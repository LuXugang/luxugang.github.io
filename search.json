[{"title":"Automaton（三）（Lucene 8.4.0）","url":"/Lucene/gongjulei/2020/0821/Automaton%EF%BC%88%E4%B8%89%EF%BC%89/","content":"点击这里\n","categories":["Lucene","gongjulei"],"tags":["自动机","prefix","wildcard","util"]},{"title":"Automaton","url":"/Lucene/gongjulei/2019/0417/Automaton/","content":"点击这里\n","categories":["Lucene","gongjulei"],"tags":["自动机","prefix","wildcard","util"]},{"title":"BinaryDocValues","url":"/Lucene/DocValues/2019/0412/BinaryDocValues/","content":"  BinaryDocValues同 SortedDocValues、SortedNumericDocValues一样，在实际应用中最多的场景用于提供给搜索结果一个排序规则。在搜索结果的排序阶段，实际是按照BinaryDocValuesFiled的域值根据字典序进行比较，而SortedDocValues则是比较域值对应的ord值来作比较，所以BinaryDocValues在排序上性能远不如SortedDocValues，当然BinaryDocValues最突出的优点对固定长度域值的索引最大化的优化，看完下面的介绍后就明白了。在后面的文章中会详细介绍BinaryDocValues的应用，在这里重点是讲述其生成的索引文件数据结构。\n 数据结构\n dvd\n图1：\n\n TermsValue\n  BinaryDocValuesFiled同SortedDocValuesFiled一样都是单值域，即一篇文档中只能有一个相同域名的域值，所以TermsValue中记录的就是每篇文档中的完整域值，并且不使用前缀存储，这里强调的是，TermsValue只记录域值，即所有的域值都是挨个写入到.dvd文件中，不同其他索引文件，一般索引文件都会先记录一个域值的长度，然后再记录域值的值，这样在读取阶段才能明确在.dvd文件中读取的数据区间。BinaryDocValues则是通过TermsIndex字段来映射，随后会介绍。\n图2：\n\n Term\n  每个BinaryDocValuesFiled的完整域值，并且可能有重复的域值。\n DocIdData\n  DocIdData中记录包含当前域的文档号。\n  如果IndexWriter添加的document中不都包含当前域，那么需要将包含当前域的文档号记录到DocIdData中，并且使用IndexedDISI类来存储文档号，IndexedDISI存储文档号后生成的数据结构单独的作为一篇文章介绍，在这里不赘述。\n TermsIndex\n  TermsIndex中记录了TermsValue中每一个域值相对于第一个域值的在.dvd文件中的偏移，在读取阶段，如果需要读取第n个域值的数据，那么只要计算 第n个和第n+1个的TermsIndex的差值，就可以获得第n个域值在.dvd文件中的数据区间，实现随机访问。源码中实际是一个下标值为文档号，数组元素是TermsIndex的数组结构，每一个域值的TermsIndex采用PackedInts进行压缩存储。其数据结构在其他DocValues的文章中有介绍，这里不赘述。\n dvm\n图3：\n\n FieldNumber\n  域的编号。\n DocvaluesType\n  Docvalues的类型，本文中，这个值就是 BINARY。\n TermsValueMeta\n图4：\n\n offset\n  .dvd文件中TermsValue在文件中的开始位置。\n length\n  length为 TermsValue在.dvd文件中的数据长度。\n  在读取阶段，通过offset跟length就可以获得所有的 TermsValue数据。\n DocIdIndex\n  DocIdIndex是对.dvd文件的一个索引，用来描述 .dvd文件中DocIdData在.dvd文件中的开始跟结束位置。\n 情况1：\n图5：\n\n  如果IndexWriter添加的document中都包含当前域，那么只需要在DocIdIndex中添加标志信息即可。\n 情况2：\n图6：\n\n  如果IndexWriter添加的document中不都包含当前域，那么.dvd文件中需要将包含当前的域的文档号信息都记录下来。\n offset\n  .dvd文件中存放文档号的DocIdData在文件中的开始位置。\n length\n  length为DocIdData在.dvd文件中的数据长度。\n  在读取阶段，通过offset跟length就可以获得所有的DocIdData数据。\n numDocsWithField\n  包含当前域的文档的个数。\n MinLength\n  记录所有域值长度最小的。\n MaxLength\n  记录所有域值长度最大的。\n  记录MinLength跟MaxLength目的在于，如果两个值相同，那么就不需要.dvd文件中的TermsIndex字段、.dvm文件中的TermIndexMeta字段。因为在读取阶段，所有的域值的长度都相同，那么TermsValue中的数据是等分的。就很容易并且随机访问找到每篇文档中的域值。\n TermsIndexMeta\n图7：\n\n Offset\n  TermsIndex数据段在.dvd文件的开始位置。\n DIRECT_MONOTONIC_BLOCK_SHIFT\n  DIRECT_MONOTONIC_BLOCK_SHIFT用来在初始化byte buffer[]的大小，buffer数组用来存放每一个域值TermsIndex。\n Min、AvgInc、Length、BitsRequired\n  图2中的TermsValue中所有域值都是连续存储，那么还需要增加TermsIndex信息来描述一个term的长度，才能在读取阶段分离出每一个term。而TermsIndex的信息会经过编码然后压缩存储，这四个字段是用于解码的参数，详细的介绍见文章DirectMonotonicWriter&amp;&amp;Reader。\n TermsIndexLength\n  TermsIndex数据段在.dvd文件的数据长度。\n  TermsIndexLength结合Offset，就可以确定 TermsIndex在.dvd文件中的数据区间。\n 结语\n  BinaryDocValues在处理域值时，不做跟ord值的映射，不做前缀存储，在特定场景下存储固定域值长度，域值长度较小时，查询域值速度优于SortedDocValues，排序性能接近SortedDocValues。\n这是DocValues类型的关于.dvd、.dvm文件构造的最后一篇文章，我们只有在了解每一个DocValues的构造原理后，才能结合实际应用，使用合适的DocValues类型。\n","categories":["Lucene","DocValues"],"tags":["DocValues"]},{"title":"BinaryDocValues（Lucene 8.7.0）","url":"/Lucene/DocValues/2020/1121/BinaryDocValues-8-7-0/","content":"  在文章BinaryDocValues中我们介绍了Lucene 7.5.0版本的数据结构，并且在文章索引文件的生成（二十一）之dvm&amp;&amp;dvd中介绍了Lucene 8.4.0中其数据结构的生成。阅读本文前建议先看下上述的两篇文章，因为很多重复的内容不会在本文中展开。\n 数据结构\n  在Lucene 8.5.0之前，BinaryDocValues的数据结构如下所示，以Lucene 7.5.0为例：\n图1：\n\n  图1中，TermsValue字段中的每个term存储时既不使用前缀存储，也不进行去重处理，更没有使用压缩存储，这种设计的初衷在与读取阶段能避免解码以及解压的操作，使得有更高的读取性能。然而这必然在某些场景下导致索引文件.dvd较大。\n  在Lucene8.5.0中，Mark Harwood在对TermsValue使用LZ4进行压缩存储后，发现其索引文件.dvd的磁盘占用能大幅度降低，并且居然索引（Indexing）以及搜索的性能并没有降低，反而有更大的提升。它在一次RP中给出了测试数据。\n图2：\n\n  图2中，通过比较主分支以及PR可以明显看出使用压缩存储后，读写性能居然提高不少，大佬Mike McCandless看到后的反应感觉也是受到了点惊吓呀：\n图3：\n\n  不过随后在Lucene专用的benchmark中，发现其读写性能并没有显著提高。图2中的测试数据被认为是一种特殊的场景。既然没有影响读写性能，并且的确能减少索引文件.dvd的磁盘占用，由于TermsValue中的term集合中不是去重的，那么当使用LZ4时能有较高的压缩率，因为LZ4的核心原理就是找出相同的数据流进行压缩。\n  使用Lucene专用的benchmark的讨论以及测试数据可以见阅读这个issue： https://issues.apache.org/jira/browse/LUCENE-9211。\n TermsValue的压缩存储\n  在索引阶段，每处理32（32这个值的选择是对各种候选值测试后的较优解）个BinaryDocValue的域值，就生成一个block。所有的域值以字节流的方式写入到这个block中，并且通过DocLengths字段来描述每一个域值的长度，即占用的字节数量，使得读取阶段能从字节流中准确的读取出每一个域值，如下所示：\n图4：\n\n  图4中，每处理32个域值就就生成一个Block。\n DocLengths\n  该字段根据这个32个域值的长度是否都相等有不同的两种数据结构：\n 所有Term的长度都相等\n  长度相等描述的是term占用的字节数量相同，此时DocLengths的数据结构如下所示：\n图5：\n\n  既然所有term的长度都相等，那么只要记录第一个term的长度即可，将这个term的长度和固定值1实现组合存储写入到DocLengths字段，固定值1用来在读取阶段区分DocLengths的数据结构。\n 至少有一个Term跟其他Term的长度不相等\n  这种情况下，需要记录所有term的长度：\n图6：\n\n  除了第一个term长度，它需要跟固定值0实现组合存储外，其他term的长度依次写到DocLengths中。固定值0用来在读取阶段区分DocLengths的数据结构。\n CompressedTerms\n图7：\n\n  该字段中，所有term使用LZ4压缩存储。\n TermsIndex\n  上文中说到，每处理32个BinaryDocValue的域值，就生成一个block。在处理的过程中，会使用临时文件记录每个block在索引文件.dvd中的起始读取位置，随后这些信息将被写入到TermsIndex中。\n图8：\n\n  图8中，红框标注的即临时文件。\n图9：\n\n  图9中的Address就是临时文件记录的信息，不过这些Address是通过DirectMonotonicWriter&amp;&amp;Reader编码处理，故编码元数据保存到索引文件.dvm。在读取阶段，结合索引文件.dvd中的的Address信息以及索引文件.dvm中的编码元数据，就能获取到Block在索引文件.dvd中的起始读取位置。\n 索引文件.dvm\n  我们先看下索引文件.dvm中的TermsValueMeta字段跟索引文件.dvd的关系：\n图10：\n\n  接着我们先看下索引文件.dvm中的TermsIndexMeta字段跟索引文件.dvd的关系：\n图11：\n\n  图11中，TotalChunks跟BINARY_BLOCK_SHIFT是相比较Lucene8.5.0之前新增的字段。另外Offset跟TermsIndexLength两个字段用来描述索引文件.dvd中TermsIndex的数据区间。\n TotalChunks\n  该字段描述的是图10中，索引文件.dvd中TermsValue中的Block的数量。\n BINARY_BLOCK_SHIFT\n  该字段描述了在索引阶段处理多少个BinaryDocValue的的域值就生成一个Block。\n MaxUncompressedBlockLength\n  该字段描述了图4中所有Block中CompressedTerms压缩前的最大长度，该值在读取阶段用于定义一个字节数组的长度，该字节数组用于存储解压后的CompressedTerms。\n  上文中我们说到索引文件.dvd中的Address使用DirectMonotonicWriter&amp;&amp;Reader编码处理，实际上每处理 (2 &lt;&lt; DIRECT_MONOTONIC_BLOCK_SHIFT) 个Address就生成一个Block，并生成该Block对应的编码元数据，即图11中蓝框标注的Min、AvgInc、Length、BitsRequired。为了便于介绍，图11中只画出了一个Block的编码元数据。\n 结语\n  上文中未介绍的字段可以阅读文章文章BinaryDocValues以及索引文件的生成（二十一）之dvm&amp;&amp;dvd。\n","categories":["Lucene","DocValues"],"tags":["DocValues"]},{"title":"Automaton（二）（Lucene 8.4.0）","url":"/Lucene/gongjulei/2020/0727/Automaton%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"点击这里\n","categories":["Lucene","gongjulei"],"tags":["自动机","prefix","wildcard","util"]},{"title":"Bkd-Tree","url":"/Lucene/gongjulei/2019/0422/Bkd-Tree/","content":"Bkd-Tree作为一种基于K-D-B-tree的索引结构，用来对多维度的点数据(multi-dimensional point data)集进行索引。Bkd-Tree跟K-D-B-tree的理论部分在本篇文章中不详细介绍，对应的两篇论文在附件中，感兴趣的朋友可以自行下载阅读。本篇文章中主要介绍Bkd-Tree在Lucene中的实现，即生成树的过程。\n 预备知识\n如果只是想了解Bkd-Tree生成过程，那么这节内容可以跳过，这块内容是为介绍索引文件.dim、.dii作准备的。\n 点数据\n点数据(Point Data)，源码中又称为点值(Point Value)，它是由多个数值类型组成。\n图1：\n\n上图中由4个int类型数值组成一个点数据/点值，并且根据点数据中的数值个数定义了维度个数。上图中即有四个维度。同一个域名的点数据必须有相同的维度个数，并且在当前7.5.0版本中，维度个数最多为8个。\n int numPoints\nnumPoints是一个从0开始递增的值，可以理解为是每一个点数据的一个唯一编号，并且通过这个编号能映射出该点数据属于哪一个文档(document)。映射关系则是通过docIDs[ ]数组实现。\n int docIDs[ ]数组\ndocIDs[ ]数组在PointValuesWriter.java中定义，数组下标是点数据的编号numPoint，数组元素是点数据所属的文档号。由于一篇文档中可以有多个点数据，所以相同的数组元素对应的多个数组下标值，即numPoints，即点数据，都是属于同一个文档。\n图2：\n\n上图中只添加了2篇文档，处理顺序按照文档号的顺序，所以文档0的点数据的numPoints的值为0，另外一篇文档可以有多个点数，所以numPoints的值分别为1、2。生成的docIDs[]数组如下：\n图3：\n\n int ord[ ]数组\nord数组的数组元素为numPoints，下面的一句话很重要：ord数组中的元素是有序的，排序规则不是按照numPoints的值，而是按照numPoints对应的点数据的值。这里ord数组的用法跟SortedDocValues中的sortedValues[]数组是一样的用法。例如根据图2中的点数据，如果我们按照第三个维度的值，即&quot;99&quot;、“23”、&quot;12&quot;来描述点数据的大小关系，那么ord数组如下图所示：\n图4：\n\n这里先提一句，在生成BKD-Tree之后，叶子节点中的点数据会根据某个维度进行排序的，并且所有叶子节点中的点数据的大小关系就存放在ord[]数组中，后面的内容会详细介绍这过程。\n 流程图\n一句话概括整个流程的话就是：根据某一个维度将点数据集划分为两部分，递归式将两部分的点数据子集进行划分，最终生成一个满二叉树。\n图5：\n\n 点数据集\n图6：\n\n点数据集即为待处理的点数据集合。\n 是否要切分？\n图7：\n\n如果数据集的个数大于1024个，那么需要进行拆分。在源码中并不是通过判断数据集的个数，而是在建立Bkd-Tree之前就预先计算出当前数据会对应生成节点(node)的个数（可以认为每个节点中的数据都是空的），然后采用深度遍历方式处理每一个节点，通过节点编号来判断是否为叶子节点。如果不是叶子节点，说明要切分(节点赋值)。\n 选出切分维度\n图8：\n\n一个点数据中有多个维度，例如图1中就有四个维度。\n\n先计算出切分次数最多的那个维度，切分次数记为maxNumSplits，如果有一个维度的切分次数小于 (maxNumSplits  / 2) ，并且该维度中的最大跟最小值不相同，那么令该维度为切分维度。\n计算出每一个维度中最大值跟最小值的差值，差值最大的作为切分维度(篇幅原因，下面的例子中仅使用了这种判定方式)。\n\n条件1优先条件2。\n 点数据集排序\n图9：\n\n当确定了切分维度后，我们对当前节点中的点数据集进行排序，排序规则根据该每个点数据中的该维度的值，排序算法使用最大有效位的基数排序(MSB radix sort)。\n 切分出左子树点数据集、切分出右子树点数据集\n图10：\n\n执行完排序操作后，当前节点中的点数据集数量为N，那么将前 (N / 2)个的点数据集划分为左子树，剩余的划分为右子树。\n这么划分的目的使得无论初始的点数据集是哪种数据分布，总是能生成一颗满二叉树。\n 是否退出\n图11：\n\n当前节点不需要切分，需要判断下算法是否需要退出。\n 结束\n\n当前节点是满二叉树的最右子树，那么算法结束，可以退出。\n当前树中只有一个节点，且该节点不需要切分，那么算法结束，可以退出。\n\n 返回上一层\n\n当前处理的节点是左子树节点或者是非最右子树节点，说明该节点是由 划分左右子树生成的，即算法还处在递归中，当不需要划分后，返回到递归的上一层。\n\n 例子\nLucene 7.5.0版本源码中当一个节点中的点数据个数大于1024才会进行切分，为了能简单示例，例子中假设一个节点中的点数据个数大于2个才会进行切分，并且点数据的维度为2。\n 点数据集\n图12：\n\n上图中一共有8个点数据，每个点数据有两个维度。为了描述方便，下面统称为x维度，跟y维度。\n 处理节点1\n\n是否要切分：初始的数据集作为第一个节点，即节点1开始进行切分，该节点中有8个数据，大于节点切分的条件值2，所以需要切分。\n选出切分维度：x维度的最大值跟最小值的差值为7 ，而y维度的最大值跟最小值的差值为9，所以当前节点的切分维度为y维度。\n点数据排序：对8个点数据按照y维度的值进行排序，排序后的结果如下:\n\n&#123;1,2&#125; -&gt; &#123;4,3&#125; -&gt; &#123;3,4&#125; -&gt; &#123;4,6&#125; -&gt; &#123;6,7&#125; -&gt; &#123;2,8&#125; -&gt; &#123;8,9&#125; -&gt; &#123;7,11&#125;\n\n切分出左子树数据集、切分出右子树数据集：当前节点个数为8，从排序后的点数据中取前一半的点数据划为左子树(节点2)，剩余的划为右子树(节点3)。\n\n左子树：&#123;1,2&#125;、&#123;4,3&#125;、&#123;3,4&#125;、&#123;4,6&#125;右子树：&#123;6,7&#125;、&#123;2,8&#125;、&#123;8,9&#125;、&#123;7,11&#125;\n图13：\n\n 处理节点2\n\n是否要切分：节点2中有4个数据，大于节点切分的条件值2，所以需要切分。\n选出切分维度：x维度的最大值跟最小值的差值为3 ，而y维度的最大值跟最小值的差值为4，所以当前节点的切分维度为y维度。\n点数据排序：对4个点数据按照y维度的值进行排序，排序后的结果如下:\n\n&#123;1,2&#125;、&#123;4,3&#125;、&#123;3,4&#125;、&#123;4,6&#125;\n\n切分出左子树数据集、切分出右子树数据集：当前节点个数为4，从排序后的点数据中取前一半的点数据划为左子树(节点4)，剩余的划为右子树(节点5)。\n\n左子树：&#123;1,2&#125;、&#123;4,3&#125;右子树：&#123;3,4&#125;、&#123;4,6&#125;\n图14：\n\n 处理节点4、5\n源码中对叶子结点还有一些处理，目的是为了生成索引文件作准备，在随后的介绍索引文件.dii、.dim时候会介绍跟叶子节点相关的知识，这篇文章主要介绍生成Bkd-Tree的过程。\n 处理节点3\n\n是否要切分：节点3中有4个数据，大于节点切分的条件值2，所以需要切分。\n选出切分维度：x维度的最大值跟最小值的差值为6 ，而y维度的最大值跟最小值的差值为4，所以当前节点的切分维度为x维度。\n点数据排序：对4个点数据按照x维度的值进行排序，排序后的结果如下:\n\n&#123;2,8&#125;、&#123;6,7&#125;、&#123;7,11&#125;、&#123;8,9&#125;\n\n切分出左子树数据集、切分出右子树数据集：当前节点个数为4，从排序后的点数据中取前一半的点数据划为左子树(节点6)，剩余的划为右子树(节点7)。\n\n左子树：&#123;2,8&#125;、&#123;6,7&#125;右子树：&#123;7,11&#125;、&#123;8,9&#125;\n图15：\n\n 处理节点6、7\n同节点4、5\n 结语\n本篇文件介绍了Bkd-Tree在Lucene中的实现，即生成满二叉树的过程，再以后介绍索引文件.dii、.dim中会继续讲一些细节的东西。另外在随后的文章中会介绍Bkd-Tree插入和更新的内容。\n点击下载Markdown文件\n","categories":["Lucene","gongjulei"],"tags":["bkd","index","point"]},{"title":"BooleanQuery","url":"/Lucene/Search/2018/1211/BooleanQuery/","content":"BooleanQuery常用来对实现多个Query子类对象的进行组合，这些Query子类对象会组成一个Cluase实现组合查询。每一个Query都有四种可选，分别描述了匹配的文档需要满足的要求，定义在 BooleanClause类中，如下：\npublic static enum Occur &#123;    /** Use this operator for clauses that &lt;i&gt;must&lt;/i&gt; appear in the matching documents. */    MUST     &#123; @Override public String toString() &#123; return &quot;+&quot;; &#125; &#125;,    /** Like &#123;@link #MUST&#125; except that these clauses do not participate in scoring. */    FILTER   &#123; @Override public String toString() &#123; return &quot;#&quot;; &#125; &#125;,    /** Use this operator for clauses that &lt;i&gt;should&lt;/i&gt; appear in the      * matching documents. For a BooleanQuery with no &lt;code&gt;MUST&lt;/code&gt;      * clauses one or more &lt;code&gt;SHOULD&lt;/code&gt; clauses must match a document      * for the BooleanQuery to match.     * @see BooleanQuery.Builder#setMinimumNumberShouldMatch     */    SHOULD   &#123; @Override public String toString() &#123; return &quot;&quot;;  &#125; &#125;,    /** Use this operator for clauses that &lt;i&gt;must not&lt;/i&gt; appear in the matching documents.     * Note that it is not possible to search for queries that only consist     * of a &lt;code&gt;MUST_NOT&lt;/code&gt; clause. These clauses do not contribute to the     * score of documents. */    MUST_NOT &#123; @Override public String toString() &#123; return &quot;-&quot;; &#125; &#125;; &#125;\n MUST (+)\n满足查询要求的文档中必须包含查询的关键字。\n SHOULD (&quot; &quot;)\n满足查询要求的文档中包含一个或多个查询的关键字。\n FILTER (#)\n满足查询要求的文档中必须包含查询的关键字，但是这个Query不会参与文档的打分。\n MUST_NOT (-)\n满足查询要求的文档中必须不能包含查询的关键字。\n 组合查询\n例子：“+a b -c d”\n转化为代码如下：\nBooleanQuery.Builder query = new BooleanQuery.Builder();query.add(new TermQuery(new Term(&quot;content&quot;, &quot;a&quot;)), BooleanClause.Occur.MUST);query.add(new TermQuery(new Term(&quot;content&quot;, &quot;b&quot;)), BooleanClause.Occur.SHOULD);query.add(new TermQuery(new Term(&quot;content&quot;, &quot;c&quot;)), BooleanClause.Occur.MUST_NOT);query.add(new TermQuery(new Term(&quot;content&quot;, &quot;d&quot;)), BooleanClause.Occur.SHOULD);\n满足查询要求的文档必须包含 “a”,，不能包含“c”,，可以包含“b”，“d”中一个或者多个，包含的越多，文档的分数越高\n BooleanQuery的方法\n 设置minimumNumberShouldMatch\npublic Builder setMinimumNumberShouldMatch(int min) &#123;    ......   &#125;\n当查询有多个SHOULD的Query对象时，满足查询要求的文档中必须包含minimumNumberShouldMatch个Query的关键字\n 构建CreateWeight对象\npublic Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException &#123;    ...    return new BooleanWeight(query, searcher, needsScores, boost);  &#125;\nQuery对象的子类都会重写这个方法。对于BooleanQuery的createWeight(…)实现，只是调用了对象组合中的所有Query子类的createWeight(…)方法分别生成Weight对象，然后将这些对象封装到BooleanWeight对象中。TermQuery的createWeight()的具体实现看博客的文章 TermQuery。\n 重写Query\npublic Query rewrite(IndexReader reader) throws IOException &#123;    ...  &#125;\nBooleanQuery的rewrite(…)跟createWeight(…)相同的是都是调用对象组合中所有Query子类的rewrite(…)方法，但是并不是所有的Query都需要重写。比如TermQuery，他就没有重写父类的rewrite(…)方法，而对于PrefixQuery(前缀查询)，则必须要重写, 重写后的PrefixQuery会生成多个TermQuery，最后组合成BooleanQuery。\n例子：前缀查询关键字  &quot;ca*&quot;, 重写后，会变成 &quot;car&quot;, &quot;cat&quot;, ...每一个关键字作为TermQuery，组合成BooleanQuery进行查询，所以一般都禁用PreFixQuery，容易抛出TooManyClause的异常。\nBooleanQuery的rewrite(…)实现中一共有9个逻辑(下面的会对每一种逻辑进行标注，比如说 逻辑一)，根据BooleanQuery中的不同的组合(MUST, SHOULD, MUST_NOT, FILTER的任意组合), 会至少执行1个多个重写逻辑，我们对最常用的组合来描述重写的过程。\n 只有一个SHOULD或MUST的TermQuery\n 重写第一步\n直接返回。。。不需要重写。\n// 逻辑一if (clauses.size() == 1) &#123;      BooleanClause c = clauses.get(0);      Query query = c.getQuery();      if (minimumNumberShouldMatch == 1 &amp;&amp; c.getOccur() == Occur.SHOULD) &#123;        return query;      &#125; else if (minimumNumberShouldMatch == 0) &#123;        switch (c.getOccur()) &#123;          case SHOULD:          case MUST:          // 直接返回原Query。            return query;          case FILTER:            // no scoring clauses, so return a score of 0            return new BoostQuery(new ConstantScoreQuery(query), 0);          case MUST_NOT:            // no positive clauses            return new MatchNoDocsQuery(&quot;pure negative BooleanQuery&quot;);          default:            throw new AssertionError();        &#125;      &#125;    &#125;\n 多个SHOULD的TermQuery\n 重写第一步\n首先遍历BooleanQuery中的所有Query对象，调用他们自身的重写方法，由于TermQuery不需要重写，所以直接返回自身。\n// 逻辑二&#123;     // 重新生成一个BooleanQuery的构建器，准备对重写后的Query进行组合。    BooleanQuery.Builder builder = new BooleanQuery.Builder();    // 设置一样的MinimumNumberShouldMatch。    builder.setMinimumNumberShouldMatch(getMinimumNumberShouldMatch());    boolean actuallyRewritten = false;    for (BooleanClause clause : this) &#123;      Query query = clause.getQuery();      // 调用Query子类的rewrite(...)方法      // 我们的例子中都是TermQuery，所以直接返回自身this。      Query rewritten = query.rewrite(reader);      if (rewritten != query) &#123;        actuallyRewritten = true;      &#125;      builder.add(rewritten, clause.getOccur());    &#125;    // 由于我们例子中的的BooleanQuery的Query子类都是TermQuery，不需要重写，所以就不用生成新的BooleanQuery对象    if (actuallyRewritten) &#123;      return builder.build();    &#125;&#125;\n 重写第二步(可选)\n如果minimumNumberShouldMatch的值 &lt;= 1那么需要执行第二步。\n当有多个相同的TermQuery，并且是SHOULD，会将这些相同的TermQuery封住成一个BoostQuery，增加boost的值。\n// 逻辑七// 这段代码的逻辑跟 逻辑八 一毛一样，往下找一找^-^,就不赘述了if (clauseSets.get(Occur.SHOULD).size() &gt; 0 &amp;&amp; minimumNumberShouldMatch &lt;= 1) &#123;      Map&lt;Query, Double&gt; shouldClauses = new HashMap&lt;&gt;();      for (Query query : clauseSets.get(Occur.SHOULD)) &#123;        double boost = 1;        while (query instanceof BoostQuery) &#123;          BoostQuery bq = (BoostQuery) query;          boost *= bq.getBoost();          query = bq.getQuery();        &#125;        shouldClauses.put(query, shouldClauses.getOrDefault(query, 0d) + boost);      &#125;      if (shouldClauses.size() != clauseSets.get(Occur.SHOULD).size()) &#123;        BooleanQuery.Builder builder = new BooleanQuery.Builder()            .setMinimumNumberShouldMatch(minimumNumberShouldMatch);        for (Map.Entry&lt;Query,Double&gt; entry : shouldClauses.entrySet()) &#123;          Query query = entry.getKey();          float boost = entry.getValue().floatValue();          if (boost != 1f) &#123;            query = new BoostQuery(query, boost);          &#125;          builder.add(query, Occur.SHOULD);        &#125;        for (BooleanClause clause : clauses) &#123;          if (clause.getOccur() != Occur.SHOULD) &#123;            builder.add(clause);          &#125;        &#125;        return builder.build();      &#125;    &#125;\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n SHOULD(至少一个)和MUST(至少一个)的TermQuery\n 重写第一步\n同样先要执行 逻辑二，不赘述。\n 重写第二步(可选)\n同样先要执行 逻辑七，不赘述。\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n 重写第三步\n当有多个相同的TermQuery，并且是MUST，会将这些相同的TermQuery封住成一个BoostQuery，增加boost的值。\n// 逻辑八if (clauseSets.get(Occur.MUST).size() &gt; 0) &#123;      Map&lt;Query, Double&gt; mustClauses = new HashMap&lt;&gt;();      // 这里遍历所有的MUST的Clause，如果有重复的Clause，boost值就加1，描述了这个关键字的重要性      for (Query query : clauseSets.get(Occur.MUST)) &#123;        double boost = 1;        while (query instanceof BoostQuery) &#123;          BoostQuery bq = (BoostQuery) query;          boost *= bq.getBoost();          query = bq.getQuery();        &#125;        // 调用getOrDefault()查看是否有相同的clause，如果有，那么取出boost，然后对boost进行+1后，覆盖已经存在的clause。        mustClauses.put(query, mustClauses.getOrDefault(query, 0d) + boost);      &#125;      // 运行至此，如果BooleanQuery有相同的query，并且是MUST，那么将这些MUST的query合并为一个query，并且增加boost的值。      // if语句为true：说明有重复的clause(MUST), 那么需要对boost不等于1的query重写，然后跟其他的query一起写到新的BooleanQuery中。      if (mustClauses.size() != clauseSets.get(Occur.MUST).size()) &#123;        BooleanQuery.Builder builder = new BooleanQuery.Builder()                .setMinimumNumberShouldMatch(minimumNumberShouldMatch);        // 这个for循环是将那些boost值不等于1的query重写为BoostQuery。        for (Map.Entry&lt;Query,Double&gt; entry : mustClauses.entrySet()) &#123;          Query query = entry.getKey();          float boost = entry.getValue().floatValue();           if (boost != 1f) &#123;            // 重写为BoostQuery。            query = new BoostQuery(query, boost);          &#125;          builder.add(query, Occur.MUST);        &#125;        // 把其他不是MUST的clause重写添加到新的BooleanQuery中。        for (BooleanClause clause : clauses) &#123;          if (clause.getOccur() != Occur.MUST) &#123;            builder.add(clause);          &#125;        &#125;        return builder.build();      &#125;    &#125;\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n 多个MUST的TermQuery\n 重写第一步\n同样先要执行 逻辑二，不赘述。\n 重写第二步\n当有多个相同的TermQuery，并且是MUST，会将这些相同的TermQuery封住成一个BoostQuery，增加boost的值。然后执行逻辑八，已说明，不赘述。\n MUST(至少一个)和MUST_NOT(至少一个)的TermQuery\n 重写第一步\n同样先要执行 逻辑二，不赘述。\n 重写第二步\n如果在这个逻辑中返回了，那么就会返回一个MatchNoDocsQuery对象，也就是不会搜索到任何结果。\n// 逻辑四 final Collection&lt;Query&gt; mustNotClauses = clauseSets.get(Occur.MUST_NOT);    if (!mustNotClauses.isEmpty()) &#123;      final Predicate&lt;Query&gt; p = clauseSets.get(Occur.MUST)::contains;      // 判断是否MUST_NOT跟MUST或FILTER是否有相同的term      if (mustNotClauses.stream().anyMatch(p.or(clauseSets.get(Occur.FILTER)::contains)))     &#123;        return new MatchNoDocsQuery(&quot;FILTER or MUST clause also in MUST_NOT&quot;);      &#125;       // 判断是否有MatchAllDocsQuery的Query      if (mustNotClauses.contains(new MatchAllDocsQuery())) &#123;        return new MatchNoDocsQuery(&quot;MUST_NOT clause is MatchAllDocsQuery&quot;);      &#125;    &#125;\n 重写第三步\n当有多个相同的TermQuery，并且是MUST，会将这些相同的TermQuery封住成一个BoostQuery，增加boost的值。然后执行逻辑八，已说明，不赘述。\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n SHOULD(至少一个)和MUST_NOT(至少一个)的TermQuery\n 重写第一步\n同样先要执行 逻辑二，不赘述。\n 重写第二步\n执行逻辑四，不赘述。\n 重写第三步(可选)\n执行逻辑七，不赘述。\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n SHOULD(至少一个)和MUST(至少一个)和MUST_NOT(至少一个)的TermQuery\n 重写第一步\n同样先要执行 逻辑二，不赘述。\n 重写第二步\n执行逻辑四，不赘述。\n 重写第三步(可选)\n执行逻辑七，不赘述。\n 重写第四步\n执行逻辑八，不赘述。\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n SHOULD(至少一个)和FILTER(至少一个)的TermQuery\n 重写第一步\n同样先要执行 逻辑二，不赘述。\n 重写第二步\n因为对于FILTER的Query中的term，他只是不参与打分，但是搜索结果必须包含这个term，如果SHOULD的Query中也有这个term，那么将这个Query的SHOULD改为MUST, 然后minShouldMatch的值就必须少一个，注意的是FILTER的这个Query没有放到新的BooleanQuery中。\n// 逻辑六if (clauseSets.get(Occur.SHOULD).size() &gt; 0 &amp;&amp; clauseSets.get(Occur.FILTER).size() &gt; 0) &#123;      final Collection&lt;Query&gt; filters = clauseSets.get(Occur.FILTER);      final Collection&lt;Query&gt; shoulds = clauseSets.get(Occur.SHOULD);      Set&lt;Query&gt; intersection = new HashSet&lt;&gt;(filters);      // 在intersection中保留 FILTER跟SHOUL有相同的term的Query      intersection.retainAll(shoulds);      // if语句为真：说明至少有一个term，他即有FILTER又有SHOULD的Query      if (intersection.isEmpty() == false) &#123;        // 需要重新生成一个BooleanQuery        BooleanQuery.Builder builder = new BooleanQuery.Builder();        int minShouldMatch = getMinimumNumberShouldMatch();        for (BooleanClause clause : clauses) &#123;          if (intersection.contains(clause.getQuery())) &#123;            if (clause.getOccur() == Occur.SHOULD) &#123;            // 将SHOULD 改为 MUST              builder.add(new BooleanClause(clause.getQuery(), Occur.MUST));              // 对minShouldMatch的值减一，因为这个SHOULD的Query的term，同样是FILTER的term，满足匹配要求的文档必须包含这个term              minShouldMatch--;            &#125;          &#125; else &#123;            builder.add(clause);          &#125;        &#125;        // 更新minShouldMatch        builder.setMinimumNumberShouldMatch(Math.max(0, minShouldMatch));        return builder.build();      &#125;    &#125;\n 重写第三步(可选)\n执行逻辑七，不赘述。\n 重写第四步\n执行逻辑八，不赘述。\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n MUST(至少一个)和FILTER(至少一个)的TermQuery\n 重写第一步\n同样先要执行 逻辑二，不赘述。\n 重写第二步\n判断是否存在一个term对应的Query即是 MUST又是FILTER，如果存在，那么移除FILTER的Query。\n// 逻辑六if (clauseSets.get(Occur.MUST).size() &gt; 0 &amp;&amp; clauseSets.get(Occur.FILTER).size() &gt; 0) &#123;      // 获得所有的FILTER的Query      final Set&lt;Query&gt; filters = new HashSet&lt;Query&gt;(clauseSets.get(Occur.FILTER));      boolean modified = filters.remove(new MatchAllDocsQuery());      // 从filters中移除既是FILTER又是MUST的Query      modified |= filters.removeAll(clauseSets.get(Occur.MUST));      if (modified) &#123;        BooleanQuery.Builder builder = new BooleanQuery.Builder();        builder.setMinimumNumberShouldMatch(getMinimumNumberShouldMatch());        for (BooleanClause clause : clauses) &#123;          if (clause.getOccur() != Occur.FILTER) &#123;            builder.add(clause);          &#125;        &#125;        for (Query filter : filters) &#123;          builder.add(filter, Occur.FILTER);        &#125;        return builder.build();      &#125;    &#125;\n 重写第三步\n执行逻辑八，不赘述。\n下图中左边是重写前的BooleanQuery，右边是重写后的BooleanQuery。\n\n 结语\nBooleanQuery类中最重要的方法就是rewrite()方法，上面的例子中列举了最常用的几种BooleanQuery的情况，MUST，SHOULD，FILTER，MUST_NOT的他们之间不同数量的组合有着不一样的rewrite过程，无法一一详细列出。BooleanQuery中的rewrite一共有9个逻辑，都在关键处给出了注释，大家可以到我们的GitHub看这个类的源码https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java。\n点击下载Markdown文件\n","categories":["Lucene","Search"],"tags":["query"]},{"title":"BulkOperationPacked","url":"/Lucene/yasuocunchu/2019/0213/BulkOperationPacked/","content":"  BulkOperation类的子类BulkOperationPacked，提供了很多对整数(integers)的压缩存储方法，其压缩存储过程其实就是对数据进行编码，将每一个整数（long或者int）编码为固定大小进行存储，大小取决于最大的那个值所需要的bit位个数。优点是减少了存储空间，并且对编码后的数据能够提供随机访问的功能。\n例如有以下的数据&#123;1，1，1，0，2，2, 0, 0&#125;，二进制的表示为&#123;01, 01, 01, 0, 10, 10, 0, 0&#125;，存储需要32个字节大小的空间。数据中最大的值是2，需要2个bit位即可表示，所以其他数据统一用2个bit位固定大小来表示，编码后需要的空间如下图：\n图1：\n\n  如上图所示，编码后只需要2个字节的空间大小。\n encode 源码解析\n  https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/packed/BulkOperationPacked.java 中给出了详细的注释，根据不同的bitsPerValue(最大值需要的bit位个数)，BulkOperationPacked有几十个子类，但是编码操作都是调用了父类BulkOperationPacked的encode方法，仅仅是部分BulkOperationPacked子类的decode方法不同一共有四种encode方法：\n\nlong[]数组编码至byte[]数组\nint[]数组编码至byte[]数组\nlong[]数组编码至long[]数组\nint[]数组编码至long[]数组\n\n  在下面的代码中，挑选了其中一种encode方法，即将 long[]数组编码至byte[]数组，出于仅仅对encode逻辑的介绍，所以简化了部分代码，比如iterations，byteValueCount变量。这些变量不影响对encode过程的理解。下面的代码大家可以直接运行测试。\npublic class Encode &#123;  public static byte[] encode(long[] values,int bitsPerValue) &#123;    byte[] blocks = new byte[2];    int blocksOffset = 0;    int nextBlock = 0;    int bitsLeft = 8;    int valuesOffset = 0;    for (int i = 0; i &lt; values.length; ++i) &#123;      final long v = values[valuesOffset++];      // bitsPerValue指的是每一个元素需要占用的bit位数，这个值取决于数据中最大元素需要的bit位      // 也就是每一个元素不管大小，占用的bit位数都是一致的      if (bitsPerValue &lt; bitsLeft) &#123;        // 将当前处理的数值写到nextBlock中        nextBlock |= v &lt;&lt; (bitsLeft - bitsPerValue);        bitsLeft -= bitsPerValue;      &#125; else &#123;        // flush as many blocks as possible        int bits = bitsPerValue - bitsLeft;        // nextBlock | (v &gt;&gt;&gt; bits)的操作将v值存储到nextBlock中        // 然后将nextBlock的值存储到blocks[]数组中，完成一个字节的压缩        blocks[blocksOffset++] = (byte) (nextBlock | (v &gt;&gt;&gt; bits));        while (bits &gt;= 8) &#123;          bits -= 8;          // 将一个数组分成多块(按照一个8个bit大小划分)存储          blocks[blocksOffset++] = (byte) (v &gt;&gt;&gt; bits);        &#125;        // then buffer        bitsLeft = 8 - bits;        // 把v的值的剩余部分存放到下一个nextBlock中,也就是当前的v值的部分值会跟下一个v值的数据(可能是部分数据)混合存储到同一个字节中        // 这里说明了数据是连续存储，可能分布在不同的block中        nextBlock = (int) ((v &amp; ((1L &lt;&lt; bits) - 1)) &lt;&lt; bitsLeft);      &#125;    &#125;    return blocks;  &#125;  public static void main(String[] args) &#123;    long[] array = &#123;1, 1, 1, 0, 2, 2, 0, 0&#125;;    byte[] result = Encode.encode(array, 2);    for (int a: result    ) &#123;      System.out.println(a);    &#125;  &#125;&#125;\n  输出的结果是 84, -96。\n decode 源码解析\n 全解码\n  不同的BulkOperationPacked子类有着不同的decode方法，解码的方法不尽相同，在Lucene7.5.0版本中，共有14种解码方法，尽管有这么多，但是逻辑都是大同小异。由于在上文中我们设置的bitPerValue的值为2，所以我们就介绍对应的decode方法。\npublic class Decode &#123;  public static void decode(byte[] blocks, int blocksOffset, long[] values, int valuesOffset, int iterations) &#123;    for (int j = 0; j &lt; iterations; ++j) &#123;      final byte block = blocks[blocksOffset++];      // 因为bitPerValues的值为2，所以每次取2个bit位的数据即可      values[valuesOffset++] = (block &gt;&gt;&gt; 6) &amp; 3;      values[valuesOffset++] = (block &gt;&gt;&gt; 4) &amp; 3;      values[valuesOffset++] = (block &gt;&gt;&gt; 2) &amp; 3;      values[valuesOffset++] = block &amp; 3;      // 至此，我们取出了1个字节的所有数据，这些数据包含了4个编码前的数据    &#125;  &#125;      public static void main(String[] args) &#123;    long[] array = &#123;1, 1, 1, 0, 2, 2, 0, 0&#125;;    byte[] result = Encode.encode(array, 2);    System.out.println(&quot;Encode&quot;);    for (long a: result    ) &#123;      System.out.println(a);    &#125;    long [] arrayDecode = new long[8];    // 每次解码都是对一个字节操作，而编码后的byte[]数组有2个字节，所以iterations参数为2    Decode.decode(result, 0, arrayDecode, 0, 2);    System.out.println(&quot;Decode&quot;);    for (long a : arrayDecode         ) &#123;      System.out.println(a);    &#125;  &#125;&#125;\n 随机解码(随机访问)\n  上面的decode()方法对所有的数据，按照在byte[]数组中的顺序依次解码，下面介绍的就是这种编码带有的随机访问(随机解码)的功能。\n  下面的get()方法在DirectReader类中实现。根据bitsPerValue的值(encode阶段，固定大小的bit位数)，本篇博客中仅列出bitsPerValue值为2的解码方法，对应上文的encode方法。更具体的注释请查看GitHub：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/packed/DirectReader.java 。\n1. public long get(long index) &#123;2.     try &#123;3.       // 在encode阶段，每一个值用2个bit位进行存储，所以每一个block(一个字节大小)中起始位置只会有4种  可能，即第1位，第3位，第5位，第7位(计数从0开始)4.      // 即shift的值只可能是 0, 2，4，65.      int shift = (3 - (int)(index &amp; 3)) &lt;&lt; 1;6.      // 每一个数组元素都用2个bit位表示，所以跟 0x3执行与操作7.      return (in.readByte(offset + (index &gt;&gt;&gt; 2)) &gt;&gt;&gt; shift) &amp; 0x3;8.    &#125; catch (IOException e) &#123;9.      throw new RuntimeException(e);10.    &#125;11.  &#125;\n 例子（随机访问）\n  编码后的值如下图所示：\n图1：\n\n 取出原数组中下标值(index)为2的数组元素\n  根据decode中第5行代码，我们先求出shift的值为 2。截取上图中部分字节数据,大小为 index &gt;&gt; 2, 即截取8个bit位，然后根据shift的值执行无符号右移操作(第7行代码)，如下图：\n图2：\n\n  接着根据第7行代码与0x3执行与操作，如下图：\n图3：\n\n  从上面的解码过程可以看出，这种编码方式可以实现随机访问功能。\n 在Lucene中的应用\n  在DocValues中，有着广泛的应用，例如在SortDocValue中，用来存放ordMap[]数组的元素值，ordMap[]的概念会在后面介绍SortedDocValuesWriter类时候介绍。\n 结语\n  本篇博客介绍了使用BulkOperationPacked类实现对整数(long或者int)进行压缩存储，与去重编码相比，优点在于在解码时性能更高，并且能实现随机访问，在去重编码中，由于使用了差值存储，所以做不到随机访问。缺点在于当数据中出现较大的值时，压缩比就不如去重编码了。\n点击下载Markdown文件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode"]},{"title":"BulkScorer（一）（Lucene 9.6.0）","url":"/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  本篇文章介绍在查询阶段，BulkScorer相关的知识点，在查询流程中的流程点如下红框所示：\n图1：\n\n 父类BulkScorer\n  BulkScorer类的定位是一个对满足查询条件的所有文档号进行收集的最初入口。在执行完该类的方法一后，Lucene就完成了一个段中文档号的收集工作。我们仅关注下BulkScorer类中下面的两个方法，如下所示：\n 方法一\n图2：\n\n  该方法说的是在某个文档号区间内，即参数min和max组成的区间，进行文档号的收集，同时使用参数acceptDocs过滤掉被删除的文档号，最终满足查询条件的文档号使用参数collector收集存储。\n acceptDocs\n  acceptDocs是一个基于位图实现，用来描述被删除的文档号信息的对象。由于在Lucene中执行删除文档操作时，并不会去修改现有的索引文件，而是额外的在名为.liv的索引文件中记录被删除的文档号。\n  也就说，在方法一中依次处理满足查询条件的文档号时，有些文档号对应的文档尽管是满足查询条件的，但是它已经是被标记为删除的，故需要通过acceptDocs进行过滤。\n collector\n  满足查询条件的文档号会使用collector进行收集，在collector中会对文档号进行排序等操作，详细内容见Collector（一）。另外collector是LeafCollector对象，说的是对某个段进行收集。\n min、max\n  min和max用于指定一个待遍历的文档号区间，在有些子类实现中如果根据某些条件可以尽可能的降低区间的范围，那么可以降低整个查询时间（在介绍ReqExclBulkScorer时会介绍）。默认的实现中，min的值为0，max的值为Integer.MAX_VALUE。\n 方法二\n图3：\n\n  该方法描述的是遍历所有满足查询条件的文档号的开销，这个取决于不同的BulkScorer子类实现，在下文中再展开介绍。\n BulkScorer的子类\n  主要介绍下这几个最常见的子类：\n图4：\n\n DefaultBulkScorer\n  DefaultBulkScorer子类中实现图2中方法一的流程图如下所示：\n图5：\n\n 从Scorer中获取scorerIterator\n图6：\n\n  Scorer的概念会在以后的文章中详细介绍，在本篇文章中我们只需要暂时知道，该对象会提供一个DocIdSetIterator抽象类（下文中简称为DISI）的对象scorerIterator，通过该对象我们就可以获取到满足查询条件的所有文档号，并且在DocIdSetIterator类的不同实现中，获取每一个文档号的逻辑也是不同的。\n DocIdSetIterator\n  DISI是一个有状态的，对non-decreasing类型的文档号集合进行遍历的迭代器，并且集合中的文档号都是满足查询条件的（被删除的文档号也在集合中）。non-decreasing指的是集合中的每个文档号都大于等于排在它前面的文档号。例如下面的集合就是non-decreasing：\n&#123;1, 2, 2, 3, 4&#125;\n  这个类中有四个核心的方法：\n 方法一（DISI）\n图7：\n\n  该方法体现出DISI是具有状态的，它返回DISI当前的状态值，即目前遍历到的文档号。\n  注释中说到，如果在调用nextDoc()或者advance(int)方法前就调用当前方法，会返回-1，意思是DISI的最初的状态值是-1，也就是文档号为-1，当然了，该值不是一个合法的文档号。另外如果遍历完所有的文档号，那么当前方法会返回一个NO_MORE_DOCS作为结束标志，该值即Integer.MAX_VALUE。\n图11：\n\n 方法二（DISI）\n图8：\n\n  该方法会基于docID()中的文档号（当前状态值），返回在集合中排在它后面，下一个位置的文档号，随后将状态值更新为该位置的文档号。\n  如果DISI当前的状态值已经是集合中的最后一个文档号，那么调用该方法会返回NO_MORE_DOCS，并且将状态值更新为NO_MORE_DOCS。注意的是，不应该在这种情况下继续调用该方法，否则会导致不可预测行为。\n 方法三（DISI）\n图9：\n\n  该方法会返回集合中第一个大于等于target的文档号，并且将状态值更新为这个文档号。\n  如果未在集合中找到，那么会返回NO_MORE_DOCS。并且将状态值更新为NO_MORE_DOCS。\n 方法四（DISI）\n图10：\n\n  该方法描述的是遍历DISI中的文档号集合的开销，在有些子类实现中该值就是集合中文档号数量的一个准确值，然而在有些实现则是一个估计值。\n DocIdSetIterator的子类\n  下图给出的是实现比较简单的一个子类：\n图12：\n\n 是否可以获取实现Skip Docs的DISI？\n图13：\n\n  如果从数据集的角度去思考如何提高查询性能，通常可以通过以下两种方式实现：\n\n减少数据集的规模\n收集到足够的结果后，提前退出\n\n  这里的第二点在图5的collector收集docId中实现，见Collector（四）。第一点则是通过可以实现Skip Docs的DISI对象competitiveIterator结合图6 Scorer中获取的scorerIterator，将这两个DISI对象组合成一个新的DISI对象filteredIterator来实现减少数据集的规模。\n  注意点：只有在获取TopN的查询中，才有可能获取到实现Skip Docs的DISI对象competitiveIterator\n filteredIterator的实现逻辑\n  在收集了TopN篇文档号，随后继续遍历scorerIterator时，如果根据排序规则，可以添加到TopN中时，就会尝试获取/更新competitiveIterator，通过competitiveIterator实现skip docs。\n  目前Lucene中有基于BKD树（LUCENE-9280）以及基于倒排（LUCENE-10633）来获得competitiveIterator。见文章查询TopN的优化之NumericDocValues（一）、查询TopN的优化之NumericDocValues（二）了解基于BKD树获取competitiveIterator。\n  下面的例子中，主要是表达优化思想，并不是真实的实现逻辑：\n图14：\n\n  图14中假设是Top3的查询，如果没能获取competitiveIterator，那么需要遍历集合中所有的文档号。优化后，在收集完Top3后，根据Top3中的竞争力最小的信息（基于排序规则对应的域值）获取一个集合为[7, 99]的competitiveIterator，意思是根据排序规则，在这个集合区间范围外的文档是没有竞争力（non-competitive），也就是没有必要去处理这些文档号。随后在处理完第56篇文档号，再次更新了competitiveIterator。\n 遍历filteredIterator\n图15：\n\n docId是否在区间内\n  这里的区间指的是方法一中参数max跟min组成的区间，由于从filteredIterator中依次获得的文档号是递增的，所以当出现文档号不在区间内时，就可以直接退出遍历。\n docId不是被删除的文档号并且满足二阶段遍历？\n  在这个流程点，我们需要过滤被删除的文档号，尽管这些文档是满足查询条件的。\n 二阶段遍历\n  二阶段遍历在大部分查询场景中不会出现，在此流程点如果二阶段遍历为空，则认为是满足条件的。在文章二阶段遍历（TwoPhaseIterator）中详细的介绍了为什么要使用二阶段遍历，以及给出了某个场景作为例子。这里就不展开介绍了。\n collector收集docId\n  在系列文章Collector（一）中介绍常见的几个Collector，这里就不赘述了。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开介绍。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["bulkScorer"]},{"title":"BulkScorer（二）（Lucene 9.6.0）","url":"/Lucene/Search/2023/0724/BulkScorer%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本篇文章我们继续介绍BulkScorer的其他子类，下图为BulkScorer主要的几个子类，其中DefaultBulkScorer的介绍可以见文章BulkScorer（一）：\n图1：\n\n ReqExclBulkScorer\n 实现逻辑\n  ReqExclBulkScorer中包含了两个成员，一个是名为req的BulkScorer对象，另一个是名为excl的DocIdSetIterator对象，它包含了在查询条件中指定的不需要返回的文档号集合（MUST_NOT）。\n图2：\n\n  在文章BulkScorer（一）中我们说到，BulkScorer的score()方法描述的是对某个文档号区间进行遍历，期间过滤掉被删除的文档号，最后使用Collector收集文档号。在子类ReqExclBulkScorer的实现中，则是根据excl中的文档号将req的文档号集合划分为一个或多个更小的集合。\n图3：\n\n  图2中，excl中的文档号将req中待遍历的结合划分为3个区间。在源码实现中，每一个区间对应一次BulkScorer的score()方法的调用，差别在于不同的文档号区间范围。\n 使用场景\n  例如在使用BooleanQuery时，特定查询条件下会使用到ReqExclBulkScorer，其具体逻辑会在介绍BooleanQuery中获取BulkScorer的文章中展开介绍。\n TimeLimitingBulkScorer\n  TimeLimitingBulkScorer用于为BulkScorer设定一个超时时间，查询超时后通过抛出异常的方式结束BulkScorer的scorer方法的调用。\n  TimeLimitingBulkScorer封装了一个名为in的BulkScorer对象，允许用户设定一个名为queryTimeout的QueryTimeout对象作为查询超时条件。\n图4：\n\n  QueryTimeout类很简单，类中就包含一个shouldExit的方法，下文中会介绍该方法在何时会被调用。\n图5：\n\n  超时条件通过IndexSearcher类中的setTimeout()方法设定。\n图6：\n\n 实现逻辑\n  TimeLimitingBulkScorer实现BulkScorer的score()的逻辑中，跟ReqExclBulkScorer相似的地方是将封装的BulkScorer对象（图3中的in）的遍历区间拆封成多个一个或多个区间，每遍历完一个区间就调用图4中的shouldExit()方法判断是否已经超时。区间的划分规则则是基于根据文档号数量，每个区间中的文档号数量为interval，其公式如下所示：\nint interval = LastInterval + (LastInterval &gt;&gt; 1)\n  LastInterval的值为上一个区间的interval，并且LastInterval的初始值为100。\n\n第一个区间中的文档号数量：即初始值100\n第二个区间中的文档号数量：(100 + 100 &gt;&gt;2) =  150\n第三个区间中的文档号数量：(150 + 150 &gt;&gt; 2) = 225\n\n图7：\n\n 使用场景\n  当在IndexSearcher对象中指定了setTimeout()方法后，所有的BulkScorer对象都会进行超时判断。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开介绍。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["bulkScorer"]},{"title":"BytesRefHash","url":"/Lucene/gongjulei/2019/0218/BytesRefHash/","content":"BytesRefHash类是专门为BytesRef对象作优化的一种类似hashMap的数据结构，该类的主要用途就是将所有的BytesRef对象存储到一个连续的存储空间中，并且使得能在查询阶段达到 0(1)的时间复杂度。\n BytesRefHash的一些变量\n byte[] [] buffers;\n二维数组buffers[][]用来存储ByteRef对象，所有的BytesRef对象都连续的存储在buffers[][]数组中\n int termID;\ntermID是从0开始的一个递增的值，每个BytesRef根据它存储到buffers[][]的先后顺序获得一个唯一的termID\n int[] ids;\nids[]数组下标是BytesRef对利用MurmurHash算法计算出的hash值，ids[]数组元素则是termID\n int[] bytesStart;\nbytesStart[]数组下标是termID，数组元素是termID对应的BytesRef值在buffers[][]中的起始位置\n 例子\n这里用一个例子来描述上文中介绍的BytesRefHash的那些变量之间的关系，存储的内容如下\n\n上图中所有BytesRef对象通过MurmurHash算法计 算出的hash值通过公式 hash &amp; hashMask散落到ids[]数组后的情况如下图。其中hashMask的值为15，即当前ids[]数组大小减1，另外ids[]数组中的元素初始值为 -1。\n ids[]数组\nids[]数组初始值大小为16，数组元素(termID)用来作为bytesStart[]数组的索引\n\n#### bytesStart[]数组\nbytesStart[]数组元素用来作为buffers[][]二维数组的索引\n\n#### buffers[] []二维数组\nbuffers[][]数组中存放了 BytesRef对象的原始值，每一个BytesRef对象按块(block)连续的存放，每一个block中包含了BytesRef对象的长度跟原始值，存放长度的作用用来在读取阶段描述应该读取数组中多长的数据，注意的是存储长度占用的字节根据BytesRef的大小可能占用1个或者2个字节，下面的例子中，存储所有的BytesRef对象的长度只需要1个字节\nterm值与BytesRef对象的关系\n| Term | BytesRef(十六进制) |BytesRef(十进制)| 长度 |\n| :--: | :-----: |:-----:|:--: |\n| mop  | [6d, 6f, 70] |[109, 111, 112]| 3 |\n| moth | [6d, 6f, 74, 68] |[109, 111, 116, 104]| 4 |\n|  of  | [6f, 66] |[111, 102]| 2 |\n| star | [73, 74, 61, 72] |[115, 116, 97, 114]| 4 |\n\n#### 三个数组的映射关系\n三个数组之间的关系如下图\n\n 结语\n本篇博客通过例子介绍了 BytesRefHash类如何存储BytesRef对象，但并没有以代码的形式给出，要理解BytesRefHash这个类中实现逻辑，其类中最重要的一个方法就是 add(BytesRef bytes)，这个方法中的源码详细注释大家可以看我的GitHub：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/BytesRefHash.java ，对应的demo在这里：https://github.com/luxugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/utils/BytesRefHashTest.java\n点击下载Markdown文件\n","categories":["Lucene","gongjulei"],"tags":["index","BytesRef"]},{"title":"Changes（Lucene 8.7.0）","url":"/Lucene/2020/1106/Changes-8-7-0/","content":"  2020年11月3号，Lucene发布了最新的版本8.7.0，本篇文章将会对Change Log中几个变更展开介绍下。\n LUCENE-9510\n  该issue的原文如下：\nIndexing with an index sort is now faster by not compressing temporary representations of the data. \n  上文大意为：当设置了段内排序IndexSort后，索引（Indexing）的速度比以前更快了，因为不再压缩临时的数据。\n  这里的temporary representations of the data指的是存储域的域值，它最终存储到索引文件.fdt中。在文章索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm中我们说到，在添加每篇文档的过程中，当生成一个chunk时，域值会被压缩处理并写入到chunk中。如下所示：\n图1：\n\n  如果设置了段内排序，那么在flush阶段会读取chunk中的域值，然后写入到新的chunk中，并生成新的索引文件.fdt（因为持续添加/更新/删除文档的过程中，是无法排序的，所以只能在flush阶段，即生成一个段时才能排序）。这意味着排序后的文档对应的域值分布在不同的chunk中，就会导致随机访问，即会出现chunk的重复解压，这将导致索引（Indexing）速度的降低。那么在索引期间生成的用于描述存储域的索引文件就是下文中 temporary files：\nWe noticed some indexing rate regressions in Elasticsearch after upgrading to a new Lucene snapshot. This is due to the fact that SortingStoredFieldsConsumer is using the default codec to write stored fields on flush. Compression doesn&#x27;t matter much for this case since these are temporary files that get removed on flush after the segment is sorted anyway so we could switch to a format that has faster random access.\n  在Lucene 8.7.0中，索引期间使用了一个TEMP_STORED_FIELDS_FORMAT的新的codec来处理存储域，使得不再对域值进行压缩处理：\n图2：\n\n  由图2可见，尽管调用了compress接口，实际上就是简单的按字节拷贝而已。\n  另外触发生成chunk的条件也被改为处理一篇文档就生成一个chunk，目的就是减少处理的时间，因为我们知道，如果chunk中包含多个文档号，这些文档号的信息（即图1中的chunk）都是经过编码处理的，读取一个文档的存储域信息就需要把其他文档的存储域信息都执行解码，关键是这些文档的信息很有可能并不是下一次处理需要的信息：\n图3：\n\n  如果不知道maxDocsPerChunk的作用，请阅读文章索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm。\n  最终在flush阶段，生成新的索引文件.fdt时，就使用默认的codec，生成过程就跟文章索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm一致了。\n LUCENE-9447, LUCENE-9486\n  从Lucene 8.7.0开始，对生成一个chunk的触发条件进行了修改，即图1中chunk中包含的文档数量上限值、域值的大小上限值得到了提高。这两个issue中解释了原因，并给出了测试数据，感兴趣的可以点进去看下。\n LUCENE-9484\n  在文章构造IndexWriter对象（二）中我们知道，IndexWriter对象可以通过IndexSort实现段内的排序，使得随后生成的段都是段内有序的，并且具有相同的排序规则。另外IndexWriter在构造期间，如果索引目录中已经存在一些段，我们称之为旧段，如果这些段的排序规则跟IndexWriter中配置的不一致，那么会导致IndexWriter对象初始化失败。在Lucene 8.7.0之前， 我们需要根据IndexWriter中的IndexSort的排序规则对旧段重新排序才可以，降低了用户体验。当前issue中正是解决了这个问题。通过SortingCodecReader类封装旧段对应的reader，然后利用IndexWriter中的addIndex方法即可。使用SortingCodecReader类时，通过制定了旧段对应的reader跟IndexWriter中的IndexSort排序规则，Lucene会对reader进行重排。\n LUCENE-8962\n  在多线程执行索引（Indexing）的过程中，当某个线程执行了flush( )、commit( )操作或者getReader( )实现NRT操作时，会使得所有的线程将对应的DWPT执行flush，可能导致生成许多的小段（small segments）。在文章查询原理（三）中我们说到，当存在多个段时，搜索的过程为依次（单线程执行搜索操作）遍历每一个段，最后对每个段的结果进行合并。大佬Michael McCandless提出了一个讨论，是否能在commit()、getReader( )的方法返回前就能将这些小段进行合并，使得减少搜索期间遍历的段的数量，降低查询时间。\n  故在从Lucene 8.6.0开始（Lucene 8.7.0中进行了优化），通过构造IndexWriter期间指定的配置项maxFullFlushMergeWaitMillis来实现在commit( )跟getReader( )调用期间实现对一些小段的合并。注意的是，这里的执行段的合并会阻塞commit( )跟getReader( )这两个方法，故通过maxFullFlushMergeWaitMillis来指定超时时间。如果在超时时间内没有完成小段的合并，则commit( )跟getReader( )继续执行，在这两个方法返回后，可能还存在多个小段。 maxFullFlushMergeWaitMillis的默认值为0，表示不会在commit( )跟getReader( )的调用期间执行小段的合并。\n","categories":["Lucene"],"tags":["Changes"]},{"title":"Collector（一）","url":"/Lucene/Search/2019/0812/Collector%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  在搜索阶段，每当Lucene找到一个满足查询条件的文档（Document），便会将该文档的文档号（docId）交给Collector，并在Collector中对收集的文档号集合进行排序（sorting）、过滤（filtering）或者用户自定义的操作。\n  本篇文章将根据图1中的类图（Class diagram），介绍Lucene常用的几个收集器（Collector）：\n图1：\n\n Collector处理文档\n  下图中描述的是Collector处理文档的流程：\n图2：\n\n 获得LeafCollector\n图3：\n\n  当索引目录中存在多个段时，我们需要从每个段中分别找出满足查询条件的文档，LeafReaderContext即用来描述某一个段的信息，并且通过它能获得一个LeafCollector对象，在本篇文章中我们只要知道LeafReaderContext有这个功能（functionality）即可，在后面介绍IndexReader的文章中会展开。\n  在搜索阶段，通过Collector类的方法来获得LeafCollector对象，下面是Collector类的代码，由于Collector类是一个接口类，并且只有两个接口方法，故列出并介绍：\npublic interface Collector &#123;    LeafCollector getLeafCollector(LeafReaderContext context) throws IOException;      boolean needsScores();    &#125;\n 接口方法 getLeafCollector\n  通过该方法获得一个LeafCollector对象，Lucene每处理完一个段，就会调用该方法获得下一个段对应的LeafCollector对象。\n  LeafCollector对象有什么作用：\n\n首先看下LeafCollector类的结构：\n\npublic interface LeafCollector &#123;    void setScorer(Scorer scorer) throws IOException;     /** 参数doc即文档号docId*/    void collect(int doc) throws IOException;&#125;\n\nsetScorer方法：调用此方法通过Scorer对象获得一篇文档的打分，对文档集合进行排序时，可以作为排序条件之一，当然Scorer对象包含不仅仅是文档的打分值，在后面介绍查询的文章中会展开\ncollect方法：在这个方法中实现了对所有满足查询条件的文档进行排序（sorting）、过滤（filtering）或者用户自定义的操作的具体逻辑。在下文中，根据图1中不同的收集器（Collector）会详细介绍collect方法的不同实现\n\n 接口方法 needsScores\n  设置该方法用来告知Lucene在搜索阶段，当找到一篇文档时，是否对其进行打分。如果用户期望的查询结果不依赖打分，那么可以设置为false来提高查询性能。\n 处理一篇文档\n图4：\n\n  当Lucene找到一篇满足查询条件的文档，会调用LeafCollector的setScorer(Scorer score)方法来执行获得文档打分的流程，随后在获得文档号docId流程后获得一个docId，最后调用LeafCollector的collect(int doc)方法（参数doc即文档号docId）来实执行处理该文档的流程，在该流程中，实现对文档进行排序（sorting）、过滤（filtering）或者用户自定义的操作。\n TimeLimitingCollector\n  在介绍完Collector处理文档的流程后，我们依次介绍图1中的收集器。\n  TimeLimitingCollector封装了其他的Collector，用来限制Collector处理文档的时间，即设定了一次查询允许的最长时间timeLimit。如果查询的时间超过timeLimit，那么会抛出超时异常TimeExceededException。\n  在哪些流程点会判断查询超时：\n\n调用Collector.getLeafCollector(LeafReaderContext context)方法时会执行超时判断，即图3中的是否还有 LeafReaderContext的流程点\n调用LeafCollector.collect(int doc)方法时会执行超时判断，即图4中的处理该文档的流程点\n\n  如何实现超时机制：\n\n\n通过后台线程、解析值resolution、计数器counter实现、timeLimit\n\n计数器counter：AtomicLong类型，用来描述查询已花费的时间\n解析值resolution：long类型的数值，触发查询超时的精度值\n后台线程：Thread.setDaemon(true)的线程\ntimeLimit：上文已经介绍\n\n\n\n后台线程先执行counter的累加操作，即调用counter.addAndGet(resolution)的方法，随后调用Thread.sleep(resolution)的方法，如此反复。收集文档号的线程在 判断查询超时的流程点处通过counter.get()的值判断是否大于timeLimit\n\n\n  使用这种超时机制有什么注意点：\n\n由于后台线程先执行counter的累加操作，随后睡眠，故收集文档号的线程超时的时间范围为timeLimit - resolution 至 timeLimit + resolution 的区间，单位是milliseconds，故查询超时的触发时间不是精确的\n由上一条可以知道，resolution的值设置的越小，查询超时的触发时间精度越高，但是性能越差（例如线程更频繁的睡眠唤醒等切换上下文行为）\n由于使用了睡眠机制，在运行过程中实时将resolution的值被调整为比当前resolution较小的值时（比如由20milliseconds调整为5milliseconds），可能会存在调整延迟的问题（线程正好开始睡眠20milliseconds）\nresolution的值至少为5milliseconds，因为要保证能正确的调用执行Object.wait(long)方法\n\n 贪婪（greedy）模式：\n  在开启贪婪模式的情况下（默认不开启），如果在LeafCollector.collect( )中判断出查询超时，那么还是会收集当前的文档号并随后抛出超时异常，注意的是如果在Collector.getLeafCollector( )中判断出查询超时，那么直接抛出超时异常。\n 结语\n  剩余的Collector展开介绍会造成本篇文章篇幅过长，将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["collector","indexSort"]},{"title":"Collector（二）","url":"/Lucene/Search/2019/0813/Collector%EF%BC%88%E4%BA%8C%EF%BC%89/","content":" Collector（二）\n  本文承接Collector（一），继续介绍其他的收集器。\n  图1是Lucene常用的几个Collector：\n图1：\n\n FilterCollector\n  FilterCollector类是一个抽象类，它用来封装其他的Collector来提供额外的功能。\n PositiveScoresOnlyCollector\n  PositiveScoresOnlyCollector首先过滤出文档的打分值大于0的文档号，然后将文档号交给封装的Collector，由于collect方法（collect方法的介绍见Collector（一））比较简单，故列出：\npublic void collect(int doc) throws IOException &#123;    if (scorer.score() &gt; 0) &#123;        in.collect(doc);    &#125;&#125;\n  其中in是PositiveScoresOnlyCollector封装的Collector，scorer即一个Scorer对象（见Collector（一））。\n CachingCollector\n  CachingCollector可以缓存Collector收集的一次搜索的结果，使得其他的Collector可以复用该Collector的数据。\n  CachingCollector缓存了哪些数据：\n\nList&lt;LeafReaderContext&gt;：在Collector（一）中我们提到，LeafReaderContext描述的是一个段内的信息，当索引目录中存在多个段，那么我们需要用List来缓存所有的LeafReaderContext\nList&lt;int[ ]&gt; docs：一个段中可能有多个满足查询条件的文档，所以使用int[ ]来缓存那些文档的文档号，当索引目录中存在多个段时，需要用List来缓存每一个段中的所有文档号集合\nList&lt;float[ ]&gt; scores：一个段中所有满足查询条件的文档的打分值使用float[ ]缓存，当索引目录中存在多个段时，需要用List来缓存每一个段中的所有文档的打分值集合\n\n  图1中NoScoreCachingCollector、ScoreCachingCollector两者的区别在于是否缓存文档的打分值。\n CachingCollector缓存流程图\n  图2是Collector处理文档的过程，每个流程点在Collector（一）已作介绍，不赘述：\n图2：\n\n  CachingCollector缓存流程图跟图2类似，故用红框标记出不同处：\n图3：\n\n CachingCollector复用流程图\n图4：\n\n  另外，CachingCollector可以设置允许缓存文档个数最大值。\n  在缓存阶段，当缓存的个数超过阈值，那么清空此前缓存的所有数据，另变量cache的值为false，即这次的缓存操作置为失败，故在复用CachingCollector时需先检查cache的值是否为true。\n TopDocsCollector\n  TopDocsCollector类在收集完文档后，会返回一个的TopDocs对象，TopDoc对象是什么不重要，在这篇文章中我们只需要知道收集后的文档信息按照某种排序规则有序的存放在TopDoc对象中，该对象为搜索结果的返回值。根据不同的**排序（sorting）**规则，TopDocsCollector派生出图1中的三种子类：\n\nDiversifiedTopDocsCollector\nTopScoreDocCollector\nTopFieldCollector\n\n  其中，根据一定的**过滤（filtering）**规则，TopScoreDocCollector、TopFieldCollector还分别派生出两个子类：\n\nTopScoreDocCollector\n\nSimpleTopScoreDocCollector\nPagingTopScoreDocCollector\n\n\nTopFieldCollector\n\nSimpleFieldCollector\nPagingFieldCollector\n\n\n\n  上文中我们给出了TopDocsCollector的7个子类，结合图2中的流程，他们之间的流程差异仅在于处理该文档这个流程点，即collect(int doc)方法的不同的实现（见Collector（一））\n  故在下文中，只介绍每个Collector的collect(int doc)方法的具体实现。\n TopScoreDocCollector\n  TopScoreDocCollector类的排序规则为 “先打分，后文档号”：\n\n先打分：即先通过文档的打分进行排序，打分值越高，排名越靠前\n后文档号：由于文档号是唯一的，所以当打分值相等时，可以再通过文档的文档号进行排序，文档号越小，排名越靠前。\n\n  根据过滤规则，我们接着介绍TopScoreDocCollector的两个子类：\n\nSimpleTopScoreDocCollector：无过滤规则\nPagingTopScoreDocCollector：有过滤规则，具体内容在下文展开\n\n SimpleTopScoreDocCollector\n  SimpleTopScoreDocCollector的collect(int doc)流程图：\n图5：\n\n score是否大于堆顶元素的score?\n图6：\n\n  使用优先级队列PriorityQueue来存放满足搜索条件的文档信息（文档信息至少包含了文档打分score以及文档号docId），分数最低的文档信息位于堆顶，堆的大小默认为段中的文档总数（用户也可以指定堆的大小，即用户期望的返回结果TopN的N值）。\n  为什么判断条件是score等于堆顶元素的score的情况下也不满足：\n\n因为collect(int doc)方法接受到的文档号总是按照从小到大的顺序，当score等于堆顶元素的score时，当前文档号肯定大于堆顶元素的文档号，根据上文中TopScoreDocCollector的排序规则，故不满足\n\n 调整堆\n图7：\n\n  替换堆顶元素后，我们需要调整堆重新找到分数最低的文档信息，调整的规则同样按照“先分数，后文档号”。\n PagingTopScoreDocCollector\n  PagingTopScoreDocCollector是带有过滤规则的Collector，用来实现分页功能。\n  在SimpleTopScoreDocCollector中如果满足搜索条件的文档个数有M个，其中N为用户期望返回的个数（即TopN），为了便于理解，我们这里假设M &gt; 2N，那么第一次搜索后，返回的搜索结果，即N篇文档，任意一篇的打分值score都是大于等于剩余的（M - N）篇文档中的任意一篇，如果使用了PagingTopScoreDocCollector，我们可以就从 （M - N）篇文档中继续找出N篇文档，即执行第二次搜索。该PagingTopScoreDocCollector可以使得通过多次调用IndexSearcher.searchAfter(ScoreDoc after, Query query, int TopN)的方法来实现分页功能，其中ScoreDoc对象after即过滤规则。下面给出ScoreDoc类的部分变量：\npublic class ScoreDoc &#123;    public float score;    public int doc;    ... ... &#125;\n  score为上文N篇文档中分数最低的打分值，doc为对应的文档号，在下文中会介绍如何使用ScoreDoc作为过滤规则来实现分页功能。\n  PagingTopScoreDocCollector的collect(int doc)流程图：\n图8：\n\n  除了红色的流程点，其他流程点跟SimpleTopScoreDocCollector是一样的，不赘述。\n 是否已经被收集了？\n图9：\n\n  是否已经被收集了描述的是该文档号是否已经在前面的搜索中被收集了，判断的条件如下，如果为true，说明该文档已经被收集了：\nscore &gt; after.score || (score == after.score &amp;&amp; doc &lt;= after.doc)\n\nscore：该值描述的当前文档的打分值\nafter：该值即上文中的ScoreDoc对象\n\n  由于after.score是前面所有分页搜索的结果中分数最低的文档，所以如果当前文档的打分值大于after.score，必定该篇文档已经在前面某次分页搜索中被收集过了。\n  如果score &gt; after.score不为true，还要考虑score == after.score的情况，在图6 中我们知道如果两篇文档的打分值一样，那么文档号较大的不会被收集，所以在如果当前的文档号小于等于after.doc，必定该篇文档已经在前面某次分页搜索中被收集过了。\n  从上面的介绍可以看出，如果一个段中有M篇文档满足搜索条件，在使用分页搜索的情况，每一次Collector都需要处理这M篇文档，只是在每一次的分页搜索时选出N篇文档。\n 结语\n  基于篇幅原因，剩余的Collector在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["collector","indexSort"]},{"title":"Collector（三）","url":"/Lucene/Search/2019/0814/Collector%EF%BC%88%E4%B8%89%EF%BC%89/","content":"  本文承接Collector（二），继续介绍其他的收集器。\n  图1是Lucene常用的几个Collector：\n图1：\n\n TopDocsCollector\n TopFieldCollector\n  在Collector（二）的文章中，我们介绍了TopScoreDocCollector收集器以及它的两个子类SimpleTopScoreDocCollector、PagingTopScoreDocCollector，它们的排序规则是&quot;先打分，后文档号&quot;，TopFieldCollector的排序规则是“先域比较（FieldComparator），后文档号”。\n\n先域比较（FieldComparator）：根据文档（Document）中的排序域（SortField）的域值进行排序。\n后文档号：由于文档号是唯一的，所以当无法通过域比较获得顺序关系时，可以再通过文档的文档号进行排序，文档号越小，排名越靠前（competitive）\n\n  我们先通过例子来介绍如何使用TopFieldCollector排序的例子，随后介绍排序的原理。\n  本人业务中常用的排序域有SortedNumericSortField、SortedSetSortField，其他的排序域可以看SortField类以及子类，在搜索阶段如果使用了域排序，那么Lucene默认使用TopFieldCollector来实现排序。\n 例子\n SortedNumericSortField\n图2：\n\n图3：\n\n\nSortedNumericSortField：根据文档（document）中NumericDocValuesField域的域值进行排序，如果文档中没有这个域，那么域值视为0\n\n图2为索引阶段的内容，我们根据域名为“sortByNumber”的NumericDocValuesField域的域值进行排序，其中文档0、文档4没有该域，故它的域值被默认为0，它们按照文档号排序\n图3为搜索阶段的内容，使用SortedNumericSortField对结果进行排序，所以按照从小到大排序（图3中参数reverse为true的话，那么结果按照从大到小排序），那么排序结果为：\n\n\n\n文档1 --&gt; 文档0 --&gt; 文档4 --&gt; 文档3 --&gt; 文档2\n SortedSetSortField\n图4：\n\n图5：\n\n\nSortedSetSortField：根据文档（document）中NumericDocValuesField域的域值进行排序，如果文档中没有这个域，那么域值视为null，被视为&quot;最小&quot;\n\n图3为索引阶段的内容，允许设置相同域名的SortedSetSortField有多个域值，这么做的好处在于，在搜索阶段，我们可以选择其中一个域值来进行排序，提高了排序的灵活性\n图4为搜索阶段的内容，使用域名为&quot;sortByString&quot;的SortedSetSortField域的域值进行排序，其中文档0、文档4没有该域，故它的域值被视为null，它们之间按照文档号排序\n\n\n\n  如何在搜索阶段选择排序域值：\n\n通过SortedSetSelector.Tyte来选择哪一个域值，SortedSetSelector提供了下面的参数\n\nMIN：选择域值最小的进行排序，例如上图中文档1、文档2、文档3会分别使用域值&quot;a&quot;、“c”、“b”作为排序条件，图5中即按照这个规则排序，由于参数reverse为false，所以排序结果从小到大排序，其中文档0、文档4的排序域值为null：\n\n\n\n文档0 --&gt; 文档4 --&gt; 文档1 --&gt; 文档3 --&gt; 文档2\n\nMAX：选择域值最大的进行排序，例如上图中文档1、文档2、文档3会分别使用域值&quot;y&quot;、“z”、“x”作为排序条件，由于参数reverse为false，所以排序结果从小到大排序，其中文档0、文档4的排序域值为null：\n\n文档0 --&gt; 文档4 --&gt; 文档3 --&gt; 文档1 --&gt; 文档2\n\nMIDDLE_MIN：选择中间域值，如果域值个数为偶数个，那么中间的域值就有两个，则取较小值，例如上图中文档1、文档2、文档3会分别使用域值&quot;f&quot;、“e”、“d”作为排序条件，，由于参数reverse为false，所以排序结果从小到大排序，其中文档0、文档4的排序域值为null：\n\n文档0 --&gt; 文档4 --&gt; 文档3 --&gt; 文档2 --&gt; 文档1\n\nMIDDLE_MAX：选择中间域值，如果域值个数为偶数个，那么中间的域值就有两个，则取较大值，例如上图中文档1、文档2、文档3会分别使用域值&quot;h&quot;、“i”、“j”作为排序条件，，由于参数reverse为false，所以排序结果从小到大排序，其中文档0、文档4的排序域值为null：\n\n文档0 --&gt; 文档4 --&gt; 文档1 --&gt; 文档2 --&gt; 文档3\n  SortedNumericSortField也可以在索引阶段设置多个具有相同域名的不同域值，其用法跟SortedSetSortField一致，不赘述。\n  接下来我们根据过滤（filtering）规则，我们接着介绍TopFieldCollector的两个子类：\n\nSimpleFieldCollector：无过滤规则\nPagingFieldCollector：有过滤规则，具体内容在下文展开\n\n SimpleFieldCollector\n  SimpleFieldCollector的collect(int doc)方法的流程图：\n图6：\n\n点击查看大图\n 预备知识\n IndexWriterConfig.IndexSort(Sort sort)方法\n  在初始化IndexWriter对象时，我们需要提供一个IndexWriterConfig对象作为构造IndexWriter对象的参数，IndexWriterConfig提供了一个setIndexSort(Sort sort)的方法，该方法用来在索引期间按照参数Sort对象提供的排序规则对一个段内的文档进行排序，如果该排序规则跟搜索期间提供的排序规则（例如图3的排序规则）是一样的，那么很明显Collector收到的那些满足搜索条件的文档集合已经是有序的（因为Collecter依次收到的文档号是从小到大有序的，而这个顺序描述了文档之间的顺序关系，下文会详细介绍）。\n  以下是一段进阶知识，需要看过文档的增删改以及文档提交之flush系列文章才能理解，看不懂可以跳过：\n\n我们以图2作为例子，在单线程下（为了便于理解），如果不设置索引期间的排序或者该排序跟搜索期间的排序规则不一致，文档0~文档4对应的文档号分别是：0、1、2、3，Lucene会按照处理文档的顺序，分配一个从0开始递增的段内文档号，即文档的增删改（四）中的numDocsInRAM ，这是文档在一个段内的真实文档号，如果在索引期间设置了排序规则如下所示：\n\n  索引期间，图7\n\n  搜索期间，替换下图3的内容，使得图2中的例子中 搜索期间跟查询期间的有一样的排序规则，图8\n\n\n（重要）在图7、图8的代码条件下，传给Collector的文档号依旧分别是 0、1、2、3，但是这些文档号并不分别对应文档0~文档4了，根据排序规则，传给Collector的文档号docId跟文档编号的映射关系：\n\n图9：\n\n  由图9可以知道，Collector还是依次收到 0 ~ 4的文档号，但是对应的文档号已经发生了变化，因为这些文档在索引期间已经根据域名为&quot;sortByNumber&quot;的SortedNumerricSortField域的域值排好序了。\n  （极其重要）尽管在索引期间已经对段内的文档进行了排序，实际上文档0~文档4在段内的真实文档号依旧是：0、1、2、3，只是通过图9中的数组实现了映射关系，故给出下图：\n图10：\n\n  图10中通过数组实现的映射关系即Sorter.DocMap对象sortMap，在flush阶段，生成sortMap（见文档提交之flush（三））。\n 结语\n  TopFieldCollector相比较Collector（二）中TopScoreDocCollector，尽管他们都是TopDocsCollector的子类，由于存在索引期间的排序机制，使得TopFieldCollector的collect(int doc)的流程更加复杂，当然带来了更好的查询性能，至于如何能提高查询性能，由于篇幅原因，会在下一篇介绍图6的collect(int doc)的流程中展开介绍。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["collector","indexSort"]},{"title":"Constructing an HNSW Graph（Lucene 9.8.0）","url":"/Lucene/Index/2024/0126/ConstructinganHNSWGraph-html/","content":"Lucene has implemented the HNSW (Hierarchical Navigable Small World) logic based on the paper ‘Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs [2018].’ This article, in conjunction with the Lucene source code, introduces the implementation details during the construction process.\n Overview\nFigure 1：\n\nLet’s first introduce some basic knowledge about the completed HNSW (Hierarchical Navigable Small World) graph, briefly explained through Figure 1 from the paper:\n\nHierarchical Structure: The HNSW graph has a multi-layered structure, where each layer is an independent graph. The number of layers is usually determined based on the size and complexity of the dataset. At the top layer (as shown in Figure 1 as layer=2), there are fewer nodes, but each node covers a broader range. Conversely, at the bottom layer (layer=0), there are more nodes, but each node covers a relatively smaller area.\nNode Connections: In each layer, nodes are connected to their neighbors through edges. These connections are based on distance or similarity metrics, meaning each node tends to connect with other nodes closest to it.\nNeighbor Selection: The choice of which nodes to consider as neighbors is based on certain heuristic rules aimed at balancing search efficiency and accuracy. Typically, this involves maintaining the diversityof neighbors and limiting the number of neighbors for each node.\nSearch Path: Both the construction and querying of the HNSW graph involve a search process. The search path starts from a global entry node at the highest layer and then descends layer by layer until reaching the bottom layer. In each layer, the search follows the layer’s connection structure to find nodes closest to the target node.\n\n Implementation\nTo introduce the construction of the HNSW graph in Lucene, we will explain it through the process of adding/inserting a new node:\nFigure 2：\n\n New Node\nFigure 3：\n\nIn Lucene, each Document can only have one vector with the same field name, and each vector is assigned a node identifier (nodeId) in the order they are added. This nodeId is a value that starts from 0 and increases incrementally. In the source code, the new node is represented by this nodeId. When we need to calculate the distance between two nodes, we can find the corresponding vector values of the nodes through this mapping relationship for calculation.\n Calculating the Target Level\nFigure 4：\n\nThe target level is calculated as a random number that follows an exponential distribution.\nThe HNSW graph has a multi-layered structure, and the target level determines in which layers the new node will be added and establish connections with other nodes.\nFor example, if the target level is 3, then the node will be added to layers 3, 2, 1, and 0, respectively.\n Calculation Formula\nIn the source code, the target level is calculated using the formula: -ln(unif(0,1)) * ml, where unif(0,1) represents a uniformly distributed random value between 0 and 1, and ml is defined as 1/ln(M), where M(defined in source code as maxConn) is the maximum number of connections, i.e., a node can connect with up to M other nodes(2*M at the bottom). In the source code, the default value for M is 16.\nThe theoretical and experimental basis for using this formula is detailed in the paper; this article does not elaborate on it.\n Are There Any Unprocessed Levels Remaining?\nFigure 5：\n\nAfter calculating the target level, the process starts from the highest level and works downwards, layer by layer, until the new node is added to all levels and connections with other nodes are established. This completes the insertion of the new node.\n Retrieving the Entry Node\nFigure 6：\n\nAn entry point is a starting point that guides the insertion of a new node into a certain layer (there may be multiple entry points, as described below).\nThe purpose of inserting a new node into a layer is to establish its connections with other nodes in that layer. Therefore, on one hand, it first connects with the entry node, and on the other hand, it tries to establish connections with the neighbors of the entry node, the neighbors of the neighbors of the entry node, and so on, based on a greedy algorithm. This process will be detailed in the flow point Identifying Candidate Neighbors in the Current Layer.\n Types of Entry Nodes\nThe types of entry nodes can be divided into: global entry nodes and layer entry nodes:\n\nLayer Entry Nodes: In the insertion process, each layer may have one or more entry nodes (each new node may have different entry nodes in each layer). These nodes are determined in the process of descending through layers and are used to guide the search on each level. In other words, the entry nodes of the current layer are the TopK nodes connected to the new node in the previous layer (as will be introduced later). If the current layer is already the highest layer, then the entry node for that layer is the global entry node.\nGlobal Entry Node: Also known as initial entry node, it is a single node used as the entry node for the highest layer. When adding a new node, and when the target level of the new node is higher than the current number of layers in the HNSW graph, this node will serve as the new global entry node.\n\n Overview of Obtaining Layer Entry Nodes\nObtaining layer entry nodes can be summarized in two scenarios, as shown below:\nFigure 7：\n\n\n\nTarget Layer &gt; Current Layer (highest layer in the graph): In layers 4, 3, and 2, the addition of the new node introduces new layers, so the new node can be directly added to these layers. There is no need to consider the entry node in these cases; in layers 1 and 0, the approach is the same as the other scenario described below.\n\n\nTarget Layer &lt;= Current Layer: Before reaching the target layer, we need to start from the highest layer and obtain the entry node for each layer, descending layer by layer.\n\nLayer 3: As it is the highest layer, the global entry node serves as the entry node for this layer. Finding the candidate neighbors for the new node in this layer , and the Top1 node (called ep3)  from the candidate neighbor collectionis selected as the entry node for layer 2.\nLayer 2: ep3 serves as the entry node for this layer, and the process of finding candidate neighbors for the new node and selecting the Top1 node (called ep2) as the entry node for layer 1.\nLayer 1: ep2 serves as the entry node, and the process of finding candidate neighbors for the new node and the TopK node collection (called ep1) from the candidate neighbors is selected as the entry nodes for layer 0.\nLayer 0: The node collection named ep1 serves as the entry nodes for this layer.\n\nIt is worth noting that in layers 1 and 2, only the Top1 (closest neighbor) from the previous layer’s neighbor collection is used as the entry node, while in layer 0, a collection of TopK nodes is selected. The rationale behind this is to balance the needs for search efficiency and accuracy:\n\nChoosing the closest neighbor as entry node: This focuses on quickly narrowing down the search area and getting as close as possible to the target node.\nUsing multiple neighbors  as entry node: This improves the comprehensiveness of the search, especially in complex or high-dimensional data spaces. Using multiple entry nodes allows exploring the space from different paths, increasing the chances of finding the best match.\n\n\n\nWhy Descend Layer by Layer Instead of Directly Reaching the Target Level\nIn Figure 7 notice that, when Target Layer &lt;= Current Layer, the approach is to descend layer by layer rather than starting directly from the target level. The considerations include:\n\nFast Navigation in Higher Layers: In higher layers, the connections between nodes cover larger distances. This means that searching in higher layers can quickly skip irrelevant areas and rapidly approach the target area of the new node. Jumping directly to the target layer might miss this opportunity for rapid approach.\nGradually Refining the Search: Starting from higher layers and descending layer by layer allows the search process to become progressively more refined. In each layer, the search adjusts its direction according to the connection structure of that layer, approaching the position of the new node more precisely. This layered refinement process helps in finding a more accurate nearest neighbor.\nAvoiding Local Minima: Direct searching in the target layer could result in falling into local minima, where the nearest neighbor found is not the closest neighbor globally. Descending layer by layer helps avoid this as each layer’s search is based on the results of the previous layer, offering a more comprehensive perspective.\nBalancing Search Costs: While descending layer by layer might seem more time-consuming than jumping directly to the target layer, it is often more efficient. This is because fewer search steps are needed in higher layers, whereas direct searching in the denser lower layers might require more steps to find the nearest neighbor.\n\n Identifying Candidate Neighbors in the Current Layer\nFigure 8：\n\nIdentifying candidate neighbors in the current layer is a search process of a greedy algorithm. It starts from the entry node, initially considering it as the node that seems closest to the new node. If a node’s neighbor appears closer to the newly inserted node, the algorithm shifts to that neighbor and continues exploring its neighbors. Eventually, it finds the TopN closest nodes (note that these TopN nodes may not necessarily be the closest to the new node). The flowchart is as follows:\nFigure 9：\n\n Candidate Neighbor Collection\nThe data structure for the candidate node collection is a min-heap, sorted by node distance scores, with closer distances receiving higher scores. Node distance refers to the distance between the new node and the candidate neighbors.\n Key Points in the Greedy Algorithm’s Search Process\n\nDistance Score Threshold: During the search process, a threshold called minCompetitiveSimilarity is continuously updated, which is the top element of the min-heap. If the distance between a neighbor and the new node is less than this threshold, nodes connected to this neighbor are no longer processed.\nRecording Visited Nodes: Due to the interconnections between nodes, the greedy algorithm can easily revisit the same nodes. Recording visited nodes improves search performance.\nEntry Node for the Next Layer: The TopN from the candidate neighbor collection of the current layer will serve as the entry nodes for the next layer, as mentioned above, the Top1 and TopK (here, K is defined in the source code by a variable named beamWidth, with a default value of 100).\n\n Close Enough Neighbors vs. Absolute Closest Neighbors\nThe greedy algorithm’s search process means that it might not find the absolute closest neighbors but usually finds neighbors close enough:\n\nThe algorithm’s goal is to find neighbors close enough to the new node. Here, “close enough” means that while the neighbors found may not be the absolute closest (i.e., some nodes closer to the new node are not chosen as neighbors), they are sufficiently close to effectively represent the new node’s position in the graph.\nEfficiency and Accuracy Balance: In practical applications, finding the absolute closest neighbors can be very time-consuming, especially in large-scale or high-dimensional datasets. Therefore, the algorithm often seeks a balance point, finding close enough neighbors within an acceptable computational cost.\nGreedy Search Strategy: HNSW uses a greedy algorithm to progressively approximate the nearest neighbors of the new node. This means that at each step, the algorithm chooses the neighbor that currently appears closest to the new node. This method usually finds close enough neighbors quickly but does not always guarantee finding the absolute closest neighbors.\nPractical Considerations: In most cases, finding “close enough” neighbors meets the needs of most application scenarios, such as approximate nearest neighbor search. This approach ensures search efficiency while providing relatively high search accuracy.\nLimiting Neighbor Numbers: To control the complexity of the graph, the number of neighbors for each node is usually limited. This means that even if closer nodes exist, they might not be chosen as neighbors of the new node due to the limit on the number of neighbors.\n\n Selecting Candidate Neighbors Based on Diversity\nFigure 10：\n\nAlthough we have already found the TopN “close enough” neighbors in the previous step, Identifying Candidate Neighbors in the Current Layer, we still need to examine these neighbors for diversity based on the following factors. Neighbors that do not meet the diversity criteria will not be connected, and diversity checking is achieved by calculating the distances between neighbors:\n\nCovering Different Areas: If the neighbors of a node are far apart from each other, it means they cover different areas around the node. This distribution helps quickly locate different areas during the search process, thereby improving search efficiency and accuracy.\nAvoiding Local Minima: In high-dimensional spaces, if all neighbors are very close, the search process may fall into local minima, where the nearest neighbor found is not the closest globally. Distance differences between neighbors provide more search paths to avoid this situation.\nEnhancing Graph Connectivity: Neighbors with distance differences can enhance the connectivity of the graph, making the paths from one node to another more diverse. This is important for quickly propagating information or finding optimal paths in the graph.\nAdapting to Different Data Distributions: In real applications, data is often not uniformly distributed. Ensuring distance differences between neighbors can better adapt to these non-uniform data distributions, ensuring the graph structure effectively covers the entire data space.\nBalancing Exploration and Exploitation: In search algorithms, there needs to be a balance between exploration (exploring unknown areas) and exploitation (using known information). Distance differences between neighbors help this balance, as it allows the algorithm to explore multiple directions from a node, rather than being limited to the closest neighbors.\nImproving Robustness: In dynamically changing datasets, the distribution of data points may change over time. If a node’s neighbors have a certain distance difference, this helps the graph structure adapt to these changes, maintaining its search efficiency.\n\n Diversity Check\nThe checking process flowchart is as follows:\nFigure 11：\n\n Neighbor Collection\nThe elements in the neighbor collection are N nodes selected from the candidate neighbor collection after diversity checking, and they will become the official neighbors of the new node.\nNote that in layer 0, a node can connect to up to 2*maxConn neighbor nodes, while in other layers, it can connect to a maximum of maxConn nodes, which is the size of the neighbor collection. The default value of maxConn is 16.\nIn the source code, the NeighborArray object represents a node’s neighbor information:\nFigure 12：\n\n\nsize: The number of neighbors\nscore: An array of distance scores between the node and its neighbors\nnode: An array of neighbor node identifiers\nscoresDescOrder: Whether the node and score arrays are sorted in ascending or descending order by distance score\nsortedNodeSize: This is not relevant for this article\n\n Sorting the Candidate Neighbor Collection\nAs mentioned earlier, the candidate neighbor collection is a min-heap, with the top element being the farthest distance. Since the subsequent flow Does the Neighbor Node Meet the Diversity Criteria?? requires starting from the closest (highest score), the Sorting the Candidate Neighbor Collection in the source code involves writing the heap elements into a NeighborArray called scratch, sorted in ascending order by distance score.\n Checking Each Candidate Neighbor for Diversity\nStarting with the highest scored element in scratch, compare its distance with each neighbor in the neighbor collection d(neighbor, new node's other neighbors). If there is at least one other neighbor such that d(neighbor, new node) is less than d(neighbor, new node's other neighbors), it does not meet diversity. If it meets diversity, add this candidate neighbor node to the neighbor collection; it will become an official neighbor of the new node in that layer.\n Attempting to Add the New Node as a New Neighbor of Its Neighbors\n图13：\n\nThis process point is closely related to the graph’s update strategy. When a new node is added to the graph, it is necessary to update the graph structure according to specific rules or strategies. This includes which nodes should be included in the new node’s neighbor list, as discussed above, and deciding which existing nodes should add the new node to their neighbor lists. These update strategies typically consider the following aspects:\n\nMaintaining Graph Connectivity: The update strategy aims to maintain good connectivity of the graph, ensuring effective navigation from any node to others.\nOptimizing Search Efficiency: By appropriately updating neighbor lists, the structure of the graph can be optimized, thereby improving the efficiency of subsequent search operations.\nMaintaining Neighbor Diversity: Update strategies often aim to maintain diversity among neighbors, which helps to enhance the comprehensiveness and accuracy of searches.\nControlling Graph Size and Complexity: To avoid making the graph overly complex, update strategies may include limiting the maximum number of neighbors for a node.\nAdapting to Data Changes: In dynamically changing datasets, update strategies help the graph adapt to the addition of new data, maintaining its ability to reflect the current state of the dataset.\n\n图14：\n\nThe neighbor collection contains the new node’s official neighbors, and a unidirectional connection has already been established with them. If the neighbor’s neighbor list of the new node has not yet reached the connection limit, i.e., 2*maxConn in layer 0 and maxConn in other layers, then the new node can be added as a neighbor of its neighbor. Otherwise, after being added to the neighbor’s neighbor list, a diversity check is performed (following the same logic as described earlier), and the node with the least diversity is removed from the list. Of course, this least diverse node might be the new node or some other node in the list.\n Conclusion\n  This article outlines the process of constructing an HNSW graph in Lucene 9.8.0, and the subsequent graph data will be stored using index files .  However, currently, this article is only available in Chinese.\n","categories":["Lucene","Index"],"tags":["vec","vem","vemf","vemq","veq","vex","hnsw","eps"]},{"title":"Collector（四）","url":"/Lucene/Search/2019/0815/Collector%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  本文承接Collector（三），继续介绍其他的收集器。\n  图1是Lucene常用的几个Collector：\n图1：\n\n TopDocsCollector\n TopFieldCollector\n  根据过滤（filtering）规则，TopFieldCollector派生出的两个子类：\n\nSimpleFieldCollector：无过滤规则\nPagingFieldCollector：有过滤规则，具体内容在下文展开\n\n SimpleFieldCollector\n  SimpleFieldCollector的collect(int doc)方法的流程图：\n图2：\n\n点击查看大图\n  在介绍每个流程之前，先介绍下TopFieldCollector中的几个变量：\n\ntrackMaxScore：该值是TopFieldCollector类的构造参数，用来描述是否需要记录所有满足查询条件的文档集合中最高的文档打分值，用maxScore用来记录该最大值\ntrackDocScores：该值是TopFieldCollector类的构造参数，用来描述是否需要记录所有满足查询条件的文档的文档打分值\ntotalHits：该值描述了Collector处理的满足搜索条件的文档数量，每当进入图2的流程，该值就递增一次。\n\n  如果业务中不需要用到文档的打分值或者maxScore，强烈建议另这两个参数为false，因为找出maxScore或者文档的打分值需要遍历所有满足查询条件的文档，无法提前结束Collector工作（canEarlyTerminate），在满足查询提交的文档数量较大的情况下，提前结束Collector的收集工作能显著提高查询性能。canEarlyTerminate会在下文中介绍\n 记录文档打分值最高的文档\n图3：\n\n  如果参数trackMaxScore为true，那么Collector每处理一篇文档，就要记录该文档的打分值score，如果score大于当前maxScore的值，则更新maxScore的值。\n 添加文档信息\n图4：\n\n  使用优先级队列PriorityQueue来存放满足搜索条件的文档信息（文档信息至少包含了文档打分score以及文档号docId），分数最低的文档信息位于堆顶，堆的大小默认为段中的文档总数（用户也可以指定堆的大小，即用户期望的返回结果TopN的N值）。\n  如果堆没有满，那么将文档号交给FieldComparator，FieldComparator的概念在FieldComparator的文章中介绍了，不赘述，它用来描述文档间的排序关系（从代码层面讲，通过FieldComparator实现了优先级队列PriorityQueue的lessThan()方法），接着添加文档信息到堆中。\n 设置bottom值\n图5：\n\n  在添加文档信息到堆中流程后，如果此时堆正好满了，那么我们需要设置bottom的值，即从我们已经处理的文档中找出最差的（the weakest，sorted last），使得当处理下一篇文档时，只需要跟这个最差的文档进行比较即可。\n 仅统计满足查询条件的文档个数\n图6：\n\n  在堆满的情况的下，并且collectedAllCompetitiveHits为true，直接可以退出，尽管直接退出了，还是统计了totalHits的值，所以从collectedAllCompetitiveHits的命名方式也可以看出来只是统计了totalHits。\n  满足下面条件的情况下，collectedAllCompetitiveHits会为true：\ncanEarlyStopComparing == true &amp;&amp; canEarlyTerminate == false\n\ncanEarlyStopComparing：该值描述了是否可以提前结束域比较，在Collector（三）我们提到，当索引期间通过IndexWriterConfig.setIndexSort(Sort sort)设置的排序规则与搜索期间提供的排序规则一致时，Collector收到的文档集合已经是有序的，在堆已满的情况下，后面处理的文档号就没有比较的必要性了，那么canEarlyStopComparing的值会被true，每次获取一个段的信息时设置canEarlyStopComparing，即调用getLeafCollector(LeafReaderContext context) 时候设置（见Collector（一））。\ncanEarlyTerminate：该值描述了是否可以提前结束Collector的收集工作，canEarlyTerminate设置为true需要满足下面的条件：\n\ntrackTotalHits == false &amp;&amp; trackMaxScore == false &amp;&amp; canEarlyStopComparing\n  如果满足上面的条件，Lucene会通过抛出异常的方式结束Collector，该异常会被IndexSearcher捕获。这样的好处在于能提高查询性能。比如说某一次查询，我们需要返回Top5，但是满足搜索条件的文档数量有10000W条，那么在Collector中当处理了5篇文档后（文档在段中是有序的），就可以直接返回结果了。\n  如果条件不满足，即canEarlyTerminate的值为false，那么尽管我们已经收集了Top5的数据（查询结果不会再变化），但是要继续遍历处理剩余的9995篇文档，因为我们需要记录totalHits（如果trackTotalHits为true）或者需要获得打分值最大的文档（如果trackMaxScore为true），所以此时collectedAllCompetitiveHits为true，继续处理下一篇文档\n 更新最差的（the weakest，sorted last）文档\n图7：\n\n  通过与域比较器（FieldComparator）的bottom值比较，如果比该值更好（competitive），那么先替换bottom，然后重新算出新的bottom，随后还要替换堆顶元素，然后调整堆，算出新的堆顶元素，最后退出继续处理下一篇文档。\n 无法提前结束域比较\n图8：\n\n  由于通过域比较后，当前文档比bottom还要差，那么先通过canEarlyStopComparing判断出能不能提前结束比较，如果canEarlyStopComparing为false，则退出并处理下一篇文档。\n  canEarlyStopComparing为false说明段中的文档没有按照搜索期间的排序规则进行排序，所以当前已经收集的TopN未必是最终的搜索结果，所以退出处理下一篇文档。\n 设置collectedAllCompetitiveHits为true\n图9：\n\n  可以提前结束域比较，即canEarlyStopComparing为true，并且不可以提前结束Collector的收集工作，即canEarlyTerminate为false，那么同时满足这两个条件就可以设置collectedAllCompetitiveHits为true了。使得处理下一篇文档时就可以走图6中的流程了。\n 提前结束Collector的收集工作\n图10：\n\n  可以提前结束Collector的收集工作，那么我们先估算剩余满足查询条件的文档数量，通过线性估算出实现，估算方法不展开介绍，没有实际意义。\n  接着设置一个earlyTerminated的值为true，用户在得到查询结果后可以通过该值来了解Collector提前结束收集工作这个事件。\n  通过抛出CollectionTerminatedException异常的方式来实现，大家可以点击链接看下源码中对这个异常的解释。\n PagingFieldCollector\n  PagingFieldCollector同Collector（二）中的PagingTopScoreDocCollector一样，相对于SimpleFieldCollector实现了分页功能，分页功能的介绍见Collector（二），不赘述，collect(int doc)的流程图是相似的，并且用红圈标记出不同处。\n  PagingFieldCollector的collect(int doc)方法的流程图：\n图11：\n\n点击查看大图\n 是否已经被收集了\n  是否已经被收集了描述的是该文档号是否已经在前面的搜索中被收集了，判断的条件如下，如果为true，说明该文档已经被收集了：\ntopCmp &gt; 0 || (topCmp == 0 &amp;&amp; doc &lt;= afterDoc\n\ntopCmp：该值描述的当前文档与FileComparator的top值进行比较后的值，top值描述的是之前所有分页搜索的结果中最差的文档，在初始化PagingFieldCollector对象时需要用户提供该值（通过上一次的查询结果就能获得），并设置top的值。如果topCmp &gt; 0，说明当前文档比最差的文档好（competitive），必定该篇文档已经在前面某次分页搜索中被收集过了。\nafterDoc：该值描述的是之前所有分页搜索的结果中最差的文档的文档号，在Collector（三）中我们说到，如果域比较无法区得出排序结果，由于文档号是唯一的，所以再根据文档号进行比较，文档号大的比文档号小的差（uncompetitive），所以在topCmp == 0 情况下，如果当前文档号小于等于afterDoc，必定该篇文档已经在前面某次分页搜索中被收集过了。\n\n 结语\n  在下一篇文档中，我们将继续介绍最后一个Collector，即DiversifiedTopDocsCollector，这将是Collector系列文章的最后一篇。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["collector","indexSort"]},{"title":"DirectMonotonicWriter&&Reader","url":"/Lucene/yasuocunchu/2020/1030/DirectMonotonicWriter&&Reader/","content":" DirectMonotonicWriter&amp;&amp;Reader\n  DirectMonotonicWriter类用来存储单调递增的整数序列（monotonically-increasing sequences of integers），使用了先编码后压缩的存储方式，DirectMonotonicReader类则是用来解码跟解压。\n  每当处理1024个数据，将会执行编码跟压缩的操作，并生成两个block，分别用来存储编码元数据以及压缩的数据，我们以索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm为例：\n图1：\n\n图2：\n\n  图1、图2中，每处理1024个NumDoc，先进行编码，生成的编码元数据由四部分组成：Min、AvgInc、Offset、BitRequired组成图2中的NumDocMeta字段，编码后的数据将被压缩处理，并写入到图1中的NumDocsBlock字段中。\n 先编码后压缩\n  先编码后压缩的流程图如下所示：\n图3：\n\n 准备阶段\n图4：\n\n  图4中，准备数据为一个整数，强调的是当前处理的整数总是比上一个整数值大，因为DirectMonotonicWriter只能处理单调递增的整数序列，是否生成两个block的条件即上文中提到的是否已经添加了1024个整数，否则就添加到buffer数组中，等待满足条件。。\n 数据标准化\n图5：\n\n  数据标准化的处理方式为将数据，即buffer[ ]数组中的元素，按某种方式缩放，使之落入一个更小的数值区间。\n  数据标准化的目的在于降低存储空间，即减少索引文件的大小，需要执行以下几个步骤：\n\n步骤一：计算平均值avgInc\n步骤二：缩放数据\n步骤三：计算最小值min\n步骤四：无符号处理\n\n 计算平均值avgInc\n  由于buffer[ ]数组中的元素是递增的，所以将数组中的最后一个以及第一个元素的差值除以数组中的元素数量就可以获得平均值avgInc。代码如下：\nfinal float avgInc = (float) ((double) (buffer[bufferSize-1] - buffer[0]) / Math.max(1, bufferSize - 1));\n 缩放数据\n  通过下面的方式进行第一次缩放：\nfor (int i = 0; i &lt; bufferSize; ++i) &#123;    final long expected = (long) (avgInc * (long) i);    buffer[i] -= expected;&#125;\n  上述代码中，通过与avgInc做减法来实现缩放，并且数值越大，缩减的效果越明显。代码第2行中的 i 可以理解为权重值，来描述缩减的程度。\n  为什么要缩放数据？\n  在后续的处理中，buffer数组中的每个元素都会使用**固定位数**的方式进行存储（否则就无法读取了），由于buffer数组中的元素是递增的，如果不进行缩放，每个元素占用的bit数量会按照数组中最大的元素所需的bit数量，会造成极大的存储浪费。\n  我们用一个例子来展示缩放的效果，完整demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/python/DirectMonotonicTest/ReductionTest.py  ， 该例子使用Python实现了处理逻辑。\n  缩放前的数值分布：\n图6：\n\n  正如上文中所说的，buffer数组是个递增序列。\n  缩放后的数值分布：\n图7：\n\n  缩放后的大致数值区间由原来的[0, 60000]，变成了[-750, 1000]。\n 计算最小值min\n  计算最小值的目的是为了下一步做准备，由于在第一次缩放数据的过程中，buffer[ ]数组中的元素可能不再是递增的了，所以只能通过逐个遍历数组元素来找到最小值，代码如下：\nfor (int i = 1; i &lt; bufferSize; ++i) &#123;    min = Math.min(buffer[i], min);&#125;\n 无符号处理\n  在缩放数据后，buffer中的有些数据变成了负数，由于随后将会使用PackedInts存储，该方法只支持存储无符号的数值，故结合上一步中获得的最小值min，通过下面的方法使得所有的数值进行无符号处理。\nfor (int i = 0; i &lt; bufferSize; ++i) &#123;   buffer[i] -= min;   maxDelta |= buffer[i];&#125;\n图8：\n\n  上述代码的第四行中，maxDelta通过或操作获得存储一个数值需要占用的bit数量，即图2中的BitRequired。\n  无符号处理后的数值分布如下所示：\n图9：\n\n  最终数值区间大致处于[0, 1750]。\n 压缩存储\n图10：\n\n  压缩存储的过程即为将buffer中的数据生成一个block，例如在索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm中就是生成一个图1中的NumDocBlock，另外由于每1024个数据就生成一个block，当生成多个block时，通过记录offset来描述每一个block的起始读取位置，offset即上文中说道的编码元数据之一。\n  压缩方式使用DirectWriter&amp;&amp;DirectReader实现，这里不赘述。\n 元数据存储\n图11：\n\n  在上述的流程中，我们获得了编码元数据：Min、AvgInc、Offset、BitRequired，那么将这些数据生成一个block，例如图2中的NumDocMeta。\n 特殊情况\n  当buffer数组中的元素集合满足等差数列的性质时，那么可以特殊处理，使得更有效的降低存储空间。因为我们只需要存储等差数列的公差即可，根据上文中平均值avgInc的计算方式可知avgInc即公差值，那么此时只需要存储编码元数据，即只需要生成一个block。\n  哪种场景满足这种特殊情况？\n  在文章索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm中，当生成一个chunk的条件都是因为达到了阈值maxDocsPerChunk，就能满足这种特殊情况，因为每个chunk中的文档数量要么都是128或者512。\n点击下载附件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode"]},{"title":"DirectWriter&&DirectReader","url":"/Lucene/yasuocunchu/2019/1223/DirectWriter&&DirectReader/","content":" DirectWriter&amp;&amp;DirectReader\n  阅读本篇文章需要前置内容：BulkOperationPacked，下文中会列出在文章BulkOperationPacked中涉及的代码，但是不会展开介绍。\n  DirectWriter&amp;&amp;DirectReader两个类用来处理long类型的数据集（数组类型），其中DirectWriter用来在写数据时使用BulkOperationPacked将long类型的数据转换成byte类型，而DirectReader则是将byte类型的数据恢复成long类型。使用byte类型数据存储的组件都可以使用DirectWriter&amp;&amp;DirectReader实现压缩存储，比如以字节为单位存储索引文件内容的Directory。\n 压缩实现\n  在BulkOperation中，根据bitPerValue的值，对应的压缩实现不尽相同，但是DirectWriter只支持下列的bitPerValue：\nfinal static int SUPPORTED_BITS_PER_VALUE[] = new int[] &#123;    1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 40, 48, 56, 64&#125;;\nbitPerValue是什么：\n  使用BulkOperation提供的压缩实现，要求数据集中的所有数据按照固定位数存储，固定位数即bitPerValue。\nbitPerValue怎么计算：\n  选取数据集中的最大值，它的有效数据占用的bit位个数为bitPerValue。\n  例如有下面的数据集：\n数组一：\nlong []values = &#123;2, 278, 23&#125;;\n  数组一中的元素对应的二进制表示如下所示：\n表一：\n\n\n\n数组元素\n二进制\n\n\n\n\n2\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00000010\n\n\n278\n00000000_00000000_00000000_00000000_00000000_00000000_00000001_00010110\n\n\n23\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00010111\n\n\n\n  数组一中的最大值为278，它的有效数据占用的bit位个数如表一中红色标注所示，共9位，故bitPerValue的值为9，并且数值2、23都使用9个bit来存储。\n待处理的数据集的bitPerValue不是SUPPORTED_BITS_PER_VALUE中的一员怎么办：\n  如果待处理的数据集的bitPerValue不属于SUPPORTED_BITS_PER_VALUE数组中的数组元素之一，那么另下一个比bitPerValue大的数组元素作为新的bitPerValue。例如表一中，bitPerValue的值为9，那么重新另bitPerValue为12，即，数值2、278、23都使用12个bit位存储。\n为什么只支持SUPPORTED_BITS_PER_VALUE中的bitPerValue：\n  因为这些bitPerValue对应的压缩实现是读写性能最佳的，衡量读写性能的指标如下：\n\n写性能：写入一个数值需要涉及的block个数，越少性能越高\n读性能：读取一个数值对应的block的个数，越少性能越高，另外字节对齐的数据读取性能越高，否则需要额外计算字节首地址\n\n例如我们有以下的数据集：\n数组二：\nlong []values = &#123; 120, 69, 23, 25&#125;;\n  数组一中的元素对应的二进制表示如下所示：\n表二：\n\n\n\n数组元素\n二进制\n\n\n\n\n120\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_01111000\n\n\n69\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_01000101\n\n\n23\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00010111\n\n\n25\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00011001\n\n\n\n  表二中，数组二的bitPerValue为7，如果我们使用该bitPerValue，即每个数值使用7个bit位存储，那么处理结束后如下所示，同时给出bitPerValue为8时的结果：\n图1：\n\n  从图1可以看出，在读取阶段，如果bitPerValue为7，那么需要两个字节才能读取到69、23、25的值，另外需要计算首地址跟偏移才能找到在字节中的起始位置，而如果bitPerValue为8，那么只需要一个字节即可，并且不需要计算偏移地址。很同时也可以看出，这是空间换时间的设计。\n  我们再看下写阶段的情况，从图1同样能明显看出bitPerValue为7时，写入一个数值需要跨越2个block，我们从代码中也可以看出其性能差异的原因：\n图2：\n\n  图2截取自BulkOperationPacked，bitPerValue为7时，相比较bitPerValue为8时需要执行额外的位移操作，如第31行代码所示。更重要的是当处理相同数量的数值时，需要更多的遍历次数，在文章BulkOperationPacked中，为了简化介绍BulkOperationPacked的原理，我们并没有介绍如何计算遍历次数，只是简单的将待处理的数据集数量作为遍历次数，即图8中第31行的values.length，在源码中，图1中的第8行代码应该是这样的：\n图3：\n\n  遍历次数是如何计算的：\n  在后续介绍PackedWriter的文章中我们再讲述这个问题，本篇文章中我们只要知道bitPerValue为7时，相比较bitPerValue为8在处理相同数量的数据集时，需要更多的遍历次数。\n如何选择bitPerValue的值作为SUPPORTED_BITS_PER_VALUE中的一员：\n  我们再次列出SUPPORTED_BITS_PER_VALUE中的成员：\nfinal static int SUPPORTED_BITS_PER_VALUE[] = new int[] &#123;    1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 40, 48, 56, 64&#125;;\n  我们先看下bitPerValue大于等于8的情况，当bitPerValue为8、16、24、32、40、48、56时，这几个值总能保证待处理的数值能按照字节对齐（8的倍数）处理，故读写性能是很快的，我们想要了解的是，bitPerValue的值不是8的倍数的情况下，为何还将12、20、28、48作为SUPPORTED_BITS_PER_VALUE的成员。\n  这是一种出于空间使用率与读写性能的折中处理，例如bitPerValue的值为9时，如果只能选择16，那么每一个数值的额外空间开销为（16 - 9）/ 16 = 0.4375，故我们需要在(8，16)的区间内找出一个空间开销与读写性能兼顾的bitPerValue，那么12是最好的选择，因为它的读写性能在这个区间内是最佳的，同时额外空间开销降低到         （12 - 9）/ 12 = 0.25。\n为什么在区间(8，16)中选取bitPerValue的值是12:\n  首先在这个区间内bitPerValue的写性能是差不多的，因为它们的值不是8（一个字节）的倍数，都需要跨block存储，所以我们看它们在读性能的差异性，同样地，由于不能保证每个数值都是按照字节对齐存储，故我们只能通过平均读取一个数值需要计算首地址的次数来判断读的性能：\n  我们有如下的数据集数组三：\n数组三：\nlong[] values = &#123;69, 25, 261, 23&#125;;\n  数组三中的元素对应的二进制表示如下所示：\n表三：\n\n\n\n数组元素\n二进制\n\n\n\n\n69\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_01000101\n\n\n25\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00011001\n\n\n261\n00000000_00000000_00000000_00000000_00000000_00000000_00000001_00000101\n\n\n23\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00010111\n\n\n\n  数组三中，最大值为261，故此时的bitPerValue为9，我们用bitPerValue为9以及bitPerValue为12来处理数组三的数据，如下所示：\n图4：\n\n  从图4可见，如果需要读取261的数值，在bitPerValue=9中，由于261对应的首地址没有按照字节对齐，那么需要额外的计算出首地址的值，因为读取byte[ ]数组总是按字节读取，而在bitPerValue=12的情况下，则不需要计算：\n\nbitPerValue=9：每读取8个数值，需要计算7次首地址，平均读取一个数值需要计算0.875次首地址\nbitPerValue=12：每读取2个数值，需要计算1次首地址，平均读取一个数值需要计算0.5次首地址\n\n  故当bitPerValue=12时，读性能较高。\n  如果我们连续读取byte[ ]数组中的元素，在bitPerValue=9的情况下，第一个数值是不需要计算首地址的，而下一个不需要计算首地址的数值是第9个，所以每读取8个数值，需要计算7次首地址。其实我们只需要计算8跟bitPerValue的最小公约数就能计算出不同的bitPerValue对应的平均读取一个数值需要计算首地址的次数。\n  下图是在区间(8, 16)之间，各个bitPerValue的取值对应的平均读取一个数值需要计算首地址的次数：\n表4：\n\n\n\nbitPerValue\n平均读取一个数值需要计算首地址的次数\n\n\n\n\n9\n7/8 = 0.875\n\n\n10\n3/4 = 0.6\n\n\n11\n7/8 = 0.875\n\n\n12\n1/2 = 0.5\n\n\n13\n7/8 = 0.85\n\n\n14\n3/4 = 0.6\n\n\n15\n7/8 = 0.875\n\n\n\n  由表4可以看出，当bitPerValue=12时，平均读取一个数值需要计算首地址的次数的最少，所以读的性能最高。\n 结语\n  在应用方面，在索引阶段生成索引文件.dvm、dvd时，就使用这两个类来实现数据压缩。\n点击下载附件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode"]},{"title":"Directory（上）","url":"/Lucene/Store/2019/0613/Directory%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"  Directory类用来维护索引目录中的索引文件，定义了创建、打开、删除、读取、重命名、同步(持久化索引文件至磁盘)、校验和（checksum computing）等抽象方法。\n  索引目录中不存在多级目录，即不存在子文件夹的层次结构(no sub-folder hierarchy)。\n  其子类如下图所示，另外下图中只列出了Lucene7.5.0的core模块中的子类，在其他模块，比如在misc模块中还有很多其他的子类：\n图1：\n\n点击查看大图\n  接下来一一介绍其子类。\n BaseDirectory\n  BaseDirectory同样是一个抽象类，提供了其子类共有的获取索引文件锁的方法，即维护了一个LockFactory对象，索引文件锁的概念已经在前面的文章中介绍，这里不赘述。\n  下图中为BaseDirectory类的子类：\n图2：\n\n FSDirectory\n图3：\n\n  FSDirectory作为一个抽象类，提供了其子类共有的创建、删除、重命名、同步(持久化索引文件至磁盘)、校验和（checksum computing）等方法，这些方法在介绍完三个子类后再叙述。\n  FSDirectory的三个子类主要的不同点在于它们各自实现了打开、读取索引文件的方法。\n SimpleFSDirectory\n\n打开索引文件：使用Files的newByteChannel方法来打开一个索引文件，比如说通过DirectoryReader.open(IndexWriter)读取索引文件信息会调用此方法\n读取索引文件：使用FileChannelImpl读取索引文件，使得可以随机访问索引文件的一块连续数据。\n\n  随机访问索引文件的一块连续数据在Lucene中是很重要的，例如图4中画出了.doc索引文件的数据结构，索引文件按照域（field）划分，在读取阶段，Lucene总是按域逐个处理，所以需要获取每一个域在.doc索引文件中的数据区域。\n图4：\n\n  使用SimpleFSDirectory有以下注意点：\n\n该类不支持并发读取同一个索引文件，多线程读取时候会被处理为顺序访问（synchronized(FileChannelImpl)）如果业务有这方面的需求，那么最好使用NIOFSDirectory或者MMapDirectory\n如果有多个线程读取同一个索引文件，当执行线程被打断(Thread.interrupt()或Future.cancel())后，该索引文件的文件描述符(file descriptor)会被关闭，那么阻塞的线程随后读取该索引文件时会抛出ClosedChannelException的异常，不过可以使用RAFDirectory来代替SimpleFSDirectory，它使用了RandomAccessFile来读取索引文件，因为它是不可打断的(not interruptible)。RAFDirectory已经作为一个旧的API(legacy API)被丢到了misc模块中，它同样不支持并发读取索引文件，所以跟SimpleFSDirectory很类似，不展开介绍\n\n NIOFSDirectory\n\n打开索引文件：使用Files的FileChannel.open方法来打开一个索引文件\n读取索引文件：使用FileChannelImpl读取索引文件，使得可以随机访问索引文件的一块连续数据。\n\n  使用NIOFSDirectory有以下注意点：\n\n该类支持并发读取同一个索引文件，但是它存在跟SimpleFSDirectory一样的多线程下执行线程被打断的问题，如果业务中存在这个情况，那么可以使用RAFDirectory来代替NIOFSDirectory\n另外如果Lucene是运行在Windows操作系统上，那么需要注意在SUN’s JRE下的一个BUG\n\n MMapDirectory\n\n打开索引文件：使用内存映射(memory mapping)功能来打开一个索引文件，例如初始化MMapDirectory时，如果索引目录中已存在合法的索引文件，那么将这些文件尽可能的都映射到内存中，或者通过DirectoryReader.open(IndexWriter)读取索引文件信息会打开IndexWriter收集的索引文件数据\n读取索引文件：将索引文件全部读取到内存中(如果索引文件在磁盘上)\n\n  如果内存映射失败，导致的原因可能是内存中连续的虚拟地址空间的数量（unfragmented virtual address space）不足、操作系统的内存映射大小限制等，更多的原因可以看这里MapFailed，Lucene7.5.0中根据不同的情况提供了下面几种出错信息：\n\n非64位的JVM：MMapDirectory should only be used on 64bit platforms, because the address space on 32bit operating systems is too small\nWindows操作系统：Windows is unfortunately very limited on virtual address space. If your index size is several hundred Gigabytes, consider changing to Linux\nLinux操作系统：Please review ‘ulimit -v’, ‘ulimit -m’ (both should return ‘unlimited’), and ‘sysctl vm.max_map_count’\n内存不足：Map failed。JVM传递过来的内存不足的堆栈信息是各种嵌套OOM信息(nested OOM)，容易让使用者困惑，所以Lucene将复杂的堆栈信息替换为一条简单的信息，即&quot;Map failed&quot;\n\n 内存映射I/O技术\n  以下内容选自&lt;&lt;Linux/UNIX系统编程手册(下册)&gt;&gt;，文字太多，直接上个截图吧：\n图5：\n\n  内存映射I/O之所以能够带来性能优势的原因如下：\n图6：\n\n  内存映射I/O优缺点：\n图7：\n\n 取消映射(unmap)\n  由于JVM的限制，并没有提供取消映射的方法，故在某些JDK版本会存在这么一个问题，即用户调用了FileChannel的close()方法，但是无法关闭操作系统层面的该文件的文件描述符，直到GC回收才能关闭。这样的情况会导致以下的问题，调用了FileChannel的close()方法并且在GC回收前的时间区间内，执行了删除或者覆盖文件的操作，如果是Windows平台，那么会导致抛出异常，不过在其他的平台，基于&quot;delete on last close&quot;的语义，还是能正确的执行，不过会有短暂的额外的磁盘开销(该文件还未被删除)，虽然不会有影响但是还要需要知晓这个问题，上述描述的问题可以查看这个BUG。\n  针对无法通过JDK提供的显示方法来取消映射的问题，Lucene提供了一个替代方法（workaround），即unmapHackImpl()方法， 使得可以用户在调用了FileChannel的close()方法后能通过一个native的invokeExact方法来取消映射，unmapHackImpl()的实现不展开，感兴趣的可以点击这里查看实现逻辑。\n  使用unmapHackImpl()方法有以下必须满足的要求：\n\nJVM必须是Oracle Java或者 OpenJDK 8或更高的版本\nlucene-core.jar必须有相关的权限(策略文件中保证)，即permission java.lang.reflect.ReflectPermission “suppressAccessChecks”，以及permission java.lang.RuntimePermission “accessClassInPackage.sun.misc”\n\n  最后如果出于某些原因，不需要使用unmapHackImpl()方法，那么可以通过setUseUnmap()来取消该功能。\n  使用MMapDirectory有以下注意点：\n\n该类支持并发读取同一个索引文件，但是它存在跟SimpleFSDirectory一样的多线程下执行线程被打断的问题，如果业务中存在这个情况，那么可以使用RAFDirectory来代替MMapDirectory\n可以使用setPreload()方法来设置是否提前将文件映射到物理内存中\n\n 如何选择FSDirectory\n  由于操作系统的多样性，Lucene无法用一个FSDirectory类来满足所有的平台要求，因此在FSDirectory类中提供了open()方法，让Lucene根据当前的运行平台来选择一个合适的FSDirectory对象，即为用户从SimpleFSDirectory、MMapDirectory、NIOFSDirectory中选出一个合适的FSDirectory对象，当然用户可以通过new的方式直接使用这些FSDirectory对象。\n  根据不同的条件使用对应的FSDirectory对象：\n\nJRE环境是64位并且支持unmapHackImpl()方法：使用MMapDirectory，例如Linux、MacOSX、Solaris、Windows 64-bit JREs。(判断是否支持unmapHackImpl()方法的逻辑见MMapDirectory)\n如果上面的条件都不满足并且当前平台是Windows：使用SimpleFSDirectory，例如Windows上其他的JREs\n如果上面的条件都不满足：使用NIOFSDirectory\n\n FSDirectory类中的方法\n  FSDirectory提供了其子类共有的创建、删除、重命名、同步(持久化索引文件至磁盘)、校验和（checksum computing）等方法。\n 创建\n  创建一个用来存放某个索引文件信息的对象，核心部分即使用BufferedOutputStream对象来存放数据。\n 删除\n  在一些情况下需要删除索引文件，至少包括以下情况：\n\n索引合并：即段合并，被合并的段中的索引文件会被删除\n删除旧的commit()：如果是IndexWriter使用了KeepOnlyLastCommitDeletionPolicy策略，那么每当有新的commit()操作，就会生成一个新的Segment_N文件，并且随后删除上一个，即Segment_N-1文件\n\n  另外有一个pendingDeletes的Set对象，当索引文件无法被删除时，pendingDeletes会记录该文件，并且在执行创建、删除、重命名、同步时会尝试再次删除这些文件，本该被删除的索引文件如果还留在索引目录中，可能会导致一些问题，比如被错误的合并、被错误的重命名(下文会介绍)。\n  当删除该索引文件并且失败后，此索引文件会被添加到pendingDeletes中，导致无法被删除的原因，至少包括以下情况：\n\n在Windows平台，当删除一个索引文件，并且该索引文件的句柄仍然被打开着，那么就会抛出异常，同时在Catch语句中将该文件名添加到pendingDeletes\n\n 重命名\n  使用Files.move()方法实现重命名。\n 同步\n  将内存中的索引文件同步(持久化)到磁盘，使用FileChannel.force()方法，如果同步某个文件抛出I/O异常，那么往上传递，如果同步的是目录，捕获异常，这样区分的目的由于篇幅原因不展开叙述，感兴趣请看这篇博客，这篇博客是Lucene源码中的推荐，博客内容介绍了同步磁盘的知识点。如果链接失效，在附件中可以看到对应的PDF。\n 校验和\n  校验和的内容在后面介绍CodeUtil类时会详细介绍。\n ByteBuffersDirectory\n  ByteBuffersDirectory使用堆，即通过一个ConcurrentHashMap来存储所有存储索引文件，其实key是索引文件名。\nprivate final ConcurrentHashMap&lt;String, FileEntry&gt; files = new ConcurrentHashMap&lt;&gt;();\n  ByteBuffersDirectory适合用于存储体积较小，不需要持久化的临时索引文件，在这种情况下比MMapDirectory更有优势，因为它没有磁盘同步的开销。\n RAMDirectory\n  RAMDirectory使用自定义的byte[]数组来存储索引文件信息，并且该数组最多存放1024个字节，所以如果索引大小为indexSize个字节，那么内存中就会有( indexSize / 1024 )个byte[]数组，当indexSize超过hundred megabytes后时会造成资源浪费，比如回收周期(GC cycles)问题。\n  RAMDirectory已经被置为@Deprecated，所以不详细展开。\n 结语\n  本文介绍Lucene7.5.0的core模块中的BaseDirectory类及其子类。\n点击下载Markdown文档\n","categories":["Lucene","Store"],"tags":["directory","mmap"]},{"title":"Directory（下）","url":"/Lucene/Store/2019/0615/Directory%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"  在Directory（上）中，介绍了BaseDirectory类，它作为Directory的子类，该类及其子类实现了维护索引文件的所有操作，即创建、打开、删除、读取、重命名、同步(持久化索引文件至磁盘)、校验和（checksum computing）等等，而Directory的其他子类，不具备上述的维护索引文件的操作，而是封装了上述Directory类，提供更多高级功能。\n图1：\n\n  强调是，图1中只列出了Lucene7.5.0的core模块中的类图。\n FilterDirectory\n  FilterDirectory类作为一个抽象类，它的子类对封装的Directory类增加了不同的限制(limitation)来实现高级功能。\n图2：\n\n SleepingLockWrapper\n  在文章索引文件锁LockFactory中，索引目录同一时间只允许一个IndexWriter对象进行操作，此时另一个IndexWriter对象(不同的引用)操作该目录时会抛出LockObtainFailedException异常：\n图3：\n\n  在使用了SleepingLockWrapper后，会捕获LockObtainFailedException异常，同时等待1秒（默认值为1秒）后重试，如果在重试次数期间仍无法获得索引文件锁，那么抛出LockObtainFailedException异常。\n TrackingTmpOutputDirectoryWrapper\n  该类用来记录新创建临时索引文件，即带有.tmp后缀的文件。IndexWriter在调用addDocument()的方法时，flush()或者commit()前，就会生成.fdx、.fdt以及.tvd、.tvx索引文件，而如果IndexWriter配置IndexSort，那么在上述期间内就只会生成临时的索引文件，TrackingTmpOutputDirectoryWrapper会记录这些临时索引文件，在后面介绍IndexWriter时会展开介绍：\n图4：\n\n TrackingDirectoryWrapper\n  该类用来记录新生成的索引文件名，不会记录从已有的索引目录中读取的索引文件名，比如在初始化Directory对象阶段会先读取索引目录中的索引文件。\n LockValidatingDirectoryWrapper\n  该类使得在执行创建、删除、重命名、同步(持久化索引文件至磁盘)的操作前都会先检查索引文件锁的状态是否有效的，比如说如果用户手动的把write.lock文件删除，那么会导致索引文件锁失效。\n  IndexWriter中使用了该类来维护索引文件。\n NRTCachingDirectory\n  该类维护了一个RAMDirectory，并封装了另一个Directory类，使用该类需要定义两个重要参数：\n\nmaxMergeSizeBytes：允许的段合并生成的索引文件大小最大值\nmaxCachedBytes：RAMDirectory允许存储的索引文件大小总和最大值\n\n  NRTCachingDirectory的处理逻辑就是根据下面的条件来选择使用RAMDirectory或者使用封装的Directory来存储索引条件\nboolean doCache = (bytes &lt;= maxMergeSizeBytes) &amp;&amp; (bytes + cache.ramBytesUsed()) &lt;= maxCachedBytes\n  其中bytes为索引文件的大小，cache.ramBytesUsed()为RAMDirectory已经存储的所有索引文件大小总和，当doCache为真，则继续使用RAMDirectory存储该索引文件，否则使用封装的Directory。\n FileSwitchDirectory\n  在前面的介绍中我们知道，索引文件都存放同一个目录中，使用一个Directory对象来维护，而FileSwitchDirectory则使用两个Directory对象，即primaryDir跟secondaryDir，用户可以将索引文件分别写到primaryDir或者secondaryDir，使用primaryExtensions的Set对象来指定哪些后缀的索引文件使用primaryDir维护，否则使用secondaryDir维护，另外primaryDir或者secondaryDir可以使用同一个目录。\n图5：\n\n  图5中，primaryExtensions对象指定后缀为fdx、fdt、nvd、nvm的索引文件由primaryDir维护，即存放到 data01目录中，其他的索引文件存放到data02中，由secondaryDire维护，执行一次添加文档操作后，索引目录如下图：\n图6：\n\n Lucene50CompoundReader\n  该类仅用来读取复合文件(Compound File)，所以它仅支持打开、读取。比如当我们在初始化IndexWriter时，需要读取旧的索引文件，如果该索引文件使用了复合文件，那么就会调用Lucene50CompoundReader类中的方法来读取旧索引信息。\n点击下载Markdown文件\n","categories":["Lucene","Store"],"tags":["directory","mmap"]},{"title":"DisjunctionMaxQuery（Lucene 8.9.0）","url":"/Lucene/Search/2021/0804/DisjunctionMaxQuery%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  本系列的内容将会先介绍DisjunctionMaxQuery在Lucene中的实现原理，随后再介绍在Elasticsearch中的应用。我们先直接通过图1中DisjunctionMaxQuery的注释跟图2的构造函数来简单了解下这个Query的功能：\n图1：\n\n图2：\n\n  灰色框标注的注释中说道：DisjunctionMaxQuery中封装了多个子Query（subqueries，这些子Query之间的关系相当于BooleanQuery中SHOULD的关系），即图2中的参数disjuncts。每个子Query都会生成一个文档集合，并且如果某篇文档被多个子Query命中，虽然这篇文档的打分值在不同的子Query中是不同的，但是这篇文档的最终打分值（在收集器Collector中的打分值）会选择分数最高的。不过如果图2中构造函数的参数tieBreakerMultiplier是一个不为0的合法值（合法取值范围为[0, 1)），那么文档的打分值还会所有考虑命中这个文档的所有子Query对应的打分值，其计算公式如下：\nfinal float score = (float) (max + otherSum * tieBreakerMultiplier);\n  上述公式中，max为子Query对应最高的那个打分值，而otherSum则为剩余的子Query对应打分值的和值，由于tieBreakerMultiplier的取值范围为[0, 1]，所以当tieBreakerMultiplier0时，文档的最终打分值为max；而当tieBreakerMultiplier1时，则是所有子Query对应的打分值的和值，这个时候DisjunctionMaxQuery就相当于minimumNumberShouldMatch1的BooleanQuery。在源码中，如果tieBreakerMultiplier1，那么将会使用BooleanQuery进行查询。如下所示：\n图3：\n\n  图3中，当tieBreakerMultiplier==1，会从disjuncts中读取所有的子Query，用于生成一个新的BooleanQuery，并且子Query之间的关系为should。\n  使用DisjunctionMaxQuery很有帮助的（useful）一个场景是：多个域中都包含某个term时，我们可以对不同的域设置加分因子（boost factor），如果某篇文档中包含这些域，我们获得的文档打分值可以只是加分因子最高的那个域对应的打分值，而不像在BooleanQuery中，文档打分值是所有域对应的打分值的和值。在随后介绍在Elasticsearch的应用中再详细展开。\n 例子\n  我们用一个例子来理解下图1中灰色框标注的注释：\n图4：\n\n BooleanQuery（minimumNumberShouldMatch == 1）\n图5：\n\n图6：\n\n  图5中，文档0跟文档1中由于只包含了&quot;title&quot;跟&quot;body&quot;域的一个，并且查询条件BooleanQuery的minimumNumberShouldMatch的值为1，那么文档0跟文档1分别满足titleTermQuery跟bodyTermQuery，文档分数分别为0.113950975跟0.082873434。由于文档2同时满足了titleTermQuery跟bodyTermQuery这两个子Query ，所以文档2的分数为0.113950975跟0.082873434的和值，即0.1968244。\n  正如上文中说道：BooleanQuery中，文档打分值是所有域对应的打分值的和值。\n DisjunctionMaxQuery（tieBreakerMultiplier == 0）\n图7：\n\n图8：\n\n  对于文档0跟文档1，跟使用BooleanQuery获得打分值是一致的，他们不会tieBreakerMultiplier 的影响，因为这两篇文档分别只满足一个Query的条件。而对于文档2，它同时满足titleTermQuery跟bodyTermQuery这两个子Query后对应的文档分数分别为0.113950975跟0.082873434，由于tieBreakerMultiplier == 0，意味着最终文档2的打分值只会选择打分最高的子Query，对应的分数，即0.113950975。结合上文中给出的计算公式，其计算过程如下：\nfinal float score = (float) (0.113950975 + 0.082873434 * 0);\n  正如上文中说道：如果某篇文档被多个子Query命中，虽然这篇文档的打分值在不同的子Query中是不同的，但是这篇文档的最终打分值（在收集器Collector中的打分值）会选择分数最高的。\n DisjunctionMaxQuery（tieBreakerMultiplier == 1）\n图9：\n\n图10：\n\n  对于文档0跟文档1，他们不受tieBreakerMultiplier 的影响，因为这两篇文档分别只满足一个Query的条件。而对于文档2，它同时满足titleTermQuery跟bodyTermQuery这两个子Query后对应的文档分数分别为0.113950975跟0.082873434，由于tieBreakerMultiplier == 1，该值不为0，意味着文档2的打分值不但会选择子Query对应的打分值最高的那个，还会考虑其他子Query对应的打分值。结合上文中给出的计算公式，其计算过程如下：\nfinal float score = (float) (0.113950975 + 0.082873434 * 1);\n 结语\n  基于篇幅原因，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","DisjunctionMaxQuery"]},{"title":"DocIdSet（Lucene 8.9.0）","url":"/Lucene/gongjulei/2021/0623/DocIdSet/","content":"  本篇文章将介绍Lucene中用于存储文档号的数据结构DocIdSet，该抽象类的具体实现有以下几种，我们将一一介绍。\n图1：\n\n  图1中，DocsWithFieldSet跟RoaringDocIdSet已经介绍过了。建议先看下文章RoaringDocIdSet，因为该实现中会使用到图1中的其他DocIdSet对象，有助于理解。\n  无论在索引阶段还是搜索阶段，都会用到DocIdSet来存储文档号。例如在索引阶段，使用DocsWithFieldSet收集正排信息对应的文档号、使用RoaringDocIdSet存储查询缓存；在查询阶段，使用IntArrayDocIdSet或者BitDocIdSet来存储满足数值范围查询（PointRangeQuery）条件的文档号等等。\n  DocIdSet不仅定义了存储文档号集合使用的数据结构，还定义了如何读取文档号集合的功能，即迭代器。\n 迭代器\n  这里说的迭代器即源码中的DocIdSetIterator类，该类定义了四个抽象方法描述了迭代器所能提供的功能。\n 第一个方法：获取下一个文档号\n图2：\n\n  当我们使用迭代器遍历文档号集合时，调用该方法会返回一个合法的文档号，如果返回的文档号的值为Integer.MAX_VALUE，说明迭代器中合法的文档号已经读取结束。\n上文中的&quot;合法&quot;是什么意思\n  &quot;下一个&quot;的含义在不同的子类有不同的定义方式，如果某个子类根据自身规则能返回一个文档号，说明这个文档号是合法的。\n 第二个方法：获取某个文档号的下一个文档号\n图3：\n\n  文档号集合中的文档号是有前后关系的，该方法通过指定一个参数target，该参数指定了一个文档号，返回这个文档号的下一个文档号。同样的，文档号之间的前后关系也是因不同的子类实现而异。\n 第三个方法：获取当前的文档号\n图4：\n\n  在调用了第一个跟第二个方法后会获得一个文档号，那么通过调用第三个方法可以获得这个文档号。\n 第四个方法：获取迭代器的开销\n图5：\n\n  正如这个方法注释描述那样，一般情况下（即一般的子类实现）迭代器的开销描述的是迭代器中的文档号集合中的文档数量，但还是取决于不同的子类如何定义开销的计算方式。\n DocIdSet的子类实现\n  接着我们开始介绍图1中的各个子类，介绍每一个子类使用哪种数据结构存储文档号集合以及如何实现迭代器的功能。\n IntArrayDocIdSet\n  从这个子类的命名方式就可以大致了解存储文档号集合的数据结构的类型了，即Int类型数组：\n图6：\n\n  由于目前IntArrayDocIdSet只有在DocIdSetBuilder（根据文档号集合的疏密程度，构建不同DocIdSet对象的类，在写完这篇文章后会介绍它）中使用，所以IntArrayDocIdSet的设计不具有通用性，这也是为什么构造函数没有用public修饰（图6代码第33行）。我们以一个例子来介绍在源码中该类是如何被使用的。\n图7：\n\n  在DocIdSetBuilder中收集了一个文档号集合后，首先根据文档号从小到大排序，最终该集合有两种类型：有序单值、有序多值。\n\n有序单值：docs数组中不存在重复的文档号。\n有序多值：docs数组中存在重复的文档号。例如在TermRangeQuery中，会使用DocIdSetBuilder收集每个term对应的文档号，那么会出现这种情况。\n\n  为了实现docs数组的复用，通过length指定一个上界，描述docs数组中合法的文档号。\n IntArrayDocIdSet的迭代器实现\n  简单的介绍下IntArrayDocIdSet的迭代器实现：\n\n第一个方法：从docs数组中获取下一个数组元素\n第二个方法：由于docs是有序的，故通过二分法先找到target在数组中的下标，然后获取下一个下标对应的数组元素\n第三个方法：见上文中第三个方法的描述\n第四个方法：返回docs数组中合法文档号的数量，即length的值\n\n BitDocIdSet\n  该子类实现使用BitSet对象存储文档号，FixedBitSet是BitSet的一种实现方式。可以阅读文章FixedBitSet了解其存储文档号的方式。\n图8：\n\n BitDocIdSet的迭代器实现\n  简单的介绍下BitDocIdSet的迭代器实现：\n\n第一个方法：如果当前文档号为doc，那么该方法会通过调用第二个方法实现，其参数为 doc + 1\n第二个方法：见文章FixedBitSet中关于nextSetBit(int index)方法的介绍\n第三个方法：见上文中第三个方法的描述\n第四个方法：图8中，BitDocIdSet的构造函数参数中cost即迭代器的开销，即取决于使用BitDocIdSet的调用方。例如在查询缓存中，会使用BitDocIdSet存储文档号，该场景中的开销为缓存的文档号数量；在文章RoaringDocIdSet中说到，如果一个block中的文档号集合处于稀疏跟稠密之间，那么会使用BitDocIdSet存储文档号集合，该场景中的开销为block中的文档数量\n\n ShortArrayDocIdSet\n  同IntArrayDocIdSet一样，从这个子类的命名方式就可以大致了解存储文档号集合的数据结构的类型了，即short类型数组：\n图9：\n\n ShortArrayDocIdSet的迭代器实现\n  简单的介绍下ShortArrayDocIdSet的迭代器实现：\n\n第一个方法：从docIDs数组中获取下一个数组元素\n第二个方法：由于docIDs数组是有序的，故通过二分法先找到target在数组中的下标，然后获取下一个下标对应的数组元素\n第三个方法：见上文中第三个方法的描述\n第四个方法：返回docIDs数组的长度，即docIDs数组中的元素数量\n\n  该子类目前只在RoaringDocIdSet中使用，当block中的文档号集合稀疏时，会使用ShortArrayDocIdSet存储。\n  由于在RoaringDocIdSet中，文档号的类型是int类型，即32位，其中利用文档号的高16位判断属于哪一个block，故只有低16位存储在block中，所以可以使用16位的short类型数组存储。\n NotDocIdSet\n  从图9的构造函数可以看出，该子类封装了一个DocIdSet，故该子类存储文档号集合的方式取决封装的DocIdSet。\n  NotDocIdSet中存储的文档号集合实际是一个&quot;反文档号集合&quot;。如果图10中的参数maxDoc的值为10000，并且合法的文档号集合为[0~9997]，那么图10中的参数DocIdSet中实际存放的只有9998、9999两个文档号。\n  该子类目前只在RoaringDocIdSet中使用，当block中的文档号集合稠密时，会使用ShortArrayDocIdSet存储&quot;反文档号集合&quot;，然后用NotDocIdSet封装。由于每个block的空间为216此方法，故maxDoc的值为216。\n图10：\n\n NotDocIdSet的迭代器实现\n  简单的介绍下NotDocIdSet的迭代器实现：\n\n第一个方法：如果当前文档号为doc，那么该方法会通过调用第二个方法实现，其参数为 doc + 1\n第二个方法：根据参数target，从图10的参数DocIdSet中找到大于等于target的文档号，我们称之为nextSkippedDoc，如果nextSkippedDoc跟target相等，说明target属于&quot;反文档号集合&quot;。即不是合法的文档号，此时重复执行第二个方法，并且此时的参数设为target + 1，直到与方法返回的nextSkippedDoc不相等即找到合法的文档号\n第三个方法：见上文中第三个方法的描述\n第四个方法：开销即图10中构造函数的maxDoc\n\n RoaringDocIdSet\n  在文章RoaringDocIdSet已经详细的介绍了RoaringDocIdSet。在本文内容的基础上，我们可以进一步更详细的介绍下RoaringDocIdSet存储文档号集合的方式。\n图11：\n\n  在RoaringDocIdSet中我们说到，存储方式按照block划分，int类型的文档号的前16bit用于判断属于哪个block，即前16bit用作图11中DocIdSet[ ]数组的下标值，然后根据block中文档号集合的稀疏/稠密类型会进一步使用不同的DocIdSet子类存储文档号的低16bit。\n RoaringDocIdSet的迭代器实现\n  取决于每个block中的DocIdSet的子类类型。另外第四个方法，即迭代器的开销为文档号数量。\n 结语\n  下一篇文章中，我们将会介绍上文中提到的DocIdSetBuilder，即DocIdSet的构造器，它会根据一定的条件来选择不同的DocIdSet子类来存储文档号的集合。\n点击下载附件\n","categories":["Lucene","gongjulei"],"tags":["disi","docId","docIdSet"]},{"title":"DocValues","url":"/Lucene/DocValues/2019/0218/DocValues/","content":" DocValues\n在搜索引擎中，我们通常都是对域名(field)构建倒排索引(inverted index)，实现了域值(Values)到文档(document)的映射，而DocValues则是构建了一个正向索引，实现文档到域值的映射。下面是官方给出的DocValues的介绍https://wiki.apache.org/solr/DocValues=\n What docvalues are:\n\nNRT-compatible: These are per-segment datastructures built at index-time and designed to be efficient for the use case where data is changing rapidly.\nBasic query/filter support: You can do basic term, range, etc queries on docvalues fields without also indexing them, but these are constant-score only and typically slower. If you care about performance and scoring, index the field too.\nBetter compression than fieldcache: Docvalues fields compress better than fieldcache, and “insanity” is impossible.\nAble to store data outside of heap memory: You can specify a different docValuesFormat on the fieldType (docValuesFormat=“Disk”) to only load minimal data on the heap, keeping other data structures on disk.\n\n What docvalues are not:\n\nNot a replacement for stored fields: These are unrelated to stored fields in every way and instead datastructures for search (sort/facet/group/join/scoring).\nNot a huge improvement for a static index: If you have a completely static index, docvalues won’t seem very interesting to you. On the other hand if you are fighting the fieldcache, read on.\nNot for the risk-averse: The integration with Solr is very new and probably still has some exciting bugs!\n\n DocValues的类型\nDocValues目前主要有五种类型，随后的博客中会一一详细介绍\n\nSORTED_SET\nSORTED_NUMERIC \nNUMERIC\nSORTED\nBINARY\n\n","categories":["Lucene","DocValues"],"tags":["DocValues"]},{"title":"Elasticsearch-8.2","url":"/Elasticsearch/2022/0905/Elasticsearch-8-2/","content":"点击这里\n","categories":["Elasticsearch"],"tags":["Elasticsearch"]},{"title":"FST（一）","url":"/Lucene/yasuocunchu/2019/0220/FST%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  FST（Finite State Transducer）算法的概念在这篇博客中并不涉及，网上有太多的资料啦，写的都非常的不错。这里推荐这位网友的介绍：https://www.shenyanchao.cn/blog/2018/12/04/lucene-fst/ 。如果链接失效了，可以看附件中的副本。本文中，我们基于一个例子来介绍在Lucene中如何构建FST。感谢网友关新全的分享，基于他的分享使得我在看源码的时候事半功倍，在此基础上，增加一些更加贴近源码的内容。同样的，关新全同学分享的文章在附件中。\n 准备工作\n  为了便于理解，先介绍几个概念。\n current[ ]数组\n  构建FST后生成的信息都最终保存到字节数组current[ ]数组中，即生成FST的最终结果就是该数组。\n Node（节点）、Arc（弧、边）\n  在下文的介绍中使用节点跟弧（边），即有向无环图的概念来描述生成的过程，Node之间使用一个Arc连接，一个Node跟一个或多个Node连接。\n  Node还具有两种状态：UnCompiledNode和CompiledNode。在源码中，使用两个对象来描述这两种状态，如下所示：\n图0：\n\n  当开始处理某个输入值时，会将它的信息生成Node跟Arc，此时Node的状态为UnCompiledNode，当Node以及它的出度对应的Arc（出边）的信息写入到current[ ]之后，Node的状态转变为CompiledNode。 例如图1中当开始处理的输入值为mop时，会生成4个状态为UnCompiledNode的Node以及3个Arc，其中Node1的出边为arc1，以此类推。我们给出源码中UnCompiledNode类的定义来理解Node跟Arc的关系：\n图0-1：\n\n图0-2：\n\n  图0-2描述的是Arc在源码中的定义，由图0-1可知，一个状态为UnCompiledNode的Node使用Arc&lt;T&gt;数组来描述它的出度的值，可以见该值是大于等于0的，即一个状态为UnCompiledNode的Node中可以包含多个Arc。\n  图0-1跟图0-2中的其他变量的概念在下文中我们会根据需要一一介绍。\n图1：\n\n label、target\n  基于图1我们先介绍下图0-2中Arc类的两个成员域：label和target，假设有一个输入值 “mop”，会使用三个Arc&lt;T&gt;对象来分别描述“m”，“o”，“p”的三个字符的信息，并且这三个对象分别是三个状态为UnCompiledNode的Node的出边。描述“m”的Arc&lt;T&gt;对象arc1，它是Node1的出边，对象arc1中的label的值是 109，即“m”的ASCII值；对象arc1的target的值即Node2，这里暂时不讨论isFinal、nextFinalOutput的含义，后面会涉及。另外如果Node没有任何的出度，那么它是一个终止节点。\n output\n  output值是某个输入的附加值（payload），比如有输入值”mop”，它的附加值是100，但是对于分别封装了&quot;m&quot;，“o”，“p”的三个arc，附加值100应该存放哪个arc的output字段呢？下面通过一个例子来说明：\n例子：输入为 “mop”、“mt”，附加值分别是100、91。\n 处理“mop”， 100\n图2：\n\n  只处理输入&quot;mop&quot;时，附加值是放在“m”的arc中。\n 处理“mt”， 91\n图3：\n\n  上图中我们能发现，如果后面的输入跟前一个输入有相同的前缀，那么相同前缀对应的每一个arc的output都有可能被更新，在这个例子中，&quot;mop&quot;的附加值100被拆分为91跟9，因为“mt”的附加值是91，取两者的较小值来更新封装了“m”的arc中的output值。\n另外上图中封装了“p”的arc怎么没有了，多出了一个值2 ？这里我们暂时不用关心，下文会解释。\n current[ ]数组\n  上文中我们提到，构建FST后的信息都最终保存到字节数组current[ ]数组中，该信息实际上是转为CompiledNode状态后的Node的新的出边信息的集合，为了便于介绍，我们称新的出边信息为四元组信息，它包含index、output、label、flag四个信息，其中index跟output可能为空。以下对四元组信息的介绍不必马上理解，特别是flag的概念。在下文中结合构建过程再回过头来看比较合适，这点很重要：\n index\n  当前arc描述的字符不是输入值的最后一个字符时，会存储一个index值来指向下一个字符的四元组信息中的flag值在current[]数组中的下标值。\n output\n  该值同图0-2中Arc类中的的output字段的值，如果output为0，那么就不用存储，在查询阶段，会对一个输入值的所有arc的output值进行整合，合成一个完整的output。例如图2中，我们查询一个&quot;mop&quot;时，会将&quot;m&quot;、“o”、&quot;p&quot;携带的output值进行整合，最终输出100。\n label\n  label值即我们的输入值中的一个字符，例如图1中的“mop”，对应三个label值，其中的一个label值就是109，即“m”的ASCII值。\n flag\n  由于构建FST后，所有的信息都存在current[ ]数组中，这过程实际是一个编码过程，在构建阶段，需要生成flag，使得在读取阶段，能根据flag的值进行解码，在Lucene 8.4.0中， flag有以下的值，不过本篇文章并没有全部列出，在后续的几篇文章中再给出：\n\n\n\nFlag\nValue\nDescription\n\n\n\n\nBIT_FINAL_ARC\n1\narc对应的label是某个term的最后一个字符\n\n\nBIT_LAST_ARC\n2\narc是Node节点中的最后一个Arc，上文中我们说到一个UnCompiledNode状态的Node可以包含多个arc\n\n\nBIT_TARGET_NEXT\n4\n上一个由状态UnCompiledNode转为CompiledNode状态的Node是当前arc的target节点, 它实际是用来描述当前的arc中的label不是输入值的最后一个字符，例如&quot;mop&quot;中，“m”、&quot;o&quot;就不是输入值mop的最后一个字符\n\n\nBIT_STOP_NODE\n8\narc的target是一个终止节点，例如图2中，描述&quot;p&quot;的arc，它的target就是一个终止节点（终止节点的概念上文中已介绍）\n\n\nBIT_ARC_HAS_OUTPUT\n16\narc有output值(output不为0)，例如图2中，描述&quot;m&quot;的arc就具有output值，而描述&quot;o&quot;、&quot;p&quot;的arc就没有output值\n\n\n\n NodeHash&lt;T&gt;类\n  NodeHash&lt;T&gt;类用来对Node计算hash值，使得能实现FST的特性之一，即后缀存储，在下文中你将会知道，每个具有唯一性的Node，它包含所有的arc对应的四元组信息只需要存储一遍。\n  每当对Node执行UnCompiledNode向CompiledNode的转化时，会计算Node的hash值。\n UnCompiledNode&lt;T&gt;[ ] frontier\n  frontier为存放状态为UnCompiledNode的Node的数组，例如图1中，frontier中就存放了Node1，Node2，Node3、终止节点四个状态为UnCompiledNode的Node。\n lastFrozenNode\n  lastFrozenNode为long类型变量，当一个Node的状态由UnCompiledNode转为CompiledNode之后，它包含的所有的arc对应的四元组信息会被写入到current[ ]中，lastFrozenNode会被置为第一个arc的四元素信息中的flag值在current[ ]中的下标值。另外由于终止节点没有arc信息，即没有四元组信息写入到current[ ]中，那么终止节点在执行状态变化后，会返回一个固定值 -1。\n 算法基本步骤\n  构建FST的过程分为三个步骤：\n\n步骤一：处理上一个输入与当前输入不相同Node节点，将Node节点中的所有arc的四元组信息写入到current[]数组中\n\n该过程即上文中提到的，Node由UnCompiledNode状态转变为CompiledNode状态的过程\n\n\n步骤二：将当前输入写入到frontier[]数组中\n\n该过程即上文中提到的，当开始处理某个输入值时，会将它的信息生成Node跟Arc，并且这些Node的状态为UnCompiledNode\n\n\n步骤三：处理上一个输入与当前输入相同的前缀值，调整output值。\n\n该过程为图3中的字符&quot;m&quot;携带的output值由原先的100调整为91\n\n\n\n 例子\n  下面通过一个例子来介绍FST的构建过程，输入跟附加值如下：\nString[] inputValues = &#123;&quot;mop&quot;, &quot;moth&quot;, &quot;pop&quot;, &quot;star&quot;, &quot;stop&quot;, &quot;top&quot;&#125;;long[] outputValues = &#123;100, 91, 72, 83, 54, 55&#125;;\n  注意的是输入顺序必须是有序的，这样才能获得一个最小FST，例子中的输入已经默认有序的。\n 输入mop\n  这是第一个输入，所以没有写入current[]数组的操作。将&quot;mop&quot;以及附加值100生成Node跟Arc即可，即步骤二。\n图4：\n\n 输入moth\n  输入moth后的执行步骤如下：\n\n步骤一：我们要处理上一个输入“mop”跟当前输入&quot;moth&quot;不相同的Node，可以看出不相同的Node是图4中Node3后面的终止节点，所以要将这个节点的所有arc对应的四元组信息写入到current[]数组中，并获得四元组信息中的flag值在在current[]中的下标值，即该终止节点的状态由UnCompiledNode转为CompiledNode，由于终止节点没有出度，即不包含arc，所以返回值是固定值 -1，最后更新lastFrozenNode为 -1。\n步骤二：将当前输入写入到frontier[]数组中，由于&quot;m&quot;、“o&quot;对应的Node已经存在，故只需要写入&quot;t”、&quot;h&quot;对应的Node跟Arc信息即可，如图5所示。\n步骤三：更新Node1中&quot;m&quot;的附加值，由100变成91。并且&quot;p&quot;的附加值由0变为9。\n\n图5：\n\n 输入pop\n  输入pop后，与上一个值&quot;moth&quot;没有相同的节点，所以我们要将图5中的Node2、Node3、Node4、终止节点的所有arc信息写入到current[]数组中，即执行步骤一，注意的是Node1中的所有arc只有在所有输入处理结束后才会处理。\n  处理的顺序为：节点间按照从后往前，节点内的arc按照写入到该节点的顺序，在当前情况下，根据图5，处理的arc顺序应该是： 终止节点 --&gt; Node4（h） --&gt; Node3（p --&gt; t） --&gt; Node2(o)，我们一一介绍：\n 处理终止节点\n  终止节点返回值为固定值-1，并更新lastFrozenNode为 -1，current[]数组不变。\n图6：\n\n 处理Node4 （h）\n  Node4中只有“h”对应的arc，根据flag的定义，该arc满足以下几个flag：\n\nBIT_LAST_ARC：它是Node4中的最后一个arc\nBIT_TARGET_NEXT：arc的target节点的值跟lastFrozenNode一致都为-1\nBIT_FINAL_ARC：&quot;h&quot;为&quot;moth&quot;的最后一个字符\nBIT_STOP_NODE：arc的target是一个终止节点（BIT_STOP_NODE）\n\n  所以flag的值为  BIT_LAST_ARC（2）+ BIT_TARGET_NEXT（4） + BIT_FINAL_ARC（1） +  BIT_STOP_NODE（8） = 15，然后将flag跟“h”的ASCII值，即四元组信息的flag跟label值，写入到current[]数组中，最后更新lastFrozenNode的值为2，该值为flag在current[]数组中的下标值。\n图7：\n\n 处理Node3（p -&gt; t）\n  根据上文的内容，我们知道Node3包含了两个arc，并且&quot;p&quot;对应的arc先于&quot;t&quot;对应的arc添加到frontier[ ]数组中，根据上文中提到的节点内的处理顺序，我们先处理&quot;p&quot;对应的arc。\n 处理 “p”\n  &quot;p&quot;对应的arc，满足以下几个flag：\n\nBIT_FINAL_ARC：&quot;p&quot;是&quot;mop&quot;的最后一个字符\nBIT_STOP_NODE：arc的target是一个终止节点\nBIT_ARC_HAS_OUTPUT：arc有output值，该值为9\n\n  所以flag的值为 BIT_FINAL_ARC（1）+ BIT_STOP_NODE（8）+ BIT_ARC_HAS_OUTPUT（16） = 25。由于Node3中还有未处理的arc，所以暂时不能更新lastFronzenNode的值。\n 处理“t”\n  &quot;t&quot;对应的arc，满足以下几个flag：\n\nBIT_LAST_ARC：它是Node3中的最后一个arc\nBIT_TARGET_NEXT：arc的target节点的值为Node4，而最新的lastFronzenNode的值是Node4对应生成的，故满足BIT_TARGET_NEXT\n\n  所以flag的值为 BIT_LAST_ARC（2）+ BIT_TARGET_NEXT（4） = 6。由于Node3中所有的arc处理结束，那么更新lastFronzenNode的值为7，7是Node3的第一个arc的四元组信息的flag值在current[]数组中的下标，如图8所示。\n  这里有个注意点就是，当按照 p -&gt; t的顺序处理结束后，会对刚才存储的数据执行逆置操作。所以“t”跟“p”对应的arc的数据在current[]数组中的位置如下图所示，这么做的目的是为了在读取阶段能正确的解码：\n图8：\n\n 处理Node2（o）\n  &quot;o&quot;对应的arc，满足以下几个flag：\n\nBIT_LAST_ARC：arc是Node2节点中的最后一个arc\nBIT_TARGET_NEXT：arc的target节点为Node3，而最新的lastFronzenNode的值是Node3对应生成的，故满足\n\n  所以flag的值为 BIT_LAST_ARC(2) + BIT_TARGET_NEXT(4) = 6。更新lastFronzenNode的值为9， 9是Node2的第一个arc的四元组信息的flag值在current[]数组中的下标，如图9所示。\n  最后，由于&quot;m&quot;对应的arc的target是Node2，Node2在处理结束后，即状态由UnCompiledNode转为CompiledNode之后，&quot;m&quot;对应的arc的target会被更新为9，9是Node2的第一个arc的四元组信息的flag值在current[]数组中的下标。\n  为什么要将&quot;m&quot;对应的arc的target会被更新为9\n  为了在读取阶段能正确的解码，不过具体的原因将会在下一篇文章中才会展开。\n图9：\n\n  步骤一执行结束后，接着执行步骤二，即把&quot;pop&quot;以及附加值72生成Node跟Arc即可，添加到frontier[ ]数组中，如下图：\n图10：\n\n 输入star\n  输入star后，与上一个输入值&quot;pop&quot;没有相同的节点，所以我们要将Node2、Node3、终止节点的所有arc信息写入到current[]数组中。同样的，Node1中的所有arc只有在所有输入处理结束后才会处理。\n  处理的顺序为：节点间按照从后往前，节点内的arc按照写入到该节点时候的顺序，在当前情况下，处理的arc顺序应该是： 终止节点 --&gt; Node3（p）–&gt;Node2（o）\n 处理终止节点\n  终止节点返回值为固定值-1，并更新lastFrozenNode为 -1，current[]数组不变\n图9：\n\n 处理Node3（p）\n  &quot;p&quot;对应的arc，满足以下几个flag：\n\nBIT_LAST_ARC：arc是Node3节点中的最后一个arc\nBIT_FINAL_ARC：&quot;p&quot;是&quot;pop&quot;的最后一个字符\nBIT_TARGET_NEXT：arc的target节点为终止节点，上一个lastFrozenNode的值为终止节点对应的值，故相同\nBIT_STOP_NODE：arc的target是一个终止节点\n\n  所以flag的值为 BIT_FINAL_ARC(1) + BIT_LAST_ARC(2) + BIT_TARGET_NEXT(4) + BIT_STOP_NODE(8) = 15。更新lastFronzenNode的值为11， 11是Node3的第一个arc的flag在current[]数组中的下标，如图11所示。\n图11：\n\n 处理Node2 （o）\n  这个过程就不赘述，跟上面的逻辑没区别。\n图12：\n\n  步骤一执行结束后，接着执行步骤二，即把&quot;star&quot;以及附加值83生成Node跟Arc即可，添加到frontier[ ]数组中，如下图：\n图13：\n\n 输入stop\n  这个过程就不赘述了，跟上面的逻辑没区别。\n图14：\n\n 输入top\n  输入top后，与上一个值&quot;stop&quot;没有相同的节点，所以我们要将图14中的Node2、Node3、Node4、终止节点的所有arc信息写入到current[]数组中，同样的，Node1中的所有arc只有在所有输入处理结束后才会处理。\n  处理的顺序为：节点间按照从后往前，节点内的arc按照写入到该节点时候的顺序，在当前情况下，处理的arc顺序应该是： 终止节点–&gt;Node4（p） --&gt; Node3（a --&gt; o） --&gt;Node2（t）。\n 处理终止节点\n  终止节点返回值为固定值-1，并更新lastFrozenNode为 -1，current[]数组不变。\n 处理Node4 （p）\n  处理“p”对应的arc时，根据NodeHash，之前存储“pop”的第二个“p”对应的arc所属的Node的Hash值相同，那么Node4中的所有arc对应的四元组信息就不用重复存储到current[ ]数组中，该四元组信息中的flag在current[ ]数组中的下标值为11（见图14），所以current[]没有改变。\n 处理Node3 （a -&gt; o）\n 处理“a”\n  &quot;a&quot;对应的arc，满足以下几个flag：\n\nBIT_ARC_HAS_OUTPUT：arc有output值(29)\n\n  所以flag的值是BIT_ARC_HAS_OUTPUT(16) = 16，另外&quot;o&quot;的target的值是大于0的，说明“a”不是输入值（star）的最后一个字符，target的值描述下一个字符的信息在current[]数组中的读取位置，该值为15，故写入到current[ ]数组的四元组信息为label（“a”）、flag（29）、index（15）。这里的下一个字符其实就是&quot;t&quot;，而&quot;t&quot;的对应的四元组信息中的flag在current[ ]数组中的下标值就是15。\n 处理“o”\n  “o”对应的arc，满足以下几个flag：\n\nBIT_LAST_ARC：arc为Node3节点中的最后一个arc\n\n  所以flag的值是2，另外&quot;o&quot;的target的值是大于0的，说明“o”不是输入值（stop或者top）的最后一个字符，这个值描述下一个字符（“p”）信息在current[ ]数组中的读取位置，该值为11，也就是Node4对应的四元组信息中flag在current[ ]中的下标值。\n 处理Node2 （t）\n  &quot;t&quot;对应的arc，满足以下几个flag：\n\n\nBIT_LAST_ARC：它满足 Node2节点中的最后一个arc\n\n\nBIT_TARGET_NEXT：arc的target节点的值跟lastFrozenNode是相同的\n\n\n  所以flag的值为 BIT_LAST_ARC(2) + BIT_TARGET_NEXT(4) = 6。最后更新lastFrozenNode的值为24。24是Node2的第一个arc的flag在current[]数组中的下标。\n图15：\n\n  步骤一执行结束后，接着执行步骤二，即把&quot;top&quot;以及附加值55生成Node跟Arc即可，添加到frontier[ ]数组中，如下图：\n图16：\n\n 处理top\n  “top”是最后的输入，所以在将“top”写入到frontier[]数组后，直接处理，处理过程不赘述，跟上面的逻辑没什么区别, 由于“top”跟“pop”有2个相同的后缀值，所以处理完“top”后，current[]数组没有变化。\n图17：\n\n 最后处理Node1节点\n  不赘述，跟前面的逻辑是差不多的。\n图18：\n\n 结语\n  本篇博客介绍了FST的构建过程，并且只介绍了current[ ]数组中的四元组信息的生成方式，相信看完之后，同学们还存在一的疑惑，但是在下一篇文章介绍了读取current[ ]解码内容后，即读取过程，相信定能豁然开朗。\n  另外该例子的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/util/FSTTest.java。\n点击下载附件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode","fst"]},{"title":"FST（二）（Lucene 8.4.0）","url":"/Lucene/yasuocunchu/2020/1009/FST%EF%BC%88%E4%BA%8C%EF%BC%89/","content":" FST（二）（Lucene 8.4.0）\n  在文章FST（一）（必须先阅读该篇文章）中我们通过一个例子，简单的描述了Lucene是如何使用一个字节数组current[ ]存储FST信息的，为了能更好的理解读取过程，我们需要另外给出例子（差别在于把&quot;mop&quot;改成了&quot;mo&quot;），输入数据以及对应FST的信息如下。\nString[] inputValues = &#123;&quot;mo&quot;, &quot;moth&quot;, &quot;pop&quot;, &quot;star&quot;, &quot;stop&quot;, &quot;top&quot;&#125;;long[] outputValues = &#123;100, 91, 72, 83, 54, 55&#125;;\n图1：\n\n  如果用节点跟边的关系来描述图1中的FST信息见下图：\n图2：\n\n  由图1可以看出FST的两个特性，相同前缀存储和相同后缀存储:\n\n相同前缀存储：\n\nmo、moth的相同前缀&quot;mo&quot;\nstop、star的相同前缀&quot;st&quot;\n\n\n相同后缀存储：\n\npop、top的相同后缀&quot;op&quot;\npop、top、stop的相同后缀&quot;p&quot;\n\n\n\n 读取FST\n  在文章FST（一）中我们说到，对于某个term的每一个字符对应的FST信息将以一个四元组信息（至少包含label跟flag）被写入到current[ ]数组中，即index、output、label、flag，其中flag跟index用于找出这个term的每一个字符在current[ ]数组中的起始读取位置以及读取区间，label是这个字符的ASCII，output则是这个term的完整或部分附加值，当获取了所有的字符后就能获得完整的附加值。例如&quot;mo&quot;这个term，它包含2个字符，这几个字符在图1的数组current[ ]数组中的数据区间如下表所示：\n表一：\n\n\n\n字符\n下标值区间\n\n\n\n\nm\n[35, 38]\n\n\no\n[5, 7]\n\n\n\n  接着我们根据顺序读取跟随机读取两种方式来展开介绍，在此之前我们先介绍下flag：\n表二：\n\n\n\nFlag\nValue\nDescription\n\n\n\n\nBIT_FINAL_ARC\n1\narc对应的label是某个term的最后一个字符\n\n\nBIT_LAST_ARC\n2\narc是Node节点中的最后一个Arc，上文中我们说到一个UnCompiledNode状态的Node可以包含多个arc\n\n\nBIT_TARGET_NEXT\n4\n上一个由状态UnCompiledNode转为CompiledNode状态的Node是当前arc的target节点, 它实际是用来描述当前的arc中的label不是输入值的最后一个字符，例如&quot;mop&quot;中，“m”、&quot;o&quot;就不是输入值mop的最后一个字符\n\n\nBIT_STOP_NODE\n8\narc的target是一个终止节点\n\n\nBIT_ARC_HAS_OUTPUT\n16\narc有output值(output不为0)\n\n\nBIT_ARC_HAS_FINAL_OUTPUT\n32\n某个term的最后一个字符带有附加值output\n\n\n\n  表二中的Flag相比较文章FST（一），多了BIT_ARC_HAS_FINAL_OUTPUT，在那一篇文章中未直接给出的原因在于，那篇文章中的例子的特殊性使得不会使用到该flag，这也是为什么本篇文章我们需要换一个例子。\n  无论哪种读取方式，总是从current[ ]数组中最后一个有效数据，即下标38，的位置开始读取，又因为四元组信息中至少包含flag跟label信息，故直接从最后一个有效数据开始往前读取两个数组元素，获得label的值为&quot;m&quot;、flag的值为16，接着根据flag的值的组合，执行不同的读取逻辑，如下图所示：\n图3：\n\n 顺序读取\n  顺序读取描述的是从FST中最小的term开始，根据字典序依次读取所有的term，在下文中的介绍将会了解到，整个过程相当于一次深度遍历。\n 读取&quot;mo&quot;\n  根据下标值38对应的数组元素，即flag，该值为16，该值只包含了BIT_ARC_HAS_OUTPUT（16）那么会执行以下的流程：\n图4：\n\n  图4流程中，结合表二中flag的介绍可以知道，由于不包含BIT_STOP_NODE，说明当前字符不是某个term的最后一个字符；由于不包含BIT_TARGET_NEXT，说明在current[ ]数组中，离当前字符&quot;m&quot;的最近的一个四元组信息（下标值区间为[ 31, 34]）对应的字符不是当前字符&quot;m&quot;的下一个字符，那么此时需要在数组中读取四元组信息中的index，使得能找到下一个字符的起始读取位置，即下标值为7对应的数组元素，它是下一个字符的四元组信息中的flag。\n  根据上面的介绍，我们目前获得了字符&quot;m&quot;、附加值91，以及下一个字符对应的flag，那么根据这个flag继续读取。\n  由于四元组信息至少包含flag和label、所以我们可以知道目前我们正在处理字符&quot;o&quot;，根据flag的值，即39， 包含了BIT_ARC_HAS_FINAL_OUTPUT（32）、BIT_TARGET_NEXT（4）、BIT_LAST_ARC（2）、BIT_FINAL_ARC（1），那么它对应的流程图如下所示：\n图5：\n\n  由于包含BIT_TARGET_NEXT，说明在current[ ]数组中，离当前字符&quot;o&quot;的最近的一个四元组信息（下标区间为[3, 4]）对应的字符是当前字符&quot;o&quot;的在某一个term（即&quot;moth&quot;）中的下一个字符，但是由于包含BIT_FINAL_ARC，说明当前字符&quot;o&quot;同时是某个term（即&quot;mo&quot;）的最后一个字符，那么我们可以知道存在&quot;mo&quot;以及以&quot;mo&quot;为前缀的term，另外由于包含了BIT_LAST_ARC，意味着不存在其他不以&quot;mo&quot;为前缀的term。\n 读取&quot;moth&quot;\n  由于在读取&quot;mo&quot;的字符&quot;o&quot;时，它对应的flag中包含BIT_TARGET_NEXT，意味着最靠近字符&quot;o&quot;对应的四元组信息的字符就是字符&quot;o&quot;的在某一个term的下一个字符，由图1可见，即下标区间[3, 4]对应的字符&quot;t&quot;，它对应的四元组信息中的flag值为6，包含了BIT_TARGET_NEXT（4）、BIT_LAST_ARC（2），对应的流程图同图5。\n  由于包含了BIT_TARGET_NEXT，并且不包含BIT_FINAL_ARC，那么说明&quot;t&quot;不是某个term的最后一个字符，并且下一个字符为离当前字符&quot;t&quot;的最近的一个四元组信息（下标区间为[1, 2]）对应的字符，即字符&quot;h&quot;。字符&quot;h&quot;对应的四元组信息中的flag值为15，包含了BIT_STOP_NODE（8）、BIT_TARGET_NEXT（4）、BIT_LAST_ARC（2）、BIT_FINAL_ARC（1），对应的流程图如下：\n图6：\n\n  由于包含了BIT_STOP_NODE，说明没有下一个字符，此时就获得了完整的term，即&quot;moth&quot;’。\n  字符&quot;h&quot;是最后一个字符了，为什么还有BIT_TARGET_NEXT\n  如果你看过文章FST（一）就会知道，它的target是-1，表示没有合法的下一个字符了。\n  通过&quot;mo&quot;、“moth&quot;的介绍已经可以了解读取FST的逻辑，对于BIT_LAST_ARC还需要一点补充，当不包含BIT_LAST_ARC时，说明结点中包含多个Arc，那么在源码中会保留该节点前的字符信息，使得不用重复读取，即实现了深度遍历的逻辑。以图2为例，当读取&quot;star&quot;的字符&quot;a&quot;时，就会知道节点9还有其他的arc，那么此时会保留节点9之前的信息，即字符&quot;s”、“t&quot;的信息，使得读取完&quot;star&quot;后，再读取&quot;stop&quot;时不用再次读取&quot;s”、&quot;t&quot;的信息。\n 随机读取\n  随机读取描述的是给定一个term，从FST信息中读取该term的附加值，其基本过程为在每一个节点中进行线性扫描（Linear scan），直到term中每一个字符都能被找到，例如我们给出的输入为&quot;stbae&quot;，结合图2，先在节点1中尝试找到字符&quot;s&quot;，随后在节点8中尝试找到字符&quot;t&quot;，最后在节点9中尝试找到字符&quot;b&quot;，由于节点9中不存在字符&quot;b&quot;对应的arc，那么停止查找并返回，即FST中不存在该term。\n 结语\n  本文简单的介绍了FST信息的基本读取过程，在后面的文章中我们将继续介绍Lucene中生成FST的一些其他特性的使用以及优化读取的内容。\n点击下载附件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode","fst"]},{"title":"FixedBitSet","url":"/Lucene/gongjulei/2019/0404/FixedBitSet/","content":"FixBitSet类在Lucene中属于一个工具类（Util），它的其中一个用途用来存储文档号，用一个bit位来描述（存储）一个文档号。该类特别适合存储连续并且没有重复的int类型的数值。最好情况可以用8个字节来描述64个int类型的值。下面通过介绍几个FixBitSet类的方法来理解这个类的存储原理。本篇文章纯属充数。。。直接看源码的话不会花很多时间，写这篇文章的原因主要是出于总结，因为好几个月前我看过了这个类的源码，今天准备写关于NumericDocValues的文章时再次遇到这个类时，发现又忘了，囧。\n 构造函数\npublic FixedBitSet(int numBits) &#123;  &#125;\n构造一个FixedBitSet对象，参数numBits用来确定需要多少bit位来存储我们的int数值。如果我们另numBits的值为300，实际会分配一个64的整数倍的bit位。因为比300大的第一个64的倍数是 320 (64 * 5)，所以实际上我们可以存储 [0 ~319]范围的数值。最终根据320的值，我们获得一个long类型的bit[]数组，并且bit[]数组初始化为大小5。在这里我们发现bit[]数组的每一个元素是long类型，即64bit，所以5个元素一共有 64 * 5 共 320个bit位。\n void set(int index)方法\npublic void set(int index) &#123;        // 将index根据64进行划分，比如 0~63都属于一个wordNum, 64~127属于另一个wordNum        int wordNum = index &gt;&gt; 6;      // div 64        // 计算出当前文档号应该放到64个bit位(long类型)的哪一位        long bitmask = 1L &lt;&lt; index;        // bits[]是个long类型的数据        bits[wordNum] |= bitmask;    &#125;\n 例子：\n图1：\n\n 添加 3\n根据set()方法的逻辑：1. 计算出wordNum的值：3 &gt;&gt; 6,即wordNum = 0，说明3应该存放在bit[]数组下标为0的元素中。2. 计算出bitmask的值，即计算出在64个bit位中的偏移，bitmask = 0b1000。3. 与bit[0]的值执行或操作。\n图2：\n\n 添加 67\n根据set()方法的逻辑：1. 计算出wordNum的值：67 &gt;&gt; 6,即wordNum = 1，说明67应该存放在bit[]数组下标为1的元素中。2. 计算出bitmask的值，即计算出在64个bit位中的偏移，bitmask = 0b1000。3. 与bit[1]的值执行或操作。\n图3：\n\n 添加 120\n根据set()方法的逻辑：1. 计算出wordNum的值：120 &gt;&gt; 6,即wordNum = 1，说明120应该存放在bit[]数组下标为1的元素中。2. 计算出bitmask的值，即计算出在64个bit位中的偏移，bitmask = 0b00000001_00000000_00000000_00000000_00000000_00000000_00000000_00000000。3. 与bit[1]的值执行或操作。\n图4：\n\n 添加179、195、313\n不赘述，大家可以自己算下是不是跟下图中一致。\n图5：\n\n通过上面的例子可以看到，如果我们存储的是连续的值，那么压缩率是很高的。当然同时可以看出无法处理有相同值的问题。\n boolean get(int index)方法\nget()方法可以实现随机访问，来确定index的值是否在bit[]数组中。\npublic boolean get(int index) &#123;    int i = index &gt;&gt; 6;               // div 64    long bitmask = 1L &lt;&lt; index;    // 如果bit为1，说明index在bit[]数组中。    return (bits[i] &amp; bitmask) != 0;  &#125;\n 结语\nFixedBitSet类中还有一些其他的方法，比如说prevSetBit(int index)方法来找到第一个比index小的值和nextSetBit(int index)方法来找到第一个比index大的数，在Lucene中，常用FixedBitSet类来存储文档号，并且在通过prevSetBit(int index)或者nextSetBit(int index)来遍历文档号。\n点击下载Markdown文档\n","categories":["Lucene","gongjulei"],"tags":["docId","bit"]},{"title":"FieldComparator&&LeafFieldComparator","url":"/Lucene/Search/2019/0415/FieldComparator&&LeafFieldComparator/","content":"  当满足搜索要求的文档被TopFieldCollector收集后，我们可以通过FieldComparator类来对这些结果（文档document）进行排序，并同时可以实现TopN的筛选。\n 排序类型\n  在介绍如果通过FieldComparator实现排序前，我们先介绍下排序类型，根据我们的排序对象，提供了下面几种类型。\n SCORE\n  根据文档打分(相关性 relevance)进行排序。打分越高越靠前。\n DOC\n  根据文档号进行排序，文档号越小越靠前\n STRING\n  根据String类型数据，即字符串对应的ord值(int类型)进行排序，ord越小越靠前(ord值的概念在SortedDocValues有详细介绍)。\n STRING_VAL\n  根据String类型数据，即字符串进行排序，上文的STRING通过对应的ord进行排序，而在STRING_VAL则是按照字典序逐个比较字符串的每一个字符。\n  通常情况下排序速度没有STRING快。比如只使用了BinaryDocValues的情况下，就只能使用这种排序类型，因为BinaryDocValues没有为String类型的域值设置ord值。\n INT\n  根据int类型数值进行排序，数值越小越靠前。\n FLOAT\n  根据float类型数值进行排序，数值越小越靠前。\n LONG\n  根据long类型数值进行排序，数值越小越靠前。\n DOUBLE\n  根据double类型数值进行排序，数值越小越靠前。\n CUSTOM\n  自定义类型，自定义实现一个实现FieldComparator类的子类。\n REWRITEABLE\n  设置一个排序类型为可变的，使得每次搜索时候我们可以随时更新为新的排序规则。\n 流程图\n  我们一个例子来说明排序的过程，在这个例子中，我们使用STRING_VAL来排序，即在索引阶段必须在每篇文档中定义BinaryDocValuesField，在介绍例子前，先介绍下排序的流程图。\n图1：\n\n 初始化\n图2：\n \n  在初始化阶段，根据设置TopN(必须设置)来初始化一个value[ ]数组，数组大小为N。value[ ]数组用来存放文档号对应的域值，在下面的例子中，即BinaryDocValuesField的域值，N的值为3。\n 文档号\n图3：\n \n  由于比较的过程是在Collector类中的collect(int doc)中进行的，所以输入只能是满足搜索要求的文档号。\n 队列满？\n图4：\n \n  即value[ ]数组中的元素个数是否等于数组的最大容量。\n 添加\n图5：\n \n  根据文档号取出对应的BinaryDocValuesField的域值，将域值存放到value中。如何根据文档号取出域值在这里不赘述，如果你已经看过了BinaryDocValues这篇文章，那么就知道过程啦。\n 设置bottom\n图6：\n \n  在添加的过程中，当队列满了，那么我们需要设置一个bottom值，bottom的值为数组中最小的那个（当前例子中按照字典序），设置bottom的目的在于，当有新的添加时，只要跟bottom做比较，如果大于bottom，那么就替换bottom，否则就直接跳过。\n 大于bottom？\n图7：\n \n  判断新的添加是否大于bottom。\n 替换bottom\n图8：\n \n  新的添加大于bottom，那么替换bottom。\n 更新bottom\n图9：\n \n  替换了bottom后，需要调整bottom，因为新的添加只跟bottom作了比较，它有可能比其他的域值更大。\n 例子\n图10：\n\n图11：\n\n  文档会按照从小到大的传到collect(int doc)方法中，所以域值的出现顺序即 “c”、“b”、“d”、“e”、“a” 。\n 处理文档号0、1、2\n  本例子中TopN的N值为3，所以添加这三篇文档时，直接将对应的域值添加到value[]数组即可。添加结束后，队列已满，故需要设置bottom值。\n图12：\n \n 处理文档号3\n  文档号3对应的域值为&quot;e&quot;， 它比bottom值&quot;d&quot;小，所以直接跳过。\n 处理文档号4\n文档号4对应的域值为&quot;a&quot;，它比bottom值&quot;d&quot;大，所以替换bottom，然后再更新bottom。\n图13：\n \n 结语\n  本篇文章介绍了FileComparator的排序过程，出于仅原理的理解，只介绍了基本的流程，其细节的部分比如说，如果排序对象为STRING，并且在搜索阶段存在多个IndexReader时，当IndexSearcher切换IndexReader时，还要考虑在不同的IndexReader中同一个域值可能有不用的ord值的情况。完整的过程大家看源码吧，然后就是由于源码比较简单，所以我并没有添加源码注释~\n","categories":["Lucene","Search"],"tags":["FieldComparator","LeafFieldComparator"]},{"title":"ForceMerge（一）（Lucene 8.8.0）","url":"/Lucene/Index/2021/0527/ForceMerge%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  在执行了flush、commit等方法后，Lucene会基于段的合并策略对索引目录中的段集合进行合并操作。Lucene在IndexWriter类中也提供了额外的方法允许用户可以主动去执行段的合并操作。\n ForceMerge概述\n  本篇文章将基于IndexWriter类中的下面两个方法来介绍下强制合并的内容。\n图1：\n\n图2：\n\n  在图1中的方法实际是另参数doWait为true，随后调用了图2中的方法。该参数的作用在下文中会展开。\n我们先通过源码中该方法的部分注释来描述下ForceMerge的概念：\n图3：\n\n  图3中的注释说到，强制合并ForceMerge是通过合并策略进行段的合并，并将段的数量减少到maxNumSegments以下，maxNumSegments即图1中的参数，即用户期望的合并后的段的数量。\n  接着还提到了一方面ForceMerge操作是开销极大的，特别是maxNumSe gments的值被设置的很小的场景。另一方面建议最好是对那些不会再发生更改的段进行强制合并。在随后的介绍中我们将会明白这段注释的含义。\n图4：\n\n  图4中的注释说到，如果使用了索引文件采用了复合模式，那么在ForceMerge期间，索引目录中要求额外2倍的存储空间，如果没有采用复合模式，那么需要额外的3倍的存储空间。\n  接着还提到建议在ForceMerge后执行一次commit操作，能减少索引目录的存储占用。原因是在ForceMerge后，被合并的段仍然会被保留在索引目录中，故索引目录中同时存在新段跟旧段，意味着索引目录中有冗余的索引数据。但是在执行了commit后，会对索引目录进行检查（见文章文档提交之commit（二）中的执行检查点(checkPoint)工作介绍），如果旧段（被合并的段）不被任何reader占用，IndexWriter会删除这些旧段，达到free up disk space的目的。\n图5：\n\n  图5中的注释说到，一般情况下，强制合并后可以降低索引的大小，一方面是小段被合并了，另一方面是那些段中被标记为删除的文档会真正的从段中移除（不明白？请看索引文件之liv的介绍）。\n图6：\n\n  图5中的注释说到，强制合并开始后，其他线程新增的段不会参与这次的强制合并。同样的，下文中将会解释这段注释的含义。\n ForceMerge的流程图\n图7：\n\n 执行flush\n图8：\n\n  该流程点执行了一次flush，该流程点操作跟直接调用IndexWriter.flush()方法是一致的，其详细过程可以阅读系列文章文档提交之flush（一）。\n为什么在强制合并前要执行一次flush操作\n  其最重要的一个原因是，在多线程下，其他线程的文档增删改操作可能尚未flush成段，这些操作中可能包含删除信息（文档删除信息、DocValues的更新信息），并且这些删除信息可能会作用（apply）到索引目录中的段，有助于在随后的合并中能生成更小的段。当然这是一个try best的过程，因为在执行了当前流程点后可能其他线程又会产生新的删除信息，这也是解释了为什么图3的注释中建议只对不会再发生变化的段进行强制合并。\n 收集所有的段\n图9：\n\n  介绍该流程点前，我们需要先了解三个知识点：oneMerge、pendingMerges、runningMerges。\n oneMerge\n  oneMerge的概念在文章LogMergePolicy已经详细介绍，这里我们可以简单的理解为：在段的合并过程中，根据段的合并策略会从索引目录中找到n个待合并的段集合，其中每个待合并的段集合用一个oneMerge来描述。换句话说就是oneMerge的数量描述了IndexWriter需要执行合并的次数，每个oneMerge中包含了将要合并为一个新段(New Segment)的段集合。\n pendingMerges\n  pendingMerges是一个线程共享的容器（List对象）。根据段的合并策略获取了一个或多个oneMerge后，这些oneMerge会被暂时保存到pendingMerges。执行合并的所有线程总是同步的从这个容器中取出一个OneMerge来执行段的合并操作，这句话特别重要。\n runningMerges\n  对于某个正在执行段的合并的线程，该线程会依次获取pendingMerges中的每一个oneMerge，对oneMerge中的段集合执行合并操作。同时会将这个oneMerge添加到runningMerges中。runningMerges正是用来描述哪些段正在合并中，在合并结束后，runningMerges会移除这个oneMerge。runningMerges也是一个线程共享的容器（Set对象）。\npendingMerges、runningMerges的使用场景\n\n场景一\n\n由于段的合并操作是允许后台线程执行的（取决于段的合并调度），所以如果线程A正在执行段的合并，此时线程B调用了IndexWriter.close()方法，那么在这个方法中会通过判断pendingMerges跟runningMerges是否为空来判断线程B是否能正确的关闭。线程B会等待所有的合并操作结束后再关闭。详细可以见这个方法： https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java 中的 waitForMerges()方法\n\n\n场景二\n\n在多线程下，当执行段的合并操作的线程数量大于阈值maxThreadCount（默认6，允许配置）并且pendingMerges不为空，说明合并进度太慢了（merging has fallen too far behind），并且只有maxThreadCount个线程正在执行合并操作（见文章段的合并调度MergeScheduler中maxThreadCount的介绍），故当前线程将被stall（调用Object.wait(250)方法）。\n\n\n\n  OK，我们继续介绍图9的流程点。在这个流程点中通过一个segmentsToMerge容器来收集当前内存中所有的段，包含IndexWriter的segmentInfos(见文章构造IndexWriter对象（三）中SegmentInfos的概念)、pendingMerges、runningMerges包含的段。\n  至此会有同学会产生这样的疑问，把pendingMerges、runningMerges也添加到此次的强制合并，会不会出现旧段被多次用于合并的问题。这个问题在图7中的流程点是否存在可以合并的段说明原因。\n  另外在图9中，执行当前流程点使用了synchronized关键字，这里使用同步的目的是确定了此次强制合并的IndexWriter的segmentInfos。其他线程的flush、commit操作可能会修改segmentInfos，故这里实现了segmentInfos的读写同步。同时这里使用同步机制的解释了图6中的注释。\n 结语\n  基于篇幅，剩余内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","forceMerge"]},{"title":"ForceMerge（二）（Lucene 8.8.0）","url":"/Lucene/Index/2021/0603/ForceMerge%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  我们紧接文章ForceMerge（一），继续介绍剩余的内容，先给出强制合并的流程图：\n图1：\n\n 是否存在可以合并的段？\n图2：\n\n  当前流程点的内容对应为源码中IndexWriter类的updatePendingMerges(…)方法，如下所示：\n图3：\n\n  如果用一句话描述该方法的用途，那就是：根据段的合并策略从上一个流程点（收集所有的段）收集的段集合中筛选出真正参与合并的段集合（段集合由一个或者多个OneMerge（见文章ForceMerge（一）的介绍）组成）。\n  图3中，这个方法使用synchronized修饰，对应为图1中的第二次synchronized。\n  另外根据不同的合并触发类型，该方法中会有三种方式来获得段集合。我们先了解下合并的触发类型。\n 合并触发类型\n图4：\n\n SEGMENT_FLUSH\n  该类型对应的注释在当前版本已经是不准确的描述了。在早期版本中，以Lucene 6.6.0为例，在执行了doFlush( )期间，会调用IndexWriter的doAfterSegmentFlushed( )方法，在这个方法中如果需要进行合并，那么此时的合并触发类型为SEGMENT_FLUSH，而在文章文档提交之flush（一）中我们知道，doFlush( )描述的正是DWPT（见文章文档的增删改（二）的介绍） flush成一个段的过程。\n  在当前版本中（Lucene 8.8.0），仅仅是在执行了修改索引（见文章文档的增删改（一））的操作后，并且如果产生的删除信息超过了阈值（见文章文档的增删改（五）中flushDeletes、流程点执行flush策略，设置flushDeletes的介绍），那么在执行完这次修改索引的操作后会触发SEGMENT_FLUSH的合并。\n FULL_FLUSH\n  这种类型的注释是正确的，即当执行了IndexWriter类中的flush()、commit()、getReader()方法以及NRT（NRT中会调用getReader()方法）操作后，会触发合并。\n EXPLICIT\n  用户主动的合并操作属于这种类型。比如IndexWriter类中提供了maybeMerge( )的方法允许用户主动调用合并操作，如图5所示。另外强制合并的操作也属于当前的类型。\n图5：\n\n MERGE_FINISHED\n  由于允许多线程执行段的合并操作，对于某个线程来说，在执行完操作后，会再次尝试执行段的合并操作。例如在段的调度策略ConcurrentMergeScheduler会使用到这种类型：\n图6：\n\n  在文章段的合并调度MergeScheduler中我们知道，ConcurrentMergeScheduler会开启一个后台线程进行段的合并，即图6的描述。图6中蓝框标注的doMerge(…)方法即执行段的合并。该方法执行结束后，在红框标注的runOnMergeFinished(…)方法中会再次尝试进行段的合并。上述方法在ConcurrentMergeScheduler类中。\n CLOSING\n  在IndexWriter类中，提供了shutdown( )的方法表示当前IndexWriter即将关闭，不再提供索引的增删改等操作。由于段的合并的操作可以是后台线程执行（取决于段的合并调度MergeScheduler），那么Lucene会在IndexWriter关闭前执行这种类型的段的合并操作。如果是CLOSING类型，那么执行段的合并操作的线程将会按照最大的磁盘写入量执行（见文章段的合并调度MergeScheduler中关于最大磁盘写入量的概念）。\n COMMIT、GET_READER\n  这两种合并触发类型在Lucene 8.6.0之后开始依次添加的。在Lucene8.6.0版本之前，我们知道每次执行flush()、commit()、getReader()(比如NRT操作)之后会进行段的合并操作。然而在Lucene8.6.0之后，Lucene开始支持在执行commit()、getReader()期间就进行了段的合并，这些合并的触发类型就是COMMIT或GET_READER。\n  关于在执行commit()、getReader()期间进行段的合并的内容，感兴趣的同学可以看文章Changes（Lucene 8.7.0）中关于LUCENE-8962的介绍。\n 流程图\n  上文中我们说到，图3中的方法，它描述的功能是根据不同的合并触发类型，使用三种方式来获得段集合。其不同方式的选择逻辑如下所示：\n图7：\n\n 三种方式的选择\n  图7的流程图中，三种方式的选择对应于源码中的代码块如下所示：\n图8：\n\n  图8中第2320行代码的findForcedMerges(…) 方法即方式一，通过该方法将会获得段集合。并且这种方式正是段的强制合并获得段集合的入口方法。对于其他两种方式，方式二以及方式三，本系列文章暂不展开介绍。\n 注册待合并的段\n图9：\n\n  当通过三种方式中的一种获取了待合并的段集合（一个或多个OneMerge）后，这些OneMerge会依次进行注册操作。如果注册成功，OneMerge中包含的段将用一个称为mergingSegments的容器存放。该容器的作用可以理解成这些段添加了一个状态，该状态描述的是这些段正在执行段的合并操作。同时还会添加到pendingMerges（见文章ForceMerge（一））容器中。\n什么情况下会注册失败\n  在注册的过程中，会判断每一个段是否已经处于其他线程的合并操作中，即通过mergingSegments是否包含这个段来判断。如果OneMerge中至少一个段已经在mergingSegments中，那么注册将会失败，意味着将不会被添加到pendingMerges容器中，那么就不会进行合并操作。这里需要再次重复下在文章ForceMerge（一）中提到的内容，即pendingMerges是线程共享的容器，执行合并的所有线程总是同步的从这个容器中取出一个OneMerge来执行段的合并操作。\n  上文的内容同时描述了这么一个事实：多个线程在执行图7中的三个方式中的任意一个后获得了一个或多个OneMerge，这些OneMerge不一定能参与段的合并操作\n注册待合并的段还有什么其他作用\n  由于允许并发执行索引的增删改跟段的合并操作，那么存在这么一种场景，在索引提交阶段，如果某个段中的所有文档都满足删除的条件，这个段会被直接删除。如果此时这个段正在执行段的合并操作，那么就会出现空指针问题。所以在删除某个段前可以通过检查mergingSegments中是否包含此段来判断是否要删除这个段。\n\n在文章文档提交之flush（六）介绍流程点发布FlushedSegment时我们知道，某些段在索引提交阶段会被丢弃。我们可以看下源码中这个方法来直观的理解：\n\n图10：\n\n  如果一个段的文档都满足删除信息（has 100% deleted documents）时，该方法将被调用。红框标注的注释解释了mergingSegments的作用：如果当前段正在合并中，那么把这个段留在readerPool中即可。\n  注意的是，图10的方法属于IndexWriter类型，并且也用synchronized修饰了。\n 结语\n  在下一篇文章中，我们将继续介绍图7中的方式一的实现方式。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","forceMerge"]},{"title":"GeoQuery（一）（Lucene 8.8.0）","url":"/Lucene/Search/2021/0817/GeoQuery%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  本系列文章将介绍下Elasticsearch中提供的几个地理查询（Geo Query）在Lucene层的相关内容。Elasticsearch 7.13版本中提供了以下的Geo Queries：\n\ngeo_bounding_box query\ngeo_distance query\ngeo_polygon query\ngeo_shape query\n\n 预备知识\n  为了能深入理解GeoQuery，我们需要先介绍下两个预备知识：\n\nLucene中点数据的索引与查询\nGeoHash编码\n\n Lucene中点数据的索引与查询\n 点数据\n  Lucene中，点数据域用于存储数值类型的信息，并且点数据可以是多维的，例如图1中的第48行、49行分别写入了一个int类型的二维点数据跟三维点数据。\n图1：\n\n  其他类型的点数据如下所示：\n图2：\n\n  基于篇幅，后续的文章中只会介绍跟上文中的Geo Queries相关的点数据域。\n 索引（indexing）和搜索（search）点数据\n  在文章索引文件之dim&amp;&amp;dii中介绍了Lucene中点数据对应的索引数据结构，以及在文章Bkd-Tree中通过一个例子简单的介绍了如何对点数据集合进行划分，并用于生成一棵BKD树，以及在系列文章索引文件的生成（八）~（十四）中详细的介绍了生成索引文件之dim&amp;&amp;dii的过程，最后在系列文章索引文件的读取（一）~（四）中介绍了索引文件之dim&amp;&amp;dii的读取过程，即点数据的搜索过程。\n  如果阅读过上述的文章，那么相信能非常容易理解GeoQuery在Lucene中的实现。\n GeoHash编码\n  接着我们将先介绍区域编码的概念，随后介绍下在Elasticsearch中GeoHash编码的实现。\n 区域编码（domain encode）\n  区域编码描述的是使用唯一的编码值来描述平面上的一块区域。对于一个有边界的二维空间，可以在水平方向或者垂直方向对空间进行划分，那么空间将被划分为四块区域。这四块子区域可以用2个bit号来进行编码。如果规定在水平方向划分后，左边的区域用0表示，右边的区域的用1表示；在垂直方向划分后，下面的区域用0表示，上面的区域用1表示，那么这四个区域就可以分别用00、01、11、10进行编码，如下所示：\n图3：\n\n  按照上述的切分方式，如果我们继续分别对四块区域进行水平方向跟垂直方向的划分，如下所示：\n图4：\n\n  区域编码常用于计算空间里面点与点之间的距离关系，从这种区域编码方式能发现：拥有相同前缀的长度越长，空间上的位置就越接近，不过反过来就不一定成立了，因为空间中的两个点可能非常接近，但是这两个点所属的区域可能拥有很短的前缀，甚至没有相同前缀。\n  图5中，**黑点跟红点没有相同的编码前缀，跟绿点有相同的编码前缀，但是黑点**跟红点更为接近。\n图5：\n\n 查找附近的点\n  使用区域编码来实现查找附近的点的直观方法就是找出与查询点所属的编码值前缀最长的区域，这些区域中的点都是认为是在查询点附近的。由于存在图5中描述的问题，所以这种方式会遗漏一些附近点（红点）。\n  为了解决这个遗漏问题，我们可以把查询点所属的区域的周围8个区域中的点都找出来就可以了，当然这种粗暴的做法的缺点是可能会获得大量的结果集。为了防止出现大量结果集，那么可以对当前最小区域再进行划分。\n  那么问题就转变为如何通过查询点所在的区域编码获取它周围8个区域的编码值了。\n 计算规则\n  以图5中**黑点**所在区域为例，区域编码为0011。它的水平编码为01，垂直编码为01，该区域上下左右四个区域的编码值如下所示：\n图6：\n\n  同理可以计算出查询点所属的区域的左上、左下、右上、右下四个区域编码值。\n 精度\n  由上面的划分方式可以看出，划分次数越多，区域越小，意味着精度越高。精度高一方面描述了区域编码值能表示更少的点，另一方面也为查找附近点能提高更少的结果集。\n Elastisearch中的GeoHash编码\n  Geohash编码指的是将经纬度坐标转换为字符串的编码方式。它同样通过区域编码的处理方式对地理位置进行了划分。由于地理位置可以用经纬度来表示，其中纬度的取值范围为[-90, 90]，经度的取值范围为[-180, 180]，即对一个有边界的二维区域进行区域编码。\n 编码\n  我们先看下Elasticsearch中如何将经纬度值转变为GeoHash编码。\n 量化\n  首先将经纬度两个值分别量化（quantizing）为区间为[0, 2^32 - 1]的值，使得可以用int类型来描述经纬度，其量化公式如下所示：\nint latEnc = (int) Math.floor(latitude / (180.0D/(0x1L&lt;&lt;32)))int lonEnc = (int) Math.floor(longitude / (360.0D/(0x1L&lt;&lt;32)))\n  比如纬度的取值范围为[-90, 90]，那么维度值-90跟90将分别量化为 0、2^32 - 1。\n 交叉编码\n  将量化后的维度、经度值，即两个32位的int类型的值交叉编码为一个64个bit的long类型的值。在这个64个bit中，奇数位共32个bit为量化后的维度值对应的32个bit，偶数位共32个bit为量化后的经度值对应的32个bit。\n  例如有一个坐标值如下所示：\nlatitude：32longitude：50\n  量化后的值为：\nint latEnc = 0b10101101_10000010_11011000_00101101int lonEnc = 0b10100011_10001110_00111000_11100011\n  最后对latEnc跟lonEnc进行交叉编码，其处理过程如下图所示：\n图7：\n\n  图7中，我们先将int类型的latEnc中的32个bit塞到一个long类型的v1中。\n图8：\n\n  同图7的处理方式一样，将int类型的lonEnc中的32个bit塞到一个long类型的v2中。\n图9：\n\n  图9中先将v2左移一位，然后 v2 跟v1执行或操作获得一个long类型的值interleave。\n  图7~图8的处理过程对应Elasticsearch中的源码BitUtil类中的interleave方法如下所示：\n图10：\n\n base32编码\n  在上文中我们获得了interleave的值后，需要对其进行base32编码，在此之后就获得了GeoHash编码。基于篇幅，该内容将在下一篇文章中展开。\n 结语\n  Elasticsearch中GeoHash编码原理基于莫顿编码，感兴趣的同学可以深入理解下：http://graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN 。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","geo","dim","dii"]},{"title":"GeoQuery（二）（Lucene 8.8.0）","url":"/Lucene/Search/2021/0902/GeoQuery%EF%BC%88%E4%BA%8C%EF%BC%89/","content":" GeoQuery（二）（Lucene 8.8.0）\n  在上一篇文章GeoQuery（一）中，我们基于下面的例子介绍了在GeoHash编码在Elasticsearch中的部分实现，我们继续介绍其剩余内容。\nlatitude：32longitude：50\n base32编码\n  在前面的步骤中，先将经纬度的值量化为两个int类型的数值，随后在交叉编码后，将两个int类型的数值用一个long类型的表示。为了便于介绍，我们称这个long类型的值为interleave。最后，interleave在经过base32编码后，我们就能获得GeoHash编码值。\n  同样的我们根据Elasticsearch中的源码来介绍其过程。由于在es中地理位置的精度等级为12（Geohash类中的PRECISION变量定义，如图1所示），加上base32核心处理方式是将5个bit用一个字符（char）描述，故意味着interleave的高60个bit是有效的，同时低4个bit可以用来描述其精度值。\n图1：\n\n 低4个bit\n  上一篇文章中我们获得的interleave的值如下所示：\n图2：\n\n  当低4个bit用来描述精度12（0b1100）后，interleave的值如下所示：\n图3：\n\n 高60个bit\n  接着就可以对高60个bit进行base32编码了。总体流程为从这60个bit的末尾开始，每次取出5个bit，其对应的十进制值作为编码表的下标值，在编码表中找到对应的字符，最终生成一个包含12个字符的字符串。如下所示：\n图4：\n\n  从图4中可以看出，我们首先取出5个bit，以10110为例，它对应的十进制的值为22，随后将22作为编码表BASE_32的下标值，取出数组元素q，把该值写入到GeoHash编码中。为了图片的整洁性，故图4中只描述了将60个bit的末尾15个bit进行编码的过程。当所有的bit都处理结束后，其最终的GeoHash编码如下所示：\n图5：\n\n为什么介绍GeoHash编码\n  在Elasticsearch的文档中我们可以知道，以geo_bounding_box query为例，它支持提供GeoHash编码跟经纬度进行地理位置查询，如下所示：\n图6：\n\n 结语\n  下一篇文章将正式开始介绍Elasticsearch中提供的Geo Queries。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","geo","dim","dii"]},{"title":"HNSW图的构建（Lucene 9.8.0）","url":"/Lucene/Index/2024/0118/HNSW%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/","content":"  Lucene基于论文Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs [2018]实现了HNSW的逻辑，本篇文章结合ChatGPT-4跟Lucene源码介绍构建过程中的实现细节，另外构建HNSW使用的索引数据结构可以参考文章索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）。\n 概述\n图1：\n\n  我们先通过论文中的这张图，简要的介绍下构建完成的HNSW图的一些基本知识：\n\n层次结构：HNSW图具有多层结构，每一层都是一个独立的图。层数通常是根据数据集的大小和复杂性确定的。在最高层（即图中的layer=2），节点数量最少，但每个节点覆盖的范围最广；而在最低层（即图中的layer=0），节点数量最多，但每个节点覆盖的范围相对较小。\n节点连接：在每一层中，节点通过边与其邻居相连接。这些连接是基于距离或相似性度量的，意味着每个节点倾向于与离它最近的其他节点相连。\n邻居的选择：选择哪些节点作为邻居是基于一定的启发式规则的，这些规则旨在平衡搜索效率和准确性。通常，这涉及到保持邻居的**多样性（Diversity）**和限制每个节点的邻居数量。\n搜索路径：构建/查询HNSW图都有一个搜索过程，搜索路径会从最高层的全局入口节点开始，然后逐层下降，直到达到最低层。在每一层中，搜索会根据当前层的连接结构来寻找离目标节点最近的节点。\n\n 实现原理\n  我们通过一个新节点的添加/插入过程来介绍下Lucene中HNSW图的构建：\n图2：\n\n 新节点\n图3：\n\n  Lucene中每一个Document只能有一个相同域名的向量，并且按照添加顺序为每一个向量映射一个节点编号（nodeId），它是一个从0开始递增的值。新节点在源码中即节点编号，当我们需要计算两个节点之间的距离时，就可以通过映射关系找到节点对应的向量值进行计算。\n 计算目标层级\n图4：\n\n  目标层级是通过一个遵循指数分布计算出的随机数。\n  HNSW图具有多层结构，目标层级意味着新节点将被添加到哪些层中，并且在每一层与其他节点建立连接。\n  例如目标层级为3，那么这个节点将被依次添加到第3、2、1、0层中。\n 计算公式\n  源码中通过公式：−ln⁡(unif(0,1)) * ml 计算目标层级，其中unif(0,1)表示从0到1之间均匀分布的一个随机值，ml定义为 1/ln(M)，其中M是最大连接数，即最多可以跟M个节点进行连接。源码中M的默认值为16。\n  使用该公式基于的理论以及实验基础在论文有详细的介绍，本文不详细展开。\n 是否还有未处理的层级？\n图5：\n\n  计算出目标层级后，将从最高层开始，从上往下逐层处理，直到新节点添加到所有层并建立与其他节点的连接，就完成了新节点的插入。\n 获取入口节点\n图6：\n\n  入口节点（entry point）是指导新节点插入到某一层中的起始点（可能存在多个入口节点，见下文介绍）。\n  将新节点插入到某一层的目的是建立它在这层中跟其他节点的连接。因此一方面首先跟入口节点进行连接，另一方面基于贪心算法尝试与入口节点的邻居、入口节点的邻居的邻居等等建立连接。其过程将会在流程点找出当前层的候选邻居中详细展开。\n 入口节点的类型\n  入口节点的类型可以分为：全局入口节点和层级入口节点。\n\n层级入口节点：在插入过程中，每一层都可能有一个或多个入口节点（每一个新节点在每一层的入口节点可能是不同的）。这些节点是在逐层下降过程中确定的，用于在每个层级上引导搜索。也就说当前层的入口节点是上一层中与新节点连接的节点集合中TopK个节点（下文会介绍）。如果当前层已经是最高层，那么该层的入口节点为全局入口节点\n全局入口节点：又名初始入口节点，是单一的一个节点，用于最高层的入口节点。添加新节点时，并且当新节点的目标层级大于当前HNSW图中的层数时，该节点将作为新的全局入口节点\n\n 获取层级入口节点概述\n  获取层级入口节点可以概括为两种情况，如下所示：\n图7：\n\n\n目标层级 &gt; 当前层级（图中最高层）：在层级4、3、2中，添加新节点引入了新层，因此直接将新节点添加到这几层即可。添加过程中不需要考虑获取入口节点的问题；在层级1、0中，获取方式跟下面的另一种情况相同\n目标层级 &lt;= 当前层级：\n\n层级3：由于是最高层，因此全局入口节点将作为该层的入口节点，并找到新节点在这层中的候选邻居，并且将候选邻居集合中的Top1节点（称为ep3）作为层级2的入口节点\n层级2：ep3作为当前层的入口节点，找到新节点在这层中的候选邻居，并且将候选邻居集合中的Top1节点（称为ep2）作为层级1的入口节点\n层级1：ep2作为当前层的入口节点，找到新节点在这层中的邻居，并且将候选邻居集合中的TopK节点集合（称为ep1）作为层级0的入口节点\n层级0：ep1这个节点集合将作为当前层的入口节点\n\n\n\n  注意到的是，上文中层级1、2只将上一层中新节点邻居中的Top1（最近的邻居）作为入口节点，而在层级0中选择了上一层的TopK节点集合，其理由是搜索效率和准确性的需求平衡：\n\n\n选择最近的邻居：侧重于快速缩小搜索范围，并尽可能快地接近目标节点。\n\n\n使用多个邻居：提高搜索的全面性，特别是在复杂或高维的数据空间中。使用多个入口节点可以从不同的路径探索空间，增加找到最佳匹配的机会。\n\n\n 为什么逐层下降，而不是直接到达目标层级\n  图7中，当目标层级&lt;=当前层级时，是从当前层级逐层下降，而不是直接从目标层级开始开始，考虑的因素有：\n\n高层的快速导航：在高层，节点之间的连接覆盖了更大的距离。这意味着在高层进行搜索可以快速跳过不相关的区域，迅速接近新节点的目标区域。如果直接跳到目标层，可能会错过这种快速接近的机会。\n逐步精细化搜索：从高层开始并逐层下降允许搜索过程逐步变得更加精细。在每一层，搜索都会根据该层的连接结构调整方向，以更精确地接近新节点的位置。这种逐层精细化的过程有助于找到更准确的最近邻。\n避免局部最小值：如果直接在目标层进行搜索，可能会陷入局部最小值，即找到的最近邻并不是全局最近的邻居。逐层下降有助于避免这种情况，因为在每一层的搜索都是基于上一层的结果，从而提供了更全面的视角。\n平衡搜索成本：虽然逐层下降可能看起来比直接跳到目标层更耗时，但实际上它通常更高效。这是因为在高层进行的搜索步骤较少，而直接在密集的低层进行搜索可能需要更多的步骤来找到最近邻。\n\n 找出当前层的候选邻居\n图8：\n\n  找出当前层的候选邻居是一个贪心算法的搜索过程，它从入口节点卡开始，首先将其作为看起来最接近新节点的节点，如果一个节点的邻居看起来更接近新插入的节点，算法会转向那个邻居节点，并继续探索其邻居。最终找到TopN个距离较近（TopN中不一定是离新节点最近的节点），流程图如下：\n图9：\n\n 候选邻居集合\n  候选节点集合的数据结构是一个最小堆，按照节点距离打分值排序，距离越近，打分值越高。节点距离描述的是新节点跟候选邻居的距离。\n 贪心算法的搜索过程中的一些知识点\n\n距离打分值阈值：搜索过程中会一直更新一个名为minCompetitiveSimilarity的阈值，即最小堆堆顶元素，如果某个邻居与新节点的距离小于该阈值，则不再处理跟这个邻居相连接的节点。\n记录已经访问过的节点：由于节点之间的相互连接，贪心算法很容易重复访问相同的节点，通过记录已经访问过的节点，提高搜索性能\n下一层的入口节点：当前层的候选邻居集合中的TopN将作为下一层的入口节点，即上文中提到的Top1跟TopK（这里的K在源码中是通过名为beamWidth的变量定义，默认值为100）。\n\n 足够近的邻居和绝对最近的邻居\n  贪心算法的搜索过程意味着它可能不会找到绝对最近的邻居，但通常会找到足够接近的邻居：\n\n算法的目标是找到与新节点足够接近的邻居节点。这里的“足够接近”意味着虽然找到的邻居可能不是绝对意义上最近的邻居（即有些离新节点更近的节点没有成为邻居），但它们与新节点的距离足够近，可以有效地代表新节点在图中的位置。\n效率与准确性的平衡：在实际应用中，寻找绝对最近的邻居可能非常耗时，特别是在大规模或高维的数据集中。因此，算法通常会寻找一个平衡点，即在可接受的计算成本内找到足够近的邻居。\n贪心搜索策略：HNSW使用贪心算法来逐步逼近新节点的最近邻居。这意味着在每一步，算法都会选择当前看起来最接近新节点的邻居。这种方法通常能快速找到足够近的邻居，但不保证总是找到绝对最近的邻居。\n实用性考虑：在大多数情况下，找到“足够近”的邻居已经能够满足大部分应用场景的需求，如近似最近邻搜索。这种方法在保证搜索效率的同时，也能够提供相对高的搜索准确性。\n限制邻居数量：为了控制图的复杂性，通常会限制每个节点的邻居数量。这意味着即使存在更近的节点，也可能因为邻居数量限制而不被选为新节点的邻居。\n\n 基于多样性筛选候选邻居\n图10：\n\n  尽管在上一个流程点找出当前层的候选邻居中已经找到了TopN个&quot;足够近&quot;的邻居，但基于下面几个因素考虑，我们还要对这些邻居进行多样性的考察，不满足多样性的邻居则不会进行连接，并且通过计算邻居之间的距离实现多样性的检查：\n\n覆盖不同区域：如果一个节点的邻居彼此之间距离较远，这意味着它们覆盖了该节点周围的不同区域。这种分布有助于在搜索过程中快速定位到不同的区域，从而提高搜索的效率和准确性。\n避免局部最小值：在高维空间中，如果所有邻居都非常接近，可能会导致搜索过程陷入局部最小值，即找到的最近邻并不是全局最近的邻居。邻居间的距离差异有助于提供更多的搜索路径，从而避免这种情况。\n增强图的连通性：具有距离差异的邻居节点可以增强图的连通性，使得从一个节点到另一个节点的路径更加多样化。这对于在图中快速传播信息或找到最优路径非常重要。\n适应不同的数据分布：在实际应用中，数据往往不是均匀分布的。通过确保邻居间的距离差异，可以更好地适应这些不均匀的数据分布，确保图结构能够有效地覆盖整个数据空间。\n平衡探索和利用：在搜索算法中，需要平衡探索（探索未知区域）和利用（利用已知信息）之间的关系。邻居间的距离差异有助于这种平衡，因为它允许算法从一个节点出发探索多个方向，而不是仅限于最近的邻居。\n提高鲁棒性：在动态变化的数据集中，数据点的分布可能会随时间变化。如果节点的邻居具有一定的距离差异，这有助于图结构适应这些变化，保持其搜索效率。\n\n 多样性检查\n  检查流程图如下所示：\n图11：\n\n 邻居集合\n  邻居集合中的元素是从候选邻居集合中进行多样性检查后，筛选出的N个节点，它们将做新节点的正式邻居。\n  注意的是，第0层中，节点最多可以连接2*maxConn个邻居节点，其他层最多可以连接maxConn个，也就是邻居集合的大小。其中maxConn的默认值为16。\n  源码中通过NeighborArray对象来表示节点的邻居信息：\n图12：\n\n\nsize：邻居的数量\nscore：节点跟邻居的距离打分值数组\nnode：邻居的节点编号数组\nscoresDescOrder：node跟score数组按照距离打分值的升序还是降序排序\nsortedNodeSize：本篇文章不需要关注这个\n\n 候选邻居集合排序\n  上文中我们提到候选邻居集合是一个最小堆，即堆顶为距离最远的元素。由于后续节点是否满足多样性要求从距离最近（打分越高）的开始，因此这里的候选邻居集合排序在源码中是将堆中元素写入到一个NeighborArray中，我们称之为scratch，并且按照距离打分值升序排序。\n 逐个候选邻居进行多样性检查\n  从scratch中距离打分值最高的元素开始，比较它与邻居集合中每一个邻居的距离d(邻居，新节点的其他邻居)，如果至少存在一个其他邻居使得d(邻居，新节点)小于d(邻居，新节点的其他邻居)，则不满足多样性，如果满足则把这个候选邻居节点添加到邻居集合中，它将作为新节点在该层中的正式邻居。\n 尝试将新节点作为它邻居的新邻居\n图13：\n\n  这个流程点与图的更新策略密切相关。当新节点被添加到图中时，需要根据特定的规则或策略来更新图的结构。这包括上文中已经介绍的新节点的邻居列表应该包括哪些节点以及决定哪些现有节点应该将新节点加入其邻居列表。这些更新策略通常考虑以下几个方面：\n\n保持图的连通性：更新策略旨在保持图的良好连通性，确保可以从任一节点有效地导航到其他节点。\n优化搜索效率：通过适当地更新邻居列表，可以优化图的结构，从而提高后续搜索操作的效率。\n维护邻居多样性：更新策略通常旨在保持邻居的多样性，这有助于提高搜索的全面性和准确性。\n控制图的大小和复杂性：为了避免图变得过于复杂，更新策略可能包括限制节点的最大邻居数量。\n适应数据变化：在动态变化的数据集中，更新策略有助于图适应新数据的加入，保持其反映数据集当前状态的能力。\n\n图14：\n\n  邻居集合中包含了新节点的正式邻居，并且已经跟他们建立了单向的连接。如果新节点的邻居的邻居列表还未达到连接数上限，即第0层的2*maxConn和其它层的maxConn，那么新节点就可以作为它邻居的邻居。否则先添加到邻居的邻居列表后，再进行多样性检查（检查逻辑同上文中的描述），移除多样性最差的节点，当然这个最差的节点可能是新节点，也有可能是其他节点。\n 结语\n  无\n","categories":["Lucene","Index"],"tags":["vec","vem","vemf","vemq","veq","vex","hnsw","eps"]},{"title":"ImpactsDISI（Lucene 9.6.0）","url":"/Lucene/Search/2023/0804/ImpactsDISI/","content":"  在文章BulkScorer（一）中，我们介绍了抽象类DocIdSetIterator类，而ImpactsDISI是DocIdSetIterator的其中一种实现，当排序规则为文档打分值时，使得在查询TopN遍历文档时，可以跳过那些不具备竞争力的文档。\n图1：\n\n 前置知识\n  在展开介绍ImpactsDISI原理之前，我们需要先了解一些Lucene中的其他知识点，包括Impact、打分公式、跳表。\n Impact\n  文章索引文件的读取（十二）中详细的介绍了其概念，本文中我们只简单介绍提下Impact中几个重要的内容。\n  Impact是Lucene中的一个类，在生成索引阶段，某个term在一篇文档中的词频freq和标准化值(normalization values)norm会使用Impact对象记录，并且写入到索引文件.doc中。\n图2：\n\n图3：\n\n  图3红框标注的内容即某个term在一篇文档中的freq和norm。注意的是，由于使用差值存储freq和norm，如果当前norm跟上一个norm的差值为0，则只存储freq。\n 打分公式\n  当前版本Lucene9.6.0的打分公式如下所示：\n图4：\n\n  当需要计算包含某个term的文档的打分值时就会调用图3中的score方法。红框中的注释大意是：\n\n如果norm相等，那么freq较大对应的文档打分值会相等或者更高\n如果freq相等，那么norm较小对应的文档打分值会相等或者更高\n\n  这意味我们不需要真正的去调用图3中的打分方法获取文档的精确的打分值，而是可以仅通过freq和norm这两个值就可以粗略判断文档之间的打分值高低。\n 跳表\n  文章索引文件的生成（三）之跳表SkipList、索引文件的生成（四）之跳表SkipList详细介绍了Lucene中跳表的构建过程以及读取过程，本文中不再赘述。Lucene中通过分块（block）、分层（level）的方式对文档号构建跳表。\n图5：\n\n  图5是只基于Lucene中跳表的的实现思想，并不是真正的实现方式。该图描述的是对文档号集合[0, 3455]一共3456个文档号进行跳表的构建。Lucene默认每处理128（即图5中的skipInterval，源码中的变量）个文档号就构建一个block（分块），该block在源码中对应为图3中level=0层中第一个SkipDatum，并且每生成3（即图5中的skipMultiplier，源码中的变量，默认值为8）个SKipDatum就在上一层，即level=1层构建一个新的SkipDatum（分层）。\n  图5中每一个SkipDatum跟图3的索引文件的对应关系如下所示：\n图6：\n\n高清大图\n ImpactsDISI的文档遍历\n  ImpactsDISI作为DocIdSetIterator的子类，其核心为如何实现图7中的抽象方法advance(int target)方法，该方法描述都是从满足查询条件的文档号集合中找到下一个大于等于target的文档号。\n  如果target的值为3，并且有下面两个文档号集合：\n集合一：[0,2,3,4,5]集合二：[0,2,4,5] \n  对于集合一，advance方法的返回值为3；对于集合二，advance方法的返回值为4.\n图7：\n\n  该方法在ImpactsDISI中的实现流程图如下：\n图8：\n\n 名词解释\n  先介绍下流程图中的一些名词：\n\nblock：在图6的level=0层，每处理128个文档号就构建一个block。在level=0层对应SkipDatum\n- maxScore：某个block的所有文档打分的最大值\n- minCompetitiveScorer：在执行TopN的查询时，当已经收集了N篇文档，通过规则为文档打分值的优先级队列进行排序后，堆中最小的文档打分值即minCompetitiveScorer。如果是升序，意味着后续收集到的文档的打分值必须大于该值才被认为是具有竞争力的\n- NO_MORE_DOCS：Lucene中定义的边界值，常在遍历文档时使用，表示遍历结束\n\n 不需要进行skip\n图9：\n\n  我们先介绍下根据target更新block。它描述的是下一个进行遍历的block。在文章索引文件的生成（四）中介绍的哨兵数组skipDoc记录了每一层当前遍历的block，使得可以快速定位target属于某个block。\n  在level=0层，每128篇文档号就生成一个block，即图6中的SkipDatum。注意的是每个SkipDatum中impacts的Impact数量是小于等于128个，意味着如果这个block的maxScore大于minCompetitiveScorer，说明至少包含一篇文档是具有竞争力的，那么这个block中的所有文档我们都需要处理才能明确知道哪些文档是具有竞争力的，所以这种情况下就不能进行skip。\n为什么每个SkipDatum的impacts的Impact数量是小于等于128个?\n  首先一个block中的文档数量为128个，另外在索引阶段我们不需要记录某个term在这128篇文档中的Impact信息，即freq和norm。因为我们记录Impact的目的是为了在查询阶段能计算出block的maxScore值，根据上文中介绍的打分公式，如果某个term在两篇文档中的norm相同，那么只需要记录freq较大的Impact（详细的例子见索引文件的读取（十二））。\n  如果当前target跟上一个target不在同一个block中，那么就需要更新block。否则就直接返回当前target，因为它对应的文档打分值有可能是具有竞争力的。\n 进行skip\n图10：\n\n  当前target跟上一个target不在同一个block，那么在执行根据target更新block后，先要计算这个block中的macScore，如果maxScore大于等于minCompetitiveScore，那么返回当前target，因为它对应的文档打分值有可能是具有竞争力的。\n  在执行能否skip到其他block中?时，就可以通过跳表中其他层的SkipDatum的Impact信息来进行skip，例如下图中level=1的红框标注的SkipDatum中它包含了下一层，即level=0中三个（源码中默认是8）SkipDatum中的所有Impact。如果根据这些Impact计算出的分数还是小于minCompetitiveScore，那么就可以跳过level=0这三个block。\n图11：\n\n高清大图\n  上文中如果能实现skip，那么在流程点更新target中会将target的值更新为384。\n 结语\n  Lucene中，设计跳表的目的是为了能在遍历文档号时实现skip。而ImpactDISI利用之前已有的机制，通过额外新增的Impact索引信息实现排序规则为文档打分值的TopN查询的skip。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["disi","impact"]},{"title":"IndexOrDocValuesQuery（Lucene 8.9.0）","url":"/Lucene/Search/2021/0710/IndexOrDocValuesQuery/","content":"  本篇文章介绍下Lucene中用于优化范围（数值类型point或者term类型）查询的Query：IndexOrDocValuesQuery。\n  我们先通过这篇BLOG（如果链接失效，可以查看附件中的PDF）来概述下为什么会设计IndexOrDocValuesQuery。\n 求交集的问题\n  在文章多个MUST的Query的文档号合并中我们说到，执行一个由多个子Query组成，并且它们的关系为MUST的BooleanQuery时，其过程是先根据每个子Query获取一个有序的文档号集合，然后基于开销最小的子Query对应的文档号集合（我们称这个集合为leader iteration，其他的集合成为follow iteration）依次遍历这个集合，每次取出一个该集合中的文档号后，去其他集合中判断是否存在该文档号，当所有集合都存在相同的文档号，那么该文档号满足查询条件。\n 如何计算Query的开销\n  基于不同类型的Query，计算方式各不相同，但大部分情况下描述的是满足Query条件的文档数量。\n  例如在文章查询TopN的优化之NumericDocValues（二）中介绍了查询BKD树的开销的方法（见该文章中如何估算新的迭代器中的文档号数量的介绍）。\n  例如在TermQuery中，该查询的开销为包含term的文档数量，而该值保存在索引文件.tim、.tip中，如下图所示：\n图3：\n\n  图3中，红框标注的DocFreq字段描述的是包含某个term的文档数量。\n  上述的处理方式存在这么一个问题，为了便于介绍，我们假设只有两个文档号集合求交集，即只有一个leader iteration，一个follow iteration。如果leader iteration中只包含2个文档号，而follow iteration中包含海量的文档号。\n\n问题一：构建follow iteration占用了不必要的额外内存开销，因为follow iteration中最多只有2篇文档号是满足查询条件的\n问题二：极端情况下需要遍历follow iteration中所有的文档号才能判断是否存在交集\n\n 解决方案\n  BLOG中首先给出了一个解决的方向，或者说给出了一个查询计划（query plan）的设计方式：\n\n遍历所有的文档号：使用倒排（term或者point）\n\n由于在DocValues的数据结构中，正排值是按照文档号顺序先后存储的，如果通过正排值获取包含它的所有的文档号，其时间复杂度是线性的，相当的慢。比如说如果10篇文档中包含相同的正排值，那么通过这个正排值找到包含它的文档号集合需要查找5次。而在倒排中，包含某个term的所有文档号是集中存储的（见文章索引文件之doc、索引文件.tim、.tip介绍）\n\n\n匹配指定的文档号：使用正排\n\n正排能根据文档号随机访问正排值（见文章索引文件的读取（五）之dvd&amp;&amp;dvm的介绍），能解决问题一跟二。points对应的BKD树是按照值（点数据）有序存储的，根据文档号查找是否存在某个值意味着需要先根据该值找到包含该值的文档号集合，即会出现上文中提到的问题二\n\n\n\n IndexOrDocValuesQuery\n  接着我们根据IndexOrDocValuesQuery来介绍上文中的解决方案的具体实现方式。首先看下这个Query的注释简单了解下：\n图1：\n\n  图1中红框标注的注释说的是：IndexOrDocValuesQuery封装了两个分别通过倒排（points或者terms）跟正排实现查询的Query，根据这两个Query的查询性能会选出一个去执行真正的查询（这个过程即执行查询计划）。IndexOrDocValuesQuery特别适用于范围查询，因为在执行范围查询时，当根据term或者point value找到包含它的文档号集合后，需要对这个集合进行排序操作，而这个过程的开销可能会很大，使用IndexOrDocValuesQuery可能（见下文开销的比较小节的介绍）可以避免这个开销。\n  另外在注释中还给出了使用IndexOrDocValuesQuery的例子，即构建了两个范围查询的Query，他们的查询条件是相同（范围查询的上下界以及查询的域名是相同）。其中一个使用BKD树进行查询，另一个使用DocValues，即正排数据结构，进行查询。注意的是要想使用IndexOrDocValuesQuery，必须在索引阶段同时使用DocValues跟Points对相同的数据生成索引信息。\n  图1中蓝框标注的注释说的是：注释中的例子的查询性能在两种场景中性能比较好。\n\n场景一：IndexOrDocValuesQuery作为leader iteration，基于BKD树找出所有满足条件的文档集合的场景（不过文档号的排序开销仍然没法消除）\n场景二：IndexOrDocValuesQuery作为follow iteration，通过leader iteration指定的文档号来匹配IndexOrDocValuesQuery对应的文档集合是否存在的场景（基于DocValues）\n\n ScorerSupplier（可跳过）\n  在继续介绍IndexOrDocValuesQuery之前，我们先介绍下ScorerSupplier类，该类就是因为设计IndexOrDocValuesQuery而新增的。\n  当然这个小结如果看的不是很明白也不会影响对IndexOrDocValuesQuery的理解，可以直接跳过，因为理解ScorerSupplier需要一些其他知识点，比如Scorer对象，在本文中不会做出介绍。\n图2：\n\n  图2中该类的注释中说到：ScorerSupplier用来提供Scorer对象，会先进行开销计算来决定生成哪种Scorer。结合本文介绍的IndexOrDocValuesQuery，下文中我们将会知道，在IndexOrDocValuesQuery中是如何选择哪个Query对应的Scorer。图2中的get(long leadCost)方法的参数leadCost，当前Query作为follow iteration时，leadCost为lead iteration的开销值。\n 两个Query的选择\n  我们通过一个例子来介绍IndexOrDocValuesQuery如何选择它封装的两个Query。\n图4：\n\n  图4中，代码第89行的IndexOrDocValuesQuery封装了两个Query，PointRangeQuery（我们称之为主Query，对应源码中的indexQuery）跟DocValuesRangeQuery（我们称之为副Query，对应源码中的dvQuery），随后IndexOrDocValuesQuery跟代码第85行的TermQuery组成一个BooleanQuery，并且它们的关系为MUST。\n  主Query跟副Query的选择逻辑中如下所示：\nif (threshold &lt;= leadCost) &#123;    return 主Query;&#125; else &#123;    return 副Query;&#125;\n leafCost\n  本文的开头介绍了leader iteration的概念，leader iteration对应的Query的开销即leafCost。\n如何选择TermQuery还是IndexOrDocValuesQuery对应的文档号集合作为leader iteration？\n  选择方式很简单，即比较两个Query的开销，开销小的即胜出。这里想要着重说明的是IndexOrDocValuesQuery是如何计算开销的：主Query的开销作为IndexOrDocValuesQuery的开销。以图4为例，PointRangeQuery的开销将作为IndexOrDocValuesQuery的开销。\n threshold\n  threshold的计算方式为：\nfinal long threshold = cost() &gt;&gt;&gt; 3;\n  其中cost方法返回的是主Query的开销。以图4为例，即cost()方法返回的就是PointRangeQuery的开销。\n 结语\n  简单的总结：IndexOrDocValuesQuery即利用了倒排实现了包含某个term的所有文档号的快速收集，又利用了正排实现了通过文档号能随机访问某个term。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","IndexOrDocValuesQuery"]},{"title":"IndexSortSortedNumericDocValuesRangeQuery （一）（Lucene 9.0.0）","url":"/Lucene/Search/2022/0314/IndexSortSortedNumericDocValuesRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  我们先通过IndexSortSortedNumericDocValuesRangeQuery类的注释了解下这个Query。\n图1：\n\n  图1中红框标注的注释说到，范围查询可以通过利用Index Sort来提高查询效率。如果查询条件的域field正好是用于段内排序的域，那么就可以通过二分法找到满足查询条件的两个文档号。这两个文档号分别作为一个区间的上下界，满足查询条件的文档号肯定都在这个区间内。\n  对于红框注释有两个细节需要注意：\n\n用于段内排序的域可以设置多个排序规则，这些排序规则有先后顺序，但是只有第一个排序规则（primary sort）对应的域跟查询条件的域相同才能提高查询效率\n上文中说到的区间，这个区间内的文档号不是都满足查询条件的，只能保证满足查询条件的文档号肯定都在这个区间内\n\n  下文中，我们将会这两个注意的细节作出详细的介绍。\n  图1中蓝框标注的注释说到，只有同时满足下面的条件才能实现这种优化执行策略（optimized execution strategy）：\n\n条件一：索引是有序的，第一个排序规则对应的field必须跟查询条件的域相同\n条件二：范围查询条件的域必须是SortedNumericDocValues或者NumericDocValues\n条件三：段中每一篇文档中最多只能包含一个SortedNumericDocValues或者NumericDocValues\n\n  条件三的限制换个说话就是：一篇文档中要么不包含SortedNumericDocValues或者NumericDocValues，要么只能包含一个。具体原因在下文中说明。\n  如果上述三个条件不同时满足，那么就无法使用这个优化执行策略，这次查询就会交给fallbackQuery（下文会介绍）去执行。\n  图1中灰框标注的注释说道，fallbackQuery的查询条件必须跟IndexSortSortedNumericDocValuesRangeQuery一致，返回的结果必须是相同的文档集合并且每篇文档的分数是固定的。\n  因为IndexSortSortedNumericDocValuesRangeQuery的查询逻辑是先尝试用IndexSort机制进行查询，如果无法同时满足上文中说道的三个条件，那么就将这次查询委托（delegate）给fallbackQuery。\n  灰框标注的例子中可以看出，代码66行的LongPoint.newRangeQuery即fallbackQuery，它的查询条件跟代码67行的IndexSortSortedNumericDocValuesRangeQuery是一样的。\n  从这个例子也可以看出，fallbackQuery跟IndexSortSortedNumericDocValuesRangeQuery的查询条件保持一致需要使用者自己来保证，意味着用户想要自己编写一个效率较高的数值范围查询有较高的学习成本，他至少要了解并且在索引阶段写入SortedNumericDocValues或者NumericDocValues，还要了解各种数值类型范围查询，比如IndexOrDocValuesQuery，PointRangeQuery（一）等等。所以社区已经开始着手开发一些&quot;sugar&quot;域跟Query（见LUCENE-10162），使得用户能简单透明的使用数值类型的范围查询，让Lucene来帮助用户生成一个高效的Query。\n  我们看下Elasticsearch 8.0的NumberFieldMapper类中是如何生成一个高效的数值类型范围查询Query的，以Mapping类型为int的字段为例：\n图2：\n\n  图2描述了生成一个用于int类型的字段的范围查询的过程：首先通过IntPoint.newRangeQuery生成一个PointRangeQuery，我们称之为indexQuery；如果该字段开启了DocValues，那么通过SortedNumericDocValuesField.newSlowRangeQuery生成一个DocValues的范围查询，我们称之为dvQuery, 然后将indexQuery跟dvQuery封装为一个IndexOrDocValuesQuery；最后如果分片的索引根据查询域排序了，那么进一步将IndexOrDocValuesQuery封装为IndexSortSortedNumericDocValuesRangeQuery，此时IndexOrDocValuesQuery即上文中的fallbackQuery。\n  在文章IndexOrDocValuesQuery中详细的介绍了这个Query，本文中我们简单的提一下：IndexOrDocValuesQuery既利用了倒排中根据term快速获取满足查询条件的文档号集合能力，又利用了正排中根据文档号能快速check是否存在某个term的能力。使得IndexOrDocValuesQuery不管作为leader iterator还是follow iterator都能获得不错的读取性能。\n  回到图2中构造过程，的确需要相当大的学习成本才能构造成一个高效的Query。LUCENE-10162的目标在于期望用户通过编写类似IntField.NumericRangeQuery(String field, long lowerValue, long upperValue)这种方式就可以获得图2中的Query。\n 利用IndexSort实现高效查询\n  不管是哪种Query的实现，其需要解决的最重要的核心问题是这个Query如何提供一个迭代器DocIdSetIterator，迭代器中包含了满足查询条件的文档号集合，以及定义了读取这些文档号的方式。查询性能取决于迭代器的实现方式。\n BoundedDocSetIdIterator\n  BoundedDocSetIdIterator即在利用了IndexSort后，IndexSortSortedNumericDocValuesRangeQuery实现的迭代器，注意的是这个迭代器的名字有书写错误typo，应该是BoundedDocIdSetIterator，也许会在这个LUCENE-10458合并后修正。\n  我们看下这个迭代器中包含的信息，即该类的成员变量：\n图3：\n\n  图3中的firstDoc、lastDoc中用来描述一个左闭右开的文档号集合的区间，这个区间内至少包含了满足查询条件的所有文档号。对于firstDoc跟lastDoc是如何计算的，我们将在下一篇文章中展开。\n为什么说BoundedDocSetIdIterator中至少包含了满足查询条件的所有文档号\n  因为某些文档中虽然不包含查询条件对应的域的信息，但是Lucene会给这篇文档添加一个默认值来参与段内的文档排序。该默认值就是MissingValue。这些被设置了MissingValue的文档号就有可能被统计到迭代器中。\n 例子\n我们通过一个例子介绍下上述的问题，demo地址：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo9.0.0/src/main/java/facet/MissingValueTest.java 。\n图4：\n\n图5：\n\n图6：\n\n  图4中，我们定义了一个IndexSortSortedNumericDocValuesRangeQuery的范围查询，其中代码66、67定义了查询条件的上下界分别为 1, 100，代码72行定义了查询条件的域名为&quot;number&quot;，显然，只有文档1跟文档2中包含&quot;number&quot;域，并且其域值满足查询条件，而文档0中不包含&quot;number&quot;域的信息，所以在图6中，满足查询条件的文档号分别是文档1跟文档2。\n  图4的例子满足了利用IndexSort的三个条件，故IndexSortSortedNumericDocValuesRangeQuery会生成图5中的BoundedDocSetIdIterator迭代器。但是通过断点可以看到firstDoc跟lastDoc组成的左闭右开的区间中的文档号集合包含了文档0、文档1、文档2。这是因为在图4中我们定义了MissingValue的值为3，所以文档0也被BoundedDocSetIdIterator收集了（其被收集的原因也会在下一篇文章中展开介绍）。\n  如果图4中不设置MissingValue，那么BoundedDocSetIdIterator的信息是这样的：\n图7：\n\n图8：\n\n  由于在设置了MissingValue后，BoundedDocSetIdIterator中可能会包含不满足查询条件的文档号，所以在BoundedDocSetIdIterator中，如图8所示，它还包含一个成员变量delegate，它是上文中fallbackQuery对应的迭代器。因为fallbackQuery的迭代器中包含的文档号肯定是满足查询条件的，所以在读取BoundedDocSetIdIterator的文档号时，每次都会去delegate中检查是否存在这个文档号，来保证返回数据的准确性。\n 结语\n  剩余内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","IndexSortSortedNumericDocValuesRangeQuery"]},{"title":"IndexSort（一）（Lucene 8.9.0）","url":"/Lucene/Index/2021/0915/IndexSort%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  段内排序IndexSort是Lucene在索引（Indexing）阶段提供的一个功能，该功能使得在执行flush、commit或者NRT操作后，新生成的段其包含的文档是有序的，即在索引阶段实现了文档的排序。\n  在之前的一些文章中已经简单的介绍了IndexSort，例如在文章构造IndexWriter对象（一） 说到，通过在IndexWriter的配置信息中添加IndexSort信息来开启段内排序的功能；在文章文档提交之flush（三）中提到了对文档进行段内排序的时机点；在文章Collector（三）中提到了在Search阶段，如何通过IndexSort实现高效（提前结束）收集满足查询条件的文档集合。\n  本系列文章将会详细介绍IndexSort在索引阶段相关的内容，以及它将如何影响索引文件的生成、段的合并、以及在查询阶段的用途。\n IndexSort的应用\n  我们先通过一个例子来了解如何使用IndexSort这个功能。完整的demo地址见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.9.0/src/main/java/index/IndexSortTest.java 。\n图1：\n\n图2：\n\n  图1中的第44、45行代码定义了两个排序规则。在继续展开介绍之前，我们先简单的说下Lucene中正排索引SortedSetDocValuesField（见图2）的一些概念。\n SortedSetDocValuesField\n  使用SortedSetDocValuesField可以使得我们在同一篇文档中定义一个或多个具有相同域名、不同域值的SortedSetDocValuesField域。这种域的其中一个应用方式即在索引阶段，对于一篇文档，我们可以选择其包含的SortedSetDocValuesField域的某一个域值参与段内排序。\n  例如在图2中，文档3中（代码第80、81行）定义了2个域名为&quot;sort0&quot;，域值分别为&quot;b1&quot;、“b2&quot;的SortedSetDocValuesField域。并且在图1中的第44行代码定义了一个段内排序规则，该规则描述的是每个文档会使用域名为&quot;sort0”，并且将最小的域值来参与排序。那么对于文档3，它将使用域值为&quot;b1&quot;（字符串使用字典序进行排序）参与段内排序。\n SortedSetSelector.Type\n  图2中SortedSetSelector.Type.MIN规定了使用最小的域值参与段内排序。SortedSetSelector.Type的所有选项如下所示：\n图3：\n\n  图3中MIN、MAX就不做介绍了，我们说下MIDDLE_MIN跟MIDDLE_MAX。\n MIDDLE_MIN、MIDDLE_MAX\n  如果域值的数量是奇数，那么MIDDLE_MIN、MIDDLE_MAX具有相同的作用，比如有以下的域值：\n&#123;&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;, &quot;b5&quot;&#125;\n  那么将会选择&quot;b3&quot;参与排序。\n  如果域值的数量是偶数，假设有以下的域值：\n&#123;&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;, &quot;b5&quot;, &quot;b6&quot;&#125;\n  那么在MIDDLE_MIN条件下会选择&quot;b3&quot;、在MIDDLE_MAX下会选择&quot;b4&quot;。\n  另外使用SortedSetDocValuesField的一个场景是，我们在搜索阶段可以根据SortedSetDocValuesField对查询结果进行排序，并且可以通过指定不同的SortedSetSelector.Type获取不同的排序结果。\n 排序方式概述\n  我们接着看下图1。图1中定义了两个规则，那么在段内排序的过程中，先按照&quot;sort0&quot;进行排序，当无法比较出先后关系时，接着按照&quot;sort1&quot;进行排序，如果两个排序规则都无法比较出先后关系，则最终比较文档的添加顺序。\n 文档之间的排序比较方式\n图4：\n\n图5：\n\n  图4中，我们的搜索条件没有对结果增加额外的排序规则，那么查询结果将会按照段内排序后的顺序输出。\n  我们结合图2，分别介绍下一些文档之间排序比较的方式。\n 文档0\n  这里说的文档0指的是图2中添加的顺序，如图2中第53行的代码所示。\n  根据&quot;sort0&quot;的排序规则，将按照域值从大到小排序（SortedSetSortField的第二个参数reverse的值为true），所以文档0~3这四篇文档将分别使用&quot;c1&quot;、“b1”、“b1”、“b1&quot;进行比较，另外由于文档4中没有&quot;sort0&quot;域，那么它将被排到最末位置。可见根据&quot;sort0”，只能确定文档0是排在最前面的以及文档4是排在最后面，如下所示。故需要通过&quot;sort1&quot;对文档1、2、3进一步排序。\n文档0 --&gt; 文档1、2、3 --&gt; 文档4\n 文档1、2、3\n  根据&quot;sort1&quot;的排序规则，将按照域值从大到小排序，所以文档1、2、3这四篇文档将分别使用&quot;e2&quot;、“f2”、&quot;e2&quot;进行比较，可见文档2在这三篇文档中排在最前面，由于文档1、3无法通过&quot;sort1&quot;区分出先后关系，并且没有其他的排序排序规则了，那么由于文档1先被添加，故文档1排在文档3前面，如下所示：\n文档0 --&gt; 文档2 --&gt; 文档1 --&gt; 文档3 --&gt; 文档4\n 搜索阶段的排序\n  上文中说到SortedSetDocValuesField可以用于在索引阶段排序，同样的它也可以用于搜索阶段的排序。在设置了图1中的排序规则前提下，如果我们在搜索阶段提供了以下的排序规则：\n图6：\n\n  其查询结果如下所示：\n图7：\n\n  其排序的比较过程跟IndexSort是一样的，这里就不赘述了。\n 用于排序的域\n  下图中列出了其他可以用于段内排序的域：\n图8：\n\n  这些域的使用方法可以通过查看每个类的注释就可以完全理解，故不展开介绍了。\n 结语\n  下一篇文章中，我们将介绍段内排序在索引阶段的排序方式、排序时机点以及其他相关内容。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["index","IndexSort"]},{"title":"IndexedDISI（一）（Lucene 8.4.0）","url":"/Lucene/gongjulei/2020/0511/IndexedDISI%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  IndexedDISI工具类在Lucene中用来存储Norm/DovValues对应的文档号，其实现原理借鉴了roaring bitmaps（见文章RoaringDocIdSet），本文先通过介绍在Lucene7.5.0中的实现来理解其原理，接着会介绍在Lucene8.4.0中的优化实现。\n IndexedDISI写入文档号\n Block\n  使用IndexDISI存储的数据结构如下所示：\n图1：\n\n  图1中，每个block用来描述最多2^16个文档号信息，例如第一个block中描述的文档号集合为[0, 2^16 - 1]，在处理某个文档号时，根据下面的公式来找到存储该文档号对应的block：\nint block = docId &gt;&gt;&gt; 16\n  如果当前处理的文档号为 3，那么根据上面的公式 block = 3 &gt;&gt;&gt; 16 = 0，那么文档号3将被存储在第一个block中，如果当前处理的文档号为 65538，根据上面的公式 block = 65538 &gt;&gt;&gt; 16 = 1，那么文档号65538将被存储在第二个block中。\n 稠密度（density）\n  在一个block中，根据block中存储的文档号数量划分为三种稠密度：\n\nALL：block中存储的文档号数量为2^16个\nDENSE：block中存储的文档号数量范围为 [4096, 2^16 - 1]，使用FixedBitSet存储文档号\nSPARSE：block中存储的文档号数量范围为 [1, 4095]，使用short类型数组存储文档号\n\n  存储文档号使用的数据结构根据稠密度各不相同，我们只介绍介绍DENSE跟SPARSE，至于为什么不介绍稠密度ALL，我们将在系列文章索引文件的生成（十五）之dvm&amp;&amp;dvd中作出解释。\n DENSE\n  该稠密度对应的block的数据结构如下所示：\n图2：\n\n BlockId\n  BlockId为block的编号，例如图1中，第一个block的编号为0，第二个block的编号为2。\n图3：\n\n  上文中我们说到，每个block中描述了216个文档号，另外通过图3可知，blockId最大值为65535，可见IndexDISI最多描述232个文档号。\n  BlockId的作用：\n  blockId用于计算baseId，计算公式为：\nbaseId = (blockId * 65536)\n  在使用了baseId后，在每个block中只要存储块内文档号就行了，意味着，在一个block中存储的文档号的取值范围为 0 ~ 65535，在读取阶段， 通过blockId跟块内文档号就可以获得原始文档号。\n docIdNumber\n  docIdNumber为文档数量，它描述了当前block中存储的文档号数量。\n word\n  word是一个long类型的值，long类型的数值占用64个bit，每一个bit用来描述一个文档号，另外上文中说到block中最多描述2^16个文档号，那么图2中word的数量就是固定的 65536/64，即1024个。\n  对于blockId为0的block中的第一个word，该word描述的文档号范围为 0 ~ 63，第二个word描述的文档号范围为 64 ~ 127，看下面的例子：\n图4：\n\n  图4中，block的编号为0的第一个word中存储了3个文档号{3, 57, 60}，如果没看懂，建议先阅读文章工具类之FixedBitSet 。\n SPARSE\n  该稠密度对应的block的数据结构如下所示：\n图5：\n\n  稠密度为SPARSE时，文档号直接用short类型数组存储即可，在上文中说到，块内文档号的取值范围为0 ~ 65535，那么文档号的最大值需要16bit才能表示，而short类型的值占用2个字节，故采用short类型的数组存储。\n IndexedDISI读取文档号\n  我们接着介绍IndexedDISI读取文档号的过程，在介绍了其读取过程后就可以明白为什么从Lucene8.0.0版本开始，对IndexedDISI的写入进行了优化。\n  在Lucene最常用的应用中，我们在Collector中会对满足查询条件的文档号进行排序，排序规则通常使用DocValues来实现，而包含DocValues信息的文档的文档号就使用IndexedDISI存储，故在排序过程中，先判断这个满足查询的文档号是否包含了DocValues信息，该过程即在IndexedDISI中读取文档号，判断是否存在这个文档号。\n  判断一个文档号是否在IndexedDISI中可以简单的划分为两个步骤：\n\n步骤一：找到所属block\n步骤二：判断block是否包含此文档号\n\n 步骤一：找到所属block\n  通过下面的公式先找到文档号属于哪一个block：\nint block = docId &gt;&gt;&gt; 16\n  该过程也就是计算出上文中提到的block编号blockId，通过逐个比较block中的BlockId字段找到所属block，可见找到所属block的时间复杂度为O(n)，如下所示：\n图6：\n\n 步骤二：判断block是否包含此文档号\n  当找到所属block之后，我们继续在块内寻找，根据不同的稠密度，查找方式也各不相同。\n DENSE\n  这种稠密度使用了word来存储文档号，我们通过下面的公式可以计算出查询的文档号应该在第n个word中：\ndocId &gt;&gt;&gt; 6\n  注意的是此时docId是块内文档号，最后通过与操作就可以判断是否包含此文档号（看不懂？说明你没有看文章工具类之FixedBitSet ）。\n  在计算应该在第n个word的过程中，我们需要做一件事情，那就是必须知道当前查询的文档号它是IndexedDISI存储的第几个文档号（可能不包含当前查询的文档号），我们称之为 段内编号，基于上文中的存储方式，我们只能通过累加前n -1 个block中的文档数量来获得，意味着我们必须依次处理每一个block（通过累加读取block中的DocIdNumber字段的值段内编号），故时间复杂度为O(n)。\n  为什么要获得当前查询的文档号在block中的段内编号\n  因为如果block中有这个当前查询的文档号，那么我们还要取出DocValues的值，才能在Collector中进行排序比较，并且需要通过段内编号才能找到当前查询文档号对应的DocValues的值，这块内容的介绍将在系列文章索引文件的生成（十五）之dvm&amp;&amp;dvd中作出解释。\n SPARSE\n  这种稠密度就是简单的遍历short[ ]数组来判断，不赘述。\n 结语\n  通过上述的介绍，可以发现两个步骤的时间复杂度均为O(n)，在Lucene8.0.0之后，会通过查找表（lookup table）提高查询性能，基于篇幅将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","gongjulei"],"tags":["disi","docId","doc"]},{"title":"IndexedDISI（二）（Lucene 8.4.0）","url":"/Lucene/gongjulei/2020/0514/IndexedDISI%EF%BC%88%E4%BA%8C%EF%BC%89/","content":" IndexedDISI（二）（Lucene 8.4.0）\n  在文章IndexedDISI（一）（阅读本文中之前，需要该前置文章）中我们介绍了在Lucene7.5.0中IndexedDISI的实现原理， 本文基于Lucene 8.4.0，将介绍优化后的IndexedDISI，即使用查找表（lookup table）提高了查询性能。\n  我们先根据源码中的注释看下优化的目的与方式，也可以直接查看IndexedDISI.java文件中的Javadoc：\nTo avoid O(n) lookup time complexity, with n being the number of documents, two lookup tables are used: A lookup table for block offset and index, and a rank structure for DENSE block index lookups.\n  上述大意是，查找的时间复杂度为O(n)，其中n是文档号的数量，优化方式为通过两个查找表来提高查询性能：\n\n第一个查找表使用offset跟index实现block之间的跳转，在源码中，使用int类型数组来存储offset跟index的信息，该数组的变量名为jumps\n第二个查找表使用rank结构（structure）实现在block的稠密度为DENSE中的word之间的跳转，数组的变量名为rank\n\n图1：\n\n 第一个查找表jumps\n  我们先看实现block之间跳转的查找表jumps数组，在jumps数组中，index跟offset作为一对（pair）来描述一个跳转信息：\n\noffset：该值描述了block在字节流的起始读取位置，例如在生成索引文件.dvd&amp;&amp;.dim中使用IndexedDISI存储文档号时，offset就描述了block在索引文件.dvd中的起始读取位置\nindex：block中第一个文档号的段内编号（见文章IndexedDISI（一））\n\n 生成jumps\n  我们通过一个例子来理解第一个查找表，如果我们以下的文档号集合：\n图2：\n\n  在文章IndexedDISI（一）我们说到，每个block用来描述最多2^16个文档号信息，例如第一个block中描述的文档号取值范围为[0, 2^16 - 1]，第二个block描述的文档号取值范围为[2^16, 2^17 - 1]，第三个block描述的文档号取值范围为[2^17, 2^18 - 1]，故图1中的文档号集合将会由3个block来存储如下所示：\n图3：\n\n 对于图3的情况，生成的jumps数组如下所示：\n图4：\n\n  从图4可以看出，index描述的的确是block中第一个文档号的段内编号，例如在第三个block中，index的值为5003，意味着这个block中存储的第一个文档号的段内编是5003，至于为什么要存储index，我们将在系列文章索引文件的生成（十五）之dvm&amp;&amp;dvd中作出解释。\n 读取jumps\n  同样通过一个例子来介绍如果通过jumps实现block之间的跳转。如果我们有一个待查询的文档号为131082，由于每个block中描述的文档号的最多为2^16个，那么通过下面的公式获得结果，我们称之为inRangeBlockIndex（源码中的变量名），可以计算出文档号131082应该由哪个block来描述，在代码中：\n130182 &gt;&gt;&gt; 16 = 2\n  由于jumps[ ]数组被存储在字节流中，所以继续根据根据下面的公式就可以获得inRangeBlockIndex为2对应的index跟offset的值：\nfinal int index = jumpTable.readInt(inRangeBlockIndex*Integer.BYTES*2);final int offset = jumpTable.readInt(inRangeBlockIndex*Integer.BYTES*2+Integer.BYTES);\n  上述公式中Integer.BYTES的值为4，jumpTable为存储jumps[ ]数组的字节流，故我们以jumps[ ]数组的第一个数组元素作为起始位置，偏移值为inRangeBlockIndex*Integer.BYTES*2 + Integer.BYTES的位置就可以读取offset，即文档号131082用第二个block来描述，至于在不在这个block中，需要进一步进行判断，下文中会展开介绍：\n图5：\n\n  在文章开头我们就说道，jumps数组是int类型，故4个数组元素占用16个字节。\n 第二个查找表rank\n  在一个block内部，我们根据block的稠密度使用不同的数据类型存储，只有稠密度为DENSE时，会使用第二个查找表rank数组来实现word之间的跳转：\n图6：\n\n 生成rank\n  生成rank数组前，需要指定denseRankPower，该值描述的是block中每n个文档号就生成一个rank，对应关系如下：\n表1：\n\n\n\ndenseRankPower\n文档数量\n\n\n\n\n7\n128\n\n\n8\n256\n\n\n9\n512\n\n\n10\n1024\n\n\n11\n2048\n\n\n12\n4096\n\n\n13\n8192\n\n\n14\n16384\n\n\n15\n32768\n\n\n\n  表一中，如果denseRankPower为7，那么block中每128个文档号就生成一个rank，由于一个word能最多表示64个文档号，意味着每2个word就生成一个rank，即图6中描述的那样。\n  在图6中，每处理两个word，就将这两个word中包含的文档数量，即bitCount写入到rank数组中，注意的是bitCount是一个累加值，在介绍完下面的例子后会发现，rank数组中的bitCount用来表示的是一个rank中第一个文档号的段内编号，例子中denseRankPower为7：\n图7：\n\n  假设图6中的第一个block中存储了上述的文档号，其中图7中文档号的数量说明这个block的稠密度为DENSE（见文章p;在文章IndexedDISI（一）），那么上述文档号在使用word存储后如下所示：\n图8：\n\n  由于denseRankPower为7，意味着每两个word就要生成一个rank，并且两个word的bitCount和值记录到rank数组中：\n图9：\n\n  图9中，每个word的bitCount描述的是它存储的文档号数量，例如第一个word中存储了文档号1、56、61共三个文档号，故bitCount的值为3，由于两个word生成一个rank，所以两个word的bitCount的和值写入到rank数组的下标值为0的位置，即红框圈出的两个bitCount的和值，另外在继续处理了第三个、第四个共两个word后，我们此时需要生成第二个rank，不仅仅要获得这，两个word中的bitCount和值，还要累加上第一个跟第二个word的和值，即3 + 2 + 3 + 2 = 10，写入到rank数组的下标值为1的位置。\n  图6中的HEB、LEB是 高八位 high eight bit、 lower eight bit的缩写，由于一个block中最多有65536个文档号，意味着bitCount的值最大值为65535，需要16个bit字节才能表示，加上rank数组是字节数组，所以需要两个数组元素即2个字节才能表示bitCount的最大值，故第一个字节用来表示高8位，第二个字节用来表示低8位。\n 读取rank\n  在读取阶段，使用rank数组进行跳转的条件如下所示：\ndenseRankPower != -1 &amp;&amp; targetWordIndex - wordIndex &gt;= (1 &lt;&lt; (denseRankPower-6) )\n  上述条件中，wordIndex是上一个文档号的段内编号，targetWordIndex是当前查询文档号的段内编号，可见只有前后两次查询的段内编号超过denseRankPower对应的文档数量才会进行跳转，理由很简单，读取阶段只会读取出denseRankPower对应的文档号信息到内存，如果满足上述条件，我们需要从磁盘（缓冲）中读取新的words，具体的读取过程由于跟读取jumps数组类似就不赘述了。\n 结语\n   在Lucene8.0.0之后，通过两个查找表使得在时间复杂度为O(n)的基础上提高了查询性能，对于表一中denseRankPower，源码中作者建议的取值范围为 [8, 12]，更多关于two lookup table的设计思想见这个issue：https://issues.apache.org/jira/browse/LUCENE-8585?jql=text ~ &quot;indexedDISI&quot; 。\n点击下载附件\n","categories":["Lucene","gongjulei"],"tags":["disi","docId","doc"]},{"title":"IntBlockPool","url":"/Lucene/gongjulei/2018/1209/IntBlockPool/","content":"在索引阶段，使用IntBlockPool来存储term(域值)的信息，在MemoryIndex中，使用此类对term在文档中的位置、payload数据进行存储，它即MemoryIndex中的倒排表，它的数据就是用这个类的对象存储的。在介绍IntBlockPool类时，我们根据这个类在MemoryIndex中实际使用过程来讲解。\n 如何介绍IntBlockPool类\n首先会贴出这个类中几个重要的方法，详细的说明方法中的逻辑过程，在讲解MemoryIndex类时，会使用图文方法来演示一个例子。IntBlockPool类的详细代码注释在这里：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/IntBlockPool.java\n IntBlockPool的方法和变量\n 几个基本的变量说明\n// 存储term的位置，词频，payload，默认是由10个大小为 INT_BLOCK_SIZE大小的一维数组组成的。public int[][] buffers = new int[10][];public final static int INT_BLOCK_SHIFT = 13;// 一维数组的大小。public final static int INT_BLOCK_SIZE = 1 &lt;&lt; INT_BLOCK_SHIFT;public final static int INT_BLOCK_MASK = INT_BLOCK_SIZE - 1;// 用来表示我们正在使用第几个一维数组(前面定义了一个包含10个一维数组的二维数组)。private int bufferUpto = -1;// 用来表示一维数组中可以使用的位置(偏移)。public int intUpto = INT_BLOCK_SIZE;// 同一时刻只选取二维数组中的一个一维数组进行数据的存储跟读取，buffer用来表示我们正在使用的那个一维数组。// 这个buffer又称为 head buffer。public int[] buffer;// 用来表示在二维数组中可以使用的位置(偏移)。public int intOffset = -INT_BLOCK_SIZE;// 分层的级别，代表下次分配新的分片(slice)的级别(层级)，数组元素越大，分片大小越大，数组元素用来作为LEVEL_SIZE_ARRAY[]数组的下标值。private final static int[] NEXT_LEVEL_ARRAY = &#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 9&#125;;// 数组元素表示新的分片的大小。private final static int[] LEVEL_SIZE_ARRAY = &#123;2, 4, 8, 16, 32, 64, 128, 256, 512, 1024&#125;;\n 获得新的分片(slice)\n在MemoryIndex中，对于有相同域名的不同域值，这些域值的倒排表信息都是存放到同一个二维数组中，每个域值都会被分配一个固定大小的分片，用来存储域值(即term)的信息，并且相同的域值会被存储多次(因为在文档中的的位置不同导致偏移值和payload的不同)。另外多个相同的域值，他们的数据不是连续存放的，通过分层的方式分布在二维数组的不同位置(稍后会作出解释)。下面获取一个新的分片的方法。\n// 需要分配size大小的分片。private int newSlice(final int size) &#123;    // 判断当前的head buffer空间是否充足，如果不够的话，就用分配新的head buffer来存储。    if (intUpto &gt; INT_BLOCK_SIZE-size) &#123;      // 空间不足的话，分配一个新的一维数组。      nextBuffer();      assert assertSliceBuffer(buffer);    &#125;    final int upto = intUpto;    // 分配size个大小的数组空间给这次的存储,然后intUpto更新。    intUpto += size;    // 指定下次分片的级别。    buffer[intUpto-1] = 1;    return upto;  &#125;public void nextBuffer() &#123;    // 判断二维数组是否存储已满，那么就扩容，并且扩容结束后，迁移数据。    if (1+bufferUpto == buffers.length) &#123;      int[][] newBuffers = new int[(int) (buffers.length*1.5)][];      System.arraycopy(buffers, 0, newBuffers, 0, buffers.length);      buffers = newBuffers;    &#125;    // 生成一个新的一维数组。    buffer = buffers[1+bufferUpto] = allocator.getIntBlock();    // 更新bufferUpto，这个值描述了我们当前正在使用第bufferUpto个一维数组。    bufferUpto++;    // head buffer数组的可使用位置(下标值)置为0。    intUpto = 0;    // 更新intOffset的值，表示在二维数组中可用的位置。    intOffset += INT_BLOCK_SIZE;  &#125;\n 往分片(slice)中写数据\n通过SliceWriter对象将数据写入到分片中。\n SliceWriter类的部分方法和变量\n// 描述在二维数组中的位置。 private int offset;\n将value值写入到分片中，如果分片剩余空间不足，那么重写分配一个新的分片。\npublic void writeInt(int value) &#123;        // 获得head buffer这个一维数组, offset &gt;&gt; INT_BLOCK_SHIFT的值就是head buffer在二维数组中的行数。      int[] ints = pool.buffers[offset &gt;&gt; INT_BLOCK_SHIFT];      assert ints != null;      // 获得在head buffer这个一维数组组内的偏移。      int relativeOffset = offset &amp; INT_BLOCK_MASK;      // if语句为真，说明分片剩余空间不足，我们需要分配新的分片(slice), 当前数组位置存储的是分片的层级。      if (ints[relativeOffset] != 0) &#123;        //  分配一个新的分片，并且返回可以存放value的下标值。          relativeOffset = pool.allocSlice(ints, relativeOffset);        // 更新下ints变量和offset变量，因为调用pool.allocSlice()后，head buffer发生了变化。        ints = pool.buffer;        offset = relativeOffset + pool.intOffset;      &#125;      // 存储value的值。      ints[relativeOffset] = value;      offset++;     &#125;private int allocSlice(final int[] slice, final int sliceOffset) &#123;    // 取出分片的层级。    final int level = slice[sliceOffset];    // 获得新的分片的层级。    final int newLevel = NEXT_LEVEL_ARRAY[level-1];    // 根据新的分片的层级，获得新分片的大小。    final int newSize = LEVEL_SIZE_ARRAY[newLevel];    // 判断是否需要分配一个新的一维数组。    if (intUpto &gt; INT_BLOCK_SIZE-newSize) &#123;      nextBuffer();      assert assertSliceBuffer(buffer);    &#125;    // 获得当前在head buffer中下一个可以使用的位置。    final int newUpto = intUpto;    // 记录下一个数组下标位置(这个位置的值在二维数组中的位置)。    final int offset = newUpto + intOffset;    // 更新head buffer中下一个可以使用的位置。    intUpto += newSize;    // Write forwarding address at end of last slice:    // 将sliceOffset位置的数组值替换为offset, 目的就是在读取数据时，被告知应该跳转到数组的哪一个位置继续找这个term的其他偏移值(在文本中的偏移量)跟payload值。    // 换句话如果一篇文档的某个term出现多次，那么记录这个term的在文本中的所有偏移值跟payload并不是连续存储的。    slice[sliceOffset] = offset;           // 记录新的分片层级。    buffer[intUpto-1] = newLevel;    // 返回可以写入数据的位置。    return newUpto;  &#125;\n存储一个新的域值时候需要调用下面的方法来分配一个分片(仅在MemoryIndex中会调用此方法,Lucene 7.5.0)。\npublic int startNewSlice() &#123;      // offset的值是 head buffer这个一维数组组内的偏移量 + head buffer这个一维数组在二维数组中的偏移量。      return offset = pool.newSlice(FIRST_LEVEL_SIZE) + pool.intOffset;    &#125;\n写入的域值跟上一个处理的域值不一样，并且已经在分片存储过，那么写入之前需要调用下面方法另offset的值为这个域值上次在分片中的结束位置\npublic void reset(int sliceOffset) &#123;     this.offset = sliceOffset;   &#125;\n 从分片(slice)中读数据\n通过SliceReader对象从分片中读取数据\n SliceReader类的部分方法和变量\n// 当前读取的位置。private int upto;// 指定了二维数组的某个一维数组。private int bufferUpto;// 一维数组的第一个元素在二维数组中的偏移量(位置)。private int bufferOffset;// 当前正在读取数据的一维数组。private int[] buffer;// limit作为下标描述了下一个存储当前term信息的位置。private int limit;// 获得分片的层级。private int level;// 这个term的最后一个信息的位置。private int end;\n从当前分片中读取一个term的数据，如果没有完全读取，那么跳转到其他分片中继续读取。\npublic int readInt() &#123;      assert !endOfSlice();      assert upto &lt;= limit;      // if语句为真，那么去下一个分片中继续读取。      if (upto == limit)        nextSlice();      return buffer[upto++];    &#125;private void nextSlice() &#123;      // Skip to our next slice      // 找到下一个存储term信息的位置。      final int nextIndex = buffer[limit];      // 获得分片的层级。      level = NEXT_LEVEL_ARRAY[level-1];      // 获得分片的大小。      final int newSize = LEVEL_SIZE_ARRAY[level];      // 计算出在二维数组中的第几层。      bufferUpto = nextIndex / INT_BLOCK_SIZE;      // 当前的一维数组的第一个元素在二维数组中的位置。      bufferOffset = bufferUpto * INT_BLOCK_SIZE;      // 取出 head buffer。      buffer = pool.buffers[bufferUpto];      // 计算出在head buffer中的起始位置。      upto = nextIndex &amp; INT_BLOCK_MASK;      // 更新limit的值。      if (nextIndex + newSize &gt;= end) &#123;        // We are advancing to the final slice        // 已经读到最后一个slice。        assert end - nextIndex &gt; 0;        limit = end - bufferOffset;      &#125; else &#123;        // This is not the final slice (subtract 4 for the        // forwarding address at the end of this new slice)        // 还有其他的slice没有读取到, 将limit的值置为下一个slice的起始位置。        limit = upto+newSize-1;      &#125;    &#125;\n 例子\n在MemoryIndex的storeTerms(…)的方法中，其逻辑就是通过IntBlockPool类依次存储一篇文档中的每一个域值的在文档中的分词位置posIncr，域值的起始位置startOffset，结束为止endOffset，负载值payload，下面是名词解释。\n文档A中包含的内容： “Tales of Tales James”， 分词器：WhiteSpaceAnalyzer，分词后的tokenStream中包含了下面4个term： “Tales”，“of”，“Tales”，“James”。\n posIncr\n   term是tokenStream的第几个token。例如 ”of“ 的posIncr的值是1，”Tales“的posIncr是0跟2，”James“的posIncr是3。\n startOffset\n   term的第一个字符的位置。例如 “of” 的startOffset的值是6， ”James“的startOffset值是15。\n endOffset\n   term的最后一个字符的位置的后一个位置。例如”of“的endOffset的值是8，endOffset与startOffset的差值就是term包含的字符个数。\n payload\n   payload，负载值，这个值用来标注一个term，你可以给这个term标注任意信息，比如可以将业务数据作为payload。如果没有指定payload的值，那么payload的值为-1。在这个例子中，所有term的payload值都是-1。\n\n文档A中的term需要保存的数据如下图\n\n\n\nterm\nposlncr\nstartOffset\nendOffset\npayload\n\n\n\n\nTales\n0\n0\n5\n-1\n\n\nof\n1\n6\n8\n-1\n\n\nTales\n2\n9\n14\n-1\n\n\nJames\n3\n15\n20\n-1\n\n\n\n 添加第一个term：Tales\n“Tales” 第一次添加到二维数组 buffers中，那么先分配一个大小2的分片(slice)，并将posIncr添加到buffers中，由于在MemoryIndex中，存储startOffset，endOffset，payload是可选项，所以当只要存放posIncr时，只要分配2个数组元素大小的分片即可，如下图：\n\n上图中，level的值指的是当需要继续添加“Tales”的其他数据时，我们需要再分配新的分片，并且下一个新分片的大小等级是1, 作为 LEVEL_SIZE_ARRAY[] 中的下标值，那么新的分片的大小就是4，我们继续添加“Tales”的其他数据。\n\n上图中，数组下标值为1对应的元素值由1改成了2，表示新的分片起始位置是2(下标值)。由于同一个term的多个分片分散在二维数组的不同位置，在搜索阶段，当我们读取完一个分片后，通过这个分片的最后一个值来确定下一个分片在二维数组中的位置。接着依次把startOffset，endOffset，payload的值分别存放进去。新分片的最后一个值还是用来存放分片等级。\n 添加第二个term：of\n“of”同样是第一次添加，所以分配一个大小为2个的分片，然后将posIncr跟分片等级level写入分片中，如下图：\n\n然后将“of”的其他数据继续写入， 同样的，需要分配新的分片，并且更改数组下标值7对应的元素值，由1变为8，表示新的分片从下标值8的位置开始，并且另新的分片的最后一个元素(下标值为11)值为1(分片层级)，如下图：\n\n 添加第三个term：Tales\n“Tales”已经存储过了，所以我们找到最后一个分片中最后一个使用的位置(在上图中就是下标值为5的位置)。这时候我们我们的分片没有剩余空间了，那么需要分配一个新的分片，并且这个新分配的位置是在下标值为12的地方，如下图：\n\n当我们给“Tales”分配了一个新的分片，并且成功的将posIncr，startOffset，endOffset写入到分片中，但是没有多余的空间存储payload了，所以我们这时候还要再分配一个分片级别为1(大小为4)的新分片，如下图：\n\npayload成功的存储到新的分片中，下标值17跟18位置的数组元素用来存放下一个“Tales”的数据(如果有的话)。\n 添加第四个term：James\n“James”同样是第一次添加，所以分配一个大小为2个的分片，然后由于要添加startOffset，endOffset，payload又要再分配大小为4的分片，过程跟添加“of”是一样的，所以最后一个term添加后的图如下：\n\n 结语\n上面的例子中，还有一些问题，如果感兴趣可以结合MemoryIndex的索引跟查询来了解，比如说，如何知道 “Tales of Tales James” 中的第一个“Tales”是第一次存储，如何知道第二个“Tales”已经存储过了，还有就是添加“of”跟“James”时，如何知道应该从二维数组中的哪个位置开始存储。\n至于读取二维数组的过程就不通过例子赘述了，从写入的逻辑就很容易看出如何读取。\n点击下载Markdown文件\n","categories":["Lucene","gongjulei"],"tags":["invertedIndex"]},{"title":"LRUQueryCache","url":"/Lucene/Search/2019/0506/LRUQueryCache/","content":"   LRUQueryCache用来对一个Query查询的结果进行缓存，缓存的内容仅仅是文档号集，由于不会缓存文档的打分（Score），所以只有不需要打分的收集器（Collector）才可以使用LRUQueryCache，比如说TotalHitCountCollector收集器，另外缓存的文档号集使用BitDocIdSet对象进行存储，在BitDocIdSet中实际使用了FixedBitSet对象进行存储。\n  即使使用了不需要打分的收集器，也不一定对所有的查询结果进行缓存，有诸多苛刻的条件，在下文中会详细介绍。\n  LRUQueryCache中缓存的Query结果是有上限限制的，在每次添加一个缓存时，根据两种阈值来判断是否需要将某个已经缓存的数据剔除，使用的算法为LRU：\n\nmaxCachedQueries：缓存的结果数量，默认值1000\nmaxRamBytesUsed：缓存占用的内存量，默认值32MB或可分配内存的5%中的较小值\n\n当然，我们可以自定义maxCachedQueries跟maxRamBytesUsed的值。\n LRUQueryCache流程图\n图1：\n\n 开始\n图2：\n\n在执行查询时，如果我们使用了一个不需要打分的Collector，那么该Query就可以进入LRUQueryCache的流程之中，比如说TermsCollector、TotalHitCountCollector等等。默认的查询使用的是TopScoreDocCollector，他需要打分，所以无法使用LRUQueryCache的功能。\n Query\n图3：\n\n查询定义的Query对象。\n 允许缓存？\n图4：\n\n允许缓存受限于诸多条件，下面一一列出：\n Query条件\n有些Query不需要缓存：\n\nTermQuery：TermQuery作为最常用的Query，源码中给出不需要缓存的理由是这种查询已经足够快(plenty fast)了。\nMatchAllDocsQuery：此Query是获得IndexReader中的所有文档号，在不使用cache的情况下获取所有结果的逻辑是从0开始遍历到IndexReader中的最大的文档号(因为每一篇文档都是满足要求的)，如果缓存这个结果的话，由于使用FixedBitSet存储了cache，在生成缓存的过程中需要编码，并且读取cache还需要解码，查询性能肯定是相比较大的。\nMatchNoDocsQuery：此Query不会匹配任何文档，所以没有缓存的必要\nBooleanQuery：BooleanQuery中没有任何子Query是不用缓存的\nDisjunctionMaxQuery：DisjunctionMaxQuery中没有任何子Query是不用缓存的\n\n满足了Query条件后，会将当前Query（Query对象的HashCode）添加到LRU算法中，并且当前Query为cache中最近最新使用，为了后面执行LRU算法做准备。\n 索引文件条件\n根据当前的索引文件条件决定是否允许缓存，比如说存放DocValues的.dvm、.dvd文件在更新后，那么就不允许缓存。\n 段（Segment）条件\n即使满足Query条件、索引文件条件，还要考虑当前段中的条件，条件跟当前段中包含的文档数量相关：\n\n最坏的情况下缓存所有子IndexReader中所有的文档号需要的内存量（worstCaseRamUsage）、目前最大可使用的内存量（totalRamAvailable：），这个值即上文中的maxRamBytesUsed，当满足(worstCaseRamUsage * 5) &lt; totalRamAvailable时就允许缓存\n当前子IndexReader中包含的文档号数量 perReaderDocNum ≥ minSize(默认值10000)，并且 perReaderDocNum占所有子IndexReader的文档总量totalDocNum满足 (perReaderDocNum / totalDocNum ≥ minSizeRatio (默认值0.03))， 其中minSize跟minSizeRatio可配\n\n 子IndexReader条件\n不是所有的IndexReader都适合缓存，比如说facet中读取taxonomy的OrdinalMappingLeafReader，在以后的文章中介绍facet会给出原因。\n 线程竞争条件\n当多个线程使用同一个IndexSearcher对象，那么cache就会成为临界区，当前线程如果访问cache发现已被其他线程占用，源码中的处理方式是不等待锁资源，即不使用LRUQueryCache，原因是在高并发下，查询被阻塞的时间可能跟查询个数成正比，反而降低了查询性能。锁资源被占用的情况有以下几种：\n\n其他线程正在添加一个cache，并且这个cache有可能不是当前线程的Query的cache\n其他线程正在读取一个cache，源码中使用的不是读写锁，所以即使当前线程可能也是读操作，也无法访问cache\n\n 不存在缓存？\n当满足缓存条件后，继续下面的流程\n图5：\n\n如果存在缓存，那么直接取出缓存就可以退出了，需要重复的是，返回的结果只是文档号集。\n图6：\n\n如果不存在缓存，那么我们需要增加缓存，但是增加缓存还存在一些额外条件：\n 允许增加缓存条件\n Query条件\n这里的Query条件跟上文中的Query条件是一样的，这里还要继续检查一遍当前的Query是否需要缓存，因为如果某个Query使用多线程在多个子IndexReader中并行查询，由于这些线程使用同一个Weight对象，并且在上文中的Query条件检查中会将当前Query添加到LRU算法中，为了避免重复添加造成错误的计数(相同Query的历史查询计数，下文会介绍)，所以在上文中的Query条件除了第一个线程，其他线程会跳过这一步骤，故在这里需要检查Query条件。\n 历史查询条件\n在满足Query条件的前提下，并且同时满足历史查询计数打到阈值，才允许增加缓存，不同的Query对象的阈值是不同的，目前Lucene 7.5.0版本中域值根据Query对象有以下几种数值：\n\n2：如果是MultiTermQuery、MultiTermQueryConstantScoreWrapper、TermInSetQuery、PointQuery(点数据类型的所有查询)，那么这些Query在历史查询中出现过2次，就允许增加缓存\n4：如果是BooleanQuery、DisjunctionMaxQuery，那么这些Query在历史查询中出现过4次，就允许增加缓存\n5：其他类型的Query\n\n 执行查询，保存结果\n当满足允许增加缓存条件后，就可以执行一次常规的查询，获得查询结果后，即文档号集，存放到FixedBitSet，即缓存。\n 执行LRU?\n图7：\n\n上文中提到，缓存的个数跟占用内存量是有上限限制的，每当添加一个缓存后，会判断是否需要执行LRU算法来剔除某个旧的缓存或者直接添加新的缓存。在随后的文章中会详细介绍在Lucene中LRU算法的实现，因为这不是LRUQueryCache专有的功能，它属于一个Util类，出于正确分类目的，会另外写一篇文章。\n 结语\n文章中一些细节并没有详细介绍，比如说 为什么有些IndexReader不允许缓存、哪些IndexReader不允许缓存、为什么段文件更新后不允许缓存，在后面的文章中会解释这些问题。\n点击下载Markdown文件\n","categories":["Lucene","Search"],"tags":["LRU","cache"]},{"title":"LZ4","url":"/Lucene/yasuocunchu/2019/0226/LZ4/","content":" LZ4\nLZ4是一种无损数据压缩算法，着重于压缩和解压的速度，并且应用广泛。在Hadoop、Linux内核、文件系统都有应用，而在Lucene中，则是使用LZ4对倒排表的数据以及词向量（termVector）进行压缩存储。在本篇文章中，介绍LZ4Fast的压缩逻辑在Lucene中的Java实现。\n 两种实现\nLucene中提供了两种LZ4的算法实现，分别是LZ4Fast跟LZ4High：\n LZ4Fast\n本文介绍的就是LZ4Fast，它是原生的LZ4算法的实现，性能跟内存开销小于LZ4High，最多使用16KB的内存。\n LZ4High\nLZ4High相比较LZ4Fast，它的压缩速度较慢，并且占用更多的内存(每个线程占用~256KB)，但是它提供了更高的压缩比，处理很大的数据时更能体现压缩比的优势。可能会随后的博客中更新：）。\n 流程简介\n压缩过程分为两步\n 步骤一：计算hash，找到相同的数据区间\n 步骤二：将相同的数据区间进行压缩存储\n 例子\n我们通过一个例子，来介绍LZ4Fast的压缩的实现：\n图1：\n\n图2：\n\n 找到第一个锚点（anchor）\n刚开始处理时，下标值为0的地方为第一个锚点\n 找到第二个锚点（anchor）\n第一个锚点的值为0，我们最终目的是获得第二个锚点的值，第二个锚点跟第一个锚点之间的数据进行压缩处理。在这个区间的数据，有两个子区间的数据是一样的，那么需要对这两个子区间进行压缩存储。由于注意的是，两个字区间可能有重叠部分。\n 处理下标值为0的数组元素\n图3：\n\nencodeArray[] 数组是经过压缩处理的数组。\n取出下标值为0以及后面的三个数组元素，4个字节组成一个int类型的数值，对这个数值进行散列，如果发生冲突，说明可能之前散列过相同的值，由于这里是第一个数据，所以不会发生这种情况。\n 处理下标值为1的数组元素\n图4：\n\n同上一步操作，类似滑动窗口的操作，窗口大小为4个字节，每一次右移一个字节，然后我们计算下标值1~下标值4的数组元素组成的int值，同样进行散列。\n 处理下标值为2、3、4的数组元素\n同样的，还是没有发生hash冲突。\n 处理下标值为5的数组元素\n图5：\n\n此时发生冲突，由于不同的值可能会有相同的hash值，所以这里还要继续判断下标值03的数组元素对应的int值是否跟下标值58的数组元素对应的int值是否一样。在这里，的确这两个区间R1、R2的数据是一模一样的，那么需要开始进行压缩存储。\n 找出相同数据的最大区间\n目前我们找到了 数组中下标值0~3跟 5~8两个相同的区间R1、R2，接着扩大这两个区间，直到满足两个区间数据相同，且是最大的区间。\n图6：\n\n注意的是，这里两个区间有重叠的区域，即下标值5~9的区域。\n 计算token值\nmatchOff：\n该值是相同数据区间R2的第一个元素的下标值，即5。\nliteralLen：\n由于直到matchOff位置才开始有相同的数据区间，所以anchor（当前值0）至matchOff的区间的数据需要原封不动的写入到encodeArray[]数组中，而literalLen代表了这段数据的长度。matchOff与anchor的差值就是literalLen，即 literalLen = matchOff- anchor，当前锚点anchor的值Wie0，所以literalLen的值为5。\nmatchLen：\nmatchLen描述了两个相同区间R1跟R2相同数据的个数，即 10。\ntoken值是literalLen跟matchLen的组合值，利用组合存储\nfinal int token = (Math.min(literalLen, 0x0F) &lt;&lt; 4) | Math.min(matchLen - 4, 0x0F);\n当前的literalLen跟matchLen-4的值没有大于0x0F,所以token = (5 &lt;&lt; 4) | (10 - 4), 即 86，二进制就是0b01010110。后面的处理过程中会有literalLen跟matchLen-4的值大于0x0F的情况。\nmatchDec：\nmatchDec是R2跟R1的两个区间第一个元素所在下标值的差值，即 5 - 0 = 5。这个值在解压的时候会作为遍历的一个条件。由于matchDec是个int类型，并且这个值不会大于（1 &lt;&lt; 16）,所以需要两个字节分别存储低8位跟高8位在解压时，根据matchDec跟matchLen的大小，恢复数据的过程略有不同，后面会介绍。\n 写入token值、anchor至matchOff的数据、matchDec\n图7：\n\n上图中 原始数据即anchor至matchOff之间的数据。\n 找到第三个锚点（anchor）\n上面的流程结束后，更新第二个锚点的值为15。按照同样的逻辑找到第三个锚点。\n 处理下标值为15~41的数组元素\n都没有发生hash冲突的情况。\n 处理下标值为42的数组元素\n图8：\n\n此时发生冲突，由于不同的值可能会有相同的hash值，所以这里还要继续判断下标值14的数组元素对应的int值是否跟下标值4245的数组元素对应的int值是否一样。在这里，的确这两个区间R1、R2的数据是一模一样的，那么需要开始进行压缩存储。\n 找出相同数据的最大区间\n然后下标值5跟下表值46的数组元素就不相同了，所以相同数据的最大区间如下图的R1跟R2\n图9：\n\n 计算token值\nmatchOff：\n该值是相同数据区间R2的第一个元素的下标值，即42。\nliteralLen：\n由于直到matchOff位置才开始有相同的数据区间，所以anchor（当前值15）至matchOff的区间的数据需要原封不动的写入到encodeArray[]数组中，而literalLen代表了这段数据的长度。matchOff与anchor的差值就是literalLen，即 literalLen = matchOff- anchor，当前锚点anchor的值为15，所以literalLen的值为27。\nmatchLen：\nmatchLen描述了两个相同区间R1跟R2相同数据的个数，即 4。\ntoken值是literalLen跟matchLen的组合值，利用组合存储\nfinal int token = (Math.min(literalLen, 0x0F) &lt;&lt; 4) | Math.min(matchLen - 4, 0x0F);\n当前的literalLen大于0x0F ,所以token = (15 &lt;&lt; 4) | (4 - 4), 即 240，二进制就是0b11110000。\nmatchDec：\nmatchDec是R2跟R1的两个区间第一个元素所在下标值的差值，即 42 - 1 = 41。这个值在解压的时候会作为遍历的一个条件。由于matchDec是个int类型，并且这个值不会大于（1 &lt;&lt; 16）,所以需要两个字节分别存储低8位跟高8位在解压时，根据matchDec跟matchLen的大小，恢复数据的过程略有不同，后面会介绍。\n 写入token值、anchor至matchOff的数据、matchDec\n图10：\n\n注意encodeArray[]数组中下标值为9的数组元素，由于token值只有高4位用来表示原始数据的长度，即最大值为15，所以literalLen的值超出的部分需要额外的一个字节存储，当前literalLen的值为27，所以额外的值为 27 - 15 = 12。\n 找到第四个锚点（anchor）\n上面的流程结束后，更新第三个锚点的值为46。按照同样的逻辑找到第四个锚点\n 处理下标值为46的数组元素\n图11：\n\n 找出相同数据的最大区间\n图12：\n\n 计算token值\nmatchOff：\n该值是相同数据区间R2的第一个元素的下标值，即46。\nliteralLen：\n由于直到matchOff位置才开始有相同的数据区间，所以anchor（当前值46）至matchOff的区间的数据需要原封不动的写入到encodeArray[]数组中，而literalLen代表了这段数据的长度。matchOff与anchor的差值就是literalLen，即 literalLen = matchOff- anchor，当前锚点anchor的值为46，所以literalLen的值为0。说明不用写原始数据到encodeArray[]数组中\nmatchLen：\nmatchLen描述了两个相同区间R1跟R2相同数据的个数，即 22。\ntoken值是literalLen跟matchLen的组合值，利用组合存储\nfinal int token = (Math.min(literalLen, 0x0F) &lt;&lt; 4) | Math.min(matchLen - 4, 0x0F);\n当前的matchLen为22，大于0x0F ,所以token = (0 &lt;&lt; 4) | 15, 即 15，二进制就是0b00001111。matchLen多出来的部分需要再用一个字节存储。\nmatchDec：\nmatchDec是R2跟R1的两个区间第一个元素所在下标值的差值，即 46 - 15 = 31。这个值在解压的时候会作为遍历的一个条件。由于matchDec是个int类型，并且这个值不会大于（1 &lt;&lt; 16）,所以需要两个字节分别存储低8位跟高8位在解压时，根据matchDec跟matchLen的大小，恢复数据的过程略有不同，后面会介绍。\n图13：\n\n注意encodeArray[]数组中下标值为42的数组元素，由于token值只有低4位用来表示原始数据matchLen的长度，即最大值为15，所以matchLen的值超出的部分需要额外的一个字节存储，当前matchLen的值为22，所以额外的值为 22 - 15 - 4 = 3。\n另外literalLen的值为0，说明这个区间内的数据已经存储过了。\n 最后5个字节的处理\n最后5个字节在上面的流程中不会被处理，LZ4Fast算法会对最后5个字节单独处理。上面的流程结束后，更新第三个锚点的值为68。同样需要存储token，不过token的值的计算方式为\nfinal int token = Math.min(literalLen, 0x0F) &lt;&lt; 4;\n 计算token值\n还有最后5个字节，所以literalLen的值为5，所以token的值为 （5 &lt;&lt; 4）= 80。\n 写入token值、最后5个字节的原始数据\n图14：\n\n至此，array[]数组压缩成encodeArray[]数组的过程结束。\n 压缩后的数据结构如下：\n图15：\n\n 结语\n本文通过一个例子实现了LZ4Fast的压缩过程，由于篇幅原因上文中如何计算hash的过程并没有给出，并且没有理解源码中提供的hash函数的含义，以及为什么要留最后的5个字节单独处理。在随后的博客中，会介绍解压过程。\n点击下载Markdown文件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode","lz4"]},{"title":"LogMergePolicy","url":"/Lucene/Index/2019/0513/LogMergePolicy/","content":"点击这里\n","categories":["Lucene","Index"],"tags":["merge","mergePolicy"]},{"title":"Lucene核心技术（一）","url":"/Lucene/Index/2021/0201/Lucene%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89/","content":" Lucene核心技术\n  本篇文章通过ppt介绍Lucene：\n Lucene核心技术（一 ）.pptx\n","categories":["Lucene","Index"],"tags":["lucene"]},{"title":"MemoryIndex（一）（Lucene 8.8.0）","url":"/Lucene/Index/2021/0426/MemoryIndex%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  从本篇文章开始介绍Lucene中提供的基于内存的搜索引擎的内容。我们通过MemoryIndex类的注释简单的了解下。\n图1：\n\n  图1中中包含了两个重点内容：\n\n单文档：使用MemoryIndex生成的索引是单文档（single-document）的，所有的索引数据都在同一篇文档中\n使用场景：不适用于数量大并且需要持久化的数据场景；适用于数量较小、瞬时实时的数据、查询频繁的场景，在随后的文章中，我们会介绍MemoryIndex在Elasticsearch中的应用场景\n\n 内存索引\n图2：\n\n  从图1中可以看出，内存索引的层次结构首先按照域名划分，属于同一个域名的索引又按照Posting、Terms、PointValues、FIeldInfo、Norm、DocValues划分（只列出了部分关键内容）。\n  在源码层面，使用了一个Map来存放所有的内存索引：\n图3：\n\n  图3中的TreeMap的key跟value跟图2中的关系如下所示，即域名为key，该域名对应的索引信息为value，在代码中用Info对象存储：\n图4：\n\n 内存索引的数据结构\n  接着我们对图4中用蓝色虚线框标注的几个不同类型索引信息分别介绍下其数据结构。\n Posting\n  Posting中存放的是倒排信息，基于磁盘的索引中的倒排信息包含了文档号docId、词频frequency、位置position、偏移offset（offset是一个二元组，即startOffset、endOffset）、负载payload共五类信息，由于我们在上文中说到生成的索引在一篇文档中，所以基于内存的索引中只要存储除去文档号docId的其他四种信息即可。\n  在源码中，使用了IntBlockPool类来存储Posting的位置position、偏移offset、负载payload这三类信息，而词频frequency则是使用一个额外的数组存放，在下文中介绍termId时再展开介绍。\n  对于IntBlockPool类，实际上使用了一个int类型的二维数组buffers来存储Posting。\n  对于某个域名对应的Posting，其数据结构如下：\n图5：\n\n  由于偏移offset、负载payload可以分别使用参数来决定是否存储，如下所示。故在不同参数下，一共有图5中三种Posting的数据结构。\n图6：\n\n  另外在图5中可以看到，并没有真正的存储payload的值，而是存放了一个索引值payloadIndex，该索引值指向另一个数据结构中的某个位置，在那个位置存放着payload的值，后面的文章中会详细展开。如果没有payload值，那么payloadIndex的值置为-1。\n 哨兵值\n  将term的Posing信息写入到二维数组buffers之前，会在二维数组buffers中预分配一段空间连续的分片，哨兵值位于分片中的最后一个位置。\n  哨兵值有两个作用：\n\n作用一：用来描述当前分片是否无法再存储更多的Posting信息。在写入到分片的过程中，如果遇到了哨兵值，说明需要再分配一个新的分片\n\n二维数组buffers中每个数组元素都会被初始化为0，而哨兵值是一个非0的值，故可以通过判断即将写入的位置的数组元素是否为0\n\n\n作用二：哨兵值用来描述新分配的分片大小\n\n 新的分片大小的分配规则\n  在存储域值时，会使用到ByteBlockPool对象存储，该对象中也有相同的哨兵值的概念，其分配规则也是类似，在后面的文章中我们再详细展开。在本文中，我们只需要知道。除了第一次分配的分配大小为2，随后新的分片的大小总是为4，其原因同样会在后面的文章中展开。\n termId\n  termId是从0开始的递增值，用来唯一代表一个term。termId从0开始递增，每处理一个新的term，termId执行+1操作，新的termId即代表这个term。\n  如果我们依次处理Jack， Jay， Jay，lily， lucy， lily 这几个term，那么这些term对应的termId如下所示：\n表一：\n\n\n\nterm\ntermId\n\n\n\n\nJack\n0\n\n\nJay\n1\n\n\nJay\n1\n\n\nlily\n2\n\n\nlucy\n3\n\n\nlily\n2\n\n\n\n  在介绍了termId之后，我们就可以介绍上文中提到的term的词频frequency的存储方式。\n  由于有了termId的概念，故使用了一个名字为freq的int 类型的数组存储即可。其中数组下标为termId，数组元素为term的词频frequency。该数组在代码中的定义如下所示：\n图7：\n\n  图7中的SliceByteStartArray对象实际也是Info（见图4）对象中的一个变量。\n  SliceByteStartArray对象中的另外两个数组，start[ ]跟end[ ]数组，他们的数组下标同样是termId，数组元素分别描述了term的Posting信息在二维数组buffers（见图5）的开始跟结束位置。因为所有term的Posting信息都存放在同一个二维数组buffers中，故在读取阶段，通过start[ ]跟end[ ]数组获得某个term的Posting信息在二维数组buffers中的起始读取位置跟结束位置。下文会通过例子详细介绍SliceByteStartArray对象。\n BytesRefHash\n  上文中介绍了term的词频frequency、位置position、偏移offset、负载payload的存储方式，term自身的值也是需要存储的，它使用BytesRefHash对象存储。在读取阶段，根据termId就能从BytesRefHash对象中获得term的值。其写入跟读取的方式在文章ByteRefHash中已经详细介绍，故本文不赘述。\n 例子\n  我们通过一个例子来介绍term的倒排信息是如何存储的。\n图8：\n\n  图8中，红框标注的两个参数都是true，意思是存储偏移offset跟负载payload的值。\n 写入逻辑\n  在介绍例子之前，我们先概述下写入的逻辑的核心过程：\n\n分片预分配：如果term从未处理过，那么在二维数组buffers中预分配一个分片，然后写入term的Posting信息\n\n预分配的分片，第一次分配的分片大小只有2个数组元素大小。因为在图8中两个参数都为false的情况下，只需要存储位置position，那么再上哨兵值，故只需要2个数组\n\n\n分片再分配：如果分片无法再存储更多的Posting信息，那么重新分配一个新的分片\n\n在往分片写入的过程中，如果遇到了哨兵值，说明当前分片无法存储更多的信息，需要再分配一个新的分片。注意的是新的分片不一定在当前分片的下一个位置，故将当前分片的哨兵值改为一个索引值，即二维数组buffers的下标值，该值对应的位置即新的分片在二维数组中的起始位置。下文的例子中会介绍这种情况\n\n\nPosting写入顺序：位置position --&gt; 偏移Offset（startOffset、endOffset）–&gt; 负载payload\n\n  根据图8的代码的，域名为&quot;author&quot;的域值在分词后将会生成四个term，并且处理的顺序分别为 jay --&gt; lily --&gt; jay --&gt; lucy。那么首先这几个term对应的termId以及Posting信息如下表所示：\n表二：\n\n\n\nTerm\nTermId\n词频frequency\n偏移Offset\n位置position\n负载payload\n\n\n\n\njay\n0\n2\n[0, 3], [9, 12]\n0， 2\n-1\n\n\nlily\n1\n1\n[4, 8]\n1\n-1\n\n\nlucy\n2\n1\n[13, 17]\n3\n-1\n\n\n\n  表二中，为了便于介绍，我们直接列出了所有term 的倒排信息。另外图8中的例子中没有设置负载payload，所以对应的payloadIndex为-1。在实际处理过程中，是通过分词器获取的。\n 添加第一个term：jay\n  由于jay这个term第一次处理，故在二维数组buffers中预分配大小为2个数组元素大小的连续空间：\n图9：\n\n 写入位置position\n图10：\n\n 写入偏移Offset、payload\n  由于当前分片的下一个写入位置是个哨兵值，故先进行分片再分配，新的分片的大小固定为4个数组元素大小。接着当前分片的哨兵值被替换为一个索引值index，用来描述新的分片在二维数组buffers中的起始位置，即2，然后写入偏移Offset的startOffset跟endOffset：\n图11：\n\n 写入SliceByteStartArray\n  在写入Posting的过程中，会将当前处理的term在二维数组buffers中的起始、结束位置、词频frequency写入到SliceByteStartArray对象（见图7）中：\n图12：\n\n 添加term：lily\n  添加的过程跟添加第一个jay是一样的，故直接给出结果：\n图13：\n\n 添加第二个term：jay\n 写入位置position\n  由图13可知，当前分片无法存储更多的数据，故先分配一个新的固定大小为4的分片，当前分片的哨兵值更新为12，然后写入位置position。注意的是SliceByteStartArray对象中的信息在写完便宜offset、负载payload后才更新\n图14：\n\n 写入偏移Offset、payload\n  由图3可知，写入偏移offset、payload需要3个数组元素大小，故在写完偏移offset后，先分配新的分配，修改哨兵值为16，然后写入payloadIndex，最后更新SliceByteStartArray中的end、freq数组：\n图15：\n\n 添加term：lucy\nemsp; 添加的过程跟添加第一个jay是一样的，故直接给出结果：\n图16：\n\n 结语\n  本文简单的介绍了MemoryIndex的概念，以及如何存储倒排信息的内容。在随后的文章将进一步介绍term自身的值、负载payload，以及其他类型索引文件的内容。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["index","memoryIndex"]},{"title":"近实时搜索NRT（一）","url":"/Lucene/Index/2019/0916/NRT%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  Lucene提供了近实时搜索NRT（near real time）的功能，它描述了索引信息发生改变后，不需要执行commit操作或者关闭IndexWriter（调用IndexWriter.close()方法）就能使得这些更改的信息**很快（quickly）**变得可见。\n  &quot;很快&quot;意味着不是马上变得可见，Lucene无法保证更改索引信息后，在某个确定的时间之后，使得最新的索引信息变得可见，这取决具体的业务。\n  近实时搜索NRT通过DirectoryReader来实现，故下面的文章中将会介绍DirectoryReader的实现原理。\n DirectoryReader\n  通过DirectoryReader，我们可以读取索引目录中已经提交（执行commit操作）或者未提交（执行flush操作）的段的信息，IndexSearcher通过DirectoryReader包含的信息来进行查询，它是一个抽象类，其类图如下所示，在下面的内容中会详细介绍DirectoryReader如何生成的：\n图1：\n\n  上图中，DirectoryReader有两个子类StandardDirectoryReader、FilterDirectoryReader，其中StandardDirectoryReader是Lucene7.5.0中默认的DirectoryReader具体实现，FilterDirectoryReader作为一个抽象类，它的两个具体实现ExitableDirectoryReader、SoftDeletesDirectoryReaderWrapper通过封装其他的DirectoryReader对象，来实现功能扩展。下面我们一一介绍图1中的每一个类。\n StandardDirectoryReader\n  StandardDirectoryReader作为Lucene7.5.0中默认且唯一的DirectoryReader类的具体实现，我们需要详细的来了解它，该类的对象可以通过个调用以下四个方法来获得：\n\n方法一：DirectoryReader.open(final Directory directory)\n方法二：DirectoryReader.open(final IndexCommit indexCommit)\n方法三：DirectoryReader.open(final IndexWriter indexWriter)\n方法四：DirectoryReader.open(final IndexWriter indexWriter, boolean applyAllDeletes, boolean writeAllDeletes)\n\n  其中通过调用方法三&amp;&amp;方法四的方法实现了NRT功能，而方法一&amp;&amp;方法二则没有,下文将会描述它们之间的差异。\n 获取StandardDirectoryReader对象的流程图\n  尽管提供了4种方法，但这些方法的实现原理都可以在一张流程图中表示：\n图2：\n\n点击查看大图\n  我们先介绍方法一&amp;&amp;方法二的所有流程点。\n 统一入口\n图3：\n\n  IndexCommit是什么：\n\n执行一次提交操作（执行commit)方法）后，这次提交包含的所有的段的信息用IndexCommit来描述，其中至少包含了两个信息，分别是segment_N文件跟Directory\n\n  如何获得IndexCommit：\n\n在文档提交之commit（二）的文章中，我们提到了一种索引删除策略SnapshotDeletionPolicy，在每次执行提交操作后，我们可以通过主动调用SnapshotDeletionPolicy.snapshot()来实现快照功能，而该方法的返回值就是IndexCommit\n\n 获取segment_N文件\n图4：\n\n  该流程只有方法二才会执行，图4中的多个流程，执行它们的最终目的是为了获得索引目录（根据方法二的参数Directory对象获得索引目录）中的segment_N文件。\n  在图4的流程中，Lucene连续两次获取索引目录中的所有文件，获得两个文件名集合file、file2，在分别对file、file2进行排序后，通过比较两个集合是否包含相同的文件名来判断是否当前索引目录是否有频繁的新的写入操作，如果有，那么通过重试的方法，直到file、file2是相同的文件集合，由于这段代码较为简洁，故直接给出：\nString files[] = directory.listAll();String files2[] = directory.listAll();Arrays.sort(files);Arrays.sort(files2);if (!Arrays.equals(files, files2)) &#123;// listAll() is weakly consistent, this means we hit &quot;concurrent modification exception&quot;continue;&#125;\n  在上面的代码中，其中directory即方法二的参数。\n  从上面的代码我们可以看出哪些信息：\n\n信息一：在多线程下，当频繁的有修改索引信息的操作时，获取一个读取索引文件的操作可能会有较高的延迟（files跟files2一直无法相等）\n信息二：从图1的流程中可以看出，方法二的流程并没有同步操作，故即使在某一时候files跟files2相等，跳出图4的重试操作，有可能索引信息马上被别的线程修改了，故在不同步索引修改方法（见文档的增删改）的前提下，不一定能获得最新的索引信息，该方法至少能保证获得调用方法二之前的索引信息\n\n  如何根据文件集合files来获得最后一次提交的segment_N文件：\n\n每次提交都会生成segment_N，其中N是一个递增的值，它描述了一个段的编号，即最新的提交对应的N值是最大的，Lucene通过遍历files中的每一个文件名，取出前缀为&quot;segment&quot;，并且段编号最大的文件，该文件即最后一次提交的segment_N文件，图5中，执行了两次提交操作，并采用索引删除策略NoDeletionPolicy（见文档提交之commit（二）），故索引目录中保留了2个segment_N文件，并且最后一次提交的segment_N文件是segment_2\n\n图5：\n\n 获得所有段的信息集合SegmentInfos\n图6：\n\n  在前面的流程中，无论是方法一还是方法二，到达此流程点时，已经获得了segment_N文件，那在图6的流程中，我们根据segment_N获取段的信息集合segmentInfos，该集合在前面的文章中已经多次介绍，它包含的一个重要信息是一个链表，链表元素是每一个段的信息：\nprivate List&lt;SegmentCommitInfo&gt; segments = new ArrayList&lt;&gt;();\n  SegmentCommitInfo的数据分散在两个索引文件中，如图7所示：\n图7：\n\n点击查看大图\n  图7中，两块黄色的框标注的内容即SegmentCommitInfo的信息，在读取segment_N阶段，先读取出SegmentCommitInfo的第一块数据，然后根据第一块数据的中的SegName（该字段的含义见segment_N，生成segment_N的时机见文档提交之commit（二））从索引文件.si（生成索引文件.si的时机见文档提交之flush（三））中读取出SegmentCommitInfo的第二块数据，两块数据组成完整的SegmentCommitInfo的信息，在源码中，这些信息即SegmentCommitInfo类的成员变量。\n 获得StandardDirectoryReader\n图8：\n\n  在前面的流程中，我们获得了每一个段的SegmentCommitInfo，在Lucene中，将SegmentCommitInfo再次封装为SegmentReader，然后将所有段对应的SegmentReader最后封装为StandardDirectoryReader。\n SegmentReader\n  SegmentReader类不展开作介绍了，我们只需要知道该类描述的是一个段的所有的信息，该信息通过封装的SegmentCommitInfo来描述。\n StandardDirectoryReader\n  所有段对应的SegmentReader集合以数组方式LeafReader[ ]被封装在StandardDirectoryReader中，如下图所示：\n图9：\n\n点击查看大图\n  图9中描述了StandardDirectoryReader中包含的最重要的一些数据，其中LeafReader[ ]数组中的元素个数跟索引文件segment_N中的黑框字段SegmentCommitInfo的个数是一致的。\n 结束\n图10：\n\n  对于方法一&amp;&amp;方法二，严格的来讲，至此我们获得了流程点获得所有段的信息集合SegmentInfos之前索引目录中最新的索引数据，由于其他线程可能通过IndexWriter并发的执行更改索引的操作，所以在多线程下，方法一&amp;&amp;跟方法二并不能实现NRT的功能。\n 结语\n  基于篇幅，剩余的内容在一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["NRT","Search"]},{"title":"近实时搜索NRT（三）","url":"/Lucene/Index/2019/0920/NRT%EF%BC%88%E4%B8%89%EF%BC%89/","content":" 近实时搜索NRT（三）\n  在近实时搜索NRT（二）的文章中我们提到，Lucene提供了四种方法来获得StandardDirectoryReader对象，这里先简单总结下这四种open方法获取StandardDirectoryReader的差异：\n\n方法一：DirectoryReader.open(final Directory directory)\n\n根据索引目录Directory中N值最大的segment_N文件（即最近的一次commit）获取已经提交的索引信息，未commit的文档的索引信息无法被读取，即使已经flush但未提交的文档的索引信息也无法被读取，故这种获取StandardDirectoryReader的方法不属于NRT\n\n\n方法二：DirectoryReader.open(final IndexCommit indexCommit)\n\n我们如果使用封装了索引删除策略NoDeletionPolicy的SnapshotDeletionPolicy，我们可以记录每一次的提交，即IndexCommit（见近实时搜索NRT（一）），在索引目录中会生成多个segment_N文件，使用此方法可以获得任意一次提交的索引信息，可以用来回溯操作，故这种获取StandardDirectoryReader的方法不属于NRT\n\n\n方法三：DirectoryReader.open(final IndexWriter indexWriter)\n\n该方法实际调用了方法四，区别在于另方法四中的两个参数applyAllDeletes为true、writeAllDeletes为false\n\n\n方法四：DirectoryReader.open(final IndexWriter indexWriter, boolean applyAllDeletes, boolean writeAllDeletes)\n\n由于在生成IndexWriter对象阶段会先读取索引目录中已有的索引信息（旧的索引信息），并且更改索引（新的索引信息）需要通过IndexWriter对象，故通过IndexWriter对象能获得索引目录中所有的索引信息，故方法三跟方法四获取StandardDirectoryReader的方法属于NRT\n\n\n\n  在近实时搜索NRT（一）、近实时搜索NRT（二）中我们了解到，无论调用哪一个上述介绍的方法，其相同的逻辑都是将一个段的信息segmentCommitInfo封装为一个LeafReader，最后将多个LeafReader封装为StandardDirectoryReader，当索引目录中的索引信息发生更改时，我们可以通过重新调用上述的方法来获得最新的StandardDirectoryReader，但是基于下面的几个考虑，Lucene提供了性能更高的openIfChanged方法来获得最新的StandardDirectoryReader：\n\n只将部分发生更改的段生成LeafReader，即仅替换StandardDirectoryReader中的那些发生变更的LeafReader\n如果索引信息没有发生变化，那么就直接返回StandardDirectoryReader，而不用执行上述四个方法的所有流程\n\n  Lucene7.5.0中提供了以下四种openIfChange方法，这四种方法：\n\n方法一：DirectoryReader.openIfChanged(DirectoryReader oldReader)\n方法二：DirectoryReader.openIfChanged(DirectoryReader oldReader, IndexCommit commit)\n方法三：DirectoryReader openIfChanged(DirectoryReader oldReader, IndexWriter writer)\n方法四：DirectoryReader openIfChanged(DirectoryReader oldReader, IndexWriter writer, boolean applyAllDeletes)\n\n openIfChange方法的流程图\n  其中openIfChange的方法一&amp;&amp;方法二、方法三&amp;&amp;方法四的逻辑需要用两个流程图图1、图2展现：\n图1：\n\n点击查看大图\n图2：\n\n点击查看大图\n  在介绍每一个流程点之前，我们先大致先说下图1、图2的流程，两个流程图总体描述了旧的StandardDirectoryReader，即oldReader（DirectoryReader的子类，见近实时搜索NRT（一））在经过一系列的流程后，判断是否需要生成新的StandardDirectoryReader，如果不需要那么返回null，否则生成一个新的StandardDirectoryReader，生成的新的StandardDirectoryReader的方法有两个，即图1、图2中用红色跟紫色标注的两个流程点，而这两种获取新的StandardDirectoryReader的方法即近实时搜索NRT（一）、近实时搜索NRT（二）介绍的四种open方法，如下图所示：\n图3：\n\n点击查看大图\n  下面我们先介绍图1中的每一个流程点。\n IndexWriter是否为空？\n图4：\n\n  IndexWriter是否为空描述的是旧的StandardDirectoryReader，即oldReader中是否持有IndexWriter对象的引用。\n  如何判断oldReader中是否持有IndexWriter对象的引用：\n\n上文中我们知道四种获取StandardDirectoryReader的open方法中，方法三&amp;&amp;方法四的参数带有IndexWriter对象，故通过这两个方法获得的oldReader中会持有IndexWriter对象的引用，同理，通过方法一&amp;&amp;方法二获得的oldReader中不持有IndexWriter对象的引用\n\n 根据Directory判断索引是否发生变化？\n图5：\n\n  如果oldReader是通过open方法中的方法一或者方法二获得的，并且调用openIfChange的方法一，由于该方法没有indexCommit参数，故该方法会执行图5中的流程。\n  方法一只提供了一个oldReader的参数，并且没有持有IndexWriter对象的引用，所以判断是否需要获取新的StandardDirectoryReader的方式为根据索引目录中最后一次commit，对比最后一次commit对应的SegmentInfos对象跟oldReader中的SegmentInfos的版本号Version是否一致，如果不一致，说明索引信息在生成oldReader之后发生了变化，那么我们需要重新生成一个新的StandardDirectoryReader，否则返回null。\n  SegmentInfos是什么：\n\n见近实时搜索NRT（一）文章中的关于流程点获得所有段的信息集合SegmentInfos的介绍\n\n  SegmentInfos的版本号Version是什么：\n\nSegmentInfos的版本号描述了内存中的SegmentInfos变更状态，内存中的任意一个段发生变化都会增加版本号，当执行了一次commit后，这次commit的对应的SegmentInfos的版本号就被写入到segment_N文件中，如下图所示：\n\n  段的什么变化会引起版本号Version的变化：\n\n这块内容十分重要，由于openIfChange的方法三&amp;&amp;方法四会也会涉及版本号的内容，故我们留到后面的文章介绍，这里先挖个坑\n\n图6：\n\n  图6中，红框标注的为某次提交对应的SegmentInfos的版本号。\n  如何根据最后一次commit获得SegmentInfos：\n\n见近实时搜索NRT（一）文章中的关于流程点获取segment_N文件、获得所有段的信息集合SegmentInfos的介绍\n\n  图5中如果根据Directory判断索引发生了变化，那么通过图3的流程获得一个新的StandardDirectoryReader。\n  如何获得新的StandardDirectoryReader：\n\n当Version不一致时，说明oldReader中的SegmentInfos跟内存中的SegmentInfos信息不一致，那么需要获得新的StandardDirectoryReader，即图5中的找出变更的LeafReader流程点，由于后面的流程也有该操作，我们留到下一篇文章介绍，这里先挖个坑\n\n segment_N文件是否一致？\n图7：\n\n  如果oldReader是通过open方法中的方法一或者方法二获得的，并且调用openIfChange的方法二，由于该方法有indexCommit，那么该方法会执行图7中的流程。\n  方法二描述的是根据参数IndexCommit来获得对应的StandardDirectoryReader，另外通过比较oldReader中对应的segment_N文件名是否跟方法二的参数IndexCommit中的segment_N文件名是否相同来判断是否需要获得新的StandardDirectoryReader。\n  如何通过oldReader获得对应的segment_N文件：\n\noldReader即StandardDirectoryReader类的对象，该类中并没有一个存放segment_N的变量，而是通过StandardDirectoryReader类中的SegmentInfos对象来获得segment_N文件，SegmentInfos对象在上文中已介绍，该对象中有一个lastGeneration，Lucene通过字符串拼接（“segment_lastGeneration”）的方式获得segment_N文件的文件名\n\n  lastGeneration什么时候被赋值的：\n\n在文档提交之commit（二）的生成新的Segment_N文件流程中我们说到，该流程会生成一个generation用来设置这次提交生成的segment_N的N值，同时generation值会赋值给lastGeneration\n\n  另外IndexCommit类中有一个变量叫做segmentsFileName，该变量用来保存segment_N的文件名。\n  当判断出segment_N的文件名不一致，那么我们需要重新获得StandardDirectoryReader，即图7中的找出变更的LeafReader流程点，同样的，详细的获得过程在下一篇文章中展开。\n 结语\n  基于篇幅，剩余的流程点将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["NRT","Search"]},{"title":"近实时搜索NRT（五）","url":"/Lucene/Index/2019/0929/NRT%EF%BC%88%E4%BA%94%EF%BC%89/","content":"  本文承接近实时搜索NRT（四），继续依次介绍每一个流程点，阅读本文章需要看过文档的增删改、文档提交之flush的系列文章。\n openIfChange方法的流程图\n  其中openIfChange的方法一&amp;&amp;方法二、方法三&amp;&amp;方法四的逻辑需要用图1、图2两个流程图展现：\n图1：\n\n点击查看大图\n图2：\n\n点击查看大图\n 从IndexWriter中获取StandardDirectoryReader\n图3：\n\n  如果oldReader是通过open方法（见近实时搜索NRT（一）、近实时搜索NRT（四））中的方法三或者方法四获得，那么oldReader就持有IndexWriter对象（见近实时搜索NRT（三）），当调用了openIfChange的方法一，该方法就会执行图3中的流程，这种方法属于NRT。\n  图3的流程描述了这么一个过程：根据oldReader中持有的IndexWriter判断索引是否发生变化，即流程点根据IndexWriter判断索引是否发生变化？，发生变化则需要重新生成StandardDirectoryReader，如果生成了新的StandardDirectoryReader，那么需要再次跟oldReader作比较，即流程点新旧StandardDirectoryReader是否一致？，如果不相同，那么返回这个新的StandardDirectoryReader，否则返回null。\n  根据IndexWriter判断索引是否发生变化需要判断下面四个条件，依次同时满足视为没有发生变化：\n\n条件一：oldReader与IndexWriter他们俩分别持有的SegmentInfos的版本号Version必须相同\n\nSegmentInfos的介绍见近实时搜索NRT（一）文章中的关于流程点获得所有段的信息集合SegmentInfos的介绍\nSegmentInfos的版本号Version的介绍见近实时搜索NRT（三）\n版本号不相同意味着执行了索引目录中一个或多个旧段发生了更改，或者生成了一个或多个新段，或者生成了还未提交的索引信息（flush操作）\n\n\n条件二：目前没有正在处理的文档，该条件由四个子条件组成，所有子条件都满足视为满足条件二\n\n子条件一：numDocsInRAM的值为0\n\n每当DWPT处理一篇文档，该值会加一（见文档的增删改的系列文章），在DWPT生成一个段后，该值会减去DWPT中包含的文档数（见文档提交之flush的系列文章），如果numDocsInRAM的不为0，说明目前有些DWPT中包含的文档信息还未生成对应的段，也就说明了oldReader需要更新\n\n\n子条件二：全局删除队列deleteQueue（见文档的增删改（四））中的结点个数为0\n\n不满足子条件二说明还有未处理的删除信息，这些删除信息可能会作用（apply）索引目录中已有的段，如果结点个数不为0，说明没有一个DWPT开始执行生成段的工作（见文档提交之flush（六）），也就是没有flush操作（主动flush或者自动flush），也就是索引信息将要变化，oldReader需要更新\n\n\n子条件三：ticketQueue（见文档提交之flush（四））中的元素个数为0\n\nticketQueue不为0，说明还有某些段没有被发布（发布的含义见文档提交之flush（五））\n\n\n子条件四：pendingChangesInCurrentFullFlush的值为false\n\n由于fullFlush操作（见文档提交之flush（一））跟openIfChange操作可以是并行的，pendingChangesInCurrentFullFlush用来保证执行完fullFlush才能使得openIfChange来判断索引是否发生变化\n\n\n\n\n条件三：所有的删除信息都已经作用到其他的段\n\n在文档提交之flush（五）中我们介绍了发布生成的段的流程，该流程中从ticketQueue中依次取出FlushTicket，随后执行的两个工作分别是将删除信息作用到其他的段和将DocValues更改信息作用到其他的段，只有这个两个工作完成了，才能保证段的信息不会再被改变，否则段可能会被改变，那么oldReader需要更新\n\n\n条件四：所有的DocValues更改信息已经作用到其他的段\n\n同条件三\n\n\n\n  需要满足条件二、条件三、条件四正是说明了我们重新获得StandardDirectoryReader中包含了未提交（commit操作）的索引信息，所以这种方法属于NRT（见近实时搜索NRT（一）文章中对NRT的定义）。\n  为什么需要同时满足上面的四个条件（重要）：\n\n任何对段进行更改的操作都会更改segmentInfos的版本号Version，所以在单线程下，实际上我们只需要判断Version就能知道索引目录中的索引信息是否发生更改，然而在多线程下，其他线程可能正在执行文档的增删改操作，而这些操作就可以通过条件二、条件三、条件四来判断（至于为什么通过这三个条件判断，我想阅读过文档的增删改以及文档提交之flush的系列文章才能明白，无法通过一两句话解释清楚），故在多线程下，如果一个正在执行方法一的线程仅仅判断segmentInfos的版本号Version无法获得实时性较好的StandardDirectoryReader\n所以这也是为什么上文中提到，满足这四个条件是依次同时满足，并且判断版本号是第一个判断条件\n\n  生成了新的StandardDirectoryReader后，为什么还要跟oldReader比较是否一致（相同）：\n\n该问题即图3中的新旧StandardDirectoryReader是否一致？的流程点，它的判断方法是通过比较新旧两个StandardDirectoryReader他们分别持有的segmentInfos的版本号Version是否一致\n首先判断的目的是为了保证方法返回值的正确性，因为openIfChange方法的返回值只有两种情况，要么null要么不是null，null说明了索引信息没有发生变化，反之发生了更改，也就是新的StandardDirectoryReader跟oldReader包含相同的索引信息\n\n  为什么已经在流程点根据IndexWriter判断索引是否发生变化？已经判断出索引信息发生变化，还会出现新的StandardDirectoryReader跟oldReader包含相同的索引信息（重要）：\n\n上文中我们提到根据IndexWriter判断索引是否发生变化需要依次判断四个条件，我们考虑这么一种情况，如果满足条件一，但是不满足条件二、条件三、条件四中的一个或多个，这种情况属于调用openIfChange()方法的线程A并未修改索引信息，其他线程正在修改索引信息，如果其他线程执行更改索引的操作并没有成功更改索引内容（例如执行删除文档的操作，然后所有段中的文档没有一篇满足删除的条件，或者添加文档失败等等），意味着segmentInfos的版本号Version不会更改，那么更改操作结束后，线程一重新获得的StandardDirectoryReader跟oldReader包含相同的索引信息仍旧相同的，那么此时返回null\n\n 结语\n  至此我们介绍完了图1中的所有流程，由于图2中的流程点与图1中的流程点是重复的，所以不对图2的流程展开介绍，那么Lucene提供的性能更高获得StandardDirectoryReader对象的openIfChange()方法就都介绍结束了。\n  在下一篇文章中，我们将会继续DirectoryReader的其他子类（见近实时搜索NRT（一）的图1）。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["NRT","Search"]},{"title":"近实时搜索NRT（二）","url":"/Lucene/Index/2019/0917/NRT%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本文承接近实时搜索NRT（一），继续依次介绍每一个流程点。\n 获取StandardDirectoryReader对象的流程图\n图1：\n\n点击查看大图\n  我们继续介绍方法三&amp;&amp;方法四的所有流程点。\n  这两种是业务中最常使用，通过IndexWriter实现NRT功能的方法，在构造IndexWriter对象期间，会读取Directory，即索引目录中的已存在的索引信息（旧的索引信息），而对索引信息的更改（新的索引信息）都需要通过IndexWriter对象，故通过IndexWriter对象，我们能获得Directory中新旧索引信息，实现NRT。\n  我们将方法三&amp;&amp;方法四的流程从图1中拆解出来，并且跟文档提交之flush（一）的流程进行对比，如下图所示：\n图2：\n\n点击查看大图\n  图2中两个流程图中用红色标注的描述了他们具有具有相同的流程。\n  为什么获取StandardDirectoryReader需要执行flush的操作：\n\n执行更改索引信息操作之后，其变更的内容并不会马上生成新段或更新旧段，例如文档的添加，在主动flush或者自动flush之前（见文档提交之flush（一）），新增的文档信息被保存在DWPT（见文档的增删改（二））中；文档的删除/更新，其删除信息被保存在删除队列（见文档的增删改（四））中，只有在flush后，这些变更的信息才会生成新的段（见文档提交之flush（三）），即生成近实时搜索NRT（一）中的SegmentCommitInfo，它最终成为StandardDirectoryReader的一个LeafReader（见[近实时搜索NRT（一）](https://www.amazingkoala.com.cn/Lucene/Index/2019/0916/NRT（一）），即变更的索引信息能在搜索阶段能被读取，即NRT机制\n\n  为什么执行更改索引信息操作之后，其变更的内容并不马上生成新段或更新旧段：\n\n假设我们每次添加一篇文档，就执行flush操作，那么一篇文档就会对应生成一个段，我们按照下面的条件分别介绍其导致的后果\n\n不使用段合并：索引目录中的段的个数跟文档个数相同，在查询原理（二）的文章中我们知道，查询阶段，我们分别从每一个段中执行查询操作，其性能可想而知\n使用段合并：根据段的合并策略LogMergePolicy或者TieredMergePolicy（默认策略），会导致及其频繁的段的合并操作，合并操作最可怕的地方就是在合并结束后需要跟磁盘同步，其磁盘同步性能影响在前面的文章已经介绍（见文档提交之commit（一））\n\n\n\n 获得StandardDirectoryReader\n图3：\n\n  在图2中红色标注的流程执行结束后，新旧索引信息都生成了SegmentCommitInfo，那么我们就可以获得StandardDirectoryReader了，其获得过程跟方法一&amp;&amp;方法二的获得StandardDirectoryReader是一致的，不赘述。\n 执行flush后的工作\n图4：\n\n  该流程在前面的文章已经介绍，在源码中调用DocumentsWriterFlushControl.finishFullFlush( )的方法，详细的介绍见文档提交之flush（六）文章中的IndexWriter处理事件章节的内容。\n 执行获得reader后的工作\n图5：\n\n  Lucene在当前流程点提供一个钩子函数doAfterFlush()方法，用户可以实现自己的业务逻辑，定义如下：\n/** * A hook for extending classes to execute operations after pending added and * deleted documents have been flushed to the Directory but before the change * is committed (new segments_N file written). */  protected void doAfterFlush() throws IOException &#123;&#125;\n 尝试段合并\n图6：\n\n  由于执行了flush的操作，故索引可能发生了变化，在每一次索引发生变化后，都需要尝试判断是否需要执行段的合并操作，其判断条件依据不同的合并策略而有所不同，合并策略的文章可以看这里：LogMergePolicy、TieredMergePolicy。\n 结语\n  我们通过调用图1中的四个方法来获得索引目录中最新的索引信息，无论哪一种方法，目的就是将索引目录中每一个段的信息生成一个LeafReader，最后将LeafReader封装为StandardDirectoryReader，然而这四种方法还存在性能问题，故Lucene提供了openIfChange的方法来提高NRT的性能，具体内容将在下篇文章中展开介绍。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["NRT","Search"]},{"title":"近实时搜索NRT（四）","url":"/Lucene/Index/2019/0925/NRT%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  Lucene7.5.0中提供了以下四种open方法来获得StandardDirectoryReader的对象，这四种方法：\n\n方法一：DirectoryReader.open(final Directory directory)\n方法二：DirectoryReader.open(final IndexCommit indexCommit)\n方法三：DirectoryReader.open(final IndexWriter indexWriter)\n方法四：DirectoryReader.open(final IndexWriter indexWriter, boolean applyAllDeletes, boolean writeAllDeletes)\n\n  这四种open方法的详细介绍见近实时搜索NRT（一）、近实时搜索NRT（二）。\n  基于性能考虑，Lucene7.5.0中同时提供了以下四种openIfChange方法，这四种方法：\n\n方法一：DirectoryReader.openIfChanged(DirectoryReader oldReader)\n方法二：DirectoryReader.openIfChanged(DirectoryReader oldReader, IndexCommit commit)\n方法三：DirectoryReader openIfChanged(DirectoryReader oldReader, IndexWriter writer)\n方法四：DirectoryReader openIfChanged(DirectoryReader oldReader, IndexWriter writer, boolean applyAllDeletes)\n\n  我们接着近实时搜索NRT（三）的内容，继续介绍openIfChange方法的流程图。\n openIfChange方法的流程图\n  其中openIfChange的方法一&amp;&amp;方法二、方法三&amp;&amp;方法四的逻辑需要用两个流程图图1、图2展现：\n图1：\n\n点击查看大图\n图2：\n\n点击查看大图\n 从IndexCommit中获取StandardDirectoryReader\n图3：\n\n  如果oldReader是通过open方法中的方法三或者方法四获得，那么oldReader就持有IndexWriter对象（见近实时搜索NRT（三）），当调用了openIfChange的方法二，该方法就会执行图3中的流程。\n  在介绍图3的的流程之前，我们先介绍一下几个预备知识。\n  段的段名（segName）是什么\n\n段名是一个下划线与数字的组合值，其中数值是一个全局同步递增的值，例如&quot;_0&quot;，每当生成一个DWPT（见文档的增删改（二）），就会生成一个新的段名，在执行flush（见文档提交之flush（一））的操作后，该DWPT会生成一个段，在flush的阶段（见文档提交之flush（三））会生成索引文件.si，段名用来作为索引文件的前缀来描述他们是同一个段中的索引文件，如图4所示\n另外在commit操作后，一个段的段名会被记录在索引文件Segment_N中，如图5中红色框标注\n另外可以在近实时搜索NRT（一）的文章中看下段名的另一个作用\n\n图4：\n\n图5：\n\n  段的信息为什么会发生变化，段中的哪些信息会发生变化：\n\n我们在近实时搜索NRT（一）的文章中已经介绍了一个段的完整信息用SegmentCommitInfo来描述，它包含了两部分的信息，用黄色框标注，其中第一部分的信息和第二部分在flush阶段生成（见文档提交之flush（三）的生成FlushedSegment的流程点），如下图所示：\n\n图6：\n\n  当我们每次调用更改索引信息的方法后，例如删除操作、更新操作、软删除操作（这些操作内容见文档的增删改（一）），索引目录中每一个段可能需要被作用（apply）这些删除信息（如何作用删除信息见文档提交之flush（二）），也就是说在每一个段中，如果段的文档如果满足删除条件，那我们需要处理这些文档，而这个过程就会导致段的信息发生变化。\n  在图6中，一个段的完整信息由两部分构成，我们先看第一部分，即索引文件.si包含的信息，这些信息在索引文件之si的文章中已经介绍，根据每个字段的具体含义可以看出他们都是固定的信息，我们称之为不可变部分。\n  同样地第二部分中每一个字段的含义在索引文件之segments_N的文章中已经介绍，并且可以看出他们不是固定的信息，当段中的文档需要被删除时，DelGen、DeletionCount字段会发生变化，当执行更新DocValues操作时，FieldInfosGen、DocValuesGen会发生变化，我们称之为可变部分。\n  oldReader对应的段信息跟IndexCommit对应的段信息下面的关系：\n\n关系一：oldReader中拥有IndexCommit相同段名的段，并且这些段的信息完全相同\n关系二：oldReader中拥有IndexCommit相同段名的段，并且段的信息不相同\n关系三：oldReader中拥有某些段，而IndexCommit不拥有这些段\n关系四：IndexCommit中拥有某些段，而oldReader不拥有这些段\n\n  故方法二的功能描述的是根据上述关系，获得参数IndexCommit中每一个段对应的SegmentReader（SegmentReader的概念见近实时搜索NRT（一）），最后生成一个StandardDirectoryReader，具体操作如下：\n图7：\n\n  图7所示的流程图是图3、图1中找出变更的LeafReader，返回新的StandardDirectoryReader两个流程点的具体展开，它总体描述了这么一个过程：生成一个StandardDirectoryReader来读取IndexCommit对应的段的信息，如果可以使用oldReader中的一个或多个SegmentReader来读取IndexCommit对应的某一个段的信息，那我们就复用这些SegmentReader，否则就生成新的SegmentReader，最后将所有的SegmentReader添加到LeafReader数组中，生成一个StandardDirectoryReader（SegmentReader、LeafReader数组、StandardDirectoryReader三者之间的关系见近实时搜索NRT（一）文章中的图9），下面我们详细的介绍下图7的流程。\n 处理oldReader中不拥有的段\n图8：\n\n  图8中，我们根据方法二的参数IndexCommit取出一个SegmentCommitInfo，首先判断oldReader中是否拥有该段，判断方式即根据oldReader中是否拥有相同段名的段，如果没有找到，即满足上文中的关系四，那么我们根据该SegmentCommitInfo生成一个新的SegmentReader，添加到LeafReader数组中。\n  如何判断oldReader中是否拥有相同段名的段：\n\n在源码中，使用了一个Map来实现段名跟SegmentReader的映射，故根据段名就能判断oldReader中是否拥有某个段\n\n  当处理完IndexCommit中所有的SegmentCommitInfo以后，我们根据LeafReader数组中的SegmentReader生成一个新的StandardDirectoryReader。\n 处理oldReader中拥有的段\n图9：\n\n  当oldReader中拥有IndexCommit中相同段名的段，那么我们通过以下两个条件来判断两个段是否一样，即判断属于上文中的关系一还是关系二：\n\n条件一：两个段是否使用相同的索引文件格式\n这里的索引文件格式有两种，一种是普通索引文件格式，另一种是复合索引文件格式。我们可以通过IndexWriterConfig.setUseCompoundFile方法来设置，采用复合索引文件格式的段生成的索引文件为复合文件，对应的索引文件即.cfs、.cfe，而普通索引文件格式的段生成的索引文件则是例如.nvd、.nvm、.pos、.pay、.doc、.tim、.tip、.dim、.dii、.tvx、tvd、.fdx、.fdt、.si\n即使两个段中包含的文档信息一样的，但索引文件格式不同还是被认为是两个不相同的段\n条件二：两个段的可变部分是否是相同的\n可变部分是否相同通过比较图6中的DelGen、FieldInfosGen两个字段来判断，上文已经介绍不赘述\n\n  不同时满足上面两个条件的话，即上文中的关系二，那么执行生成一个新的SegmentReader跟SegmentReader添加到LeafReader数组中的流程，上文已经介绍。\n  如果同时满足上面的两个条件，那么就可以复用oldReader中的SegmentReader，然后增加这个SegmentReader对应的计数值，计数值描述了引用该SegmentReader的其他Reader的个数。\n  为什么要增加这个SegmentReader的计数：\n\n在我们调用openIfChange()方法后，如果获得了一个新的StandardDirectoryReader，Lucene要求用户负责关闭oldReader，由于oldReader中的某些SegmentReader可能被新的或者其他StandardDirectoryReader复用（对象引用），所以不能直接关闭这些SegmentReader，通过计数的方式，当SegmentReader对应的计数为0时，就可以真正的关闭了。\n\n  对于上文中的关系三，说明oldReader中的那些段是在生成IndexCommit之后生成的。\n 结语\n   基于篇幅，剩余的流程点将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["NRT","Search"]},{"title":"NumericDocValues","url":"/Lucene/DocValues/2019/0409/NumericDocValues/","content":"本篇文章只是介绍NumericDocValues在.dvd、.dvm文件中的数据结构，NumericDocValues的应用跟概念介绍不会在本篇文章中赘述，大家可以参考官方文档给出的介绍。.dvd、.dvm文件存放了所有DocValues的信息，所以如果在索引阶段，还添加了其他DocValues的Document，那么他们在.dvd、.dvm文件中的布局如下图：\n图1：\n\nDocValues之间的前后关系则是根据IndexWriter的添加对应的域的先后关系来确定。同理在.dvm文件中，不赘述。\n 预备知识\n在详细介绍数据结构前，先介绍在处理NumericDocValues的过程中会碰到的一些知识点。\n 固定bit位\n源码中采用固定bit位个数来存放每一个域值，利用公式 (max - min) / gcd 来计算出存储每一个域值需要的固定bit位个数。其中max是最大的域值，min是最小的域值，gcd为所有域值的最大公约数。\n gcd (greatest common divisor，最大公约数)\n使用gcd的目的在于存储域值时，尽可能的降低空间占用。\n 例子\n3个域值分别是 150、140、135，不使用gcd的情况下，按照公式(max - min)的计算出存储每一个域值需要的固定bit位个数，那么 150 - 135 = 15，15的二进制表示为 0b00001111 即每个域值需要固定的4个bit位来存储。如果我们先求出150、140、135的最大公约数，即最大公约数为5，然后按照(max - min) / gdc来计算每个域值需要的固定bit位个数，即 (150 -135) / 5 = 3, 3的二进制表示为0b00000011，那么每个域值需要的固定bit位个数只要2个即可，所以实际存储的3个域值的按照公式 (v - min) / gcd 的结果为  3  ((150 -135) / 5 = 3)、1 ((140 -135) / 5 = 1)、0, ((135 -135) / 5 = 0)。gcd、min的值会保存到.dvm文件中。在读取阶段，通过gcd、min 就可以解码出域值。\n 按块处理域值\n在处理域值时，会分为 单个block （SingleBlock）或 多个block（MultipleBlocks）来存储域值，这么做的目的也是为了尽可能降低空间的使用。\n当需要处理的域值个数超过 16384 (NUMERIC_BLOCK_SIZE)个时，Lucene会判断采用 多个block存储是否会减少空间的使用。\n 例子\n 使用单个block存储域值\n考虑这么一种情况，如果待处理的域值个数为16644个(16384 + 260)，如果前16384个域值的取值要么是3，要么是4，并且剩余的260个域值的值都为大于2000的且各不相同的值，并且最大值为3000。很明显这16644个域值的gcd为1，所以根据(max - min) / gdc，存储每一个域值需要的bit位个数为(3000 - 3) / 1 = 2997，2997的二进制位为0b00001011_10110101,即每一个域值需要占用12个bit位。这个例子将所有的域值当成一个block进行处理。\n 使用多个block存储域值\n当待处理的域值个数达到16384个时，先将这些值作为一个block处理，由于这16384个域值不是3就是4，所以很明显gcd的值为1，所以根据(max - min) / gdc，存储每一个域值需要的bit位个数为(4 - 3) / 1 = 1， 1的二进制位为0b1，即每一个域值只要占用1个bit位。最后剩余的260个域值按照一个block处理，并且同样地按照(max - min) / gdc计算每一个域值需要的bit位个数。\n 域值种类个数小于256的优化\n这种优化的目的还是处于尽可能减少空间的使用。\n满足的优化的条件需要两点：\n 第一点：待处理的域值种类个数(不是域值的个数)不超过256。\n至于为什么是256这个值，我没有参透~不好意思。\n 第二点：预先计算判断优化后的空间使用量是否能小于优化前\n优化后的域值存储方式与非优化的方法截然不同，下面通过特定的例子来介绍\n 例子\n待处理的域值有 5、6、5、6、3000，域值种类个数为 3，即5、6、3000。\n 不优化存储\n根据公式 (max - min) / gcd，计算出存储每一个域值需要的固定bit位个数， (3000 - 5) / 1 = 2995，2995的二进制为0b00001011_10110011，即存储每一个域值需要12个bit位。最后将域值存放到dvd文件中。\n 优化存储\n优化步骤如下：给每一种域值一个编号，在源码中，对3种域值进行排序，然后赋予每一种域值一个从0开始的值，如下表：\n\n\n\n域值\n5\n6\n3000\n\n\n\n\n编号\n0\n1\n2\n\n\n接着原本应该将所有域值，即5、6、5、6、3000存放到.dvd文件中，换成对应的编号存放到.dvd文件中，即实际存储到.dvd的值为0、1、0、1、2。并且每个值需要的固定bit位的个数为编号中的最大值，在当前例子中，需要的固定bit位的个数为2，即存储每一个值只需要2个bit位。\n\n\n\n\n\n这种优化的方式需要将域值与编号的对应关系信息存放到.dvm文件中，在读取阶段，先从.dvd文件读取到一个编号，然后根据.dvm中存放的 域值与编号的映射关系，获得最终的域值。\n\n\n\n\n\n\n 数据结构\n dvd\n在下图中给出了只有NumericDocValues的.dvd、.dvm文件的数据结构。\n图2：\n\nDocIdData描述了包含NumericDocValues数据的文档号的信息。\nFieldValues描述了域值信息。\n DocIdData\n如果IndexWriter添加的document中不都包含当前域，那么需要将包含当前域的文档号记录到DocIdData中，并且使用IndexedDISI类来存储文档号，IndexedDISI存储文档号后生成的数据结构单独的作为一篇文章介绍，在这里不赘述，看这里。\n FieldValues\n 单个block存储域值\n单个block存储方式又根据是否采用了 域值种类个数小于256的优化 生成两种数据结构。\n 优化\n正如上文中的说明，优化后的域值存储方式，实际存储的是域值对应的编号，然后采用PackedInt对编号进行编码存储。PackedInt编码后的FieldValues格式在这里不赘述，在BulkOperationPacked中介绍了其中一种压缩方法。\n 未优化\n未优化的域值存储方式，只能根据 (v - min) / gcd 公式将域值存放到FieldValues中, 其中v是待存储的域值，min为所有域值的最小值，上文中的预备知识介绍了为何使用 (v - min) / gcd 公式。同样的采用PackedInt对域值进行编码存储。\n 多个block存储域值\n多个block存储方式根据域值是否都是一样生成两种数据结构。\n 域值都相同\n图3：\n\n由于域值都相同，只要往.dvd文件中写入一个固定的标记值0跟其中一个域值即可。\n 域值不都相同\n图4：\n\n bitsPerValue\n存储每一个域值需要的bit位的个数。\n min\n实际存储到.dvd文件的域值是经过 (v - min) / gcd 公式计算后的值，所以这里要记录当前block中min值，在读取阶段用来解码域值。对于每一个block，min的值可能会不同，但是gcd的值通过所有域值计算出来的，所以不用在每一个block中存储gcd。gcd的值记录在.dvm文件中。\n length\nlength用来描述在读取阶段需要往后读取的字节区间，这个字节区间内包含了当前block的所有域值信息。\n FieldValue\n当前block中的所有域值，并且使用了 PackedInt进行编码存储。\n dvm\n图5：\n\n FieldNumber\n域的编号。\n DocValuesType\nDocvalues的类型，本文中，这个值就是 NUMERIC。\n DocIdIndex\nDocIdIndex是对.dvd文件的一个索引，用来描述 .dvd文件中DocIdData在.dvd文件中的开始跟结束位置。\n 情况1：\n图6：\n\n如果IndexWriter添加的document中都包含当前域，那么只需要在DocIdIndex中添加标志信息即可。\n 情况2：\n图7：\n\n如果IndexWriter添加的document中不都包含当前域，那么.dvd文件中需要将包含当前的域的文档号信息都记录下来。\n offset\n.dvd文件中存放文档号的DocIdData在文件中的开始位置。\n length\nlength为DocIdData在.dvd文件中的数据长度。\n在读取阶段，通过offset跟length就可以获得所有的DocIdData数据。\n NumValues\n当前域的域值个数。\n NumBitsPerValueMeteData\n 情况1\n如果处理的域值都是相同的，那么只要写入标志信息即可。\n图8：\n\n 情况2\n如果处理的域值满足上文介绍的 域值种类个数小于256的优化。\n图9：\n\n length\n域值的种类的个数。在上文中，我们给出的例子是 5、6、5、6、3000，那么length的值就是3。\n FieldValue\n上文中我们提到，在.dvm文件中需要保存 域值跟编号 的映射关系，在上面的例子中，FieldValue的分别会存放，5、6、3000的域值（原始域值存储）。在读取阶段，根据读取先后顺序，给每一个域值一个从0开始计数的一个编码值，就可以获得 域值跟编号 的映射关系。\n NumBitsPerValue\n存储每一个域值需要的固定bit位个数。\n min\n用于对编码的域值进行解码。\n gcd\n用于对编码的域值进行解码。\n FieldValuesIndex\nFieldValuesIndex是对.dvd文件的一个索引，用来描述 .dvd文件中FieldValues在.dvd文件中的开始跟结束位置。\n图10：\n\n offset\n.dvd文件中存放文档号的FieldValues在文件中的开始位置。\n length\nlength为FieldValues在.dvd文件中的数据长度。\n在读取阶段，通过offset跟length就可以获得所有的FieldValues数据。\n 结语\nNumericDocValues的索引文件数据结构相对SortedDocValues比较简单。之前介绍的SortedDocValues的文章会在以后进行重写，内容保持跟本篇文章一致。\n大家可以看我的源码注释来快速理解源码，地址在这里：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer.java\n","categories":["Lucene","DocValues"],"tags":["DocValues"]},{"title":"NumericUtils（Lucene 9.8.0）","url":"/Lucene/gongjulei/2023/1128/NumericUtils/","content":"  Lucene中用BKD存储的数值无论是哪种类型（long，int，float，double，BigInteger），为了便于使用相同的代码逻辑中实现BKD的遍历以及优化存储，都统一使用字节数组byte[]描述原值（original value）并且其转化成字节数组的逻辑都在NumericUtils类中实现。\n long类型\n  我们看下long类型的值如何转化为字节数组。\n  因为long类型占用64bit，即8个字节（一个字节占8位），所以需要大小为8的字节数组。需要两步完成字节数组的转化：\n\n步骤一：将long中的最高位，即符号位，跟1执行异或操作。\n\n为了在转化为字节数组后，能让字节数组依旧有拥有大小比较的能力\n对于用字节数组表示的数值，是通过对字节数组中的每个元素进行二进制比较来判断大小。\n\n二进制的比较方式为：从最高位开始，逐位比较，当遇到第一个不相同的bit，那么bit为1的值大于bit为0的值\n不过在LUCENE-10145之后，不再是简单的两个字节数组的比较，这里简单提一下，使用更高效但依然是无符号的比较方式。\n\n\n因此在long类型中，对于一个正数，它的最高位总是0，而负数的最高位是1，如果直接将符号位写入到字节数组，会出现所有的负数都大于正数的情况\n\n\n步骤二：从long的最高位开始，每8个bit作为字节数组的一个数组元素。\n\n 例子\n  有两个long类型数值2跟-2其转化过程如下：\n图1：\n\n  \n int类型\n  跟longToSortableBytes的处理逻辑是一致，差别就是生成的字节数组大小是4，而long类型转化成的字节数组大小为8。\n double/float类型\n  Lucene中将double和float类型分别通过JDK提供的Double.doubleToLongBits(double value)以及Float.floatToIntBits(float value)将浮点型的数值转换成使用IEEE 754标准的值，即long跟int值，然后通过上文中的方法再进一步转化为字节数组。\n IEEE 754\n  我们首先重新回顾下IEEE 754这个关于浮点数算术的国际标准。\n double类型\n  对于双精度（64bit）的浮点数可以分为三个部分：\n\n符号位：（1位），决定数值的正负。0 表示正数，1 表示负数。\n指数位：（11位），用于表示数值的范围。这个部分决定了浮点数的大小。\n\n指数位是结合偏移值（指数偏置，偏置值：1023）后的值，其中一个好处是可以将负指数跟正指数统一成正指数，那么在比较两个浮点的指数位时，比较两个无符号整数一样简单，不需要考虑正负。\n\n\n尾数位：（52位）：也称为有效数位或小数位，用于表示数值的精度。\n\n float类型\n  对于单精度（32bit）的浮点数可以分为三个部分：\n\n符号位：（1位）：决定数值的正负。0 表示正数，1 表示负数。\n指数位：（8位）：用于表示数值的范围。这个部分决定了浮点数的大小。偏置值：127\n尾数位：（23位）：也称为有效数位或小数位，用于表示数值的精度。\n\n 转化\n  接着我们给出一个双精度转化的例子。比如我们有一个双精度的浮点数，也就是double类型的值：1024.0256。\n 步骤1：十进制转二进制\n  对浮点数的整数跟小数的十进制分别使用对应的规则转化为二进制：\n 1.1 整数\n  1024的二进制就是2^10。\n 1.2  小数\n  小数部分的计算逻辑如下：\n0.0256 × 2 = 0.0512 → 取 00.0512 × 2 = 0.1024 → 取 00.1024 × 2 = 0.2048 → 取 00.2048 × 2 = 0.4096 → 取 00.4096 × 2 = 0.8192 → 取 00.8192 × 2 = 1.6384 → 取 1（取小数部分作为下一步骤的输入）0.6384 × 2 = 1.2768 → 取 10.2768 × 2 = 0.5536 → 取 00.5536 × 2 = 1.1072 → 取 1... \n  直到某一步的计算结果为0，或者超过尾数位的长度（双精度为52位）。最终小数部分的二进制是：000001101000110110111000101110101100011100 。\n 1.3 整合整数跟小数两部分\n  那么十进制1024.0256对应的二进制就是：10000000000.000001101000110110111000101110101100011100。\n 步骤2：规格化\n  将步骤1中的二进制格式化为1.xxxxx的形式并且计算出指数。\n 2.1 格式化\n  即1.0000000000000001101000110110111000101110101100011100 × 2^10，也就是对步骤1中的二进制的小数点往左移动10位。\n 2.2 指数偏移\n  执行2.1后计算出的指数为10，那么结合双精度的偏置值（1023），最终的指数值为：10 + 1023 = 1033，即10000001001。\n 步骤3：获取尾数部分\n  双精度格式中，尾数部分需要 52 位。从规格化的数中取出尾数部分（不包括开头的 1），如果不够 52 位，则用 0 补足。即0000000000000001101000110110111000101110101100011100。\n 步骤4：合成IEEE 754格式\n  对于这个双精度的浮点数：1024.0256，转化后的IEEE 754为：\n\n符号位: 0 （1位）\n指数位: 10000001001（11位）\n尾数位: 0000000000000001101000110110111000101110101100011100（52位）\n\n  单精度的转化方式类似于双精度，这里不赘述。\n 负数的问题\n  尽管通过IEEE 754标准，我们可以将双精度/单精度的浮点数用long/int类型表示，但如果浮点数是负值，会存在以下的问题，我们以双精度为例：两个负的双精度浮点数在转化为long值后，这两个long值的大小关系会发生倒置，即不是转化前浮点数对应的大小关系。\n  例如我们有以下两个浮点数：-1.234和 -2.345，转成long类型后的值如下所示：\n图2：\n\n  图2可知，long类型的a于b之间的大小关系跟转化前的浮点数之间的大小关系是相反的。而两个正数之间则不会发生这样的倒置：\n图3：\n\n  当然正/负的浮点数之间依然可以靠符号位保持原来的大小关系。所以要想一个办法，不能变更负数之间的大小关系。\n 异或操作\n  看下Lucene中是如何处理负数问题的：\n图4：\n\n  图4中，bits为IEEE754标准的值。可以看到上述过程分为三步：\n\n\n第一步：带符号的右移63位的目的是取出符号位\n\n如果是正的浮点数，则结果为0，即所有bit位都是0\n如果是负的浮点数，则结果为-1，即所有bit位都是1\n\n\n\n第二步：与0x7fffffffffffffffL，这里需要注意的是 &amp;的优先级比^高。\n\n如果是正的浮点数，则结果为0b00000000_00000000_00000000_00000000_00000000_00000000_00000000_00000000\n如果是负的浮点数，则结果为0b01111111_11111111_11111111_11111111_11111111_11111111_11111111_11111111\n\n\n\n第三步：与IEEE754标准的值做异或操作\n\n如果是正的浮点数，那么bits会与0b00000000_00000000_00000000_00000000_00000000_00000000_00000000_00000000异或，由于跟0执行异或操作时不会改变任何bit位，即不对bits做调整，所以正如图4的注释说到：or back to the original，也就是说这个方法不会变更正的浮点数\n如果是负的浮点数，那么bits会与0b01111111_11111111_11111111_11111111_11111111_11111111_11111111_11111111异或，由于它的最高位是0，其他位都是1，也就是bits的符号位不变，并且其他位（指数位跟尾数位）都会取反。这样就能逆转两个负数之间的大小关系，解决了上述的负数问题\n\n为什么取反后就能逆转两个负数之间的大小关系：因为指数位跟尾数位是不考虑符号位的二进制比较\n\n\n\n\n\n","categories":["Lucene","gongjulei"],"tags":["util","byte","sortable"]},{"title":"PackedInts（一）","url":"/Lucene/yasuocunchu/2019/1217/PackedInts%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  为了能节省空间，Lucene使用PackedInts类对long类型的数据进行压缩存储，基于内存使用率（memory-efficient）跟解压速度（读取速度），提供了多种压缩方法，我们先通过类图预览下这些压缩方法。\n图1：\n\n  图1中MutableImpl类是PackedInts的内部类，其中Packed64SingleBlock是一个抽象类，它的实现如下所示：\n图2：\n\n点击查看大图\n 预备知识\n  在介绍PackedInts提供的压缩方法前，我们先介绍下几个预备知识。\n 数据类型压缩\n  根据待处理的数据的取值范围，选择占用字节更小的数据类型来表示原数据类型。\n数组一：\nlong[] oldValues = &#123;10, 130, 7&#125;;\n  long类型的数组，每个数组元素都占用8个字节，压缩前存储该数组需要24个字节（只考虑数组元素的大小），即三个数组元素的占用字节总数，它们的二进制表示如下所示：\n表一：\n\n\n\n数组元素\n二进制\n\n\n\n\n10\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00001010\n\n\n102\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_01100110\n\n\n130\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_10000010\n\n\n\n  从表一可以看出，这三个值有效的数据位最多只在低8位，故可以使用字节数组来存储这三个数据：\n数组二：\nbyte[] newValues = &#123;10, 130, 7&#125;;\n  那么压缩后的数组只需要占用3个字节。\n 固定位数按字节存储\n  在数据类型压缩的前提下，待处理的数据集使用相同的固定的bit位存储，bit位的个数由数据集中的最大值，它的数据有效bit位决定，例如有如下的数组：\n数组三：\nlong[] oldValues = &#123;10, 290, 7&#125;;\n  三个数组元素的二进制表示如下所示：\n表二：\n\n\n\n数组元素\n二进制\n\n\n\n\n10\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00001010\n\n\n290\n00000000_00000000_00000000_00000000_00000000_00000000_00000001_00100010\n\n\n7\n00000000_00000000_00000000_00000000_00000000_00000000_00000000_00000111\n\n\n\n  上表中，最大的数组元素是290，它的有效数据位为低16位，它是所有数组元素中有效数据位占用bit位最多的，那么在进行数据类型压缩后，新的数据类型既要保证数值290的精度，同时还要使得占用字节最小，故只能选择short类型的数组，并且数组元素10、7都需要跟290一样，需要存储2个字节的大小，尽管它们两个只需要1个字节就可以表示：\n数组四：\nshort[] newValues = &#123;10, 290, 7&#125;;\n  压缩后的数组只需要占用6个字节。\n为什么要选用固定字节：\n  能让所有数据用同一种数据类型表示，并且任何数据不会出现精度缺失问题，尽管压缩率不是最高，但是读取速度是非常快的。\n 固定位数按位存储\n  上文中，数组三在使用了固定位数按字节存储以及数据类型压缩之后，生成了数组四，此时三个数组元素的二进制表示如下所示：\n表三：\n\n\n\n数组元素\n二进制\n\n\n\n\n10\n00000000_00001010\n\n\n290\n00000001_00100010\n\n\n7\n00000000_00000111\n\n\n\n  表三中，我们可以发现，在使用固定位数按字节存储的前提下，仍然有高7个bit位是无效数据，那么可以通过固定位数按位存储的方式来进一步提高压缩率，该方法不会使用数据类型的压缩，只是将所有数据的有效数据位进行拼接，当拼接后的bit位达到64位时，即生成一个long值，剩余的有效数据继续拼接并生成一个新的long值，例如我们有下面的数组：\n数组五：\nlong[] oldValues = &#123;10, 290, 7, 18, 32, 23, 45, 35, 89, 291&#125;;\n  在使用固定位数按位存储之后，生成的新的数组如下所示：\n数组六：\nlong[] newValues = &#123;380695872922475610, 2534621417262022656&#125;;\n图3：\n\n点击查看大图\n  图3中，由于数组五中的最大值为291，它的有效数据占用9个bit位，所以其他的数组元素也使用9个bit位表示，那么就可以使用一个long类型的来表示7个完整的数值，由于7个数值一共占用7*9 =63个bit位，剩余的1个bit位为数组元素35的最高位0（红色虚线左侧的0），35的低8位只能存放到第二个long值中，故在读取阶段，需要读取两个long才能得到35这个数值，那么原本需要10*8=80个字节存储，现在只需要2*8=16个字节。\n block\n  block在源码中用来描述一块数据区域，在这块数据可以存放一个或多个数值，block使用数组来实现，也就是说数组的一个数组元素称为一个block，并且这个数组元素可以存放一个或多个数值。\n  例如上文中的数组三，它是一个short类型的数组，它有三个block，并且每一个block中存放了一个数值，而在数组六中，它有两个block，第一个block存放了7个完整的数值，以及1个不完整的数值(7*9 + 1)，第二个block最多可以存放6个完整的数值，以及2个不完整的数值(8 + 6*9 + 2 = 64)。\n 压缩实现\n  图1跟图2展示的是PackedInts提供的所有压缩实现，我们先对这些实现进行分类：\n表4：\n\n \n \n \n \n \n  数据分布\n  是否有填充bit\n  是否单block单值\n  实现类\n \n \n  一个block\n  否\n  是\n  Direct8\n    Direct16\n    Direct32\n    Direct64\n \n \n  是\n  否\n  Packed64SingleBlock1\n    Packed64SingleBlock2\n    Packed64SingleBlock3\n    Packed64SingleBlock4\n    Packed64SingleBlock5\n    Packed64SingleBlock6\n    Packed64SingleBlock7\n    Packed64SingleBlock8\n    Packed64SingleBlock9\n    Packed64SingleBlock10\n    Packed64SingleBlock12\n    Packed64SingleBlock16\n    Packed64SingleBlock21\n    Packed64SingleBlock32\n \n \n  两个block\n  否\n  否\n  Packed64\n \n \n  三个block\n  否\n  -\n  Packed8ThreeBlocks\n    Packed16ThreeBlocks\n\n  我们先对表4的内容做一些介绍：\n\n数据分布：该列名描述的是一个数值的信息是否可能分布在不同的block中，例如图3中的数值35，它用9个bit位来描述，其中最高位存储在第一个block，而低8位存储在第二个block中，又比如上文中的数组二跟数组四，每个数值都存储在一个block中\n是否有填充bit：例如图3中的数值35，在第一个block中存储了7个完整的数据后，该block仅剩余1个bit位，如果该bit位不存储数值35的最高位，那么该bit位就是填充bit，也就是说，如果使用了填充bit，那么一个block中不会有不完整的数值，当然内存使用率会降低\n是否单block单值：该列描述的是一个block中是否只存储一个数值，例如数组4，每一个block只存储一个short类型的数值，例如数组六，第一个block存储了7个完整的数值以及一个不完整的数值\n\n  接下来我们先介绍下每种实现中使用哪种数据类型来存储数据，然后再介绍下为什么Lucene提供这么多的压缩，以及如何选择，最后再介绍几个例子。\n Direct*\n  该系列分别使用不同的数据类型来实现一个block存储一个数值，它们属于上文中提到的固定位数按字节存储：\n\nDirect8：使用byte[]数组存储\nDirect16：使用short[]数组存储\nDirect32：使用int[]数组存储\nDirect64：使用long[]数组存储\n\n Packed8ThreeBlocks、Packed16ThreeBlocks\n  该系列使用三个block来存储一个数值：\n\nPacked8ThreeBlocks：使用byte[]数组存储\nPacked16ThreeBlocks：使用short[]数组存储\n\n Packed64SingleBlock*\n  该系列使用一个block来存储一个或多个数值，并且可能存在填充bit，它们属于上文中提到的固定位数按位存储，所有的实现使用long[]数组存储。\n Packed64\n  Packed64使用一个block来存储一个或多个数值，不会存在填充bit，它属于上文中提到的固定位数按位存储，使用long[]数组存储。\n为什么Lucene提供这么多的压缩实现：\n  这些压缩实现分别提供不同的内存使用率以及解压速度（读取速度），下面几张图是Lucene的核心贡献者Adrien Grand提供的测试数据，它使用了三台机器来测试压缩实现的压缩性能以及解压性能：\n  测试一：\n图4：\n\n图5：\n\n  测试二：\n图6：\n\n图7：\n\n  测试三：\n图8：\n\n图9：\n\n  图中的曲线对应的压缩实现如下所示：\n\n蓝色曲线Packed：Packed64\n红色曲线Single：Packed64SingleBlock*\n黄色曲线Three：Packed8ThreeBlocks、Packed16ThreeBlocks\n黄色曲线Direct：Direct*\n\n  我们先观察下Direct*与Packed64的读取速度，这两种压缩实现无论在哪一台都表现出两个极端，即Packed64读取最慢，而Direct*读取最快，而从表4我们可以看出，Packed64使用连续的bit位存储数据，待存储的数据如果没有突兀的数据，那么相对于Direct*能有很高的压缩率，例如存储连续递增的文档号，并且只存储文档差值，那么只需要1个bit位就能表示一个文档号。\n  在Lucene 4.0.0版本之前，只有Direct8、Direct16、Direct32、Direct64、Packed64、Packed32（该压缩算实现考虑的是对32位平台，我们这里不作介绍）可供选择，在这个版本中，当计算出待处理的数据集中最大值的数据有效位的bit个数后，我们称之为bitsPerValue，如果bitsPerValue为8，那么选择Direct8，如果bitsPerValue为32，那么选择Direct32，以此类推，即如果bitsPerValue不是8、16、32、64，那么就选择Packed64。\n  也就说在Lucene 4.0.0版本之前，只有bitsPerValue是8、16、32、64时，才能使用Direct*，我们考虑这么一种假设，如果bitsPerValue的值是21，并且使用Direct32存储，那么我们就能获得较好的性能，但是这么做会造成（32-21）/ 32 = 35%的空间浪费，故Adrien Grand使用long类型数组来存储，那么64个bit位的long类型可以存放3个bitsPerValue为21的数值，并且剩余的一个bit位作为填充bit，该填充bit不存储有效数据，那么我们只要读取一个block，就能读取/写入一个数值，而Packed64需要读取两个block，因为读取/写入的数值可能分布在两个block中，那么自然性能比Packed64好，而这正是空间换时间的设计，每存储3个bitsPerValue为21的数值，需要1个bit额外的空间开销，即每个数值需要1/3个bit的额外开销。\n  上述的原内容见：https://issues.apache.org/jira/browse/LUCENE-4062 。\n  上述方式即Packed64SingleBlock*压缩实现，bitsPerValue为21的数值即对应Packed64SingleBlock21，同样Adrien Grand基于这种方式做了测试，他通过10000000个bitsPerValue为21的数值使用Packed64SingleBlock21进行压缩存储，相比较使用Packed64，额外空间开销为2%，但是速度提升了44%，故使用了Packed64SingleBlock*之后，bitsPerValue不为8、16、32、64的部分bitsPerValue通过空间换时间的思想，提高了性能，在图4~图9中也能通过曲线看出来。\n为什么只有部分bitsPerValue实现了Packed64SingleBlock*压缩：\n  从表4中可以发现，只有bitsPerValue为1，2，3，4，5，6，7，8，9，10，12，16，21，32实现了Packed64SingleBlock*压缩，我们通过一个block可存放的数值个数来介绍原因：\n\n1个数值：bitsPerValue为64才能使得额外空间开销最小，每个数值的空间开销为（64-64*1）/（1）= 0，它其实就是Direct64\n2个数值：bitsPerValue为32才能使得额外空间开销最小，每个数值的空间开销为（64-32*2）/（2）= 0\n3个数值：bitsPerValue为21才能使得额外空间开销最小，每个数值的空间开销为（64-21*3）/（3）= 0.33\n4个数值：bitsPerValue为16才能使得额外空间开销最小，每个数值的空间开销为（64-16*4）/（4）= 0\n5个数值：bitsPerValue为12才能使得额外空间开销最小，每个数值的空间开销为（64-12*5）/（5）= 0.8\n6个数值：bitsPerValue为10才能使得额外空间开销最小，每个数值的空间开销为（64-10*6）/（6）= 0.66\n7个数值：bitsPerValue为9才能使得额外空间开销最小，每个数值的空间开销为（64-9*7）/（7）= 0.14\n8个数值：bitsPerValue为8才能使得额外空间开销最小，每个数值的空间开销为（64-8*8）/（8）= 0\n\n                              。。。 。。。\n\n\n\n  可以看出那些实现了Packed64SingleBlock*压缩的bitsPerValue都是基于空间开销下的最优解。\n如何选择压缩方式：\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode"]},{"title":"PackedInts（二）","url":"/Lucene/yasuocunchu/2019/1218/PackedInts%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本文承接PackedInts（一），继续介绍剩余的内容。\n 压缩实现\n  在上一篇文章中，我们介绍了Lucene 7.5.0中PackedInts提供的几种压缩实现，如下所示：\n表1：\n\n \n \n \n \n \n  数据分布\n  是否有填充bit\n  是否单block单值\n  实现类\n \n \n  一个block\n  否\n  是\n  Direct8\n    Direct16\n    Direct32\n    Direct64\n \n \n  是\n  否\n  Packed64SingleBlock1\n  Packed64SingleBlock2\n    Packed64SingleBlock3\n    Packed64SingleBlock4\n    Packed64SingleBlock5\n    Packed64SingleBlock6\n    Packed64SingleBlock7\n    Packed64SingleBlock8\n    Packed64SingleBlock9\n    Packed64SingleBlock10\n    Packed64SingleBlock12\n    Packed64SingleBlock16\n    Packed64SingleBlock21\n    Packed64SingleBlock32\n \n \n  两个block\n  否\n  否\n  Packed64\n \n \n  三个block\n  否\n  -\n  Packed8ThreeBlocks\n    Packed16ThreeBlocks\n\n我们接着介绍如何选择这些压缩实现:\n  在源码中Lucene会根据使用者提供的三个参数来选择其中一种压缩实现，即PackedInts类中的getMutable(int valueCount, int bitsPerValue,  float acceptableOverheadRatio)方法，参数如下所示：\n\nvalueCount：描述待处理的数据集的数量\nbitsPerValue：描述待处理的数据集中的最大值，它的有效数据占用的bit个数\nacceptableOverheadRatio：描述可接受的开销\n\n什么是可接受的开销acceptableOverheadRatio：\n\nbitsPerValue描述了每一个数值占用的bit位个数，acceptableOverheadRatio则是每一个数值额外的空间开销的比例，允许使用比bitsPerValue更多的bit位，我们称之为maxBitsPerValue，来存储每一个数值，计算公式如下所示：\n\nint maxBitsPerValue = bitsPerValue + (int)(bitsPerValue * acceptableOverheadRatio)\n  例如我们有以下的数据集：\n数组一：\nlong[] values = &#123;3, 8, 7, 12, 18&#125;;\n  该数组的bitsPerValue为5，如果此时acceptableOverheadRatio的值为7，那么maxBitsPerValue = 5 + 5*7 = 40，即允许使用40个bit位来存储数组一。\n  当然Lucene并不会真正的使用40个bit来存储一个数值，maxBitsPerValue只是用来描述使用者可接受的额外开销的程度。\n为什么要使用acceptableOverheadRatio：\n  使得在使用者可接受的额外开销前提下，尽量使用读写性能最好的压缩来处理，我们先看下源码中的部分代码截图：\n图1：\n\n  先说明下上图的一些内容，actualBitsPerValues的值在后面的逻辑中用来选择对应的压缩实现，actualBitsPerValues与压缩实现的对应关系如下：\n\n8：Direct8\n16：Direct16\n32：Direct32\n64：Direct64\n24：Packed8ThreeBlocks\n48：Packed16ThreeBlocks\n随后先考虑是否能用Packed64SingleBlock*（红框表示），最后才考虑使用Packed64\n\n  在第250行的if语句判断中，如果bitsPerValues的值小于等于8，并且maxBitsPerValue大于等于8，那么就使用Direct8来处理，在文章PackedInts（一）中我们知道，Direct*的压缩实现是读写性能最好的，可以看出来acceptableOverheadRatio是空间换时间的设计思想，并且压缩实现的选择优先级如下所示：\nDirect* &gt; Packed*ThreeBlocks &gt; Packed64SingleBlock* &gt; Packed64\nacceptableOverheadRatio的取值范围是什么：\n  Lucene提供了以下几种取值：\n表2：\n\n\n\nacceptableOverheadRatio\n源码中的描述\n\n\n\n\n7\nAt most 700% memory overhead, always select a direct implementation.\n\n\n0.5\nAt most 50% memory overhead, always select a reasonably fast implementation\n\n\n0.25\nAt most 25% memory overhead\n\n\n0\nNo memory overhead at all, but the returned implementation may be slow.\n\n\n\nacceptableOverheadRatio的值为7\n  如果acceptableOverheadRatio的值为7，那么不管bitsPerValue是区间[1, 64]中的哪一个值，总是会选择Direct*压缩实现。例如bitsPerValue的值为1，那么maxBitsPerValue = 1 + 1*7 = 8，那么根据图1中第250行的判断，就会使用Direct8来处理，意味着每一个数值使用8个bit位存储，由于每一个数值的有效数据的bit位个数为1，那么每个数值的额外开销为7个bit，即表2中描述的At most 700% memory overhead。\nacceptableOverheadRatio的值为0.5\n  如果acceptableOverheadRatio的值为0.5，那么总能保证选择除了Packed64的其他任意一个压缩实现，它们是比较快（reasonably fast）的实现。\nacceptableOverheadRatio的值为0.25\n  相对acceptableOverheadRatio的值为0的情况获得更多的非Packed64的压缩实现。\nacceptableOverheadRatio的值为0\n  没有任何额外的空间开销，虽然读写性能慢，但是因为使用了固定位数按位存储，并且没有填充bit（见PackedInts（一）），所以有较高的压缩率。\n Packed64的实现\n  表4中的所有压缩实现，除了Packed64，其他的实现逻辑由于过于简单就不通过文章介绍了，而Packed64的实现核心是BulkOperation，BulkOperation中根据bitsPerValue从1到64的不同取值一共有64种不同的逻辑，但他们的实现原理是类似的，故感兴趣的同学可以看文章BulkOperationPacked来了解其中的一个实现。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode"]},{"title":"PointRangeQuery（一）（Lucene 8.11.0）","url":"/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  该系列文章开始介绍数值类型的范围查询PointRangeQuery，该类数据在Lucene中被称为点数据Point Value。\n  点数据按照基本类型（primitive type）可以划分IntPoint、LongPoint、FloatPoint、DoublePoint。点数据可以是多维度的，即一个点数据可以用多个维度值来描述。下图中我们分别定义了一维、二维、三维的点数据。可以理解为oneDim是直线上的一个点，twoDim是平面上的一个点，而threeDim是一个三维空间中的一个点。\n图1：\n\n 相关文章\n  在文章索引文件之dim&amp;&amp;dii、索引文件之kdd&amp;kdi&amp;kdm中介绍了存储点数据对应的索引文件；在文章索引文件的生成（八）之dim&amp;&amp;dii到索引文件的生成（十四）之dim&amp;&amp;dii以及索引文件的读取（一）之dim&amp;&amp;dii到索引文件的读取（四）之dim&amp;&amp;dii中分别介绍了索引文件的生成、读取过程。另外在文章Bkd-Tree中介绍存储点数据使用的数据结构以及通过一个例子概述了生成一颗BKD树的过程。\n Relation\n MinPackedValue、MaxPackedValue\n  在一个段Segment中，所有的点数据按照点数据域（Point Field，即图1中的oneDim、twoDim、threeDim）进行划分，对于某个点数据域，在索引文件.kdm中会存储下面两个字段MinPackedValue、MaxPackedValue：\n图2：\n\n  MinPackedValue描述的是这个点数据域中每个维度的最小值，同理MaxPackedValue描述的是这个点数据域中每个维度的最大值。例如有某个点数据域中包含如下的二维点数据：\n&#123;2, 3&#125;, &#123;6, 2&#125;, &#123;7, 6&#125;, &#123;8, 4&#125;, &#123;5, 4&#125;, &#123;3, 5&#125;\n  可以看出在这个点数据集合中，第一个维度的最小值是2，第二个维度最小值是2，故MinPackedValue的值为{2, 2}，同理，第一个维度的最大值是8，第二个维度的最大值是6，故MaxPackedValue的值为{8, 6}。\n  如果我们把这个点数据集放到一个平面上，如下所示：\n图3：\n\n  由图3可知，MinPackedValue、MaxPackedValue不一定是索引中的数据。同时可以看出，Lucene使用这两个值生成一个矩形，索引中所有的点数据都在这个矩形内。\n lowerValue、upperValue\n  我们在定义一个PointRangeQuery时，需要指定两个参数lowerValue、upperValue，分别表示我们这次范围查询的上界跟下界。\n  如果设定的查询条件为：\nlowerValue = &#123;1, 1&#125;upperValue = &#123;7, 5&#125;\n  同MinPackedValue、MaxPackedValue一样，lowerValue、upperValue这两个点也可以形成一个矩形：\n图4：\n\n  所以从图4可以看出，对于二维的点数据，PointRangeQuery的查询核心原理即：找出两个矩形相交（重叠）部分的所有点数据。\n  这两个矩形的相交关系在源码中使用Relation定义，它描述了三种相交关系：\n图5：\n\n CELL_OUTSIDE_QUERY\n  CELL_OUTSIDE_QUERY描述的是查询条件跟索引中点数据的数值范围没有交集，即没有重叠（overlap），如下所示：\n图6：\n\n CELL_CROSSES_QUERY\n  CELL_CROSSES_QUERY描述的是查询条件跟索引中点数据的数值范围部分重叠（partially overlaps）。\n图7：\n\n图8：\n\n CELL_INSIDE_QUERY\n  CELL_INSIDE_QUERY描述的是查询条件的数值范围包含索引中所有的点数据。\n图9：\n\n 基于Relation访问节点\n  在文章索引文件的生成（十一）之dim&amp;&amp;dii中我们说到，在生成BKD树的过程中，每次生成一个内部节点（inner node），都需要计算这个节点对应的MinPackedValue、MaxPackedValue，他们描述了这个内部节点对应的所有叶子节点（leave node）中的点数据都在MinPackedValue、MaxPackedValue对应的矩形内。\n  那么当我们从根节点开始深度遍历后，查询条件跟每一个内部节点的MinPackedValue、MaxPackedValue在计算Relation后，会采取不同的方式访问其子节点。\n\nCELL_OUTSIDE_QUERY：说明当前内部节点下的所有叶子节点都不满足查询条件，那么就不用再处理这个内部节点下的所有子节点了。\nCELL_INSIDE_QUERY：说明当前内部节点下的所有叶子节点中的点数据都满足查询条件，那么随后只从当前内部节点出发执行深度遍历，并且不需要再对内部节点进行Relation的计算，直到访问到叶子节点，并读取其包含的文档号。\nCELL_CROSSES_QUERY：说明当前内部节点下的所有叶子节点只有部分满足查询条件，那么在分别访问内部节点的左右子节点（内部节点）时，都需要计算Relation。\n\n 收集文档号集合的策略\n  在深度遍历BKD树的过程中，在读取叶子节点后，文档号会被收集。在PointRangeQuery中，遍历之前会根据索引中的一些信息执行不同的收集策略：\n 策略一：根据段中的最大文档号生成文档号集合\n  执行策略一需要同时满足两个条件：\n\n条件一：段中每一篇文档都包含某个域的点数据\n条件二：索引中某个域的点数据都满足查询条件\n\n 条件一\n  如果索引文件.kdm中的DocCount字段的值跟段中的文档数量segSize相同，那么满足条件一：段中每一篇文档都包含某个域的点数据。\n图10：\n\n  在生成索引文件.kdm期间，会使用FixedBitSet来收集文档号，FixedBitSet使用类似bitmap原理存储文档号，所以它不会重复存储相同的文档号。DocCount字段描述的是包含某个点数据域的文档数量，所以即使一篇文档中定义了多个相同域名的点数据域，对于DocCount只会执行+1操作。\n图11：\n\n  另外，段中的文档号数量segSize通过索引文件.si获得，在代码中可以通过reader.maxDoc()方法获得。\n 条件二\n  通过比较图10中的MinPackedValue、MaxPackedValue与查询条件的上下界进行比较，如果他们的Relation为CELL_INSIDE_QUERY，那么满足条件二：索引中某个域的点数据都满足查询条件。\n  执行策略一后，我们就可以在不遍历BKD树的情况下收集到满足查询条件的结果，即[0, reader.maxDoc()]这个区间的文档集合。\n 策略二：反向收集文档号信息\n  执行策略一需要同时满足三个条件：\n\n\n条件一：段中每一篇文档都包含某个域的点数据\n\n同策略一中的条件一，不赘述\n\n\n\n条件二：每篇文档中只包含一个某个点数据域的点数据\n\n如果索引文件.kdm中的PointCount字段（见图10）的值跟DocCount相同，那么满足条件二\nPointCount字段描述的是所有文档中的所有某个点数据的点数据的数量，比如一篇文档中定义了3个相同域名的点数据域，对于PointCount会执行+3操作，而DocCount只会执行+1操作\n\n\n\n条件三：满足查询条件的点数据数量（估算值cost，下一篇文章中会介绍cost的计算方式）占段中的文档数量的一半以上（&gt; 50%）\n\n\n  这三个条件针对的是对于这类索引数据的优化：如果所有文档中有且只有一个某个点数据域的点数据，如果cost大于文档数量的一半，那么就收集不满足查询条件的文档号。见源码中的注释：\nIf all docs have exactly one value and the cost is greater than half the leaf size then maybe we can make things faster by computing the set of documents that do NOT match the range.\n  执行策略二后，在随后遍历BKD树的过程中，我们只收集那些不满足查询条件的文档号。\n 策略三：收集满足查询条件的文档号\n  在无法满足策略一跟策略二的条件，那么就执行默认的策略三，即在随后遍历BKD树的过程中，我们收集那些满足查询条件的文档号。\n 节点访问规则IntersectVisitor\n  上文说道，对于策略二，它获取的是不满足查询条件的文档号，而对于策略三，它则是获取满足查询条件的文档号。不管哪一种策略，他们的相同点都是使用深度遍历读取BKD树，不同点则是访问内部节点跟叶子节点的规则，这个规则即IntersectVisitor。\n 结语\n  基于篇幅，我们将在下一篇文章中介绍节点访问规则IntersectVisitor以及策略二中条件三的cost的计算过程。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["point","query","dim","dii","rangeQuery"]},{"title":"PointRangeQuery（二）（Lucene 8.11.0）","url":"/Lucene/Search/2021/1128/PointRangeQuery%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本文承接PointRangeQuery（一），继续介绍数值类型的范围查询PointRangeQuery。\n 节点访问规则IntersectVisitor\n  上一篇文章中我们说到，在收集文档号的策略中，除了策略一，不管哪一种策略，他们的相同点都是使用深度遍历读取BKD树，不同点则是访问内部节点跟叶子节点的处理规则，这个规则即IntersectVisitor。\n  IntersectVisitor在源码中是一个接口类，我们通过介绍它提供的方法来了解：\n图1：\n\n 访问内部节点\n  正如图1中compare(…)方法中的注释说到，这个方法用来判断查询条件跟当前访问的内部节点之间的Relation（见PointRangeQuery（一）的介绍），决策出如何进一步处理该内部节点的子节点。我们先介绍下PointRangeQuery中如何实现compare(…)，随后在访问叶子节点时小结中介绍如何根据Relation作出访问子节点的策略。\n PointRangeQuery中计算Relation的实现\n  其实现过程用一句话描述为：先判断是否为CELL_OUTSIDE_QUERY，如果不是再判断是CELL_CROSSES_QUERY还是CELL_INSIDE_QUERY。\n 是否为CELL_OUTSIDE_QUERY\n  实现逻辑：依次处理每个维度，只要存在一个维度，查询条件在这个维度下的最小值比索引中的点数据在这个维度下的最大值还要大，或者查询条件在这个维度下的最大值比索引中的点数据在这个维度下的最小值还要小，那么它们的关系为CELL_OUTSIDE_QUERY。\n  我们以二维的点数据为例，并且我们称第一个维度为X维度，第二个维度为Y维度：\n图2：\n\n  图2中，在Y维度下，查询条件在Y维度下的最大值（1）比索引中的点数据在Y维度下的最小值（2）还要小，所以他们的关系是CELL_OUTSIDE_QUERY。\n图3：\n\n  图3中，在Y维度下，查询条件在Y维度下的最小值（8）比索引中的点数据在Y维度下的最大值（6）还要大，所以他们的关系是CELL_OUTSIDE_QUERY。\n CELL_CROSSES_QUERY还是CELL_INSIDE_QUERY\n  实现逻辑：在不是CELL_OUTSIDE_QUERY的前提下，只要存在一个维度，索引中的点数据在这个维度下的最小值比查询条件在这个维度下的最小值还要小，或者索引中的点数据在这个维度下的最大值比查询条件在这个维度下的最大值还要大，那么它们的关系为CELL_CROSSES_QUERY。如果所有维度都不满足上述条件，那么他们的关系为CELL_INSIDE_QUERY。\n图4：\n\n  图4中，在不是CELL_OUTSIDE_QUERY的前提下，查询条件在Y维度下的最小值不会大于6, 那么因为索引中的点数据在Y维度下的最小值（2）比查询条件在这个维度下的最小值（4）还要小，那么他们的关系为CELL_CROSSES_QUERY。\n图5：\n\n  图5中，在不是CELL_OUTSIDE_QUERY的前提下，不管是哪一个维度，都不满足索引中的点数据在这个维度下的最小值比查询条件在这个维度下的最小值还要小，也都不满足索引中的点数据在这个维度下的最大值比查询条件在这个维度下的最大值还要大，所以他们的关系为CELL_INSIDE_QUERY。\n compare(…)方法的实现\n  在文章PointRangeQuery（一）中说到，收集文档号集合有不同的策略，对于策略一由于不用遍历BKD树，所以不需要实现这个方法。而策略二跟策略三对compare(…)方法的实现有着少些的区别。\n 策略三\n  该策略的对应的实现逻辑即上文中PointRangeQuery中计算Relation的实现介绍的内容。源码中的详细实现见类PointRangeQuery#getIntersectVisitor中的compare(…)方法。\n 策略二\n  该策略首先采用同策略三一样的方式判断出查询条件跟内部节点的Relation，由于它采用反向收集文档号，所以它对应的实现也是&quot;反向Relation&quot;，由于该实现代码量较小，我们直接贴出源码：\n图6：\n\n  从图6可以看出，当代码225行计算出Relation的值为CELL_INSIDE_QUERY时，其&quot;反向Relation&quot;的值为CELL_OUTSIDE_QUERY，就如代码228行注释说的那样，如果内部节点下的所有子节点中的点数据都满足查询条件的话，那么就不用处理该内部节点的子节点，即不用再继续深度遍历该内部节点。同样的，当代码230行计算出Relation的值为CELL_OUTSIDE_QUERY，其&quot;反向Relation&quot;的值为CELL_INSIDE_QUERY。其代码231行注释中的&quot;clear all documents&quot;说的是在访问叶子节点的处理方式，我们在随后下一篇中会介绍。源码中的详细实现见类PointRangeQuery#getInverseIntersectVisitor中的compare(…)方法。\n 结语\n  基于篇幅，剩余内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["point","query","dim","dii","rangeQuery"]},{"title":"RamUsageEstimator","url":"/Lucene/gongjulei/2019/1212/RamUsageEstimator/","content":"  Lucene通过RamUsageEstimator类提供的方法来粗略的（roughly）估算Java对象在内存中的大小。计算对象大小的主要目的是配合flush策略将内存中的索引信息同步到磁盘，flush策略的概念可以看文章构造IndexWriter对象（二）中关于MaxBufferedDocs、RAMBufferSizeMB的介绍以及文章文档的增删改（中）。\n 对象的计算方式\n  Lucene中通过对象中包含的基本类型、引用对象这两者对应的大小和值来粗略的计算该对象的大小。\n 基本类型（primitive type）\n  基本类型的大小，即占用的字节大小如下表所示：\n表一：\n\n\n\n基本类型\n占用字节数量\n\n\n\n\nboolean\n1\n\n\nbyte\n1\n\n\nchar\n2\n\n\nshort\n2\n\n\nint\n4\n\n\nfloat\n4\n\n\ndouble\n8\n\n\nlong\n8\n\n\n\n 引用对象\n  引用对象对应的大小通过一些常量的组合值实现，这些常量通过虚拟机提供的信息来初始化，如下所示：\n\nCOMPRESSED_REFS_ENABLED：布尔值，如果为true说明JVM开启了指针压缩（即启用了-XX:+UseCompressedOops）\nNUM_BYTES_OBJECT_ALIGNMENT：对齐值，一个对象通过填充值（padding value）使其对象大小为NUM_BYTES_OBJECT_ALIGNMENT的整数倍（最接近的）\nNUM_BYTES_OBJECT_REF：引用对象需要占用的大小\nNUM_BYTES_OBJECT_HEADER：对象头占用的大小\nNUM_BYTES_ARRAY_HEADER：数组头占用的大小\nLONG_CACHE_MIN_VALUE：由于LongCache的存在，缓存范围（cache range）内的数值会被缓存，LONG_CACHE_MIN_VALUE为这个范围的最小值，如果某个对象A中有一个Long对象，并且Long对象的值在缓存范围内，那么该Long对象的大小不会参与对象A的大小计算\nLONG_CACHE_MAX_VALUE：同上，LONG_CACHE_MAX_VALUE为缓存范围的最大值\n\n  由于上述的常量值基于使用的JVM平台以及JVM的参数，故无法给出具体的数值，这里我们只简单介绍下如何获取这些变量的值：\n\n利用反射机制，获取两个java.lang.management.ManagementFactory以及com.sun.management.HotSpotDiagnosticMXBean中的信息来设置上述的常量\n感兴趣的同学可以点击这个链接，查看具体的常量的初始化，共50行左右的代码量，简单易懂：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/RamUsageEstimator.java\n\n 对齐计算\n  根据上文中获得的对齐值NUM_BYTES_OBJECT_ALIGNMENT计算对齐后的大小，计算逻辑如下：\npublic static long alignObjectSize(long size) &#123;    size += (long) NUM_BYTES_OBJECT_ALIGNMENT - 1L;    return size - (size % NUM_BYTES_OBJECT_ALIGNMENT);  &#125;\n  作者的环境获得的NUM_BYTES_OBJECT_ALIGNMENT的值为8，那么如果上述alignObjectSize方法的参数size的值为14，那么在对齐操作后，会获得一个比14大，并且是8的倍数的数值，即16。\n 数组大小计算\n  根据不同的数组元素类型计算方式各不相同。\n byte类型\n public static long sizeOf(byte[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + arr.length);&#125;\n boolean类型\npublic static long sizeOf(boolean[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + arr.length);&#125;\n char类型\npublic static long sizeOf(char[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) Character.BYTES * arr.length);&#125;\n short类型\npublic static long sizeOf(short[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) Short.BYTES * arr.length);&#125;\n int类型\npublic static long sizeOf(int[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) Integer.BYTES * arr.length);&#125;\n float类型\n public static long sizeOf(float[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) Float.BYTES * arr.length);&#125;\n long类型\npublic static long sizeOf(long[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) Long.BYTES * arr.length);&#125;\n double类型\npublic static long sizeOf(double[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) Double.BYTES * arr.length);&#125;\n 对象类型\npublic static long shallowSizeOf(Object[] arr) &#123;    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) NUM_BYTES_OBJECT_REF * arr.length);&#125;\n  上述的代码过于简单，大家自行看下即可。\n 计算shallow size\n  shallow size描述的是对象自身占用的内存大小，我们先通过一个例子来介绍下通过JProfiler计算出的shallow size，然后再介绍在Lucene中的计算shallow size的逻辑 。\n图1：\n\n图2：\n\n  使用JProfiler后，我们观察下图1中第27行的RamUsageEstimatorTest对象ramUsageEstimator的shallow size是多少，如下所示：\n图3：\n\n  由图3蓝色框标注的内容可以看出RamUsageEstimatorTest类中参与shallow size计算的只有非静态变量，即实例域，绿色框标注的内容为Retained size，它将object1对象的大小也参与到ramUsageEstimator对象的大小计算中，而在shallow size中，只将引用object1的大小，即NUM_BYTES_OBJECT_REF的值纳入计算。\n  接下来我们看下Lucene中计算shallow size的逻辑：\n Lucene计算shallow size流程图\n图4：\n\n Object\n图5：\n\n  该流程描述的是哪些准备数据可以用来计算shallow size，至少包含下面的种类：\n\n种类一：基本类型的数组，例如整型数组int[ ]，浮点型数组floag[ ]\n种类二：对象数组，例如图1中，定义一个RamUsageEstimatorTest[ ]对象数组。\n种类三：对象，例如图1中，RamUsageEstimatorTest对象\n种类四：基本类型，例如int、float的数值，注意的是他们会被自动装箱，故实际是计算的Integer、Float对象的shallow size，同种类三\n\n  另外通过Class.isArray()方法判断准备数据Object是属于array class还是普通对象，即not array class。\n 计算数组元素为基本类型的shallow size\n图6：\n\n  该计算方式在上文的数组计算方式章节已经介绍，不赘述。\n 计算数组元素为对象的shallow size\n图7：\n\n  该计算方式在上文的数组计算方式章节已经介绍，不赘述。\n 获取对象所有实例域\n图8：\n\n  这里所谓的&quot;所有&quot;实例域，指的是对象对应的类的继承链（inheritance chain）上实例域都参与shallow size计算。\n  例如这么一个类的继承链：\n图9：\n\n图10：\n\n  基于图10的例子，GrandSonClass对象grandSon的shallowSize的值如下所示：\n作者的运行环境中对齐值NUM_BYTES_OBJECT_ALIGNMENT为8，NUM_BYTES_OBJECT_HEADER为121. 对其计算前的和值 = NUM_BYTES_OBJECT_HEADER + fatherData1(8) + fatherData2(4) + sonData(8) + grandSonData(4) = 362. 对其计算后的和值 = 40，即找到比36大，且最小的NUM_BYTES_OBJECT_ALIGNMENT的倍数的值\n图11：\n\n  图11给出的是通过JProfiler算出的GrandSonClass对象grandSon的shallowSize的值，同样为40。\n  另外我们可以看出，非实例域是不会参数shallow size的计算，例如图10中的几个静态域。\n Lucene中对象大小计算\n  如果仅仅用通过RamUsageEstimator类计算出来的shallow size来描述一个对象的大小就过于粗略（rough）了，故针对不同的类，结合shallow size，实现类似计算Retained size的功能。\n Accountable类\n  需要计算大小的对象对应的类都需要实现Accountable类的方法，来实现不同类的类似Retained size的功能，使得尽量接近一个对象真正占用的内存大小，该类是个接口类，代码简洁，故在此列出，该类的完整代码见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/Accountable.java ：\npublic interface Accountable &#123;    // Return the memory usage of this object in bytes. Negative values are illegal.    long ramBytesUsed();    ... ...&#125;\n  我们随意找一个实现了Accountable接口的类来介绍下如何实现类似计算Retained size的功能，例如SegmentMap类，该类的完整代码见：  https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/index/OrdinalMap.java ：\nprivate static class SegmentMap implements Accountable &#123;    private static final long BASE_RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(SegmentMap.class);    private final int[] newToOld, oldToNew;        ... ...        @Override    public long ramBytesUsed() &#123;        return BASE_RAM_BYTES_USED + RamUsageEstimator.sizeOf(newToOld) + RamUsageEstimator.sizeOf(oldToNew);    &#125;&#125;\n  通过观察SegmentMap类我们可以看出，该类先计算了SegmentMap对象shallow size，即BASE_RAM_BYTES_USED，然后在ramBytesUsed() 方法中继续计算两个数组的大小，即所谓的类似Retained size的功能。\n 结语\n  在Lucene中，计算一个对象大小的逻辑根据类中的实例域的数量以及类型而各不相同，想更进一步深入理解的同学可以自行看下这个类中的计算，想必会有更大的收获：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/index/BufferedUpdates.java 。\n点击下载附件\n","categories":["Lucene","gongjulei"],"tags":["ram","usage","memory"]},{"title":"RangeField（一）（Lucene 8.4.0）","url":"/Lucene/Search/2020/0723/RangeField%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  本文将介绍Lucene中提供的范围域（RangeField），以及基于该域实现的范围查询。\n 范围域（RangeField）\n  RangeField使得一个域有了空间的概念，并且最多支持到四维（4 dimensions）：\n\n1 dimensions：该维度的域描述了一条直线上的一段长度，如果有如下的定义：\n\n图1：\n\n  用图形来描述这个域的话如下所示：\n图2：\n\n\n2 dimensions：该维度的域描述了平面上的一个矩形，如果有如下的定义：\n\n图3：\n\n  用图形来描述这个域的话如下所示：\n图4：\n\n\n\n3 dimensions：该维度的域描述了空间中的一个立方体\n\n\n4 dimensions：该维度的域描述了四次元立方体（tesseract）\n\n\n  为了便于描述，我们将以2 dimensions为例，继续下文的介绍。\n BinaryRangeDocValuesField\n  在Lucene 8.2.0版本中，新增了BinaryRangeDocValuesField，使得能根据范围域实现范围查找，在之前的版本中，已经提供了一些跟范围域的相关操作，即IntRange、DoubleRange、FloatRange类中相关的方法，在后面的文章中，将会介绍这几个类，并且将介绍它们与BinaryRangeDocValuesField在存储，查询上的差异。\n 实现类\n图5：\n\n  图5中BinaryRangeDocValuesField继承了BinaryDocValuesField，同时有四个实现类，故可以了解到，通过BinaryRangeDocValuesField存储的范围域的信息最终使用DocValues存储，即索引信息存储在索引文件.dvd&amp;&amp;.dvm中。\n 域值编码（encode）\n  以图3为例，域名为&quot;level&quot;，域值为两个int类型数组min1、max1，Lucene中所有数值类型的域值都会被重新编码为一个字节数组，并且该字节数组可以用于排序，同时字节数组之间的排序关系跟对应数值之间的排序关系是一致的，编码的过程在文章索引文件的生成（八）之dim&amp;&amp;dii中已经介绍过了，不赘述。\n  域值在转化为字节数组后，其写入到索引文件的过程跟使用BinaryDocValues是一致的，写入跟读取BinaryDocValues的过程分别见文章索引文件的生成（二十一）之dvm&amp;&amp;dvd跟索引文件的读取（六）之dvd&amp;&amp;dvm的介绍。\n 范围查询\n  以图3中的IntRangeDocValuesField为例，通过调用IntRangeDocValuesField类中的newSlowIntersectsQuery方法生成一个Query，我们看下源码中该方法的注释：\n图6：\n\n  图6的注释大意为：使用DocValues找到所有与查询条件（查询条件也是一个范围域）相交（intersect）的范围域，在下文的图16中我们会介绍如何相交的判定方式。\n  最后使用IndexSearcher.search()方法实现范围查询，其核心的流程图如下所示：\n图7：\n\n 两阶段遍历（TwoPhaseIterator）\n  在介绍图6的流程图之前，先介绍下Lucene中的两阶段遍历TwoPhaseIterator，我们先看下源码中关于两阶段遍历的注释：\n图8：\n\n  图7的注释大意为：先通过DocIdSetIterator对象获取一个文档号集合，该集合中不一定都满足查询条件，即所谓的approximation，通过nextDoc()或者advance()方法遍历该文档号集合（第一阶段），将每一个文档通过matches()方法（第二阶段），在该方法中能真正的确定文档号是否满足查询条件。\n  图6的流程中正是使用了两阶段遍历的方式，其中流程点文档集合即通过DocIdSetIterator对象获得，流程点是否还有未处理的文档号？中为两阶段遍历的第一个阶段，接着在流程点是否满足查询条件？通过matches()方法判断是否满足查询条件进行所谓的两阶段遍历的第二个阶段。\n  当前版本中，无论是哪种类型的查询，都是通过遍历DocIdSetIterator对象中的文档号来获取满足查询条件的文档号，但是当DocIdSetIterator对象中的文档集合只是所谓的approximation，那么就需要使用两阶段遍历来处理了。\n  如果我们有以下的查询条件：\n图9：\n\n  图9中查询条件为两个TermQuery的组合，在系列文章索引文件的生成（一）中我们知道，通过域名&quot;author&quot;跟域值能直接在索引文件中准确的定位到满足条件的文档号，那么上图中的这种查询就能获得一个DocIdSetIterator对象，并且它包含的文档号不是所谓的approximation，故就不要两阶段遍历的处理方式了。\n 两阶段遍历的第一个阶段\n图10：\n\n  由于BinaryRangeDocValuesField使用DocValues存储，故跟其他DocValues一样，以图3为例，包含域名为&quot;level&quot;的所有BinaryRangeDocValuesField的文档号存储在索引文件.dvd的DocIdData字段，如下所示：\n图11：\n\n  DocIdData字段在内存中用IndexedDISI来描述，而IndexedDISI是继承DocIdSetIterator的子类，如下所示，故流程点文档集合中的文档集合即上文中我们提到的DocIdSetIterator对象中的文档集合，随后开始每一个文档号，执行两阶段遍历的第一个阶段。\n图12：\n\n  另外读取图10的DocIdData字段的过程可以阅读文章索引文件的读取（五）之dvd&amp;&amp;dvm，而如何从IndexedDISI获取文档号可以阅读文章IndexedDISI（一）、IndexedDISI（二）。\n 根据文档号取出域值\n图13：\n\n  从IndexedDISI中获取了一个文档号之后，我们需要根据这个文档号找到对应的域值，以图3为例，即域名&quot;level&quot;对应的域值，在文章索引文件的读取（六）之dvd&amp;&amp;dvm中我们详细的介绍了如何根据文档号从图10的索引文件中找到对应的域值，这里不赘述。\n 两阶段遍历的第二个阶段\n图14：\n\n  我们通过一个例子来说明流程点是否满足查询条件的判断过程，该例子的完整demo见 https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/lucene/DcoValues/rangeField/IntRangeDocValuesFieldTest.java ：\n图15：\n\n  图形化图14中的范围域以及查询条件后如下所示：\n图16：\n\n  在上文中我们说到，图15中第77行的newSlowIntersectsQuery方法会找到所有与查询条件（查询条件也是一个范围域）相交（intersect）的范围域，图15中第75、76行定义了查询条件的范围域，即图16中根据lowQueryRange跟highQueryRange围成的一个绿色的框，另外图16中红框、蓝框、黄框、天蓝色分别对应图15中的四篇文章中的范围域，显而易见，红框、蓝框、黄框跟绿框的是相交的，意味着文档0、文档1、文档2满足查询条件。\n  图16中，第三个RangeField跟查询条件也是相交的，相交部分为一个点。\n  上文中通过图形很容易看出是否相交，接着我们介绍在代码中是如何判断相交的，判断是否相交的代码在matches()方法中实现，即两阶段遍历的第二个阶段。\n  由于判断相交的代码比较简单，故我们直接贴出来：\n图17：\n\n  可以看出，判断方式为逐个比较每个维度，当所有维度都满足某个条件，那么就认为是相交的，对于图15的例子，有两个维度，即图16中X、Y轴，这里的某个条件描述是两个子条件的组合，并且它们需要同时满足：\n\n子条件1：某个维度下，查询条件在该维度下的最大值必须大约等于范围域在该维度下的最小值\n子条件2：某个维度下，查询条件在该维度下的最小值必须小于等于范围域在该维度下的最大值\n\n  在同时满足上述两个子条件后，那么范围域必定是跟查询条件相交的，我们以图16中的第三个RangeField为例，在X轴的维度上，查询条件在这个维度上的最大值为8，范围域在这个维度的最小值为8，满足大于等于的关系，在Y轴的维度上，查询条件在这个维度上的最小值为0，范围域在这个维度上的最大值为10，满足小于等于的关系，故第三个RangeField跟查询提交是相交的，即图15中包含第三个RangeField的文档2满足查询条件。\n  在流程点是否满足查询条件？完成两阶段遍历的第二个阶段，最后文档号将被Collector收集。\n  基于上文中判断相交的逻辑，使得在定义一个RangeField的min跟max数组时会有一定的限制，该要求就是在每一个维度上，min数组在这个维度上的值必须小于max数组在这个维度上的值，否则在索引阶段会抛出下面的异常：\n图18：\n\n 结语\n  在下一篇文章中，我们将介绍IntRange、DoubleRange、FloatRange类中跟范围域相关的内容。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["point","dim","dii","range","field"]},{"title":"ReaderPool（一）（Lucene 8.7.0）","url":"/Lucene/Index/2020/1208/ReaderPool%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  ReaderPool类对于理解Lucene的一些机制起到了极其关键的作用，这些机制至少包含段的合并、作用（apply）删除信息、NRT（near real-time）、flush/commit与merge的并发过程中的删除信息的处理等等，所以有必要单独用一篇文章来介绍这个类。下面先给出源码中对于这个类的介绍：\n图1：\n\n  图1的javadoc中这样描述ReaderPool类：该类持有一个或多个SegmentReader对象的引用，并且是shared SegmentReader，share描述的是在flush阶段、合并阶段、NRT等不同的场景中都共用ReaderPool对象中的SegmentReader。另外IndexWriter还会使用这些shared SegmentReaders来实现例如作用（apply）删除信息、执行段的合并、NRT搜索。\n 构造ReaderPool对象\n  ReaderPool对象只在构造IndexWriter对象期间生成，正如图1中的Javadoc所描述的那样，它用来被IndexWriter使用。\n图2：\n\n  我们通过ReaderPool的构造函数来介绍在构造ReaderPool对象期间一些主要的内容：\n图3：\n\n  我们看下图3中红框标注的部分内容，它描述的是通过参数StandardDirectoryReader reader，从中依次读取它包含的SegmentReader，然后将每个SegmentReader的信息存储到代码第93行的readerMap中。\n构造函数的参数StandardDirectoryReader reader是哪里来的？\n  通过图2的流程点获取IndexCommit对应的StandardDirectoryReader获得StandardDirectoryReader，在随后流程点生成对象ReaderPool中传递给ReaderPool的构造函数。\n代码第93行的readerMap是什么？\n  readerMap是一个map容器，它是ReaderPool对象的实例变量。其中key为SegmentCommitInfo对象，value为ReadersAndUpdates对象，如下所示：\n图4：\n\n  SegmentCommitInfo对象是什么？\n  SegmentCommitInfo用来描述一个段元数据（metadata）。它是索引文件segments_N的字段：\n图5：\n\n  索引文件segments_N中用来保存描述每个段的信息的元数据SegmentCommitInfo。在图2的流程点获取IndexCommit对应的StandardDirectoryReader中通过读取索引目录中的索引文件segments_N获取每个段对应的SegmentCommitInfo，并且将它作为readerMap的key，用来区分不同的段。\n  另外readerMap的value，即ReadersAndUpdates对象，它同样描述了段中的数据，下文中我们再对其展开介绍。\n  故在构造ReaderPool的过程中，其最重要的过程就是记录当前索引目录中所有段的信息，在下文中，会介绍被记录的信息在什么情况下会被使用。\n 读取ReaderPool对象\n  我们先看下readerMap中的value，即ReaderAndUpdates对象中包含了哪些内容：\n图6：\n\n  图6中红框标注的实例变量是我们关心的内容，我们一一介绍。\n SegmentReader reader\n图7：\n\n  reader中包含的内容在文章SegmentReader（一）已经做了介绍，不赘述。\n PendingDeletes pendingDeletes\n图8：\n\n  正如图8的注释描述的一样，pendingDeletes用来存储段中**“新的”**被删除的信息，注释中further deletions即&quot;新的&quot;被删除的信息。\n  为什么要加黑突出&quot;新的&quot;这两个字？\n  以在构造ReaderPool对象期间为例，图7中的SegmentReader中可能包含删除信息，这些删除信息是在图的流程点获取IndexCommit对应的StandardDirectoryReader通过读取索引目录中一个段对应的索引文件.liv获得的，我们称之为&quot;旧的&quot;删除信息。\n  当后续索引（Indexing）过程中，如果存在删除操作，并且当前段满足这个删除条件，那么删除信息必须作用（apply）到这个段，这些删除信息称之为**&quot;新的&quot;被删除信息，它们会被添加到pendingDeletes中，更准确的描述应该是这些&quot;新的&quot;删除信息被暂存**到pendingDeletes。\n  为什么是暂存&quot;新的&quot;的删除信息？\n  如果不是暂存，那么就是持久化到磁盘，即生成新的索引文件.liv。但是每次有删除信息就执行I/O磁盘操作，这显然不是合理的设计。\n  什么时候将&quot;新的&quot;删除信息持久化到磁盘？\n  例如在执行flush、commit、获取NRT reader时。\n图9：\n\n  图9中，当用户调用了主动flush（执行IndexWriter.flush()操作），当执行到流程点更新ReaderPool，说明这次flush产生的&quot;新的&quot;删除都已经实现了apply，那么此时可以将&quot;新的&quot;删除信息生成新的索引文件.liv。\n  什么时候一个段会被作用（apply）&quot;新的&quot;删除信息\n  还是以图9的flush为例，在IndexWriter处理事件的流程中，会执行一个处理删除信息的事件，其流程图如下所示：\n图10：\n\n  图10中红框标注的两个流程点将会从段中找到满足删除条件的文档号，然后将删除信息暂存到pendingDeletes中。\n  另外在执行段的合并过程中，待合并的段在图11的流程点作用（apply）删除信息被作用&quot;新的&quot;删除信息：\n图11：\n\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["ReaderPool"]},{"title":"ReaderPool（二）（Lucene 8.7.0）","url":"/Lucene/Index/2020/1209/ReaderPool%EF%BC%88%E4%BA%8C%EF%BC%89.md/","content":"  本文承接文章ReaderPool（一），继续介绍剩余的内容。\n 读取ReaderPool对象\n  我们继续介绍ReaderPool对象中的readerMap这个map容器ReaderAndUpdates中包含的实例变量。\n图1：\n\n图2：\n\n boolean isMerging\n图3：\n\n  正如图3中的注释描述的那样，isMerging这个布尔值相用来描述一个段是否正在参与段的合并操作。如果一个段正在合并中，并且该段中的有些文档满足DocValues的更新条件（更新方式见文章文档的增删改（一）），那么更新信息将被暂存到图3红框标注的mergingNumericUpdates中，它是一个map容器。当合并结束后，暂存在mergingNumericUpdates中的更新信息将作用到合并后的新段。\n  注意的是从Lucene 4.6.0开始正如图3注释描述的那样，更新信息的确是被暂存在mergingNumericUpdates的map容器中，下图是Lucene 4.6.0中的代码：\n图4：\n\n  然而在Lucene 8.7.0中，更新信息是暂存在图2中的mergingDVUpdates的map容器的，所以图3中的mergingNumericUpdates属于书写错误。\n  什么时候isMerging的值为true？\n  以执行段的合并为例，如下所示：\n图5：\n\n  图5中，红框标注的两个流程点中根据一个段的SegmentCommitInfo的信息，从ReaderPool中的readerMap（图1所示）中找到ReaderAndUpdates，然后置ReaderAndUpdates中的isMerging的值为true。\n Map&lt;String,List&lt;DocValuesFieldUpdates&gt;&gt; pendingDVUpdates\n图6：\n\n  如果某个段没有参与段的合并，更精确的描述应该是这个段对应在ReaderPool中的isMerging的值为false时，在索引（Indexing）的过程（段的合并跟索引可以是并行操作，取决于段的合并调度）中，即执行文档的增删改的操作，当段中的文档满足DocValues的更新操作，那么更新信息会被暂存到pendingDVUpdates中。\n  为什么是暂存更新信息？\n  如果不是暂存，那么就是持久化到磁盘，即生成新的索引文件.dvd&amp;&amp;dvm。但是每次有DocValues的更新操作就执行I/O磁盘操作，这显然不是合理的设计。\n  什么时候将DocValues的更新操作持久化到磁盘？\n  例如在执行flush、commit、获取NRT reader时。\n图7：\n\n 图7中，当用户调用了主动flush（执行IndexWriter.flush()操作），当执行到流程点更新ReaderPool，说明这次flush产生的DocValues的更新信息已经实现了apply，那么此时可以将更新信息生成新的索引文件dvd&amp;&amp;dvm。\n  什么时候一个段会被作用（apply）DocValues的更新信息\n  还是以图7的flush为例，在IndexWriter处理事件的流程中，会执行一个处理删除信息的事件，其流程图如下所示：\n图8：\n\n  图10中红框标注的流程点将会为每个满足DocValues的更新操作的段记录更新信息，即将更新信息暂存到pendingDVUpdates中。\n  另外在执行段的合并过程中，待合并的段在图5的流程点作用（apply）删除信息被作用DocValues的更新信息。\n Map&lt;String,List&lt;DocValuesFieldUpdates&gt;&gt; mergingDVUpdates\n图9：\n\n  正如图9的注释描述的那样，当某个段正在进行合并时，即上文中中isMerging的值为true，mergingDVUpdates容器用来暂存在合并期间生成的新的DocValues更新信息，在合并结束后，这些更新信息将被作用新生成的段。\n  在图5的流程点提交合并中，将会读取mergingDVUpdates中暂存的更新信息，并作用到新生成的段。\n boolean poolReaders\n图10：\n\n  最后我们再介绍下ReaderPool对象中的poolReaders，它是一个布尔值，图10的注释中说到，该值的特点就像是往DVD上刻录，一旦刻录了就无法再次刻录，即该值默认为false，一旦被赋值为true，那么就不会再为true。\n哪些场景下poolReaders的值会被置为true\n  在生成IndexWriter对象时，可以指定为true，见文章构造IndexWriter对象（一）的介绍，或者是在使用NRT机制的时候。\npoolReaders设置为true后有什么用\n  目前唯一使用的场景是在执行段的合并中，如果poolReaders为true，那么在图5的流程点生成IndexReaderWarmer，至于IndexReaderWarmer的作用可以阅读文章执行段的合并（四）。\n 结语\n  通过两篇文章的介绍，想必已经深入理解了ReaderPool这个类的作用。理解了这个类，才能真正理解Lucene的段的合并、flush/commit、NRT等机制。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["ReaderPool"]},{"title":"RoaringDocIdSet","url":"/Lucene/gongjulei/2019/1008/RoaringDocIdSet/","content":"  RoaringDocIdSet的设计灵感来源于RoaringBitmap，Lucene根据自身需求有着自己的的实现方法，来实现对文档号的处理（存储，读取）。\n RoaringBitmap\n  位集合（bitsets）通常也被成为位图（bitMaps），它是一种常用的快速数据结构（fast data structures），但由于它对内存的开销较大，所以我们通常会使用经过压缩处理的bitMaps，而这便是RoaringBitmap。\n  本篇文章不会对RoaringBitmap展开介绍，因为Lucene有着自己的实现方法，感兴趣的同学可以看这两个链接：http://roaringbitmap.org、https://github.com/RoaringBitmap/RoaringBitmap。\n RoaringDocIdSet存储文档号流程图\n图1：\n\n点击查看大图\n 文档号集合\n图2：\n\n  RoaringDocIdSet只允许处理按照文档号从小到大递增的文档号集合，每当处理一个新的文档号，称之为docId，会判断与上一个处理的文档号lastDocId的大小，如果小于等于lastDocId，说明文档号集合不满足要求，抛出如下的异常：\nif (docId &lt;= lastDocId) &#123;    throw new IllegalArgumentException(&quot;Doc ids must be added in-order, got &quot; + docId + &quot; which is &lt;= lastDocID=&quot; + lastDocId);&#125;\n 是否还有未处理的文档号？\n图3：\n\n  我们从最小的文档号开始，逐个处理文档号集合中的文档号，当处理完所有的文档号，那么就完成了图1所示的流程。\n 是否处理上一个block（lastBLock）？\n图4：\n\n  block是什么：\n\n根据规则将文档号划分到一个或多个block中处理，在同一个block中的所有文档号会以同一种方式存储（存储的方式在下文中会介绍），不同block之间的存储方式没有相互联系。\n\n  规则是什么：\n\n规则如下:\n\ndocId &gt;&gt;&gt; 16\n\n由上面的规则可以看出文档号从0开始，每65536个文档号被划分为一个block，如果我们有下面的文档号集合：\n\n图5：\n\n\n划分后的block如下所示，根据规则，可以知道一个block中最多可以包含65536个文档号：\n\n图6：\n\n  我们以图5中的文档号集合为例，当处理到文档号65536时，该文档号会被划分到第二个block中，那么此时我们就需要处理第一个block，此时第一个block成为图4中的lastBlock，同时第二个block成为currentBlock。\n 根据block中的文档号数量选择存储方式\n图7：\n\n  上文中我们说到使用block来存储文档号，并且一个block中最多存储65536个文档号，但是并没有给出block的数据结构，原因是block的数据结构取决于block中包含的文档号数量。\n block的数据结构\n  block一共有两种数据结构：\n\nshort类型数组：当block中包含的文档号数量小于4096个时，使用该数据结构\nFixedBitSet：当block中包含的文档号大于等于4096个时，使用该数据结构，FixedBitSet在前面的文章中已经介绍，这里不赘述，感兴趣的可以点击链接查看：https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0404/45.html\n\n  short类型数组向FixedBitSet的转化（重要）：\n\n上文中我们说到，从图2中的文档号集合中依次处理每一个文档号，该文档号总是先用short类型数组存储，该数组的变量名为buffer，当该数组中的元素超过4096个时，那么如果下一个文档号仍然属于同一个block，那么需要先将short类型数组中所有元素存放到FixedBitSet对象中，该对象的变量名为denseBuffer，并且后续属于同一个block的文档号都使用denseBuffer存放，这就是转化的过程\n**（重要）**图7的流程没有描述转化的过程，会让读者误以为使用了两种数据结构存储了同一个block中的文档号集合\n\n 处理上一个block（lastBlock）\n图8：\n\n  根据block中包含的文档号的数量来判断稠密度：\n\n稀疏：block中存储的文档号数量小于等于4096个认为是稀疏的\n稠密：block中未存储的文档号数量小于4096个认为是稠密度，一个block最多可以存储65536个文档号，即稠密的block中存储的文档号数量大于等于61440个文档号\n既不稀疏也不稠密：block中存储的文档号数量大于4095并且小于61440个认为是既不稀疏也不稠密\n\n  不同稠密度的文档号集合如何存储：\n\n稀疏：使用short类型数组存储\n稠密：计算出那些未存储的文档号，然后使用short类型数组存储\n既不稀疏也不稠密：使用FixedBitSet存储\n\n  例如我们以下的文档号集合：\n图9：\n\n  图10描述了将图9中的文档号划分到不同的block，其中第一个block中包含了0~65533共65534个文档号：\n图10：\n\n  图11描述了进行了稠密度计算后，将处理后的block添加到block数组中，第一个block包含了65534个文档号，故认为是稠密的，由于一个block最多只能存储65536个，那么计算出那些未存储的文档号，即65534跟65535两个文档号，然后存储它们两个，而第二个block跟第三个block都只包含了2个文档号，所以认为是稀疏的，直接存储即可：\n图11：\n\n RoaringDocIdSet读取文档号\n  RoaringDocIdSet提供了两种方法来读取文档号:\n\n获取所有文档号：这种方法只需要逐个block数组的每一个元素，如果block使用short类型数组存储，那么顺序遍历该数组中的元素，如果使用FixedBitSet存储，其遍历方法见https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0404/FixedBitSet\n判断某个文档号是否存在：根据下面的规则，找到该文档号属于block数组中的哪一个block，如果block使用short类型数组存储，那么使用二分法尝试在该数组中找，如果使用FixedBitSet存储，其查找方法见https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0404/FixedBitSet\n\ndocId &gt;&gt;&gt; 16\n 结语\n  个人觉得直接看源码应该比看我写的文章能更快的了解RoaringDocIdSet😁，所以点击这个链接看下吧：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.java。\n点击下载附件\n","categories":["Lucene","gongjulei"],"tags":["docId","docIdSet","RoaringDocIdSet"]},{"title":"SIMD（Lucene 8.7.0）","url":"/Lucene/Codecs/2021/0115/SIMD/","content":"  从Lucene 8.4.0开始，在写入/读取倒排信息时，即写入/读取索引文件.doc、.pos、.pay时，通过巧妙的编码方式（下文中展开）使得C2编译器能生成SIMD（Single Instruction Multiple Data）指令，从而提高了写入/读取速度。\n SIMD（Single Instruction Multiple Data）\n  下文中关于SIMD的介绍基于以下的一些资料，如果链接失效，可以阅读文章底部附件中的备份：\n\n文章一：http://www1.cs.columbia.edu/~kar/pubsk/simd.pdf\n文章二：http://daniel-strecker.com/blog/2020-01-14_auto_vectorization_in_java/#Output Interpretation\n文章三：http://www.songho.ca/misc/sse/sse.html\n文章四：https://stackoverflow.com/questions/59725341/java-auto-vectorization-example\n文章五：https://en.wikipedia.org/wiki/SIMD\n文章六：https://prestodb.rocks/code/simd\n\n  由于本人在指令集方面有限的知识储备，只能泛泛而谈，无法准确识别上文中可能出现的错误，欢迎该方面的大佬勘误。如果能将勘误内容写到https://github.com/LuXugang/Lucene-7.5.0的issue中就更好啦。\n  SIMD指令集使得CPU能同时对多个值执行相同的操作：\n图1：\n\n  图1选自论文http://www1.cs.columbia.edu/~kar/pubsk/simd.pdf。 上图中，X、Y的值存放在128bit的寄存器中，其中X、Y的值占32bit。通过SIMD，使得可以同时计算四次运算（operand）。\n 自动向量化（Automatic Vectorization）\n  先贴出Wiki的原文：\nAutomatic vectorization, in parallel computing, is a special case of automatic parallelization, where a computer program is converted from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once\n  上文的大意为在一次处理过程中，由只能执行一对运算（operand）变成执行多对运算成为自动向量化。\n  在写完一个Java程序后，Java代码会被编译为字节码并且存放到class文件中，随后在运行之前或运行期间，字节码将再次被编译。这次字节码将被编译为机器码（native machine code）这个过程即JIT编译。\n  不同于C/C++，在编写Java代码时，没有显示的接口或者方式来指定向量计算，在Java中，完全是通过C2编译器来判断某段代码是否需要向量化。\n SIMD In Java\n  Java程序中，可以通过指定虚拟机参数查看运行期间生成的汇编指令。\n 虚拟机参数\n  添加两个虚拟机参数：-XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly。\n  另外需要下载hsdis-amd64.dylib（见附件），在Mac系统中，并将其放到/Library/Java/JavaVirtualMachines/jdk-12.jdk/Contents/Home/lib目录中即可。\n 例子1\n  在http://daniel-strecker.com/blog/2020-01-14_auto_vectorization_in_java/ 中详细的介绍了如何判断Java程序运行时是否使用了SIMD，这里不赘述展开。\n 例子2\n  通过下面的例子做一个粗糙的性能比较（JDK8），demo地址见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.7.0/src/main/java/io/simd/SIMDTest.java 。\npublic class SIMDTest &#123;    private static final int LENGTH = 100;    private static long profile(float[] x, float[] y) &#123;        long t = System.nanoTime();        for (int i = 0; i &lt; LENGTH; i++) &#123;            y[i] = y[i] * x[i];        &#125;        t = System.nanoTime() - t;        return t;    &#125;    public static void main(String[] args) throws Exception &#123;        float[] x = new float[LENGTH];        float[] y = new float[LENGTH];        // to let the JIT compiler do its work, repeatedly invoke        // the method under test and then do a little nap        long minDuration = Long.MAX_VALUE;        for (int i = 0; i &lt; 1000000; i++) &#123;            long duration = profile(x, y);            minDuration = Math.min(minDuration, duration);        &#125;        System.out.println(&quot;duration: &quot; + minDuration + &quot;ns&quot;);    &#125;&#125;\n  下图是重复执行二十次获得的duration的最小值：\n图2：\n\n  图2中Y轴的单位为ns，通过虚拟机参数-XX:UseSSE=0 -XX:UseAVX=0 -XX:-UseSuperWord来关闭C2编译器的自动向量化。\n SIMD In Lucene\n  在这个issue中，详细的介绍了基于SIMD写入/读取倒排表信息的所有相关信息。其核心为在写入/读取阶段，通过巧妙的编码方式，使得能让编译器识别这段代码应该使用SIMD指令。\n  我们通过介绍倒排表信息的压缩/解压来介绍这个巧妙的编码方式。\n 倒排表信息的压缩\n  倒排表信息即索引文件索引文件.doc、.pos、.pay中的内容，其中索引文件.doc中的文档号、词频信息，索引文件.pos中位置信息，索引文件.pay中的payload、偏移信息都会在压缩后再写入到索引文件中。\n  其压缩的核心思想就是bitPacking，也就是在文章PackedInts（一）中提到的固定位数按位存储的方式，通过bitPacking对128个long类型的整数进行压缩处理。\n  固定位数按位存储描述的是对于一组待处理的数据集合，每一个数据都用固定数量的bit存储，我们称之为bitsPerValue，并且bitsPerValue的大小取决于数据集合中的最大值所需要的bit数量，如果有以下的数据集合：\n&#123;3, 2, 9, 5&#125;\n  上述集合中的最大值为9，它对应的二进制为0b00001001,可见有效的bit数量为4，即bitsPerValue的值为4，那么只需要存储1001就可以来描述数值9。根据上述固定位数按位存储的概念，以数值2为例，故需要存储0010来描述它。\n  我们以一个例子继续介绍倒排表信息的压缩。\n  如果有一个待压缩的词频信息集合，并且bitsPerValue的值为4，我们用long类型的数组来描述这个集合，并且该集合中有128个词频信息，如下所示：\n图3：\n\n  压缩的过程为两次收缩（collapse）操作：\n 第一次收缩操作\n  根据bitsPerValue的值使用对应三种收缩方式中的一种：\n表一\n\n\n\nbitsPerValue\n收缩方式\n\n\n\n\nbitsPerValue &lt;= 8\ncollapse8\n\n\n8 &lt; bitsPerValue &lt;= 16\ncollapse16\n\n\nbitsPerValue &gt; 16\ncollapse32\n\n\n\n  表一中的三种收缩方式大同小异，故这里我们只以collapse8为例展开介绍。\n  收缩方式collapse8描述的是对于bitsPerValue &lt;= 8的待压缩的数据，对每个数据按照固定的8个bit进行压缩，这个过程为第一次收缩操作。\n  我们先对照collapse8的代码做一个简单的介绍：\n图4：\n\n  图4中，第84行的参数arr即包含128个long类型数值的数组，并且数组中的最大值不超过256，即bitsPerValue &lt;= 8。\n  另外第85行的循环次数为16，因为第86行中，每一次循环的会同时处理8个数值，由于一共有128个数值，故需要循环16次。\n  图4中的第86行代码，我们会提出两个疑问，如下所示：\n  疑问一：为什么一次循环处理8个数值？\n  正如上文介绍的，第一次收缩操作的目的是将每个数据按照固定的8个bit进行压缩，由于数组arr[ ]是long类型，那么一个占64个bit的long类型的数组元素就可以存储8个占用8个bit的数据，即将8个数组元素塞到一个数组元素中，如下所示：\n图5：\n\n  由于画图界面有限，图5中仅给出8个数值中的4个进行收缩操作。可见在第一次循环中，对8个数据各自取低位的8个bit，组合到了一个占64个bit的long类型的数组元素中。\n  图5中第一个数组元素的值未显示是因为数值对应的十进制位数较多，会影响图的美感。\n  疑问二：每次循环如何选择8个数值？\n  从图4跟图5可以看出，第一次循环选择的8个数值为数组下标为0、8、16、32 … 112的数组元素。为什么要按照这种方式选择，或者说为什么不选择下标值0~8的前8个数组元素呢以及其他方式？对于该疑问我请教了实现该方法的PMC（Project Management Committee），如下所示：\n图6：\n\n  正如Adrien Grand回复的那样，按照图4中的选择方式可以让C2编译器生成SIMD指令。至于为什么采用这种方式就能生成SIMD指令，在issue中说到，作者是受到这篇文章的启发：https://fulmicoton.com/posts/bitpacking/  （如果链接失效，可以看附件中的文章Of bitpacking with or without SSE3）。感兴趣的朋友可以阅读下，本文不展开介绍。在Java中，可以通过运行期间生成的汇编代码判断是否生成了SIMD指令。\n 第二次收缩操作\n  图5中，以数组的第一个数组元素为例，在进行了第一次收缩操作后，该数组元素中存储了8个数据，每个数据占用8个bit，又因为该数组的bitsPerValue的值为4，所以每个数据还有4个bit（高4位）是无效的，无效指的是这些bit不参与描述一个数据。那么随后会通过第二次收缩操作消除这些bit。\n  图3的数组在经过第一次收缩操作后，128个数据分布在了前16个数组元素中，即下标值0~15的数组元素存放了原始的128个数据。还是以第一个数组元素为例，该数组元素64个bit中有32个bit是无效的，那么我们通过下面的方式将这些无效的bit变成有效的，如下所示：\n图7：\n\n   图7中，第一个数组元素，即下标值为0的数组元素，先执行第一步的左移4个bit，然后跟下标值8的数组元素进行第二步的或操作。注意的是下标值为8的数组元素不需要位移操作。第二步的或操作结束后，下标值为8的数组元素中的有效bit就覆盖了第一个数组元素中的无效的bit了，意味着存储了16个数据。\n  两次收缩操作使得用来描述数据的long类型的值中所有的bit都是有效的，即都会参与用于描述数据。最终128个long类型的数据被压缩成8个long类型的数值。\n  为什么选择下标值8的数组元素跟第一个数组元素进行或操作？\n  或者说可以选择其他数组元素进行或操作吗，答案是可以的，但是源码中的选择方式可以使得C2编译器在运行期间生成SIMD指令。同样的，想深入理解这种选择方式的话可以阅读https://fulmicoton.com/posts/bitpacking/ 这篇文章。\n 结语\n  无。\n点击下载附件\n","categories":["Lucene","Codecs"],"tags":["Codecs","simd"]},{"title":"Scorer（Lucene 9.6.0）","url":"/Lucene/Search/2023/0814/Scorer/","content":"  阅读文本之前，建议先看下文章ImpactsDISI，有助于理解。先直接给出Scorer在Lucene中的注释：\n图1：\n\n  图1中的注释已经几乎不能完全用于理解Scorer类，因为这个类经过十几年的迭代，注释却没有保持更新，甚至部分描述还不准确。例如红框标注的注释说到：使用给定的Similarity对文档进行打分。由于在早期Scorer的构造函数的参数中需要提供Similarity对象，但在十年前提交的LUCENE-2876中移除了该参数。该PR merge后会更新这些注释。见旧版的Scorer类：\n图2：\n\n  因此我想通过直接介绍Scorer中的方法，让大家直观的了解Scorer在目前版本（Lucene 9.6.0）中定位。\n Scorer的抽象方法\n图3：\n\n  Scorer类继承Scorable类，因此同样会介绍父类中的方法。\n 方法一\n图4：\n\n  Scorer的Iterator()方法说的是，它能提供一个文档号迭代器，即DocIdSetIterator对象。\n  DocIdSetIterator中包含了满足查询条件（部分实现可能不满足查询条件，比如DocValuesIterator，下文会介绍）的文档号集合，以及遍历这些文档号的方式。在文章BulkScorer（一）中介绍了DocIdSetIterator的概念，并且在文章ImpactsDISI中介绍了DocIdSetIterator的一种实现方式，并且说到ImpactsDISI可以利用Impact信息来实现特殊的文档号遍历方式。\n 方法二\n图5：\n\n  通过这个方法可以看出，Scorer对象中含有状态值，即当前正在进行打分的文档号。\n  我们在文章BulkScorer（一）也提到了DocIdSetIterator对象也含有状态值。没错，Scorer对象中的状态值在很多子类实现中就是DocIdSetIterator对象中的状态值。下图是Scorer的TermScorer的部分实现：\n图6：\n\n  图6中，从第39行的代码可以看出，Iterator跟postingEnum是同一个对象，第68、59行代码分别是上文中方法一、方法二的实现。\n 方法三\n图7：\n\n  该方法用于通知Scorer对象，目前已收集到的文档中最小的打分值（历史最低打分值）。\n  通常在执行TopN查询并且使用文档打分值作为排序规则时会调用该方法。比如我们在收集器Collector中收集到N篇文档后，可以通过一个排序规则为打分值的优先级队列获取堆中最小的打分值minCompetitiveScore，意思是后续的文档的打分值只有大于minCompetitiveScore才是具有竞争力的。此时收集器就会通过调用该方法通知Scorer对象：目前具有竞争力的文档的打分值必须大于minCompetitiveScore。使得一些Scorer的子类可以基于这个minCompetitiveScore实现优化。优化的方向其实就是对满足查询条件的待遍历的文档号集合进行&quot;瘦身&quot;，跳过掉那些打分值小于等于minCompetitiveScore的文档号，或者说筛选出高于minCompetitiveScore的文档号集合。\n  我们还是以TermScorer为例，对于该方法的实现逻辑如下所示：\n图8：\n\n  图8中，TermScorer对象将minCompetitiveScore信息告知了ImpactsDISI对象。至于ImpactsDISI如何基于minCompetitiveScore实现文档号集合的&quot;瘦身&quot;，见文章ImpactsDISI。\n 方法四\n图9：\n\n  上文的方法二中描述的是当前正在进行打分的文档号，那么调用方法四就是对这篇文档进行打分。\n  下图是TermScorer中的实现方式：\n图10：\n\n  图10中，docScorer是封装了Similarity的LeafSimScorer对象，只需要提供文档号（可以根据文档号获取到标准化值）以及在这篇文档中的词频就可以进行打分（见文章ImpactsDISI中关于打分公式的介绍）。\n 方法五\n图11：\n\n  该方法是为了避免数据稀疏和零概率问题而使用的平滑技术，使得查询文档中不存在的term也可以进行打分。\n  下图是TermScorer中的实现方式：\n图12：\n\n  图12中，Lucene的打分公式支持计算平滑值，只需要将词频值设置为0即可。\n 方法六\n图13：\n\n  Scorer不仅仅提供方法一中的DocIdSetIterator对象，还会根据具体的子类需要同时实现TwoPhaseIterator，当DocIdSetIterator中的文档号集合不全部满足查询条件时，即所谓的接近（approximation）满足查询结果，这时候就可以用TwoPhaseIterator进行准确的判断某个文档号对应的文档是否满足查询条件。\n TwoPhaseIterator\n  我们通过SortedNumericDocValuesSetQuery来介绍下使用TwoPhaseIterator对接近（approximation）满足查询结果的文档号集合如何进行准确匹配。\n图14：\n\n  代码第44行，使用NumericDocValuesField.newSlowSetQuery进行查询，查询条件是代码43行中包含数值至少包含2或者3的文档。\n  由于文档中只使用了DocValues，因此不支持通过term（即例子中的2跟3两个数值）查询文档号，因此对于NumericDocValuesField.newSlowSetQuery，只能遍历所有的文档，随后每处理一篇文档就使用TwoPhaseIterator中的matches方法判断文档中是否包含2、3这两个正排值中的一个。下面给出matches的实现方式：\n图15：\n\n  由于图14中我们使用了SortedNumericDocValuesField，它属于多值的情况，即一篇文档中，某个域的正排值可以是多个。图15中代码第133行的count计算的是当前处理的文档中的正排值的数量。对于图14中的三篇文档，他们的count值分别是：3、1、2。\n  图15中的numbers是一个包含了查询条件，即图14中代码第43行的对象。代码第134行的for循环描述的是依次读取当前文档中的正排值，判断numbers是否包含，如果包含，说明当前文档满足查询条件。\n 方法七\n图16：\n\n  注释中说到调用该方法能到达文档号target所在的block，目的是获取到打分信息。\n  该方法跟方法八一样都是为了实现[block-max WAND](https://www.amazingkoala.com.cn/Lucene/Search/2020/0916/block-max WAND（一）)算法在LUCENE-8135中出现的。尽管在本篇文章发布之前还未全部完成block-max WAND算法的介绍，但我们可以通过文章ImpactsDISI了解该方法以及方法八，在本篇文章中，只概述下这两个方法：\n\n文档号在索引文件.doc中是按block进行划分存储的，默认每128篇文档号作为一个block，advanceShallow方法中会通过跳表找到target所在block\n当跳到所在block后，就可以通过调用方法八 getMaxScore计算该block中所有文档中最大的打分值。至于为什么要获取maxScore，以及计算方式，请查看文章ImpactsDISI\n\n 方法八\n图17：\n\n  见方法七中的介绍。\n 结语\n  至此，图3中的所有方法除了 getChildren和getWeight方法都已经介绍完毕。可以看出Scorer类型提供下面的功能：\n\n对文档进行打分\n\n打分逻辑取决于不同子类中定义的Similarity对象\n\n\n提供用于遍历的文档号集合\n\n该集合在大部分实现中是一个满足查询条件的文档号集合，如果不是，那么会额外提供一个TwoPhaseIterator实现准确匹配\n\n\n记录历史最低打分值，用于doc skip\n\n用于在TopN的查询中，结合跳表、文档分块存储特点，不对那些低于历史最低分值的文档进行处理\n\n\n\n点击下载附件\n","categories":["Lucene","Search"],"tags":["impact","scorer"]},{"title":"SegmentReader（一）","url":"/Lucene/Index/2019/1014/SegmentReader%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  在近实时搜索NRT的系列文章中，我们知道用于读取索引目录中所有索引信息的StandardDirectoryReader实际是使用了一个LeafReader数组封装了一个或者多个SegmentReader，而每一个SegmentReader则对应一个段中的索引信息，如下图所示：\n图1：\n\n  本篇文章将会介绍在生成SegmentReader的过程中，它获取了哪些具体的索引信息信息，更重要的是，我们还会了解到为什么通过DirectoryReader.openIfChange()（见近实时搜索NRT（三））重新打开一个StandardDirectoryReader的开销会远远的小于DirectoryReader.open()方法（见近实时搜索NRT（一））。\n 生成SegmentReader的流程图\n图2：\n\n 获取SegmentReader的元数据LeafMetaData\n  LeafMetaData包含的信息如下：\n\ncreatedVersionMajor：该值描述了SegmentReader读取的段所属的Segment_N文件，它是创建时的Lucene的主版本（major）号，见文章索引文件之segments_N中的Version字段的介绍\nminVersion：该值通过.si索引文件获得，含义见文章索引文件之si中的SegVersion字段的Min.major、Min.minor、Min.bugfix的介绍\nsort：该值同样通过.si索引文件获得，描述了段中的文档的排序规则，其含义见文章索引文件之si中的IndexSort的介绍，不赘述\n\n  下图描述了LeafMetaData包含的信息对应在索引文件中的信息：\n图3：\n\n  图3中的蓝色箭头描述的是我们可以通过segName找到索引目录中的.si索引文件，可以点击近实时搜索NRT（一）查看segName更详细的介绍。\n 获取不会发生变更的SegmentCoreReaders\n  我们首先介绍下SegmentCoreReaders包含了哪些主要信息：\n\nStoredFieldsReader：从索引文件fdx&amp;&amp;fdt中读取存储域的索引信息\nFieldsProducer：从索引文件tim&amp;&amp;tip、索引文件doc、索引文件pos&amp;&amp;pay中读取域的索引信息\nTermVectorsReader：从索引文件tvx&amp;&amp;tvd读取词向量的索引信息\nPointsReader：从索引文件dim&amp;&amp;dii中读取域值为数值类型的索引信息\nNormsProducer：从索引文件nvd&amp;&amp;nvm中读取域的打分信息\nFieldInfos：从索引文件fnm读取域的信息\nsegment：段的前缀名\n\n  上文中涉及好几个读取索引文件的操作，这里并不会详细展开读取索引文件的过程，因为太简单了。\n  不过这里得提一下，使用和不使用复合索引文件cfs&amp;&amp;cfe时，读取索引文件的区别：\n  图4、图5分别是不使用复合索引文件和使用复合索引文件时索引目录中的文件列表：\n图4：\n\n图5：\n\n  在不使用复合索引文件的情况下，获得SegmentCoreReaders只需要根据段的前缀名从索引目录中分别找到每一个非复合索引文件，随后读取即可，而在使用复合索引文件的情况下，我们需要先根据复合索引文件cfs&amp;&amp;cfe，才能读取每一个非复合索引文件的信息。\n  根据图4、图5的例子，复合索引文件的数据结构如下图所示：\n图6：\n\n  图6中每一个字段的含义已经在索引文件之cfs&amp;&amp;cfe的文章中介绍，不赘述。\n  SegmentCoreReaders中包含的索引信息中StoredFieldsReader、TermVectorsReader、PointsReader、NormsProducer是不会发生更改的内容，使得SegmentCoreReaders能够被复用，如果在未来的操作，该段中的索引信息发生更改，那么段中变更的索引信息会以其他索引文件来描述，这便是索引文件之liv、索引文件值.dvm、.dvd、索引文件之fnm，索引信息发生变更的情况以及描述变更的方式如下所示：\n\n文档被删除：被删除的文档通过索引文件之liv来描述\n文档被更新：文档的更新实际是先删除，后添加的过程，如果是更新DocValues，那么使用索引文件值.dvm、.dvd、索引文件之fnm来描述\n\n  所以在我们通过DirectoryReader.openIfChange()获取最新的StandardDirectoryReader时，即使StandardDirectoryReader中某个或多个SegmentReader发生变更，我们可以直接复用SegmentCoreReaders中的索引信息，然后只需要更新（读取）相对开销较小的索引文件之liv、索引文件值.dvm、.dvd、索引文件之fnm这些索引文件即可。\n  这里的开销指的是什么：\n\n索引文件的信息在磁盘，这里的开销的大头实际就是读取索引文件的磁盘I/O\n\n 获取段中有效的文档号集合Bits\n  Bits中包含的是段中有效的文档号（live document id），使用FixedBitSet存放这些文档号，通过FixedBitSet其实就知道了那些无效的文档号（原因见FixedBitSet的介绍）。\n  无效的文档号包含两种情况：\n\n情况一：添加一篇新的文档的过程中，该文档会先被安排一个文档号，然后再执行添加的逻辑，如果在添加的过程中发生错误导致未能正确的添加这篇文档，那么该文档的文档号被视为无效的（见文章文档提交之flush（三）中处理出错的文档的流程点）\n情况二：被删除的文档也是被视为无效的\n\n  有效的文档号集合Bits通过索引文件之liv获得。\n图7：\n\n  图7中，代码的第84、85行，删除了包含&quot;h&quot;的文档号，那么文档0、4、7三篇文档会被删除，即上文中的无效的文档号，那么对应的.liv索引文件如下所示：\n图8：\n\n  图8中各个字段的名字在文章索引文件之liv中已经介绍，不赘述，图8中的索引信息随后被读取到有效的文档号集合Bits中。\n 获取段中有效的文档号的数量numDocs\n  numDocs的值描述的是段中有效的文档号的数量，它的计算方式如下：\nnumDocs = maxDoc - delCount;\n  上述的计算方式中，maxDoc描述的是DWPT处理的文档总数（包含添加出错的文档），delCount描述的是无效的文档个数，这两个值在索引文件中的位置如下所示：\n图9：\n\n 结语\n  基于篇幅，剩余的内容在下一篇文档中展开介绍。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["reader","segment"]},{"title":"SegmentReader（二）","url":"/Lucene/Index/2019/1015/SegmentReader%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本文承接SegmentReader（一），继续介绍生成SegmentReader的剩余的流程。\n 生成SegmentReader的流程图\n图1：\n\n 获取段中最新的域信息FieldInfos\n  FieldInfos描述了段中所有域的信息，它对应的是索引文件.fnm中的内容，在索引文件之fnm的文章中详细介绍了，这里不赘述。\n  在SegmentReader（一）的文章中我们说到，在图1中的流程点获取不会发生变更的SegmentCoreReaders，SegmentCoreReaders中已经获得了一个FieldInfos，为什么这里还要获取段中最新的域信息FieldInfos呢：\n\n\n同样地在SegmentReader（一）的文章中我们说到，如果一个段中的索引信息发生更改，那么变更的索引信息会以其他索引文件来描述，即索引文件之liv、索引文件之.dvm、.dvd、索引文件之fnm，其中DocValues类型的索引发生更新时，会以索引文件之.dvm、.dvd、索引文件之fnm来描述变更的索引\n\n\n所以如果段中没有DocValues类型的索引变化时，那么我们就可以完全复用SegmentCoreReaders中所有的信息（见SegmentReader（一）），即可以完全复用下面的信息：\n\nStoredFieldsReader：从索引文件fdx&amp;&amp;fdt中读取存储域的域值的索引信息\nFieldsProducer：从索引文件tim&amp;&amp;tip、索引文件doc、索引文件pos&amp;&amp;pay中读取域的索引信息\nTermVectorsReader：从索引文件tvx&amp;&amp;tvd读取词向量的索引信息\nPointsReader：从索引文件dim&amp;&amp;dii中读取域值为数值类型的索引信息\nNormsProducer：从索引文件nvd&amp;&amp;nvm中读取域的打分信息\nFieldInfos：从索引文件fnm读取域的信息\n\n\n\n那么如果段中没有DocValues类型的索引变化时，当我们通过DirectoryReader.openIfChange()获取最新的StandardDirectoryReader时，能获得比直接调用DirectoryReader.open()有更高的性能，其实就是大大降低了读取索引文件的I/O开销\n\n\n那么如果段中DocValues类型的索引发生了变化，我们就需要重新读取索引目中的.fnm文件来获得最新的域信息FieldInfos\n\n\n  如何判断段中的DocValues类型的索引发生了变化？：\n\n通过索引文件之segments_N中的字段来获得，如下图所示：\n\n图2：\n\n  图2中红色框标注的FieldInfosGen的值如果不是 -1，那么说明段中的DocValues类型的索引更新了。\n 获取段中DocValues的信息DocValuesProducer\n  DocValuesProducer描述了DocValues的索引信息，它通过索引文件.dvd&amp;&amp;dvm获得，在这个流程点我们关注的是如何读取索引文件.dvd&amp;&amp;dvm。\n  下图描述的是包含了DocValues索引信息的一个段在索引目录中包含的索引文件，并且这里未使用复合索引文件：\n图3：\n\n  图3中红框标注的索引文件即描述了该段中的DocValues类型的索引信息，那么我们通过读取这两个文件就可以获得DocValuesProducer。\n  如果该段的DocValues类型的索引信息发生了变更，那么该段包含的索引文件如下所示：\n图4：\n\n  正如我们上文所说的，当段中的DocValues类型的索引信息发生了变更，其变更的内容用索引文件之.dvm、.dvd、索引文件之fnm来描述，即图4中用蓝框标注的3个索引文件“_0_1.fnm”、“_0_1_Lucene70_0.dvd”、“0_1_Lucene70_0.dvm”。\n  如果我们使用复合索引文件建立索引能更直观的看出DocValues类型的索引信息发生了变更后，索引目录中的索引文件的变化。\n  图5为使用复合索引文件的一个段在索引目录中包含的索引文件：\n图5：\n\n  如果该段的DocValues类型的索引信息发生了变更，那么该段包含的索引文件如下所示：\n图6：\n\n  不管是否使用复合索引文件，如果该段的DocValues类型的索引信息发生了变更，那么该段中就会包含旧的.dvd、dvm索引文件文件（“_0_Lucene70_0.dvd”、“0_Lucene70_0.dvm”）以及新的.dvd、.dvm索引文件（“_0_1_Lucene70_0.dvd”、“0_1_Lucene70_0.dvm”），那么当我们获取这个段对应的SegmentReader时就会读取新的.dvd、.dvm索引文件。\n OpenIfChange()方法\n  在调用该方式时，如果发现某个SegmentReader（我们称之为旧的SegmentReader）需要更新（见近实时搜索NRT（三）），那么我们需要获得一个新的SegmentReader，我们会先完全复用旧的SegmentReader中的SegmentCoreReaders、DocValuesProducer，然后根据图1中的Bits以图4中蓝框标注的索引文件作部分的更新。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["reader","segment"]},{"title":"SortedDocValues","url":"/Lucene/DocValues/2019/0219/SortedDocValues/","content":"SortedDocValues同NumericDocValues、SortedNumericDocValues一样，在实际应用中最多的场景用于提供给搜索结果一个排序规则。本篇文章只讲述使用了SortedDocValues后，其在.dvd、.dvm文件中的索引结构。在以后的介绍例如 facet、join、group等功能时，会详细介绍如何根据.dvd、dvm中的索引数据进行查询的过程。\n 预备知识\n下面出现的变量名皆为源码中的同名变量名。\n TermId\n在索引阶段，根据IndexWriter中添加的document的顺序，有序的处理每一个document中的SortedDocValuesField。并且对每一个SortedDocValuesField的域值赋予一个从0开始递增的termId，相同的域值具有相同的termId。\n图1：\n\n域值与termId的对应关系如下：\n\n\n\n域值\naa\ncc\nbb\nff\n\n\n\n\ntermId\n0\n3\n2\n1\n\n\n\n currentValues[]数组\ncurrentValues[]数组中，下标值为文档号docId，数组元素为termId。在索引阶段，由于处理的数据是按照IndexWriter中添加的document的顺序进行的，即按照第一篇文档(document)，文档号为0，文档号递增的顺序。所以在这个过程中，就可以实现通过数组方式来记录 文档号(docId) 跟 termId 的映射关系\n图2：\n\n sortedValues[]数组 &amp;&amp; ord\nsortedValues[]数组中的数组元素是termId，数组下标是ord值。下面的一句话很重要：数组元素是有序的，但是排序规则不是根据termId的值，而是根据termId对应的域值的字典序。\n图3：\n\n ordMap[]数组\nsortedValues[]数组中实现了 数组下标ord 到 数组元素termId的映射，而ordMap[]数组则是实现了 数组下标termId 到 数组元素 ord的映射。\n图4：\n\n 数据结构\n dvd\n图5：\n\n DocIdData\nDocIdData中记录包含当前域的文档号。\n如果IndexWriter添加的document中不都包含当前域，那么需要将包含当前域的文档号记录到DocIdData中，并且使用IndexedDISI类来存储文档号，IndexedDISI存储文档号后生成的数据结构单独的作为一篇文章介绍，在这里不赘述。\n Ords\nOrds记录了每一篇文档中SortedDocValuesField的域值对应的ord值。这里的ord值即上文中的预备知识中的ord值。\n TermsDict\nTermsDict中根据域值大小，并且按块处理，存储所有的SortedDocValuesField域值。\n图6：\n\n Block\n存储SortedDocValuesField的域值时，按照域值从小到大的顺序，并且按块处理所有的域值，相同的域值不会重复存储。\n每16个域值按照一个块（block）处理，在一个块内，只存储第1个域值的完整值，对于剩余的15个域值，只存储与前一个域值不相同后缀的部分值（所以需要按照域值大小顺序存储才能尽可能减少空间占用），即前缀存储。\n图7：\n\n FirstTermLength\n每个block的第1个域值的长度。\n FirstTermValue\n每个block的第1个域值的完整值。\n PrefixLength &amp;&amp; SuffixLength\nprefixLength是当前域值与前一个域值相同前缀的长度，如果当前当前是第10个域值，那么它跟第9个域值进行prefixLength的计算。suffixLength是除去相同前缀的剩余部分的长度。\n SuffixValue\n除去相同前缀的剩余部分的域值。\n BlockIndex\nBlockIndex中记录了每一个block相对于第一个block的在.dvd文件中的偏移，在读取阶段，如果需要读取第n个block的数据，那么只要计算 第n个和第n+1个的blockIndex的差值，就可以获得第n个block在.dvd文件中的数据区间，实现随机访问。每一个block的blockIndex采用PackedInts进行压缩存储。\n Block与ord的关系\n由于域值是从小到大写入到所有Block中， 而在上文中的预备知识中得知，sortedValues[]数组的ord的值正是描述了域值的大小关系，所以写入到block中的第1个域值就是对应ord的值0，第8个域值就是对应ord的值7，ord的值从0开始递增。\n下面的例子描述了我们搜索后的结果只要返回Top3，并且比较规则是根据SortedDocValuesField的域值进行比较。\n图8：\n\n但是满足搜索要求的文档只会将docId传入到Collector类的collect(int doc)方法中，即我们只能知道文档号的信息，无法获得当前文档中SortedDocValuesField的域值来做排序比较。\n这时候我们可以根据文档号docId从IndexedDISI中找到段内编号（见文章IndexedDISI）。段内编号作为currentValues[]数组下标值，取出数组元素，即termId，然后termId作为ordMap[]数组下标值，取出数组元素，即ord值。根据ord值我们就可以找到当前文档中的 SortedDocValuesField的域值在对应的Block中，然后遍历Block中的16个域值，直到找到我们想要的域值。\n图9：\n\n TermsIndex\nTermsDict中每遍历16个域值就处理为一个block，而在TermsIndex中则是每遍历1024个域值就记录一个域值的前缀值。\n图10：\n\n PrefixValue\n与上一个域值的相同的前缀值 加上 后缀值的第一个字节。\n例如第1个域值为 “ab”、第1023个域值为&quot;abcdasdfsaf&quot; 、第1024个域值为&quot;abceftn&quot;、第2047个域值为 “abceftop” 、第2048个域值为 “abceftoqe”，那么存储到TermsIndex的数据如下：\n图11：\n\n PrefixValueIndex\nPrefixValueIndex中记录了每一个prefixValue相对于第一个prefixValue的在.dvd文件中的偏移，在读取阶段，如果需要读取第n个prefixValue的数据，那么只要计算 第n个和第n+1个的PrefixValueIndex的差值，就可以获得第n个prefixValue在.dvd文件中的数据区间，实现随机访问。每一个prefixValue的PrefixValueIndex采用PackedInts进行压缩存储。\n PrefixValue有什么用\n在TermsDict中，我们可以根据文档号在TermsDict找到对应的SortedDocValuesField的域值，但是通过TermsIndex我们就可以判断 某个域值是不是在SortedDocValuesField中。\n例如我们需要判断域值 “abcef” 是否为SortedDocValuesField的一个域值，那么就可以使用二分法遍历PrefixValue，每次跟&quot;abcef&quot;比较，根据上面的例子，我们可以知道 “abcef” 大于““abce”，并且小于&quot;abceftoq”。\n图12：\n\n所以我们接着根据两个ord值去TermsDict中继续查找，同样的的使用二分法去遍历Block。\n dvm\n图13：\n\n FieldNumber\n域的编号\n DocvaluesType\nDocvalues的类型，本文中，这个值就是 SORTED。\n DocIdIndex\nDocIdIndex是对.dvd文件的一个索引，用来描述 .dvd文件中DocIdData在.dvd文件中的开始跟结束位置。\n 情况1：\n图14：\n\n如果IndexWriter添加的document中都包含当前域，那么只需要在DocIdIndex中添加标志信息即可。\n 情况2：\n图15：\n\n如果IndexWriter添加的document中不都包含当前域，那么.dvd文件中需要将包含当前的域的文档号信息都记录下来。\n offset\n.dvd文件中存放文档号的DocIdData在文件中的开始位置。\n length\nlength为DocIdData在.dvd文件中的数据长度。\n在读取阶段，通过offset跟length就可以获得所有的DocIdData数据。\n numDocsWithField\n包含当前域的文档的个数。\n OrdsIndex\n 情况1：\n图16：\n\n如果只有一种类型的域值，那么写入固定的标志信息即可。\n 情况2：\n图17：\n\n numberOfBitsPerOrd\nnumberOfBitsPerOrd描述了存储ord值的固定bit位个数。\n offset\n.dvd文件中存放ord的Ords在文件中的开始位置。\n length\nlength为Ords在.dvd文件中的数据长度。\n TermsDictMeta\n图18：\n\n TermsDictSize\n域值的种类个数(不是域值的个数)\n TermsDictBlockShift\n描述了一个block中有多少个域值。\n DIRECT_MONOTONIC_BLOCK_SHIFT\nDIRECT_MONOTONIC_BLOCK_SHIFT用来在初始化byte buffer[]的大小，buffer数组用来存放BlockIndex。\n Min\n记录一个最小值，在读取阶段用于解码。Min的含义请看我的源码注释。\n AvgInc\n记录一个AvgInc，在读取阶段用于解码。AvgInc的含义请看我的源码注释。\n Length\nBlockIndex的数据总长度\n BitsRequired\n经过DirectMonotonicWriter的数据平缓操作后，每个数据需要的固定bit位个数。\n MaxLength\n域值的最大长度\n BlockMeta\n图19：\n\n offset\nBlock在.dvd文件中的开始位置。\n length\nlength为Block在.dvd文件中的数据长度。\n在读取阶段，通过offset跟length就可以获得所有的所有Block数据。\n BlockIndexMeta\n图20：\n\n offset\nBlockIndex在.dvd文件中的开始位置。\n length\nlength为BlockIndex在.dvd文件中的数据长度。\n在读取阶段，通过offset跟length就可以获得所有BlockIndex数据。\n TermsIndexMeta\n图21：\n\n TERMS_DICT_REVERSE_INDEX_SHIFT\n描述每遍历多少个域值，保存一次PrefixValue。当前版本为1024。\n Min\n记录一个最小值，在读取阶段用于解码。Min的含义请看我的源码注释。\n AvgInc\n记录一个AvgInc，在读取阶段用于解码。AvgInc的含义请看我的源码注释。\n Length\nPrefixValueIndex的数据总长度\n BitsRequired\n经过DirectMonotonicWriter的数据平缓操作后，每个数据需要的固定bit位个数。\n PrefixValueMeta\n图22：\n\n offset\nPrefixValue在.dvd文件中的开始位置。\n length\nlength为PrefixValue在.dvd文件中的数据长度。\n在读取阶段，通过offset跟length就可以获得所有PrefixValue数据。\n PrefixValueIndexMeta\n图23：\n\n offset\nPrefixValueIndex在.dvd文件中的开始位置。\n length\nlength为PrefixValueIndex在.dvd文件中的数据长度。\n在读取阶段，通过offset跟length就可以获得所有PrefixValueIndex数据。\n 结语\n本文详细介绍了SortedDocValues在.dvd、.dvm文件中的数据结构，并简单介绍了为什么要写入TermsDict、TermsIndex的数据。SortedDocValues跟SortedSetDocValues是在所有DocValues中数据结构最为复杂的。另外在预备知识中提到的几个数组，它们都是在SortedDocValuesWriter类中生成，大家可以可看我的源码注释来加快SortedDocValuesWriter类的理解，源码地址：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java\n","categories":["Lucene","DocValues"],"tags":["DocValues"]},{"title":"SortedNumericDocValues","url":"/Lucene/DocValues/2019/0410/SortedNumericDocValues/","content":"SortedNumericDocValues的索引结构跟NumericDocValues几乎是一致的，所以本文不会赘述跟NumericDocValues相同部分的内容，只介绍不同的部分数据结构。两种DocValue的最常用的使用场景就是对搜索结果进行排序，使用SortedNumericDocValues相比较NumericDocValues的优点在于，一篇文档中可以设置多个相同域名不同域值的SortedNumericDocValuesField，而NumericDocValuesField在一篇文档中只允许有一个相同域名的域。因此我们可以在不更改现有索引的情况下，只修改搜索的条件（更改Sort对象）就可以获得不同的排序结果，在以后介绍facet时会详细介绍这部分内容。\n 数据结构\n dvd\n先给出NumericDocValues的.dvd文件的数据结构。\n图1：\n\n再给出SortedNumericDocValues的.dvd文件的数据结构。\n图2：\n\n两个DocValues的DocIdData跟FieldValues部分的数据结构是一样的，因为源码中他们实际调用的是同一个方法来写入这两块的数据。下面介绍不同之处DocValueCount。\n DocValueCount\n在上文中提到，索引阶段使用SortedNumericDocValues的话，一篇文档中可以有多个相同域名不同域值的SortedNumericDocValuesField，而NumericDocValues只能有一个相同域名的NumericDocValuesFIeld，如下图所示。\n图3：\n\n图4：\n\nDocValueCount描述的信息即每篇文档中包含的相同域名不同域值的域的个数。\n这些信息使用了DirectMonotonicWriter类进行了 趋势分解操作，然后使用PackedInts进行了压缩存储。DirectMonotonicWriter中的趋势分解的目的是尽可能减少空间的使用，它用来将 单调递增的整数序列(monotonically-increasing sequences of integers)进行平缓操作，使得在使用PackedInts进行压缩存储时，每一个数值能使用最少的固定bit位存储。\n这里不赘述DirectMonotonicWriter中的趋势分解过程，可以看我的源码注释来理解这个过程：[https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/packed/DirectMonotonicWriter.java]。\n dvm\n先给出NumericDocValues的.dvm文件的数据结构。\n图5：\n\n再给出SortedNumericDocValues的.dvm文件的数据结构。\n图6：\n\n上图中，除了红框标出的DocValueCountMeteData， 其他信息都是与NumericDocValues一致的。\n DocValueCountMeteData\n图7：\n\n NumDocsWithField\nNumDocsWithField描述了包含当前域的文档个数。注意的是如果.dvm中记录的NumValues值，描述的是当前域的域值个数，如果NumDocsWithField与NumValues的值相等，说明每篇文档中只有一个相同域名的SortedNumericDocValuesField，这种情况下就不用记录.dvd文件中的DocValueCount信息，并且此时NumDocsWithField跟SortedNumericDocValues在应用上几乎是一样的。\nNumDocsWithField与NumValues相同的情况下 , 最终的.dvm文件如下图所示：\n图8：\n\n Offset\nOffset描述了DocValueCount信息在.dvd文件中的开始位置。\n DIRECT_MONOTONIC_BLOCK_SHIFT\nDIRECT_MONOTONIC_BLOCK_SHIFT用来在初始化byte buffer[]的大小，buffer数组用来存放每一篇文档中班包含的域值个数。\n Min\n记录一个最小值，在读取阶段用于解码。Min的含义请看我的源码注释。\n AvgInc\n记录一个AvgInc，在读取阶段用于解码。AvgInc的含义请看我的源码注释。\n Length\n在SortedNumericDocValues使用DirectMonotonicWriter的场景中，该值永远为0，不解释。\n BitsRequired\n经过DirectMonotonicWriter的数据平缓操作后，每个数据需要的固定bit位个数。\n DataLength\nDocValueCount信息在.dvd文件中的数据长度。结合上面的Offset，在读取阶段，就可以确定应该读取.dvd文件中的某个数据区间，即DocValueCount信息。\n 结语\n由于SortedNumericDocValues与NumericDocValues的索引文件数据结构非常类似，所以本篇介绍篇幅很小。SortedNumericDocValues这个名词中的Sorted的含义只有在一篇文档中包含多个相同域名不同域值的情况下才有价值体现。在以后介绍SortedNumericDocValues的应用时，会详细介绍它跟NumericDocValues的区别，本篇文章只是介绍在.dvd、.dvm文件中的索引数据结构。\n","categories":["Lucene","DocValues"],"tags":["DocValues"]},{"title":"SortedSetDocValues","url":"/Lucene/DocValues/2019/0412/SortedSetDocValues/","content":"SortedNumericDocValues的索引结构跟SortedDocValues几乎是一致的，所以本文不会赘述跟SortedDocValues相同部分的内容，只介绍不同的部分数据结构。两种DocValue的最常用的使用场景就是对搜索结果进行排序，使用SortedSetDocValues相比较SortedDocValues的优点在于，一篇文档中可以设置多个相同域名不同域值的SortedSetDocValuesField，而SortedDocValues在一篇文档中只允许有一个相同域名的域。因此我们可以在不更改现有索引的情况下，只修改搜索的条件（更改Sort对象）就可以获得不同的排序结果，在后面介绍facet的文章中会详细介绍SortedSetDocValues的应用。\n 数据结构\n dvd\n先给出SortedDocValues的.dvm文件的数据结构。\n图1：\n\n再给出SortedSetDocValues的.dvm文件的数据结构。\n图2：\n\n两个DocValues的TermsDict、TermsIndex部分的数据结构是一模一样的，因为源码中他们实际调用的是同一个方法来写入这两块的数据，另外DocIdData的数据结构也是一样的。下面介绍不同之处OrdsAddress。\n OrdsAddress\n在上文中提到，索引阶段使用SortedSetDocValues的话，一篇文档中可以有多个相同域名不同域值的SortedSetDocValuesField，而SortedDocValues只能有一个相同域名的SortedDocValuesFIeld，如下图所示。\n图3：\n\n图4：\n\nSortedDocValues中的Ords字段，一个ord信息对应一篇文档的域值，而在SortedSetDocValues中多个ord信息对应一篇文档的多个域值，由于所有的ord都存放在Ords中，所以需要OrdsAddress，使得在读取阶段能使每一篇文档获得对应的所有ord值。\nOrdsAddress的值使用了DirectMonotonicWriter类进行了 趋势分解操作，然后使用PackedInts进行了压缩存储。DirectMonotonicWriter中的趋势分解的目的是尽可能减少空间的使用，它用来将 单调递增的整数序列(monotonically-increasing sequences of integers)进行平缓操作，使得在使用PackedInts进行压缩存储时，每一个数值能使用最少的固定bit位存储。\n这里不赘述DirectMonotonicWriter中的趋势分解过程，可以看我的源码注释来理解这个过程：[https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/packed/DirectMonotonicWriter.java]。\n如果在索引阶段，每一篇文档的最多只有一个SortedSetDocValuesField，那么生成的.dvd索引结构跟SortedDocValues就是一样。可以理解为SortedSetDocValuesField退化成了SortedDocValuesField。\n dvm\n先给出SortedDocValues的.dvm文件的数据结构。\n图5：\n\n再给出SortedSetDocValues的.dvm文件的数据结构。\n图6：\n\n从上面两张图可以看出，SortedSetDocValues的.dvm数据结构多出了SingleValue、OrdsAddressMeta两块数据，用红色框标出，另外OrdsIndex跟numDocsWithField两块的数据位置互换了一下。\n SingleValue\nSingleValue描述了是否在索引阶段，每篇文档是否最多包含一个SortedSetDocValuesField，如果是的话，那么.dvm的剩余数据结构跟SortedDocValues是一致的\n图7：\n\n OrdsAddressMeta\n图8：\n\n Offset\nOrdsAddress数据段在.dvd文件的开始位置。\n DIRECT_MONOTONIC_BLOCK_SHIFT\nDIRECT_MONOTONIC_BLOCK_SHIFT用来在初始化byte buffer[]的大小，buffer数组用来存放每一篇文档OrdsAddress。\n Min\n记录一个最小值，在读取阶段用于解码。Min的含义请看我的源码注释。\n AvgInc\n记录一个AvgInc，在读取阶段用于解码。AvgInc的含义请看我的源码注释。\n Length\n在SortedSetDocValues使用DirectMonotonicWriter的场景中，该值永远为0，不解释。\n BitsRequired\n经过DirectMonotonicWriter的数据平缓操作后，每个数据需要的固定bit位个数。\n OrdsAddressLength\nOrdsAddress数据段在.dvd文件的数据长度。\nOrdsAddressLength结合Offset，就可以确定OrdsAddress在.dvd文件中的数据区间。\n 结语\n由于SortedSetDocValues与SortedDocValues的索引文件数据结构非常类似，所以本篇介绍篇幅很小。所以先了解SortedDocValues的数据结构后，那么SortedSetDocValues的数据结构就是一目了然的。\n","categories":["Lucene","DocValues"],"tags":["DocValues"]},{"title":"TieredMergePolicy","url":"/Lucene/Index/2019/0516/TieredMergePolicy/","content":"点击这里\n","categories":["Lucene","Index"],"tags":["merge","mergePolicy"]},{"title":"block-max WAND（一）（Lucene 8.4.0）","url":"/Lucene/Search/2020/0916/block-max-WAND%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  从Lucene 8.0.0开始，Lucene新增了block-max WAND（Weak AND）算法，用于优化TopN的查询。该算法的引入可谓是一波三折，可以查看作者Adrien Grand对该算法的介绍：https://www.elastic.co/cn/blog/faster-retrieval-of-top-hits-in-elasticsearch-with-block-max-wand，下文中将围绕这篇博客展开介绍。\n TopN查询（Lucene 7.5.0）\n  我们先介绍下在Lucene 8.0.0之前，如果实现TopN查询，假设有以下的搜索条件：\n图1：\n\n  图1中，第57行、58行、59行描述了只要文档中至少包含一个域名为&quot;author&quot;、域值为&quot;lily&quot;或者&quot;lucy&quot;的信息，那么该文档就满足查询条件；第60行代码描述了，我们只需要根据文档打分返回Top3的结果。\n  假设索引目录中一共有六篇文档，每篇文档的打分如下所示：\n表一：\n\n\n\n文档号\nScore1（lily）\nScore2（Lucy）\n文档打分值（总分）\n\n\n\n\n0\n0\n0\n0\n\n\n1\n5\n4\n9\n\n\n2\n6\n7\n13\n\n\n3\n3\n8\n11\n\n\n4\n2\n6\n8\n\n\n5\n3\n3\n6\n\n\n\n  表一中，Score1描述的是域名为&quot;author&quot;、域值为&quot;lily&quot;对应的文档打分值，同理Score2，最终文档的打分值为Score1跟Score2的和值。\n  结合图1跟表一可知，当我们处理到文档3，已经获得了Top3的搜索结果，即文档1、文档2、文档3。然而在Lucene 8.0.0之前，需要对所有满足图1的查询条件的文档进行打分，然后在Collector中使用根据score排序的优先级队列来维护Top3（见文章Collector（二））。\n  通过上文的描述我们可以知道， 在查询过程中，一些打分值很低的文档号也被处理了，那有没有什么方式可以使得尽量跳过那些打分值较低的文档。\n MAXSCORE\n  在2012，Stefan Pohl介绍了MaxScore算法，该算法大意为：如果我们想查找一些文档，查询条件为这些文档中至少包含&quot;elasticsearch&quot;或者&quot;kibana&quot;，并且根据文档打分值排序获得Top10。如果能知道根据elasticsearch关键字的文档打分最大值为3、根据kibana关键字的文档打分最大值为5，当Top10中的第10篇文档，即分数最低的那篇文档的的打分值为3时，那么在随后的处理中，我们就只需要处理包含&quot;kibana&quot;的文档集合（因为那些只包含&quot;elasticsearch&quot;的文档肯定是进不了Top10的），并且只需要判断&quot;elasticsearch&quot;是否在这些文档集合中，如果在那么参与打分，并且可能更新Top10。\n  实现该算法需要两个集合：\n\n第一个集合：该集合中的任意一个term，该term在文档中的打分最高值比TopN中的最小值大\n\n这些term用来确定遍历的文档集合，即只需要处理包含第一个集合中的term的文档集合，例如上文中的&quot;kibana&quot;\n\n\n第二个集合：该集合中的任意一个term，该term在文档中的打分值最高值比TopN中的最小值小\n\n这些term用来对文档打分， 用于更新TopN中的最低分\n\n\n\n  随着更多的文档的被处理，TopN中最低的文档打分值如果变高了，那么对于第一个集合中的某些term，如果它们在文档中的打分最高值比TopN中的最小值小，就将它们从第一个集合中移除，并添加到第二个集合中，这样使得进一步减少待处理的文档数量。\n  该算法在Lucene中无法直接应用，原因如下所示：\nStefan didn&#x27;t only describe the algorithm. A couple days before the conference, he opened a ticket against Lucene, where he shared a prototype that he had built. This contribution was exciting but also challenging to integrate because we would need a way to compute the maximum score for every term in the index. In particular, scores depend on index statistics such as document frequency (total number of documents that contain a given term), so adding more documents to an index also changes the maximum score of postings in existing segments. Stefan&#x27;s prototype had worked around this issue by requiring a rewrite of the index, which meant this optimization would only work for static indexes in practice. This is a limitation that we weren&#x27;t happy with, but addressing it would require a lot of work.\n  上文中，Lucene团队不接受这种Stefan提出的issues主要是该算法需要在索引（index）阶段需要计算所有term在所有包含它的文档的文档打分值、需要变更已经生成的段文件。\n  感兴趣的同学可以看这里的详细介绍：https://issues.apache.org/jira/browse/LUCENE-4100 。\n WAND\n  WAND（Weak AND）同样是一种可用于查询TopN的算法，然而该算法的实现同Lucene中的MinShouldMatchSum（minShouldMatch &gt; 1）是相同的。我们简单的介绍下MinShouldMatchSum的实现方式，例如以下的查询条件将会使用MinShouldMatchSum：\n图2：\n\n  图2中的查询条件有三个子查询组成，描述的是：满足查询条件的文档中必须至少域名为&quot;author&quot;，域值为&quot;lily&quot;、“lucy”、&quot;good&quot;中的任意2个（即代码64行设置的minShouldMatch为2）。\n  MinShouldMatchSum算法实现中有三个核心的容器，分别是lead、head、tail，由于源码中关于这几个容器的注释，我怎么翻译都感觉不行😅，随意还是贴上原文朋友们自己品下：\n图3：\n\n  图3中，sub scorers中的每一个scorer可以简单的理解为满足某个子查询的文档信息，以图2为例，对于代码61行的子查询，他对应的scorer描述的是包含域名为&quot;author&quot;、域值为&quot;lily&quot;的文档信息。\n  在继续介绍之前，我们先理解下源码中的这么一段注释：\n图4：\n\n  图4的源码中说到，如果有n个SHOULD查询（比如图2中就是3个SHOULD查询），其中minShouldMatch的值为m，那么这种查询的开销为 n - m + 1个scorer的遍历开销，即我们不需要遍历n个scorer的文档信息就可以获得查询结果。另外图4中的某个查询的cost描述的是满足该查询的文档数量。\n  其推导思想说的是，包含n个子查询c1，c2，… cn且minShouldMatch为m的BooleanQuery，它可以转化为：\n(c1 AND (c2..cn | msm = m - 1)) OR (!c1 AND (c2..cn | msm = m))，两块部分通过&quot;或的关系&quot;（OR）组合而成\n\n(c1 AND (c2…cn|msm=m-1)) ：第一块部分描述了满足BooleanQuery查询要求的文档，如果满足子查询c1，那么必须（AND）至少满足c2…cn中任意m-1个子查询\n(!c1 AND (c2…cn|msm=m))：第二块部分描述了满足BooleanQuery查询要求的文档，如果不满足子查询c1，那么必须（AND）至少满足c2…cn中任意m个子查询\n\n根据两块部分的组合关系（OR），BooleanQuery的开销cost是这两部分的开销和\n\n\n假设子查询c1，c2，… cn是按照cost（上文中已经介绍，即满足子查询的文档数量）升序排序的，那么对于第一块部分(c1 AND (c2…cn|msm=m-1)) ，由于c1的cost最小，并且必须满足c1的查询条件，那么我们只需要遍历满足ci的文档集合即可（见文章文档号合并（MUST）），所以第一块部分的开销就是c1的开销\n对于第二块部分(!c1 AND (c2…cn|msm=m))，它可以转化为一个包含 n -1 个子查询c2，… cn且minShouldMatch为m的子BooleanQuery，所以它又可以转化为(c2 AND (c3…cn|msm=m-1)) OR (!c2 AND (c3…cn|msm=m))\n完整的类推如下所示：\n\n图5：\n\n  图5的推导图中的最后一个步骤中的(!cn−m AND (cn−m+1 ...cn ∣ msm=m))(!c_{n-m}\\ AND\\ (c_{n-m+1}\\ ... c_n\\ |\\ msm=m))(!cn−m​ AND (cn−m+1​ ...cn​ ∣ msm=m))描述的是一个包含 m 个子查询且minShouldMatch为m的子BooleanQuery，那么很明显我们只要遍历任意1个子查询对应的文档集合即可。\n  故最后得出我们只需要处理 n - m + 1个scorer。\n  基于这个理论就设计出了上文中说到的head跟tail，我们先看下源码中如何定义这两个容器：\n图6：\n\n  我们不用关心DisiWrapper跟DisiPriorityQueue是什么，统称为容器即可。初始化head时的容器大小即 n - m + 1（scorers.size() - minShouldMatch + 1），而tail的大小为 m - 1，即剩余的scorer丢到tail中，head跟size中的元素数量和为n。\n  结合上文介绍跟图3的注释就可以明白了，head中存放了用于遍历的scorers（注释中的in order to move quickly to the next candidate正说明了这点），而lead中存放了head中每一个scorer目前正在处理的相同的文档信息，并计算相同的文档信息的数量，如果该值大于等于minShouldMatch的值的话，说明文档满足查询条件。\n  上文的描述只是粗略的介绍了MinShouldMatchSum，在后面的文章中会详细介绍。在本中我们只需要理解为什么要设计head、tail既可以。\n block-max WAND\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["optimization","search","wand"]},{"title":"索引文件之vec&vem&vemf&vemq&veq&vex（Lucene 9.9.0）","url":"/Lucene/suoyinwenjian/2023/1225/vec&vem&vemf&vemq&veq&vex/","content":"  在文章索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）中介绍了Lucene 9.8.0版本向量数据相关的索引文件（必须先阅读下，很多重复的内容不会再提起），由于在Lucene 9.9.0中引入了Scalar Quantization（简称SQ）技术，因此再次对索引结构进行了改造。另外加上该issue，使得在Lucene 9.9.0中，对于向量数据的索引文件最多由以下6个文件组成，我们先给出简要的说明：\n\n.vex、.vem：HNSW信息\n.vec、.vemf：原始的向量数据，即基于SQ量化前的数据，以及文档号、文档号跟节点编号映射关系的数据\n.veq、.vemq（启用SQ才会有这两个索引文件，默认不启动）：量化后的向量数据，以及文档号、文档号跟节点编号映射关系的数据\n\n  先给出这几个索引文件的数据结构之间的关联图，然后我们一一介绍这些字段的含义：\n .vex&amp;.vem\n图1：\n\n点击查看大图\n .vec&amp;.vemf\n图2：\n\n点击查看大图\n .veq&amp;.vemq\n图3：\n\n点击查看大图\n 数据结构\n .vex&amp;.vem、.vec&amp;.vemf\n  相较于Lucene9.8.0中的索引数据结构，只是调整了某些字段的所属索引文件，比如说：\n\n对Lucene9.8.0中的元数据分别移到Lucene9.9.0中.vemf以及.vem中（调整的原因：GITHUB#12729）。\nLucene9.8.0中的.vec以及.vex的数据结构保持不变\n\n图4：\n\n点击查看大图\n图5：\n\n点击查看大图\n图6：\n\n点击查看大图\n  因此，对于Lucene9.9.0中的索引文件.vex&amp;.vem、.vec&amp;.vemf中的字段，相比较Lucene9.8.0，并没有新增或者移除字段，因此这些字段的含义在本文中就不重新介绍了，可以阅读文章索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）。\n .veq&amp;.vemq\n  如果启用SQ，那么段中会额外多出两个索引文件，即.veq&amp;.vemq。下图中除了红框标注的字段，其他的在文章索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）或者其他索引文件中同名字段有相同的含义。\n图7：\n\n QuantizedVectorDataMeta\n图8：\n\n  该字段作为元数据，Offset跟Length对应的区间，用来描述量化后的数据信息在索引文件.veq中的位置信息，见图3\n ConfidenceInterval、lowerQuantile、upperQuantile\n图9：\n\n  这三个字段用于量化操作：\n\nConfidenceInterval：（0.9~1.0之间的值）置信区间。它本来应该是用来计算分位数的，但在目前版本中，还未使用该参数。\nlowerQuantile、upperQuantile：（32位的浮点数）最大和最小分位数。引入这两个字段至少有以下的目的：\n\n离群值的影响：数据中的离群值（outliers）可能会对量化区间产生极端的影响。如果简单地选取数据的最小值和最大值作为量化的边界，一个异常的高或低值会导致整个量化区间的扩展，这会使得绝大多数的数据在量化后的动态范围内分布得非常紧凑。这样就会减少量化级别之间的区分度，增加了量化误差。\n动态范围的优化：使用分位数可以有效地切除极端的离群值，使量化区间专注于数据的核心分布区域。这样做可以优化量化级别的使用，使得数据分布更均匀，从而减少量化误差。\n\n\n\n  在以后Lucene中量化技术的文章中会介绍这几个字段的作用。\n QuantizedVectorData\n图10：\n\n  量化后的每一个向量信息由QuantizedData以及OffsetCorrection组成。\n\nQuantizedData：8bit大小，即使用一个字节来描述一个向量值，量化后的向量值的范围为[0,127]\nOffsetCorrection： 用来调整量化误差。这个偏移量是为了补偿量化值因为舍入到最近的整数而失去的一些精度。在计算向量距离的打分公式中会使用。同样的，该字段具体的使用场景将在以后的文章中展开\n\n 结语\n  本文主要介绍了因引入Scalar Quantization，向量搜索对应的索引文件的从Lucene9.8.0到Lucene9.9.0的差异。\n","categories":["Lucene","suoyinwenjian"],"tags":["index","vec","vem","vemf","vemq","veq","vex","scalar","quantization","indexFile"]},{"title":"MergeScheduler","url":"/Lucene/Index/2019/0519/MergeScheduler/","content":"点击这里\n","categories":["Lucene","Index"],"tags":["merge","mergePolicy"]},{"title":"Vector Similarity Computations FMA- style","url":"/blog/vector/2023/1222/vector-similarity-computations-fma-style/","content":"  介绍下一篇基于FMA（Fused Multiply-Add）利用SIMD的文章。\n  原文地址：https://www.elastic.co/search-labs/blog/articles/vector-similarity-computations-fma-style\n 原文\nIn Lucene 9.7.0 we added support that leverages SIMD instructions to perform data-parallelization of vector similarity computations. Now we’re pushing this even further with the use of Fused Multiply-Add (FMA).\n What is FMA\nMultiply and add is a common operation that computes the product of two numbers and adds that product with a third number. These types of operations are performed over and over during vector similarity computations.\n图1：\n\nFused multiply-add (FMA) is a single operation that performs both the multiply and add operations in one - the multiplication and addition are said to be “fused” together. FMA is typically faster than a separate multiplication and addition because most CPUs model it as a single instruction.\nFMA also produces more accurate results. Separate multiply and add operations on floating-point numbers have two rounds; one for the multiplication, and one for the addition, since they are separate instructions that need to produce separate results. That is effectively,\n图2：\n\nWhereas FMA has a single rounding, which applies only to the combined result of the multiplication and addition. That is effectively,\n图3：\n\nWithin the FMA instruction the a * b produces an infinite precision intermediate result that is added with c, before the final result is rounded. This eliminates a single round, when compared to separate multiply and add operations, which results in more accuracy.\n Under the hood\nSo what has actually changed? In Lucene we have replaced the separate multiply and add operations with a single FMA operation. The scalar variants now use Math::fma, while the Panama vectorized variants use FloatVector::fma.\nIf we look at the disassembly we can see the effect that this change has had. Previously we saw this kind of code pattern for the Panama vectorized implementation of dot product.\nvmovdqu32 zmm0,ZMMWORD PTR [rcx+r104+0x10]*vmulps zmm0,zmm0,ZMMWORD PTR [rdx+r10*4+0x10]vaddps zmm4,zmm4,zmm0\nThe vmovdqu32 instruction loads 512 bits of packed doubleword values from a memory location into the zmm0 register. The vmulps instruction then multiplies the values in zmm0 with the corresponding packed values from a memory location, and stores the result in zmm0. Finally, the vaddps instruction adds the 16 packed single precision floating-point values in zmm0 with the corresponding values in zmm4, and stores the result in zmm4.\nWith the change to use FloatVector::fma, we see the following pattern:\nvmovdqu32 zmm0,ZMMWORD PTR [rdx+r114+0xd0]*vfmadd231ps zmm4,zmm0,ZMMWORD PTR [rcx+r11*4+0xd0]\nAgain, the first instruction is similar to the previous example, where it loads 512 bits of packed doubleword values from a memory location into the zmm0 register. The vfmadd231ps (this is the FMA instruction), multiplies the values in zmm0 with the corresponding packed values from a memory location, adds that intermediate result to the values in zmm4, performs rounding and stores the resulting 16 packed single precision floating-point values in zmm4.\nThe vfmadd231ps instruction is doing quite a lot! It’s a clear signal of intent to the CPU about the nature of the computations that the code is running. Given this, the CPU can make smarter decisions about how this is done, which typically results in improved performance (and accuracy as previously described).\n Is it fast\nIn general, the use of FMA typically results in improved performance. But as always you need to benchmark! Thankfully, Lucene deals with quite a bit of complexity when determining whether to use FMA or not, so you don’t have to. Things like, whether the CPU even has support for FMA, if FMA is enabled in the Java Virtual Machine, and only enabling FMA on architectures that have proven to be faster than separate multiply and add operations. As you can probably tell, this heuristic is not perfect, but goes a long way to making the out-of-the-box experience good. While accuracy is improved with FMA, we see no negative effect on pre-existing similarity computations when FMA is not enabled.\nAlong with the use of FMA, the suite of vector similarity functions got some (more) love. All of dot product, square, and cosine distance, both the scalar and Panama vectorized variants have been updated. Optimizations have been applied based on the inspection of disassembly and empirical experiments, which have brought improvements that help fill the pipeline keeping the CPU busy; mostly through more consistent and targeted loop unrolling, as well as removal of data dependencies within loops.\nIt’s not straightforward to put concrete performance improvement numbers on this change, since the effect spans multiple similarity functions and variants, but we see positive throughput improvements, from single digit percentages in floating-point dot product, to higher double digit percentage improvements in cosine. The byte based similarity functions also show similar throughput improvements.\n Wrapping Up\nIn Lucene 9.7.0, we added the ability to enable an alternative faster implementation of the low-level primitive operations used by Vector Search through SIMD instructions. In the upcoming Lucene 9.9.0 we built upon this to leverage faster FMA instructions, as well as to apply optimization techniques more consistently across all the similarity functions. Previous versions of Elasticsearch are already benefiting from SIMD, and the upcoming Elasticsearch 8.12.0 will have the FMA improvements.\nFinally, I’d like to call out Lucene PMC member Robert Muir for continuing to make improvements in this area, and for the enjoyable and productive collaboration.\n 译文\n  在Lucene 9.7.0中，我们新增了支持，利用SIMD指令来执行数据并行化的向量相似度计算（vector Similarity）。现在，我们更进一步地使用了融合乘法加法（Fused Multiply-Add，FMA）。\n What is FMA\n  乘法跟加法是一种常见的操作，它计算两个数字的乘积，并将该乘积与第三个数字相加。在向量相似度计算过程中，这些类型的操作会一次又一次地执行。\n图1：\n\n  FMA是种单一操作，它在一次操作中执行了乘法和加法 - 乘法和加法被“融合”在一起。FMA通常比分开的乘法和加法更快，因为大多数CPU将其建模为单一指令。\n  FMA还能产生更准确的结果。分开的浮点数乘法和加法操作需要两轮，一轮用于乘法，一轮用于加法，因为它们是分开的指令，需要产生不同的结果。而FMA只有一轮舍入（round），这仅适用于乘法和加法的合并结果。这消除了与分开的乘法和加法操作相比的单一舍入，从而提高了准确性。\n图2：\n\n  然而，FMA只进行一次舍入，而这个舍入只适用于乘法和加法的合并结果。相当于：\n图3：\n\n  在FMA指令中，a * b会产生一个无限精度的中间结果，然后与c相加，最后对最终结果进行舍入。与分开的乘法和加法操作相比，这消除了一次舍入，从而提高了准确性。\n Under the hood\n  那么发生了什么真正的改变呢？我们用单个FMA操作替换了分开的乘法和加法操作。scalar variants现在使用Math::fma，而Panama vectorized variants则使用FloatVector::fma。\n  如果观察下反汇编的结果，我们可以看到其发生的变化。之前对于这类代码模式，Panama对于点积的实现如下：\nvmovdqu32 zmm0,ZMMWORD PTR [rcx+r104+0x10]*vmulps zmm0,zmm0,ZMMWORD PTR [rdx+r10*4+0x10]vaddps zmm4,zmm4,zmm0\n  vmovdqu32 指令从内存位置加载512位 packed doubleword到zmm0寄存器中。接着，vmulps 指令将zmm0的值与来自内存位置的相应packed value相乘，并将结果存储在zmm0中。最后，vaddps 指令将zmm0中的16个packed 单精度的浮点值与zmm4中的相应值相加，并将结果存储在zmm4中。\n  使用FloatVector::fma后的变化，我们可以看下下面的模式：\nvmovdqu32 zmm0,ZMMWORD PTR [rdx+r114+0xd0]*vfmadd231ps zmm4,zmm0,ZMMWORD PTR [rcx+r11*4+0xd0]\n  再次的，第一条指令与之前的示例相似，它从内存位置加载512位packed doubleword values到zmm0寄存器中。然后，vfmadd231ps（这是FMA指令）将zmm0中的值与来自内存位置的相应packed values相乘，将中间结果添加到zmm4中，执行舍入，并将结果存储在zmm4中的16位packed single precision floating-point values。\n  vfmadd231ps指令执行了多个操作，明确告诉CPU代码正在运行的计算性质。基于这一信息，CPU可以更明智地决定如何执行操作，通常会导致性能改进（如前面所述）和更高的准确性。\n Is it fast\n  总的来说，使用FMA通常会导致性能提升，但仍需要进行基准测试。不过Lucene在确定是否使用FMA时处理了许多复杂性（有些情况下不允许使用FMA，后续介绍向量搜索时会提到），因此用户不需要担心这些细节。这包括CPU是否支持FMA，Java虚拟机是否启用FMA，以及仅在已证明比分开的乘法和加法操作更快的架构上启用FMA。尽管可以看出这种启发式方法并不完美，但它在提供良好的开箱即用体验方面已经做了很多。使用FMA可以提高准确性，而未启用FMA时，现有的相似性计算没有负面影响。\n  除了使用FMA，向量相似性函数相关的内容（the suite of similarity functions）也进行了改进。 dot product、square 和 cosine distance 和Panama向量化都已更新。通过查看反汇编和实验经验应用了（apply）一些优化措施，这些措施带来了改进，有助于保持CPU忙碌的状态，主要通过更加的一致性和有针对性的循环展开（loop unrolling），以及消除循环内的数据依赖性。\n\n\nmostly through more consistent and targeted loop unrolling：这是一种优化技术，通过增加循环体中代码的实例数量来减少循环的迭代次数。这种方法可以提高程序的执行效率，因为它减少了循环控制逻辑的开销，并可能使得更多的数据在单个循环迭代中被处理。\n\n\n在循环内移除数据依赖（removal of data dependencies within loops）：这是指修改循环中的代码，以减少或消除数据依赖，从而提高性能。数据依赖可能会导致循环迭代之间的延迟，因为后续的迭代可能需要等待前一个迭代完成数据处理。通过重构代码来减少这种依赖，可以使循环的不同迭代更加独立，从而提高运行效率。综合来看，这些技术有助于提高程序处理循环时的性能，特别是在涉及大量数据和复杂计算时。\n\n\n  具体的性能提升没那么直接通过数据来确定，因为影响涵盖了multiple similarity functions and variants，但我们看到了吞吐量方面的提升，从浮点型数值的点积操作的个位数百分比的提升到cosine操作的两位数百分比的提高。基于字节的相似性函数也显示出类似的吞吐量的提升。\n Wrapping Up\n  在Lucene 9.7.0中，我们添加了通过SIMD指令来启用low-level primitive操作用于向量搜索。在即将发布的Lucene 9.9.0中，我们进一步利用更快的FMA指令，并更一致地应用于所有相似性函数的优化技术。早期版本的Elasticsearch已经受益于SIMD，即将发布的Elasticsearch 8.12.0将获得FMA改进。最后，我要感谢Lucene PMC成员Robert Muir在这个领域持续改进，并为愉快而富有成效的合作表示赞扬。\n","categories":["blog","vector"],"tags":["simd","fma","vector"]},{"title":"前导","url":"/Lucene/2018/0927/%E5%89%8D%E5%AF%BC/","content":" 简单说明\n\n博客中的Lucene版本是7.5.0，如果是其他版本的会在文章标题体现。\n如果你喜欢我的文章，可以下载每篇文章结尾的附件，附件中的MD文件，强烈推荐使用Typora阅读。\n博客中所有关于Lucene文章的代码大家可以在我的GitHub中找到，地址：https://github.com/luxugang/Lucene-7.5.0\n本博客中的文章随意转载，如果能标注转载地址就最好啦。\n文章中如果有错误，希望朋友们能指出来，邮箱：luxugang@apache.org\n\n","categories":["Lucene"],"tags":["必读"]},{"title":"倒排表（上）","url":"/Lucene/Index/2019/0222/%E5%80%92%E6%8E%92%E8%A1%A8%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"本篇文章介绍如何生成倒排表，通过一个简单的例子来讲解倒排表的底层存储结构。文章中不会给出详细的源码介绍，只有一些必要的对象，感兴趣的朋友可以看我的GitHub，对构建倒排表的源码给出了详细的注释：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java ，此类中的该方法是开始构建倒排表的入口。\n另外如果某些域使用了词向量(TermVector)，它会额外的生成倒排表，虽然写入的逻辑是类似的，但最终的倒排表的数据结构还是有区别，在后面的文章中会详细介绍。\npublic void invert(IndexableField field, boolean first) throws IOException &#123;...&#125;\n 例子\n图1：\n\n上图说明：\n\n代码第42行表示倒排表中会存放 文档号、词频、位置、偏移信息\n\ntype.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n\n代码第46行表示我们为域名为&quot;content&quot;的域值分词后&quot;book&quot;添加一个值为&quot;hi&quot;的 payload值，而域名为&quot;title&quot;的中的&quot;book&quot;以及其他term都没有payload信息，\n通过自定义分词器来实现payload，当前例子中的分词器代码可以看PayloadAnalyzer\n\n我们也可以为不同的term提供不同的payload。\n 三个关键类\n构建倒排表的过程中，主要用到下面三个类：\n ByteBlockPool类\n此类中用一个二维数组用来存放term的倒排表数据。\npublic byte[][] buffers;\n IntBlockPool类\n此类中同样用一个二维数组来存储索引信息，由于所有term的倒排表数据都紧凑的存放在ByteBlockPool对象的buffers[][]二维数组中，并且一个term的倒排表数据并非存储在一块连续的空间中，所以我们需要IntBlockPool对象中的一个二维数组来存储映射到ByteBlockPool对象的索引。\npublic byte[][] buffers;\n ParallelPostingsArray类\n在这个类中我们只要关心下面几个重要的数组。该类的对象用来处理同一种域名的数据，有几种域名就有对应数量的ParallelPostingsArray对象，也就是说下面的数组对于不同文档的相同的域是共用的，这点很重要。\n下面所有的数组的下标值都是termID，termID用来区分不同term的唯一标识，它是一个从0开始递增的值，每个term按照处理的先后顺序获得一个termID。\n int[] termFreqs；\n记录每一个term在一篇文档中的词频frequencies。\n int[] lastDocIDs；\n记录每一个term上次出现是在哪一个文档中。\n对于同一个term来说，在某一篇文档中，只有所有该term都被处理结束才会写到倒排表中去，否则的话，term在当前文档中的词频frequencies无法正确统计。所以每次处理同一个term时，根据它目前所属的文档跟它上一次所属的文档来判断当前的操作是统计词频还是将它写入到倒排表。另外包含某个term的所有文档号是用差值存储，该数组用来计算差值。\n int[] lastDocCodes；\n记录每一个term上一次出现是在哪一个文档中。跟lastDocIDs[]数组不同的是，数组元素是一个组合值，相同的是当term在新的文档中出现后，才将它上一次的文档号写入到倒排表中。\n基于压缩存储，如果一个term在一篇文档中的词频只有1，那么文档号跟词频的信息组合存储，否则文档号跟词频分开存储。\n例子：\n文档号5中某个term的词频为1，那么组合存储，将文档号左移一位，然后跟1执行或操作(5 &lt;&lt; 1) | 1 == 11。\n图2：\n\n文档号5中某个term的词频为6，将文档号左移一位，5 &lt;&lt; 1 = 10, 然后分开存储。\n图3：\n\n int[] lastPositions；\n记录每一个term在当前文档中上一次出现的位置。\n基于压缩存储，倒排表中的位置(position)信息是一个差值，这个值指的是在同一篇文档中，当前term的位置和上一次出现的位置的差值。每次获得一个term的位置信息，就马上写入到倒排表中。注意的是，实际存储到倒排表时，跟存储文档号一样是一个组合值，不过这个编码值是用来描述当前位置的term是否有payload信息。\n例子\n没有payload信息, 将位置的值左移1位：8 &lt;&lt; 1, 在读取倒排表时，如果position的二进制值最低位为0，说明没有payload的信息。\n图4：\n\n带有payload信息, 将位置的值左移1位,并与1执行或操作：(8 &lt;&lt; 1)| 1, 在读取倒排表时，如果position的二进制值最低位为1，说明带有payload的信息。\n图5：\n\n int[] textStarts；\n每一个term在ByteBlockPool对象的buffers [ ] [ ]二维数组中的起始位置。\n int[] intStarts；\n数组元素为每一个term在IntBlockPool对象的buffers[ ] [ ] 二维数组中的位置。\n写倒排表时候，IntBlockPool对象的buffers[][] 二维数组中的数据描述了对于某一个term我们应该往ByteBlockPool对象的buffers[][] 二维数组中的哪个位置写，而intStarts[]则是描述我们如何通过termID找到在IntBlockPool对象的buffers[][] 二维数组中的数据。\n在这里还需要重复的时候， 我们在当前文档第一次处理某个term时，才会将这个term上次出现的文档号跟词频写入到倒排表中， 而这个term的位置跟payload，则是马上写入。\n 存储空间的分配和扩容\n每次处理一个term，需要考虑分配跟扩容问题。\n 分配\n该term第一次处理，那么需要新分配一个连续的固定大小的存储空间，分配规则如下：\n总是预分配两块大小都为5个字节的分片，其中第一块分片存放term的文档号、词频信息，第二块分片存放term的位置、payload、offset信息。// 分片层级public final static int[] NEXT_LEVEL_ARRAY = &#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 9&#125;;// 分片大小public final static int[] LEVEL_SIZE_ARRAY = &#123;5, 14, 20, 30, 40, 40, 80, 80, 120, 200&#125;;\n 例子\n图6：\n\n 扩容\n在图6中，对于存储position、payload、offset信息的分片，如果前4个字节都被记录了，那么此时就会遇到哨兵值16，表示我们需要扩容了。\n图7：\n\n根据分片规则，扩容后获得的新分片大小为14，所以上图中下标值14~27的部分为新的分片。\n并且旧分片中下标值10~13这四个字节作为一个索引值，在读取阶段，通过读取该索引值就可以知道下一个分片在ByteBlockPool对象的buffers [ ] [ ]二维数组中的偏移位置。\n 写入过程\n 处理文档0\n 处理域名“content”\n例子使用的是自定义分词器PayloadAnalyzer,所以对于域名“content”来说，我们需要处理 “book”、“is”、&quot;book&quot;共三个term。\n 处理 “book”\n图8：\n\n ByteBlockPool对象的buffers数组\n\n下标值0~4：文档0中第一个&quot;book&quot;的长度以及对应的ASCII\n下标值10：第一个&quot;book&quot;在文档0中的位置，即0，由于有payload信息，组合存储后，位置值为（0 &lt;&lt; 1 | 1）,即1\n下标值11~14：索引值\n下标值15~17：payload的长度以及对应的ASCII\n下标值18~19：文档0中第一个&quot;book&quot;在文档0中的偏移位置以及term的长度\n\n IntBlockPool对象的buffers数组\n\n下标值0：包含&quot;book&quot;的文档号、词频信息在ByteBlockPool对象的buffers数组写入的起始位置\n下标值1：下一次遇到&quot;book&quot;时，它的position、payload、offset信息在ByteBlockPool对象的buffers数组写入的起始位置\n\n lastPositions[]数组\n\nlastPositions[]数组下标值为&quot;book&quot;的termId(0)的数组元素更新为0。\n\n termFreq[]数组\n\ntermFreq[]数组下标值为&quot;book&quot;的termId(0)的数组元素更新为2。\n\n 注意点\n\n由于文档0中的所有term没有都处理结束，所以我们还不知道&quot;book&quot;在文档0中的词频，所以上图中并没有记录 文档号、词频信息（组合存储）。\n\n 处理“book”\n图9：\n\n ByteBlockPool对象的buffers数组\n\n下标值20：第二个&quot;book&quot;在文档0中的位置，根据图8中，上一个&quot;book&quot;的位置是0，所以差值存储就是 (1 - 0) = 1，带有payload，位置信息即为 (1 &lt;&lt; 1 | 1)，即3，并且 lastPositions[]数组下标值为当前term的termId(0)的数组元素更新为1。\n下标值21~23：payload的长度以及对应的ASCII\n下标值24~25：第二个&quot;book&quot;在文档0中的偏移位置以及term的长度\n\n IntBlockPool对象的buffers数组\n\n下标值值1：下一次遇到&quot;book&quot;时，它的position、payload、offset信息在ByteBlockPool对象的buffers数组写入的起始位置**(下文不赘述这个数组的更新)**\n\n lastPositions[]数组\n\nlastPositions[]数组下标值为&quot;book&quot;的termId(0)的数组元素更新为1 (下文不赘述这个数组的更新)\n\n termFreq[]数组\n\ntermFreq[]数组下标值为&quot;book&quot;的termId(0)的数组元素更新为2 (下文不赘述这个数组的更新)\n\n 处理“is”\n图10：\n\n ByteBlockPool对象的buffers数组\n\n下标值29~31：文档0中的&quot;is&quot;的长度以及对应的ASCII\n下标值37：&quot;is&quot;在文档中的位置，即2，由于没有payload，位置信息即为 (2 &lt;&lt; 1 | 0)，即4\n下标值38~39：&quot;is&quot;在文档0中的偏移位置以及term的长度\n\n 处理域名“author”\n 处理“book”\n图11：\n\n ByteBlockPool对象的buffers数组\n\n下标值42~46：文档0中的域名&quot;title的&quot;book&quot;的长度以及对应的ASCII\n下标值52：&quot;book&quot;在文档中的位置，即0，由于没有payload，位置信息即为 (0 &lt;&lt; 1 | 0)，即4\n下标值53~54：域名&quot;title的&quot;book&quot;在文档0中的偏移位置以及term的长度\n\n 注意点\n由于是文档0中的另一个域&quot;title&quot;，所以即使在&quot;content&quot;中我们处理过&quot;book&quot;，在&quot;title&quot;域中属于一个新的term。即域之间的倒排表是独立的，尽管数据都存放在同一个ByteBlockPool对象的buffers数组中\n 处理文档1\n 处理域名“content”\n 处理“book”\n图12：\n\n ByteBlockPool对象的buffers数组\n\n下标值5~6：由于文档0已经处理结束，所以&quot;book&quot;在文档0中的词频也确定了，故可以将&quot;book&quot;在文档0中的文档号跟词频freq写入到倒排表中，由于词频值为2，故不需要组合存储，用两个字节分别存储文档号跟词频。\n下标值2528：从图11中我们可以看出只剩下下标值26、27两个字节，无法存储&quot;tem&quot;在当前文档1中的position、payload、offset的信息，故需要扩容，获得的新的分片的范围为下标值5776的区间，故下标值25~28的4个字节成为一个索引(index)，并且索引值为57。由于在图11中下标值25记录了一个length的值，所以需要把这个值放到新的分片中，即下标值57的位置。\n下标值58：&quot;book&quot;在文档1中的位置，即0，由于有payload信息，组合存储后，位置值为（0 &lt;&lt; 1 | 1）,即1\n下标值59~61：payload的长度以及对应的ASCII\n下标值62~63：&quot;book&quot;在文档1中的偏移位置以及term的长度\n\n termFreq[]数组\n\ntermFreq[]数组下标值为&quot;book&quot;的termId(0)的数组元素由图11中的2更新为1，因为这里开始存储&quot;book&quot;在文档1中的词频值了。\n\n 注意点\n\n从图12中，文档0中&quot;is&quot;跟&quot;title&quot;域中的&quot;book&quot;的文档号跟词频信息还没有写入到倒排表中，那是因为还没有在新的文档中遇到&quot;is&quot;跟&quot;title&quot;中的&quot;book&quot;。\n\n 文档处理结束\n从图12中，文档0中&quot;is&quot;跟&quot;title&quot;域中的&quot;book&quot;的文档号跟词频信息还没有写入到倒排表中，那是因为还没有在新的文档中遇到&quot;is&quot;跟&quot;title&quot;中的&quot;book&quot;。但是这些信息都保存在ParallelPostingsArray类数组中，所以不用写入到倒排表中。\n 结语\n本篇文章介绍了如何构建倒排表，逻辑简单易懂，尽管我们只添加了两篇文档，但是倒排表的所有逻辑都已经涉及了。\n点击下载Markdown文件\n","categories":["Lucene","Index"],"tags":["posting","inverted index"]},{"title":"倒排表（中）","url":"/Lucene/Index/2019/0428/%E5%80%92%E6%8E%92%E8%A1%A8%EF%BC%88%E4%B8%AD%EF%BC%89/","content":"本篇文章介绍使用了词向量（TermVector）后的域生成的倒排表，在索引阶段，索引选项（indexOptions）不为NONE的域会生成一种倒排表（上），这种倒排表的特点是所有文档的所有域名的倒排表都会写在同一张中，后续会读取倒排表来生成.doc、.pos&amp;&amp;.pay、.tim&amp;&amp;.tip、.fdx&amp;&amp;.fdx、.nvd&amp;&amp;.nvm等索引文件。而本文章中设置了TermVector的域会生成另外一张倒排表，并且一篇文档中生成单独的倒排表，同文档中的所有域名的倒排表写在同一张中，并且后续生成.tvd、.tvx文件。\n尽管有两类倒排表，但是实现逻辑是类似的，一些预备知识，下面例子中出现的各种数组、文档号&amp;&amp;词频组合存储、position&amp;&amp;payload 组合存储、倒排表存储空间分配跟扩容等概念在倒排表（上）中已经介绍了，这里不赘述。\n 两种倒排表的区别与联系\n索引选项（indexOptions）不为NONE的域.YES生成的倒排表数据结构如下：\n图1：\n\nTermVector生成的倒排表数据结构如下：\n图2：\n\n 区别与联系\n\n两种倒排表都记录了position、payload、offset信息\n倒排表生成的逻辑中，先生成图1的倒排表再生成图2的倒排表，所以term的信息只需要存储一次来尽可能降低内存的使用，而TermVector生成的倒排表获得term信息的通过索引选项（indexOptions）不为NONE的域生成的倒排表信息获得。\n\n 例子\n图3：\n\n上图说明：\n\n代码第49行表示倒排表中会存放 文档号、词频、位置、偏移信息\n代码第52行表示我们为域名为&quot;content&quot;的域值分词后&quot;book&quot;添加一个值为&quot;it is payload&quot;的 payload值，而域名为&quot;title&quot;的中的&quot;book&quot;以及其他term都没有payload信息\n通过自定义分词器来实现payload，当前例子中的分词器代码可以看PayloadAnalyzer\n\n上文中提到每篇文档都会生成一个独立的倒排表，所以我们只介绍文档0中的域生成倒排表的过程。\n图4：\n\n 写入过程\n 处理文档0\n 处理域名“content”\n例子中使用的是自定义分词器PayloadAnalyzer,所以对于域名“content”来说，我们需要处理 “the”、“book”、“is”、&quot;book&quot;共四个term。\n 处理 “the”\n图5：\n\n ByteBlockPool对象的buffers数组\n\n下标值0：&quot;the&quot;在文档0中的位置，即0，由于没有payload信息，组合存储后，位置值为（0 &lt;&lt; 1 | 0）,即0\n下标值5~6：&quot;the&quot;在文档0中的偏移位置以及term的长度\n\n IntBlockPool对象的buffers数组\n\n下标值0：包含&quot;the&quot;的位置position信息、payload在ByteBlockPool对象的buffers数组写入的起始位置**(下文不赘述这个数组的更新)**\n下标值1：下一次遇到&quot;the&quot;时，它的offset信息在ByteBlockPool对象的buffers数组写入的起始位置**(下文不赘述这个数组的更新)**\n\n lastPositions[]数组\n\nlastPositions[]数组下标值为&quot;the&quot;的termId(0)的数组元素更新为0\n\n freq[]数组\n\nfreq[]数组下标值为&quot;the&quot;的termId(0)的数组元素更新为1\n\n lastOffsets[]数组\n\nlastOffsets[]数组下标值为&quot;the&quot;的termId(0)的数组元素更新为3，目的是为了差值存储\n\n 处理 “book”\n图6：\n\n ByteBlockPool对象的buffers数组\n\n下标值10：第一个&quot;book&quot;在文档0中的位置，即1，由于带有payload信息，组合存储后，位置值为（1 &lt;&lt; 1 | 1）,即3\n下标值1114：下标值值1113 无法完全存储&quot;book&quot;在当前文档0中的payload的信息，故需要扩容，获得的新的分片的范围为下标值2033的区间，故下标值1114的4个字节成为一个索引(index)，并且索引值为20。\n下标值3031：下标值3032 无法完全存储&quot;book&quot;在当前文档0中的payload的信息，故需要扩容，获得的新的分片的范围为下标值3453的区间，故下标值3033的4个字节成为一个索引(index)，并且索引值为34。\n所以处理第一个&quot;book&quot;扩容了两次\n下标值15~16：第一个&quot;book&quot;在文档0中的偏移位置以及term的长度\n下标值2029、3437：payload的长度以及对应的ASCII\n\n textStarts[] 数组\n数组元素值14作为 索引选项（indexOptions）不为NONE的域生成的倒排表中的ByteBlockPool对象的buffers数组的下标，就可以获得&quot;book&quot;的term信息\n 处理 “is”\n图7：\n\n ByteBlockPool对象的buffers数组\n\n下标值54：&quot;is&quot;在文档0中的位置，即2，由于没有payload信息，组合存储后，位置值为（2 &lt;&lt; 1 | 0）,即4\n下标值59~60：&quot;is&quot;在文档0中的偏移位置以及term的长度\n\n 处理 “book”\n图8：\n\n ByteBlockPool对象的buffers数组\n\n下标值17~18：第二个&quot;book&quot;在文档0中的偏移位置以及term的长度\n下标值38：第二个&quot;book&quot;在文档0中的位置，即3，根据图2获得上一个&quot;book&quot;在文档0中的位置是1，所以差值就是 （3 - 1）= 2，由于带有payload信息，组合存储后，位置值为（2 &lt;&lt; 1 | 1）,即5\n下标值39~52：payload的长度以及对应的ASCII\n\n 处理域名“content”\n 处理 “book”\n图9：\n\n\n下标值64：&quot;title&quot;域的&quot;book&quot;在文档0中的位置，即0，由于没有payload信息，位置值为（0 &lt;&lt; 1 | 0）,即0\n下标值69~70：&quot;title&quot;域的&quot;book&quot;在文档0中的偏移位置以及term的长度\n\n 结语\n本篇文章介绍了如何构建TermVector生成的倒排表，在后面的文章中还会再介绍一个倒排表，即MemoryIndex中的倒排表\n点击下载Markdown文件\n","categories":["Lucene","Index"],"tags":["posting","inverted index"]},{"title":"去重编码","url":"/Lucene/yasuocunchu/2019/0130/%E5%8E%BB%E9%87%8D%E7%BC%96%E7%A0%81/","content":"去重编码是Lucene中对int类型数据的一种压缩存储方式，在FacetsConfig类中用到此方法来处理int类型数据。其优点在于，存储一个原本需要固定4个字节空间大小的int类型的数据，最好的情况下只要1个字节，最差的情况下需要5个字节。\n 处理过程\n去重编码的过程主要分三步：\n\n排序\n去重\n差值存储\n\n 关系图\n根据int数值的大小，在去重编码后存储该数值所需要的字节大小关系图如下\n\n\n\n数值范围（指数）\n数值范围（10进制）\n字节大小\n\n\n\n\n0 ~ (2^7 - 1)\n0 ~ 127\n1\n\n\n2^7 ~ (2^14 - 1)\n128 ~ 16383\n2\n\n\n2^14 ~ (2^21 - 1)\n16384 ~ 2097151\n3\n\n\n2^21 ~ (2^28 - 1)\n2097152 ~ 268435455\n4\n\n\n2^28 ~ *\n268435456 ~ *\n5\n\n\n去重编码中最重要的一点是差值存储，从上图可以看出，我们在存储一组有序的数值时，除第一个数值外，其他的数值如果只存储跟它前面数值的差值，那么可以使得达到最大的压缩比。这种方式在存储大数值时的有点更明显。\n\n\n\n\n\n例如我们有一组数据：&#123;17832，17842，17844&#125;，如果我们直接对3个数值进行存储(不存储差值)，那么最终需要9个字节才能存储这三个数值，而如果我们进行差值存储，那么我们需要存储的数据就变为： &#123;17832，10，2&#125;，其中10是17842跟17832的差值，2是17844跟17842的差值，那么最终只需要5个字节存储即可。\n 去重编码源码\n相比较源码，删除了代码中的IntsRef类，便于理解\n encode\npublic class ConvertIntToByteRef &#123;public static BytesRef dedupAndEncode(int[] ordinals) &#123;    // 对 ordinal[]数组排序，目的是为了去重跟计算差值    Arrays.sort(ordinals, 0, ordinals.length);    // 先给每一个int类型分配5个字节大小的空间, 每个字节中只有7位是有效字节(描述数值),最高位是个定界符, 所以一个int类型最多要5个字节    byte[] bytes = new byte[5*ordinals.length];    // 记录上次处理的值，用于去重判断    int lastOrd = -1;    int upto = 0;    // 遍历处理每一个int数值    for(int i=0;i&lt;ordinals.length;i++) &#123;      int ord = ordinals[i];      // ord could be == lastOrd, so we must dedup:      // 去重操作，当前处理的数值跟上一个如果一样的话，skip      if (ord &gt; lastOrd) &#123;        // 存储差值的变量        int delta;        if (lastOrd == -1) &#123;          // 处理第一个值, 只能储存原始的数值          delta = ord;        &#125; else &#123;          // 处理非第一个值，就可以储存这个值与前一个值的差值          delta = ord - lastOrd;        &#125;        // if语句为真说明delta是 0~(2^7 - 1)内的值, 需要1个字节存储        if ((delta &amp; ~0x7F) == 0) &#123;          // 注意的是第8位是0(位数从0计数), 一个byte的最高位如果是0，表示是数值的最后一个byte          bytes[upto] = (byte) delta;          upto++;          // if语句为真说明delta是 2^7 ~ (2^14 - 1)内的值, 需要2个字节存储        &#125; else if ((delta &amp; ~0x3FFF) == 0) &#123;          // 这个字节的最高位是1, 表示下一个byte字节和当前字节属于同一个int类型的一部分          bytes[upto] = (byte) (0x80 | ((delta &amp; 0x3F80) &gt;&gt; 7));          // 这个字节的最高位是0, 表示表示是数值的最后一个byte          bytes[upto + 1] = (byte) (delta &amp; 0x7F);          upto += 2;          // if语句为真说明delta是 2^14 ~ (2^21 - 1)内的值, 需要3个字节存储        &#125; else if ((delta &amp; ~0x1FFFFF) == 0) &#123;          // 这个字节的最高位是1, 表示下一个byte字节和当前字节属于同一个int类型的一部分          bytes[upto] = (byte) (0x80 | ((delta &amp; 0x1FC000) &gt;&gt; 14));          // 这个字节的最高位是1, 表示下一个byte字节和当前字节属于同一个int类型的一部分          bytes[upto + 1] = (byte) (0x80 | ((delta &amp; 0x3F80) &gt;&gt; 7));          // 这个字节的最高位是0, 表示表示是数值的最后一个byte          bytes[upto + 2] = (byte) (delta &amp; 0x7F);          upto += 3;          // if语句为真说明delta是 2^21 ~ (2^28 - 1)内的值, 需要4个字节存储        &#125; else if ((delta &amp; ~0xFFFFFFF) == 0) &#123;          bytes[upto] = (byte) (0x80 | ((delta &amp; 0xFE00000) &gt;&gt; 21));          bytes[upto + 1] = (byte) (0x80 | ((delta &amp; 0x1FC000) &gt;&gt; 14));          bytes[upto + 2] = (byte) (0x80 | ((delta &amp; 0x3F80) &gt;&gt; 7));          bytes[upto + 3] = (byte) (delta &amp; 0x7F);          upto += 4;          // delta是 2^28 ~ *内的值, 需要5个字节存储        &#125; else &#123;          bytes[upto] = (byte) (0x80 | ((delta &amp; 0xF0000000) &gt;&gt; 28));          bytes[upto + 1] = (byte) (0x80 | ((delta &amp; 0xFE00000) &gt;&gt; 21));          bytes[upto + 2] = (byte) (0x80 | ((delta &amp; 0x1FC000) &gt;&gt; 14));          bytes[upto + 3] = (byte) (0x80 | ((delta &amp; 0x3F80) &gt;&gt; 7));          bytes[upto + 4] = (byte) (delta &amp; 0x7F);          upto += 5;        &#125;        // 这里将ord保存下来是为了去重        lastOrd = ord;      &#125;    &#125;    return new BytesRef(bytes, 0, upto);  &#125;  public static void main(String[] args) &#123;    int[] array = &#123;3, 2, 2, 8, 12&#125;;    BytesRef ref = ConvertIntToByteRef.dedupAndEncode(array);    System.out.println(ref.toString());  &#125;&#125;\n最终结果用BytesRef对象表示，上面的输出结果：[2 1 5 4]\n decode\npublic class ConvertByteRefToInt &#123;  public static void decode(BytesRef bytesRef)&#123;    byte[] bytes = bytesRef.bytes;    int end = bytesRef.offset + bytesRef.length;    int ord = 0;    int offset = bytesRef.offset;    int prev = 0;    while (offset &lt; end) &#123;      byte b = bytes[offset++];      // if语句为真：byte字节的最高位是0，decode结束      if (b &gt;= 0) &#123;        // ord的值为差值，所以(真实值 = 差值(ord) + 前面一个值(prev))        prev = ord = ((ord &lt;&lt; 7) | b) + prev;        // 输出结果        System.out.println(ord);        ord = 0;        // decode没有结束，需要继续拼接      &#125; else &#123;        // 每次处理一个byte        ord = (ord &lt;&lt; 7) | (b &amp; 0x7F);      &#125;    &#125;  &#125;  public static void main(String[] args) &#123;    int[] array = &#123;3, 2, 2, 8, 12&#125;;    // 去重编码    BytesRef ref = ConvertIntToByteRef.dedupAndEncode(array);    // 解码    ConvertByteRefToInt.decode(ref);  &#125;&#125;\n 结语\n去重编码(dedupAndEncode)是Lucene中的压缩存储的方式之一，还有VInt，VLong等数据类型都是属于压缩存储，在后面的博客中会一一介绍。demo看这里：https://github.com/luxugang/Lucene-7.5.0/tree/master/LuceneDemo/src/main/java/lucene/compress/dedupAndEncodeTest\n","categories":["Lucene","yasuocunchu"],"tags":["util","encode","decode"]},{"title":"执行段的合并（一）","url":"/Lucene/Index/2019/1024/%E6%89%A7%E8%A1%8C%E6%AE%B5%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  在前面的文章中，我们介绍了段的合并策略TieredMergePolicy、LogMergePolicy，讲述了如何从索引目录中挑选出满足合并条件的一个或多个段的集合OneMerge，另外还介绍了段的合并调度MergeScheduler，讲述了如何用不同的调度策略分配一个或多个线程来调度段的合并，比如执行先后顺序，磁盘IO限制。\n  图1是文章MergeScheduler中的调度策略ConcurrentMergeScheduler的流程图，其中红色框标注的流程点描述的是一个线程执行段的合并的过程，然而基于篇幅我们并没有展开介绍，故从本篇文章开始，将通过数篇文章来介绍该流程点的逻辑：\n  为了能更好的理解图1中每一个流程点，强烈建议先看下文档提交之flush系列文章。\n图1：\n\n 执行段的合并的流程图\n图2：\n\n点击查看大图\n  我们首先概述下图2中三个阶段：\n\n合并初始化：该阶段能很快的执行结束，但是有些流程点需要获得对象为IndexWriter的对象锁，即在合并初始化阶段，一些IndexWriter的操作会同步等待，例如IndexWriter的关闭操作（shutdown()方法）、发布生成的段（见文档提交之flush的系列文章）等等\n执行真正的合并：该阶段是最耗时（time-consuming）的，故大部分的操作都不需要同步，即不需要获得对象为IndexWriter的对象锁\n合并后的收尾工作：该阶段跟合并初始化一样能很快的执行结束，但是所有的流程都需要在临界区内执行，即需要获得对象为IndexWriter的对象锁\n\n  接下来我们介绍图2中的每一个流程点\n 待合并的段集合OneMerge\n  该流程点即流程图的输入数据，该数据为OneMerge，它描述了待合并的段的信息，包含的几个重要的信息如下所示：\n\nList&lt;SegmentCommitInfo&gt; segments：使用一个链表存放所有待合并的段信息SegmentCommitInfo，其中SegmentCommitInfo用来描述一个段的完整信息（除了删除信息），它包含的信息以及对应在索引文件的内容如下图所示：\n\n图3：\n\n  SegmentCommitInfo的信息由图3中黄框标注的两部分组成，其中根据索引文件segments_N中红框标注的SegName（见文章索引文件segments_N）找到对应的索引文件si。\n  图3中两个索引文件中的字段已经在前面的文章中介绍，在这里不赘述。\n\nSegmentCommitInfo info：该字段在当前阶段是null，在后面的流程中会被赋值，它描述的是合并后的新段的信息\nList&lt;SegmentReader&gt; readers：该字段在当前阶段是null，在后面的流程中会被赋值，readers中的每一个SegmentReader描述的是某个待合并的段的信息，SegmentReader的介绍可以看SegmentReader系列文章\nList&lt;Bits&gt; hardLiveDocs：该字段在当前阶段是null，在后面的流程中会被赋值，hardLiveDocs中的每一个Bits描述的是某个待合并的段中被标记为删除的文档号集合\n\n 作用（apply）删除信息\n  在文档提交之flush（二）中我们提到，DWPT（见文档的增删改（二））转化为一个段的期间，DWPT中包含的删除信息会被处理为两种BufferedUpdates（不用关心BufferedUpdates具体包含哪些信息），如下所示：\n\n全局BufferedUpdates：作用其他段的删除信息\n私有BufferedUpdates：作用段自身的删除信息\n\n  由于段的合并操作会合并掉删除信息，故我们必须在合并之前将待合并的段包含的删除的信息正确的作用到其他段以及自身段。\n  作用（apply）删除信息的具体过程可以查看文档提交之flush（七）中的处理删除信息的流程图，本文就不赘述了。\n  为什么我们已经能获得待合并段的信息，但段中的删除信息在这个流程点可能还未作用删除信息？\n\n这个问题需要拆分成两个子问题：\n\n  子问题一：执行合并的操作时，我们是如何获得待合并段的信息：\n\nIndexWriter对象中持有一个SegmentInfos对象，该对象中的一个重要数据如下所示：\n\nprivate List&lt;SegmentCommitInfo&gt; segments = new ArrayList&lt;&gt;();\n\n链表segments中存放的是SegmentCommitInfo，上文中说道SegmentCommitInfo即一个段的完整信息（除了删除信息），段的合并策略（TieredMergePolicy、LogMergePolicy）根据IndexWriter对象中的SegmentInfos对象中的链表segments中找出满足合并条件的一个或多个段的集合OneMerge，在文章文档提交之flush（六）的发布FlushedSegment的流程图中，详细的介绍了一个段的信息SegmentCommitInfo被添加到链表segments的时机点，下图中用红色框标注：\n\n图4：\n\n  子问题二：为什么段中的删除信息在这个流程点可能还未作用删除信息：\n\n从图4中可以看到，处理删除信息的方式为将全局删除信息跟私有删除信息作为一个事件添加到eventQueue中，等待线程去执行eventQueue中的事件，即并不是马上作用（apply）删除信息，故当我们从链表segments 中得到待合并的段后，并不意味保证作用删除信息了，为了防止合并后删除信息的丢失，需要先作用删除信息\n事件、eventQueue的概念、为什么将全局删除信息跟私有删除信息作为一个事件添加到eventQueue中的问题见文档提交之flush（四），这里不赘述\n\n DocValues信息写入到磁盘\n  这个流程点的过程相对复杂，基于篇幅，我们将在下一篇文档中展开。\n 结语\n  由于执行段的合并跟文档的增删改，文档提交（commit、flush）是并发的操作，所以在介绍执行段的合并过程中肯定会有同步操作，也就是会把文档的增删改，文档提交（commit、flush）的知识点串起来讲解，故如果你不熟悉这些两个知识点，不是很适合看执行段的合并的系列文章。。。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","segment"]},{"title":"执行段的合并（三）","url":"/Lucene/Index/2019/1028/%E6%89%A7%E8%A1%8C%E6%AE%B5%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E4%B8%89%EF%BC%89/","content":" 执行段的合并（三）\n  本文承接执行段的合并（二），继续介绍执行段的合并的剩余的流程，下面先给出执行段的合并的流程图：\n图1：\n\n点击查看大图\n 获取SegmentReader的集合MergeReader\n  我们在文章执行段的合并（二）介绍更新待合并的段集合OneMerge的内容中提到，我们需要更新OneMerge中的一个变量List&lt;SegmentReader&gt; readers，readers中的每一个SegmentReader描述的是某个待合并的段的信息，而在此流程点，我们需要对readers中的每一个SegmentReader再封装为FilterCodecReader，封装后的SegmentReader用集合MergeReader描述，如下图所示：\n图2：\n\n  不过该流程只有在使用了软删除（soft delete）的机制后才会执行上述的操作，在后面的文章中会专门介绍软删除机制，届时再详细介绍当前流程。\n  如果不使用软删除机制，那么MergeReader中的SegmentReader跟OneMerge中的是一致的，即无需进行封装，如下图所示：\n图3：\n\n 生成SegmentMerger\n  在此流程点，我们用在前面的流程中生成的信息来生成一个SegmentMerger，SegmentMerger包含的主要信息如下所示：\n\nMergeReader：即上一个流程中生成的SegmentReader的集合MergeReader，它包含了待合并的段的信息\nSegmentInfo：即在初始化一个新段的流程中生成的SegmentInfo对象，在后面的流程中，合并后的索引信息将会被写入到该对象中\nMergeState：该信息将会在下一篇文章中介绍，这里我们只需要知道它生成的时机\n\n  在后面的流程中，将通过MergeReader中包含的索引信息，将合并后的索引信息写入到SegmentInfo对象中。另外如果使用了软删除的机制，那么在上一个流程点执行结束后，我们就获得了被标记为软删除的文档的数量softDeleteCount，此时需要将softDeleteCount写入到SegmentCommitInfo对象（见近实时搜索NRT（四）中关于SegmentCommitInfo对象的介绍）中。\n图4：\n\n 执行索引文件的合并\n  此流程开始真正的对每一个待合并的段中包含的索引文件执行合并操作，图5只列出索引文件间的合并先后顺序，每一种索引的详细合并过程不展开介绍，这里提供了该流程在源码中的入口，感兴趣的可以看看：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java 中的下面这个方法：\nMergeState merge() throws IOException &#123;    ... ... &#125;\n图5：\n\n  对索引文件结构感兴趣的同学可以看这个链接：Lucene7.50 索引文件介绍。\n 设置新段包含的索引文件名\n  在上一个流程执行结束后，我们就获得了合并后的新的索引文件集合，那么在当前流程我们需要将这些新的索引文件名写入到新段中，即写入到新段的SegmentInfo对象中。\n  SegmentInfo对象是什么：\n\n索引文件si中包含的索引信息读取到内存后就用SegmentInfo对象来描述，反之生成索引文件si的过程就是将SegmentInfo对象中的信息持久化到磁盘\n\n  新的索引文件名在索引文件si中的位置如下所示，用红框标注：\n图6：\n\n 生成复合索引文件\n  无论待合并的段是否使用了复合索引文件，无论是否通过IndexWriterConfig.setUseCompoundFile方法设置生成复合索引文件，合并生成的新段需要额外的判断方法来决定是否生成复合索引文件，判断流程图如下：\n 是否使用复合索引文件流程图\n图7：\n\n  在介绍图7的流程之前，我们先对图中出现的名词做下解释：\n\nmergedInfoSize：该值为合并后的新段的大小，根据不同的合并策略，mergedInfoSize的含义各不相同\n\nLogMergePolicy：该合并策略细分下面两个策略\n\n\nLogByteSizeMergePolicy：mergedInfoSize的值为新段对应的每一个索引文件（图6中的新的索引文件）的大小和值，通过JDK提供的File.size(Path)的方法来计算单个索引文件的大小。注意的是该合并策略还提供了setCalibrateSizeByDeletes()方法来让用户选择mergedInfoSize的值是否要剔除被删除的文档的索引大小（计算方式在下文介绍）\n\n\nLogDocMergePolicy：mergedInfoSize的值为新段中文档的数量maxDoc，该合并策略也提供了setCalibrateSizeByDeletes()方法来让用户选择mergedInfoSize的值是否要剔除被删除的文档的数量（计算方式在下文介绍）\n\n\n\nTieredMergePolicy：mergedInfoSize的计算方式跟LogByteSizeMergePolicy一样，差别在于mergedInfoSize的值必须扣除掉被删除的文档的索引大小\n\n\nnoCFSRatio：该值为一个阈值（用户可配置），新段的大小mergedInfoSize占索引目录中所有段的大小的百分比如果超过该值，那么就不使用复合索引文件\nmaxCFSSegmentSize：该值为一个阈值（用户可配置），如果新段的大小mergedInfoSize大于该值，那么就不使用复合索引文件\n\n  如何计算被删除的文档的索引大小：\n\n通过一个段对应的索引文件计算出的新段大小mergedInfoSize中是包含被删除的文档的索引大小的，所以只需要计算出被删除文档的数量numDelete占总的文档数量maxDoc就可以算出索引大小，其中numDelete跟maxDoc在索引文件中的位置如下所示：\n\n图8：\n\n  图8中，蓝色箭头描述的是根据SegName可以映射到对应索引文件.si，原因在前面的文章中已经介绍，不赘述，另外numDelete和maxDoc的值分别对应DeletionCount和SegSize，mergedInfoSize的计算公式为：\nmergedInfoSize*(1.0 - numDelete/maxDoc)\n\n如果使用软删除机制，并且使用了SoftDeletesRetentionMergePolicy合并策略，那么numDelete为DeletionCount跟SoftDelCount的差值，maxDoc为SegSize：\n\n图9：\n\n  至于为什么使用软删除机制后，导致numDelete的差异性，将在后面介绍软删除的文章中展开，本篇文章中我们只需要知道它会影响mergedInfoSize的计算就行了。\n  在熟悉了图7中的名词后，应该不需要介绍展开介绍每一个流程点了。\n  如果需要使用复合索引文件，那么生成复合索引文件cfs&amp;&amp;cfe即可。\n 新段对应的索引文件.si写入磁盘\n  该流程为SegmentInfo对象中的信息持久化到磁盘，生成索引文件si的过程，具体的写入过程个人觉得没有什么好讲的。。。\n 结语\n  基于篇幅，剩余的流程点将在下一篇文章中展开介绍。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","segment"]},{"title":"执行段的合并（二）","url":"/Lucene/Index/2019/1025/%E6%89%A7%E8%A1%8C%E6%AE%B5%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本文承接执行段的合并（一），继续介绍执行段的合并的剩余的流程，下面先给出执行段的合并的流程图：\n图1：\n\n点击查看大图\n DocValues信息写入到磁盘\n  在上一个流程点作用（apply）删除信息执行结束后，待合并的段中新增的删除信息目前还存储在内存中，此时需要持久化DocValues信息，即写入到磁盘。\n  删除信息可以分为下面两类：\n\n被删除的文档号：这类删除信息使用FixedBitSet存储，按照Term进行删除、按照Query进行删除、更新文档操作这三种操作找出的文档号都是被删除的文档号\nDocValues信息：这类删除信息使用链表存储，这里不展开介绍，在以后介绍软删除的文章中会展开，更新DocValues域的操作会产生这类删除信息\n\n  在源码中，该流程点有以下的TODO注释：\nTODO: we could fix merging to pull the merged DV iterator so we don&#x27;t have to move these updates to disk first, i.e. just carry them in memory:\n  也就是说没有必要在这个流程将变更的DocValues信息写入到磁盘，由于由于执行段的合并跟文档的增删改，文档提交（commit、flush）是并发的操作，DocValues还有可能被更新，故在以后的版本，将不需要在这个位置执行该流程，而是跟处理被删除的文档号一样，通过OneMerge存储（仅仅是作者的猜测，至少被删除的文档号是这么做的），OneMerge中包含的信息见文章执行段的合并（一）中的介绍，在后面的流程中会介绍处理被删除的文档号的过程。\n  至于DocValues信息从内存持久化到磁盘的过程，在以后介绍软删除的文章中会展开，在这篇文章中我们只需要知道，当前流程执行结束后，会生成新的.dvd、.dvm的索引文件。\n 初始化一个新段\n  新段newSegment即待合并的段合并后的目标段（target segment），在合并操作中初始化一个新段的过程有以下五个步骤：\n\n步骤一之获得新段的段名前缀：使用jdk提供的Long.toString(count, Character.MAX_RADIX)方法来获得，demo看这里：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/NewSegmentTest.java， 下图的例子显示的是连续获取80个段名前缀，可以看出命名方式就是09、az的组合值：\n\n图2：\n\n\n步骤二之生成SegmentInfo对象：在当前阶段SegmentInfo对象中的变量都是初始化的数据，在后面的流程中会不断的更新SegmentInfo对象中的信息\n\nSegmentInfo对象是什么：\n\n索引文件si中包含的索引信息读取到内存后就用SegmentInfo对象来描述，反之生成索引文件si的过程就是将SegmentInfo对象中的信息持久化到磁盘，所以SegmentInfo对象中的信息如图3所示\n\n\n\n\n步骤三之初始化SegmentInfo对象中的Diagnostics：如果是通过flush生成一个新的SegmentInfo对象，那么会将以下的信息初始化Diagnostics：\n\n初始化的Diagnostics包含的字段：\n\nos：运行Lucene所在的操作系统，版本号，架构，比如操作系统为Mac OS X，版本号为10.14.3，架构为x86_64\njava：java的发行商，版本号，JVM的版本号\nversion：Lucene的版本号，比如7.5.0\nsource：生成当前segment是由什么触发的，flush、commit、merge、addIndexes(facet)\ntimestamp：生成当前segment的时间戳\n\n\n由于是通过merge生成的SegmentInfo对象，所以会额外多两个字段：\n\nmergeMaxNumSegments：该字段在forceMerge中会用到，这里不开展解释\nmergeFactor：新段是由mergeFactor个旧段合并生成的\n\n\nDiagnostics在索引文件si中的位置如下图所示，红框标注：\n\n\n\n图3：\n\n\n步骤四之生成SegmentCommitInfo对象：根据SegmentInfo对象生成一个SegmentCommitInfo对象，该对象不展开介绍， 已经解释过好多次了，同样的，在这个阶段，SegmentCommitInfo对象中的变量都是初始化，在后面的流程中会更新\n步骤五之更新OneMerge：在执行段的合并（一）中我们介绍到，OneMerge在后面的流程中，它包含的变量会逐步更新，在这里OneMerge中的SegmentCommitInfo会被更新，即新段的信息被更新到OneMerge中\n\n 更新待合并的段集合OneMerge\n  在介绍该流程之前，我们先讲述下Lucene中两个很重要的类ReadersAndUpdates、ReaderPool。\n ReadersAndUpdates\n  ReadersAndUpdates用来维护一个段的信息，比如删除信息的更新，段的复用（NRT近实时搜索）等，查询、合并操作都会用到ReadersAndUpdates，它包含的几个重要的变量如下所示：\n\nSegmentCommitInfo info：该字段描述了一个段的完整索引信息（除了删除信息），见近实时搜索NRT（四）\nSegmentReader reader：当需要读取段的索引信息时，我们可以复用该对象，降低读取开销（复用、提高读取性能的概念见文章SegmentReader（一））\nPendingDeletes pendingDeletes：上文中我们说到删除信息被分为两类，被删除的文档号和DocValues信息，PendingDeletes对象中包含了一个FixedBitSet对象来存储被删除的文档号\nboolean isMerging：该字段用来描述当前段是否正在执行合并操作\nMap&lt;String,List&lt;DocValuesFieldUpdates&gt;&gt; pendingDVUpdates：如果当前段中的DocValues信息需要更新，那么DocValues信息用该Map容器存放\nMap&lt;String,List&lt;DocValuesFieldUpdates&gt;&gt; mergingDVUpdates：如果当前段中的DocValues信息需要更新，但是当前段正在更新，那么DocValues信息会先用pendingDVUpdates存放，同时用该Map容器存放\n\n ReaderPool\n  ReaderPool中包含了一个容器，其定义如下：\nprivate final Map&lt;SegmentCommitInfo,ReadersAndUpdates&gt; readerMap = new HashMap&lt;&gt;();\n  ReaderPool是IndexWriter的变量，所以ReaderPool的作用是在持有IndexWriter的情况下能通过SegmentCommitInfo找到每一个段的ReadersAndUpdates，故IndexWriter、ReaderPool、ReadersAndUpdates三者的关系如下：\n图4：\n\n  图4中ReadersAndUpdates的个数即当前索引目录中段的个数。\n  ReadersAndUpdates在什么时候生成：\n\n作用删除信息：如果当前段需要被作用删除信息，如上文描述的，删除信息会被存储到当前段的ReadersAndUpdates或pendingDVUpdates或mergingDVUpdates中，故如果ReaderPool中没有该段的ReadersAndUpdates，那么就会生成ReadersAndUpdates，生成的时间点在下面的流程图中红框标注：\n\n图5：\n\n  图5的流程图为文档提交之flush（七）中的处理删除信息的流程图。\n\n合并阶段：合并期间需要合并删除信息，故同样使用ReadersAndUpdates来获得每一个待合并的段删除信息，如果某个段在ReadPool中没有ReadersAndUpdates对象，那么先生成该ReadersAndUpdates对象\n\n  我们回到流程点更新待合并的段集合OneMerge，在当前流程点我们需要更新OneMerge中的两个变量，如下所示，OneMerge中包含的信息见文章执行段的合并（一）中的介绍：\n\nList&lt;SegmentReader&gt; readers：readers中的每一个SegmentReader描述的是某个待合并的段的信息，并且SegmentReader是通过ReadersAndUpdates获得的\nList&lt;Bits&gt; hardLiveDocs：hardLiveDocs中的每一个Bits描述的是某个待合并的段中被标记为删除的文档号集合，并且hardLiveDocs是通过SegmentReader获得的\n\n  详细关于ReaderPool的介绍，可以阅读文章ReaderPool（一）。\n 获取SegmentReader的集合MergeReader\n  该流程会涉及软删除的概念，基于篇幅，将在下一篇文章中展开。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","segment"]},{"title":"执行段的合并（五）","url":"/Lucene/Index/2019/1031/%E6%89%A7%E8%A1%8C%E6%AE%B5%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E4%BA%94%EF%BC%89/","content":"  本文承接执行段的合并（四），继续介绍执行段的合并的剩余的流程，下面先给出执行段的合并的流程图：\n图1：\n\n点击查看大图\n 提交合并\n MergeState\n在文章执行段的合并（四）中我们说到，生成MergeState的过程中完成了几个任务，根据IndexWriter是否设置了IndexSort（见文章索引文件之si中关于IndexSort的介绍）可以将任务划分为如下两类：：\n\n设置了IndexSort\n\n任务一：对每一个待合并的段进行段内排序\n任务二：对合并后的新段进行段内排序\n任务三：获得所有待合并的段的被删除的文档号与段内真正的文档号的映射DocMap[ ]\n\n\n未设置IndexSort\n\n任务三：获得所有待合并的段的被删除的文档号与段内真正的文档号的映射DocMap[ ]\n\n\n\n 任务一：对每一个待合并的段进行段内排序\n  执行任务一的目的其实是为了任务二准备的，因为只有每一个待合并的段是段内有序的，才能实现将这些段合并为一个段内有序的新段。\n  我们先通过一个例子来介绍段内排序对搜索结果的影响：\n图2：\n\n图3：\n\n  图2中描述了IndexWriter的段内排序规则，定义了两个排序规则：\n\nsortField1：根据域名为&quot;age&quot;的域值进行从小到大排序\nsortField2：根据域名为&quot;label&quot;的域值按照字典序排序\n\n  图2中的第45行代码，定义SortField[]数组时，sortField1相比sortField2有更小的数组下标，故总的排序规则为先按照sortField1进行排序，如果无法通过sortField1比较出两篇文档的先后关系，那么再使用sortField2来区分，如果两个排序规则都无法区分文档的先后关系，那么根据文档被添加的先后顺序来判断，即图3中 文档0、文档1的编号。\n图4：\n\n图5：\n\n  图3中的文档都被添加到索引之后会生成一个段，我们通过图4的代码来打印该段的文档信息，即图5的内容，可以看出当使用了IndexSort后，段内的文档按照图2中的排序规则正确的排序了，通过图5我们可以知道下面的内容：\n\ndocId为0的文档，它对应图3中的文档4，由于这篇文档没有&quot;age&quot;、“lable&quot;的DocValues域，故它被认为是&quot;最小的”，即排在最前面\ndocId为3、docId为4的文档，它们分别对应图3中的文档1跟文档3，可以看出它们是先按照sortField1而不是sortField2进行排序，否则文档3根据域名&quot;lable&quot;的域值&quot;a&quot;，它应该比文档1&quot;较小&quot;\ndocId为2，docId为3的文档，它们分别对应图3中的文档2跟文档1，可以看出这两篇文档根据sortField1无法区分大小关系，再根据sortField2能比较出文档2&quot;较小&quot;\ndociId为4，docId为5的文档，它们分别对应图3中的文档3，文档5，可以看出这两篇文档根据sortField1跟sortField2都无法区分大小关系，所以只能根据文档被添加的先后顺序来判断，文档3先被添加，所以它&quot;较小&quot;\n\n  如果我们不设置IndexSort，图4的打印结果如下所示：\n图6：\n\n  从图6中可以看出，如果不设置IndexSort，那么段内的文档顺序就是文档被添加的顺序。\n  另外在设置了IndexSort后，Collector的collect(int doc)方法依次收到的文档号也是排序的，故如果业务中对查询性能有较高要求，并且返回的结果有固定的排序规则的要求，那么我们可以将这个排序规则通过IndexSort来实现，将排序的开销扔到索引阶段。\n  上文的例子demo可以点这里查看：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/docValues/SegmentInnerSort.java 。\n  在索引阶段通过IndexSort进行段内排序，即对段内的文档进行排序，实际上不是真正的改变文档在段中的位置，因为在性能上来讲是不可能的，段内排序的实质是生成一个映射，下文中将详细的介绍段内排序的过程。\n  在索引阶段，什么时候进行段内排序：\n\nflush生成一个段的过程中进行段内排序，具体的见文章文档提交之flush（三）中流程点将DWPT中收集的索引信息生成一个段newSegment的流程图的介绍。\n\n 段内排序流程图\n  为了便于描述，图7中的流程图是按照从小到大的顺序进行排序。\n图7：\n\n 段内文档\n图8：\n\n  待排序的对象是段内所有的文档。\n 判断段内文档是否已经有序\n图9：\n\n  从段内的第二篇文档（因为要跟上一篇文档进行比较）开始，根据IndexWriter提供的段内排序规则比较当前文档是否小于上一篇文档，如果所有的文档都满足这个关系，说明段内已经有序，那么直接退出即可，否则只要有某任意两篇文档不满足上述关系，说明段内不是按照IndexWriter提供的段内排序规则有序的，故需要进行排序。\n 生成映射newToOld\n图10：\n\n  如果待排序的文档没有按照IndexWriter提供的段内排序规则有序，那么需要进行排序，并且这里使用TimSort进行排序，本篇文章中不会对介绍TimSort的逻辑，我们只关心排序结束后，会生成一个映射newToOld，当然在源码中它是一个经过压缩处理的对象，为了便于介绍，我们可以简单的理解为newToOld是一个数组，对于图3中的例子，它对应的newToOld数组如下所示：\n图11：\n\n  new指的是数组下标值，old指的是数组元素，newToOld数组实现了new到old的映射，所以段内排序并没有真正的去&quot;移动&quot;文档。如果图3中的文档都满足搜索条件，那么Collector的collect(int doc)方法依次收到的文档号即图11中的docId。\n  另外，如果阅读过文档的增删改的系列文章，图11中的数组元素，即文档编号，它是根据文档被添加到DWPT（见文档的增删改（二））中的顺序赋值的，即文章文档的增删改（四）中的numDocsInRAM。\n 生成映射oldToNew\n图12：\n\n  根据映射newToOld生成一个相反的映射，如下所示：\n图13：\n\n 任务二：对合并后的新段进行段内排序\n  该任务的逻辑相对复杂，基于篇幅，在下一篇文档中展开介绍。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","segment"]},{"title":"执行段的合并（四）","url":"/Lucene/Index/2019/1030/%E6%89%A7%E8%A1%8C%E6%AE%B5%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  本文承接执行段的合并（三），继续介绍执行段的合并的剩余的流程，下面先给出执行段的合并的流程图：\n图1：\n\n点击查看大图\n 生成IndexReaderWarmer\n  在前面流程中我们了解到，合并后生成的新段已经包含了所有固定的索引信息及部分删除信息，故在当前流程点，我们可以生成该段对应的SegmentReader对象，并将该对象添加到ReadPool（见执行段的合并（二））中，这便是生成IndexReaderWarmer的过程。\n  删除信息包括了被删除的文档号跟变更的DocValues信息。\n  SegmentReader对象中包含的内容在SegmentReader系列文章中介绍，不赘述\n  固定的索引信息是哪些：\n\n索引文件.nvd、.nvm、.pos、.pay、.doc、.tim、.tip、.dim、.dii、.tvx、.tvd、.fdx、.fdt中包含的信息\n\n  为什么是包含了部分删除信息：\n\n执行段的合并是Lucene安排一个新的线程执行的并发操作，在合并的过程中，其他执行文档增删改的线程可能生成了新的删除信息，并且新的删除信息会在随后提交合并的流程中作用当前的新段\n\n  为什么要生成IndexReaderWarmer：\n\n首先要说的是，在合并阶段生成IndexReaderWarmer需要通过IndexWriterConfig.setMergedSegmentWarmer()方法设置，默认不使用该功能\n由于执行段的合并是并发操作，使得可以并发的提前读取新段的内容，即获得SegmentReader对象（生成IndexReaderWarmer的主要目的），其他线程执行近实时搜索NRT时就无需等待合并操作结束后再去获得SegmentReader对象，要知道获得SegmentReader对象的过程是I/O操作，故可以降低NRT搜索的延迟\n\n 提交合并\n  在介绍提交合并流程前，我们先介绍下MergeState，在执行段的合并（三）的文章中我们只介绍了该对象的生成时机，即图1的生成SegmentMerger流程，由于在当前提交合并的流程中将会用到该对象，故才在此流程点展开介绍。\n MergeState\n  MergeState维护了在段的合并过程中一些共同的状态（common state），在本篇文章中我们只关心在生成MergeState的过程中完成的几个任务，根据IndexWriter是否设置了IndexSort（见文章索引文件之si中关于IndexSort的介绍）可以将任务划分为如下两类：：\n\n设置了IndexSort\n\n任务一：对每一个待合并的段进行段内排序\n任务二：对合并后的新段进行段内排序\n任务三：获得所有待合并的段的被删除的文档号与段内真正的文档号的映射DocMap[ ]\n\n\n未设置IndexSort\n\n任务三：获得所有待合并的段的被删除的文档号与段内真正的文档号的映射DocMap[ ]\n\n\n\n  在介绍每一个任务前，我们首先介绍下在初始化IndexWriter对象的过程中段内排序的非法检查的流程（见源码中的validateIndexSort()方法），如果通过IndexWriterConfig.setIndexSort(Sort)设置了段内排序，那么每次flush后生成的段，它包含的文档（document）是按照参数Sort排序的，并且如果IndexWriter对象需要读取旧的索引，即不是该IndexWriter对象生成的索引，那么需要检查旧的索引中所有段的段内排序规则，判断过程如下所示：\n 初始化IndexWriter对象的过程中段内排序的非法检查流程图\n图2：\n\n IndexWriter没有设置IndexSort\n图3：\n\n  如果IndexWriter没有设置IndexSort，那么不需要对旧索引中的段的段内排序规则进行非法检查，同时在合并阶段只需要执行任务三。\n 旧的索引集合SegmentInfos\n图4：\n\n  IndexWriter读取索引目录中的内容时，默认总是只读取最后一次提交对应的索引信息，即只读索引文件名segment_N，N值最大的那个，通过segment_N文件获取到旧的索引集合SegmentInfos，SegmentInfos中包含的一个重要的信息就是一个存放SegmentCommitInfo的链表：\nprivate List&lt;SegmentCommitInfo&gt; segments = new ArrayList&lt;&gt;();\n  索引目录中为什么可能会有多个segment_N文件：\n\n该内容已在文章索引文件之segments_N中介绍，不赘述\n\n  如何根据segment_N文件获取到旧的索引集合SegmentInfos：\n\n通过索引文件segment_N的数据结构一目了然，如下所示：\n\n图5：\n\n  图5中，根据索引文件segment_N就可以找到这次提交对应的所有段的信息（除了删除信息），即蓝色框标注的SegmentCommitInfo。\n  另外图5中蓝色箭头描述的是，通过Segname就可以从索引目录中找到对应的索引文件.si，SegName为一个段的段命\n 依次处理每一个段\n图6：\n\n  从SegmentInfos中依次取出SegmentCommitInfo，当所有的SegmentCommitInfo都处理结束后退出。\n 获取一个段的段内排序规则\n图7：\n\n  在图5中，索引文件.si中红框标注的IndexSort就是一个段的段内排序规则，如果该字段不为空，说明该段设置了段内排序规则。\n 处理设置了段内排序的段\n图8：\n\n  如果该段的段内排序规则跟IndexWriter设置的不一致，那么无法初始化IndexWriter，并且抛出如下的异常：\nthrow new IllegalArgumentException(&quot;cannot change previous indexSort=&quot; + segmentIndexSort + &quot; (from segment=&quot; + info + &quot;) to new indexSort=&quot; + indexSort);\n  上述的异常中， segmentIndexSort是段的段内排序规则，IndexSort是IndexWriter设置的排序规则，info是该段对应的SegmentCommitInfo。\n  处理旧索引中的任意一个段时发生异常退出都会导致IndexWriter无法初始化。\n 处理未设置段内排序的段\n图9：\n\n  如果一个段未设置段内排序，并且生成该段的Lucene版本号大于等于6.0.0，那么无法初始化IndexWriter，并且抛出如下的异常：\nthrow new CorruptIndexException(&quot;segment not sorted with indexSort=&quot; + segmentIndexSort, info.info.toString());\n  上述的异常中， segmentIndexSort是段的段内排序规则，info.info描述的是该段对应的索引文件.si信息。\n  从图8、图9可以看出，设置了段内排序的IndexWriter只能处理下面两种情况的旧的索引：\n\n与IndexWriter有相同段内排序规则\n未设置段内排序，并且版本号小于6.0.0\n\n  如何读取未设置段内排序，并且版本号大于等于6.0.0的旧索引\n\n可以通过IndexWriter.addIndexes(CodecReader… readers)方法实现，该方法的参数readers为旧的索引的句柄。\n\n  调用IndexWriter.addIndexes(CodecReader… readers)方法的过程实际是将旧的索引进行合并，将新生成的段添加到IndexWriter中，该方法中的合并过程也会生成MergeState，并且只有这种情况下以及处理版本号小于6.0.0的旧索引才会执行上文中提到的任务一，而这正是先介绍初始化IndexWriter对象的过程中段内排序的非法检查流程的原因😝。\n 结语\n  生成MergeState过程中的三个任务的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","segment"]},{"title":"文档号合并（MUST）","url":"/Lucene/Search/2018/1218/%E6%96%87%E6%A1%A3%E5%8F%B7%E5%90%88%E5%B9%B6%EF%BC%88MUST%EF%BC%89/","content":"  这种Query组合的文档号合并的代码是在ConjunctionDISI类中实现。本文通过一个例子来介绍文档号合并逻辑，这篇文章中对于每个关键字如何获得包含它的文档号，不作详细描述，大家可以去看我添加了详细注释的ConjunctionDISI类，相信能一目了然。GitHub地址是：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/search/ConjunctionDISI.java。\n 例子\n  添加10篇文档到索引中。如下图：\n图1：\n\n  使用WhiteSpaceAnalyzer进行分词。\n  查询条件如下图，MUST描述了满足查询要求的文档必须包含&quot;a&quot;、“b”、“c”、“e” 四个关键字。\n图2：\n\n 文档号合并\n  将包含各个关键字的文档分别放入到各自的数组中，数组元素是文档号。\n 包含“a”的文档号\n图3：\n\n 包含“b”的文档号\n图4：\n\n 包含“c”的文档号\n图5：\n\n 包含“e”的文档号\n图6：\n\n  由于满足查询要求的文档中必须都包含&quot;a&quot;、“b”、“c”、“e” 四个关键字，所以满足查询要求的文档个数最多是上面几个数组中最小的数组大小。\n  所以合并的逻辑即遍历数组大小最小的那个，在上面的例子中，即包含&quot;b&quot;的文档号的数组。每次遍历一个数组元素后，再去其他数组中匹配是否也包含这个文档号。遍历其他数组的顺序同样的按照数组元素大小从小到大的顺序，即包含**&quot;e&quot;的文档号 —&gt; 包含&quot;a&quot;的文档号 —&gt; 包含&quot;c&quot;的文档号**。\n 合并过程\n  从包含&quot;b&quot;的文档号的数组中取出第一个文档号doc1的值 1，然后从包含&quot;e&quot;的文档号的数组中取出第一个不小于 doc1 (1)的文档号doc2的值，也就是5。\n  比较的结果就是 doc1 (1) ≠ doc2 (5)，那么没有必要继续跟其他数组作比较了。因为文档号1中不包含关键字&quot;e&quot;。\n图7：\n\n  接着继续从包含&quot;b&quot;的文档号的数组中取出不小于doc2 (5)的值（在图7的比较过程中，我们已经确定文档号1~文档号5中都不同时包含关键字&quot;b&quot;跟&quot;e&quot;，所以下一个比较的文档号并不是直接从包含&quot;b&quot;的文档号的数组中取下一个值，即2，而是根据包含&quot;e&quot;的文档号的数组中的doc2(5)的值，从包含&quot;b&quot;的文档号的数组中取出不小于5的值，即9），也就是 9，doc1更新为9，然后再包含&quot;e&quot;的文档号的数组中取出不小于doc1(9)，也就是doc2的值被更新为 9：\n图8：\n\n  比较的结果就是 doc1 (9) = doc2 (9), 那么我们就需要继续跟剩余的其他数组元素进行比较了，从包含&quot;a&quot;的文档号数组中取出不小于doc1 (9)的 文档号doc3的值，也就是 9：\n图9：\n\n  这时候由于 doc1 (9) = doc3 (9)，所以需要继续跟包含&quot;c&quot;的文档号的数组中的元素进行比较，从包含&quot;c&quot;的文档号的数组中取出不小于doc1 (9)的文档号doc4的值，也就是9:\n图10：\n\n  至此所有的数组都遍历结束，并且文档号9都在在所有数组中出现，即文档号9满足查询要求。\n 结语\n  本文介绍了使用BooleanQuery并且所有的TermQuery之间是MUST关系的文档号合并原理，在后面的文章中会依次介绍 SHOULD、MUST、MUST_NOT、FILTER的TermQuery的文档号合并原理。\n点击下载Markdown文档\n","categories":["Lucene","Search"],"tags":["MUST","booleanQuery"]},{"title":"文档号合并（SHOULD）","url":"/Lucene/Search/2018/1217/%E6%96%87%E6%A1%A3%E5%8F%B7%E5%90%88%E5%B9%B6%EF%BC%88SHOULD%EF%BC%89/","content":" 多个SHOULD的Query的文档号合并\n本篇文章通过一个例子介绍如何对满足搜索要求的文档进行合并（筛选），详细的合并过程可以看我的源码注释，GitHub地址是： https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/search/BooleanScorer.java。\n 例子\n添加10篇文档到索引中。如下图：\n图1：\n\n使用WhiteSpaceAnalyzer进行分词。\n查询条件如下图。MinimumNumberShouldMatch的值为2，表示满足查询条件的文档中必须至少包含&quot;a&quot;、“b”、“c”、&quot;e&quot;中的任意两个。\n图2：\n\n 文档号合并\n本篇文章中不会介绍如何根据关键字找到对应文档的过程，只介绍了如何合并(筛选)文档号的过程。\n首先给出一个Bucket[]数组，Bucket[]数组下标为文档号，数组元素为文档出现的频率，然后分别统计包含&quot;a&quot;、“b”、“c”、&quot;e&quot;的文档数，将文档出现的次数写入到Bucket[]数组。\n 处理包含关键字“a”的文档\n将包含“a”的文档号记录到Bucket[]数组中。\n图3：\n\n 处理包含关键字“b”的文档\n将包含“b”的文档号记录到Bucket[]数组中，文档号9第二次出现，所以计数加1。\n图4：\n\n 处理包含关键字“c”的文档\n将包含“c”的文档号记录到Bucket[]数组中，文档号3、6、8再次出现，所以对应计数都分别加1；\n图5：\n\n 处理包含关键字“e”的文档\n将包含“e”的文档号记录到Bucket[]数组中\n图6：\n\n 统计文档号\n在Bucket数组中，下标值代表了文档号，当我们处理所有关键字后，我们需要遍历文档号，然后判断每一个文档号出现的次数是否满足MinimumNumberShouldMatch，为了使得只对出现的文档号进行遍历，Lucene使用了一个matching数组记录了上面出现的文档号。matching数组记录文档号的原理跟FixedBitSet一样，都是用一个bit位来记录文档号。不赘述。\n在当前的例子中，我们只要用到matching[]的第一个元素，第一个元素的值是879(为什么只要用到第一个元素跟第一个元素的是怎么得来的，在BooleanScorer类中我加了详细的注释，这里省略)\n图7：\n\n根据二进制中bit位的值为1，这个bit位的位置来记录包含查询关键字的文档号，包含查询关键字的文档号只有0，1，2，3，5，6，8，9一共8篇文档，接着根据这些文档号，把他们作为bucket[]数组的下标，去找到每一个数组元素中的值，如果元素的值大于等于minShouldMatch，对应的文档就是我们最终的结果，我们的例子中\nbuilder.setMinimumNumberShouldMatch(2);\n所以根据最终的bucket[]\n图8：\n\n只有文档号3，文档号5，文档6，文档8，文档9对应元素值大于minShouldMatch，满足查询要求。\n 结语\n本文介绍了使用BooleanQuery并且所有的TermQuery之间是SHOULD关系的文档号合并原理，在后面的文章中会依次介绍 SHOULD、MUST、MUST_NOT、FILTER的TermQuery的文档号合并原理。\n点击下载Markdown文件\n","categories":["Lucene","Search"],"tags":["booleanQuery","SHOULD"]},{"title":"文档提交之commit（一）","url":"/Lucene/Index/2019/0906/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bcommit%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  阅读本文章（必须）需要前置知识：[文档提交之flush](https://www.amazingkoala.com.cn/Lucene/Index/2019/0716/文档提交之flush（一）\n)、文档的增删改的系列文章，下文中出现的未展开介绍的变量说明已经这些文章中介绍，本文中不赘述。\n  Lucene提供了TwoPhaseCommit接口（看这里：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/index/TwoPhaseCommit.java），支持两阶段提交（2-phase commit），在这个接口中提供了3个接口方法：\n\nprepareCommit()：该方法为两阶段提交的第一阶段，在这个阶段，会完成大部分的工作，包括处理新增的文档（Document）、删除的文档、生成所有的索引文件、生成Segment_N文件、保证索引文件持久化到磁盘操作，等等\ncommit()：该方法为两阶段提交的第二阶段，在这个阶段，处理一些简单的工作，包括删除旧的Segment_N文件（egment_N文件可能有多个，取决于索引删除策略，见Segment_N文件）、备份这次提交的内容到rollbackSegments（后面的文章会介绍）中\nrollback()：在执行prepareCommit()后，如果出现错误，可以通过该方法实现回滚到备份rollbackSegments\n\n 两阶段提交流程图\n图1：\n\n  上图中并没有给出rollback()方法的流程图，在介绍完文档提交之commit的系列文章后，会展开介绍。\n图1中，用户如果调用一个prepareCommit()方法实现二阶段提交的第一阶段，会生成pendingCommit，然后执行尝试段合并流程后退出；用户如果直接调用commit()方法，会先检查是否执行了第一阶段的提交操作，即判断是否存在pendingCommit，如果不存在则先执行执行第一阶段提交的流程，随后执行执行第二阶段提交交操作，最后执行尝试段合并流程后退出。\n  接下来我们就IndexWriter.commit()方法来介绍详细的文档提交之commit的过程。\n 文档提交之commit的整体流程图\n图2：\n\n点击查看大图\n  介绍每一个流程点之前，先给出文档提交之flush的整体流程图，其中红框标注的流程点属于两个流程图相同的部分，这些内容不会赘述，已在文档提交之flush系列文章中介绍。\n图3：\n\n点击查看大图\n 检查pendingCommit\n图4：\n\n  pendingCommit是什么：\n\npendingCommit是SegmentInfos类的对象，在SegmentInfos类中最重要的一个信息如下所示，在后面的文章中我们会逐步介绍该类包含的其他信息\n\npublic final class SegmentInfos  &#123;     private List&lt;SegmentCommitInfo&gt; segments;&#125;\n\nSegmentCommitInfo描述了一个段内的索引信息（见文档提交之flush（六）），在强制发布生成的段的流程点生成，同时被保存到SegmentInfos中，SegmentInfos中的信息在后面的流程中会生成Segment_N的索引文件\n\n  当多个线程持有相同的IndexWriter对象引用进行操作时，当前线程执行IndexWriter.commit()时会先检查其他线程或自己本身之前是否生成了pendingCommit对象，如果不存在那么执行二阶段提交的第一阶段的操作，否则直接执行二阶段提交的第二阶段的操作，即跳过图3中紫色虚线划分的所有流程点，直接执行蓝色虚线中的处理旧的Segment_N文件流程点。\n  在Lucene7.5.0版本中，只允许一个pendingCommit存在，否则会抛出异常，异常内容如下：\nthrow new IllegalStateException(&quot;prepareCommit was already called with no corresponding call to commit&quot;);\n  上面的异常描述了，当生成一个prepareCommit后，必须有线程执行二阶段提交的第二阶段后 才能再次生成一个新的prepareCommit，故使用synchronized(commitLock)来实现同步IndexWriter.prepareCommit()操作，其中commitLock对象没有实际意义，只是用来实现Java对象锁的功能：\nprivate final Object commitLock = new Object()\n  为什么使用两个synchronized(commitLock)同步操作：\n\n\n红色字体的synchronized(commitLock)开始用来同步IndexWriter.commit()操作，对应图3中红色字体的synchronized(commitLock)结束\n\n\n蓝色字体的synchronized(commitLock)开始用来同步IndexWriter.prepareCommit()操作，对应图3中蓝色字体的synchronized(commitLock)结束\n\n\n 生成完整的段信息\n图5：\n\n  这几个流程点是生成完整的段信息的过程， 即DWPT（见文档的增删改）生成对应段的过程（见文档提交之flush系列文章），不赘述。\n  在图5中，流程点执行commit前的工作跟图3中的执行flush前的工作是相同的操作，作者只是为了区分commit跟flush两种不同的操作而对流程点的名称作了区分，Lucene在该流程点提供了钩子函数doBeforeFlush ()，定义如下：\n/**  * A hook for extending classes to execute operations before pending added and  * deleted documents are flushed to the Directory.  */protected void doBeforeFlush() throws IOException &#123;&#125;\n 设置commitUserData\n图6：\n\n  用户调用prepareCommit()或者commit()方法前通过调用IndexWriter.setLiveCommitData(…)来记录自定义的信息，即commitUserData，比如说记录一些业务信息来描述这次提交操作。\n  例如Git操作时，我们可以通过命令git commit -m “add README&quot;来记录这次git提交的messages信息，commitUserData好比是&quot;add README”。\n 更新索引文件的计数引用\n  在这个流程点，需要执行两个操作：\n\n操作一：拷贝SegmentInfos对象segmentInfos，segmentInfos包含了当前索引目录中所有段的信息，由于当前线程获得了对象锁synchronized(commit)跟synchronized(fullFlushLock)，所以在这个时机下能获得一个不会被改变的并且此时的segmentInfos是我们这次commit处理的内容，故执行拷贝操作，获得一份拷贝信息，获得一个新的SegmentInfos对象toCommit，在后面的处理中，我们只对（只能对）toCommit进行处理，当前线程在后续的流程中释放对象锁synchronized(fullFlushLock)之后（见图3中的synchronized(fullFlushLock)结束），其他线程就可以通过flush（主动flush或者自动flush，见文档提交之flush）生成新的段，并添加到segmentInfos中，使得在多线程下提高并发性能（commit跟flush并发执行）\n操作二，增加toCommit对应的索引文件的计数引用，由于段合并属于并发操作，其他线程执行完合并操作后，合并前的旧段（old segment）对应的索引文件也需要被删除，其删除的机制即计数引用，一个段被其他对象有N次引用时，其索引文件对应的计数引用为N，当该段没有被任何对象引用后，那么就可以删除该段对应的索引文件，故增加索引文件计数引用，使得段不会被删除，所以在当前流程中，需要增加toCommit对应的索引文件的计数引用\n\n 执行flush后的工作\n图7：\n\n  该流程在前面的文章已经介绍，在源码中调用DocumentsWriterFlushControl.finishFullFlush( )的方法，详细的介绍见文档提交之flush（六）文章中的IndexWriter处理事件章节的内容。\n 执行commit后的工作\n图8：\n\n  Lucene在当前流程点提供一个钩子函数doAfterFlush()方法，用户可以实现自己的业务逻辑，定义如下：\n/** * A hook for extending classes to execute operations after pending added and * deleted documents have been flushed to the Directory but before the change * is committed (new segments_N file written). */  protected void doAfterFlush() throws IOException &#123;&#125;\n 执行同步磁盘工作\n图9：\n\n  图3中红色标注为commit跟flush相同的流程点，在执行完这些流程点之后，索引文件已经写到了磁盘，但由于文件I/O缓冲机制，索引文件的信息（部分或全部）可能暂时被保存在内核缓冲区高速缓存中，并没有持久化到磁盘，当出现类似断电的异常，且磁盘没有备用电源的情况，索引信息可能会丢失。\n  为什么使用文件I/O缓冲机制：\n\n系统调用在操作磁盘文件时不会直接发起磁盘访问，而是仅仅在用户空间缓冲区与内和缓冲区高速缓存（kernel buffer cache）之间复制数据，在后续的某个时刻，内核会将其缓冲区的数据写入至磁盘，另外如果此时另一进程试图读取该文件的这几个字节，那么内核将自动从缓冲区高速缓存中提供这些数据，而不是从文件中。使用这个机制使得Lucene的flush()操作在将数据写入缓冲区后能直接返回而不用等待（缓慢的）磁盘操作\n对文件I/O缓冲机制感兴趣的同学可以查看书籍 &lt;&lt;Linux/UNIX系统编程手册（上册）&gt;&gt;第13章，作者：Michael Kerrisk\n\n  在commit()操作中，执行同步磁盘操作，缓冲数据和与打开文件描述符fd相关的所有元数据都刷新到磁盘，仅在对磁盘设备的传递后，即等待索引文件都被持久化到磁盘后才会返回，故这是一个相对耗时的操作。\n  另外在执行完流程点执行同步磁盘工作后，释放对象锁synchronized(commitLock)，离开IndexWriter.prepareCommit()操作的临界区。\n 结语\n  本篇文章介绍了二阶段提交的第一阶段，基于篇幅，剩余内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之commit（二）","url":"/Lucene/Index/2019/0909/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bcommit%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本篇文章承接文档提交之commit（一），继续介绍文档提交之commit的剩余流程点。\n 文档提交之commit的整体流程图\n图1：\n\n点击查看大图\n 生成新的Segment_N文件\n图2：\n\n  在文档提交之commit（一）中，我们介绍了图1的执行同步磁盘工作的流程点，在这个流程点中，最重要的一个任务是保证所有的索引文件被持久化到磁盘上，另外还需要生成一个pending_segments_N文件，其中N描述了当前索引目录中索引文件的被提交的最新的一个迭代数（generation），这是一个从数值1开始递增的值，例如我们第一次执行两阶段提交之第一阶段后，会生成pending_segments_1，第二次执行两阶段提交之第一阶段后，会生成pending_segments_2，以此类推，由于图1的两个同步机制，使得N在临界区内递增，所以更新并获得N值的实现很简单：\nprivate long getNextPendingGeneration() &#123;    // generation == -1 说明是第一次执行两阶段提交之第一阶段    if (generation == -1) &#123;        return 1;    &#125; else &#123;        return generation+1;    &#125;\n  pending_segments_N文件是什么：\n\npending_segments_N就是segments_N文件，在当前流程点生成新的Segment_N文件只是简单的将pending_segments_N重命名为segments_N，最后再次同步磁盘，保证重命名正确被执行\n\n 执行检查点(checkPoint)工作\n  在这个流程中，顺序执行以下的操作：\n\n操作一：增加这次提交对应的索引文件的计数引用（计数引用的概念见文档提交之commit（一））\n操作二：执行索引删除策略（IndexDeletionPolicy）\n操作三：减少被删除的提交对应的索引文件的计数引用\n\n 操作二：执行索引删除策略（IndexDeletionPolicy）\n  为了便于理解，我们先介绍操作二：执行索引删除策略（IndexDeletionPolicy） 。\n  IndexDeletionPolicy描述了当新的提交发生后，如何处理旧的提交，下图给出了Lucene7.5.0中核心包core中的IndexDeletionPolicy类的类图：\n图3：\n\n NoDeletionPolicy\n  该策略描述了无论有多少次新的提交，旧的提交都不会被删除，下图展示了执行5次IndexWriter.commit()方法后索引目录中部分文件：\n图4：\n\n  使用这种索引删除策略的优点在于，配合segments_N文件和commitUserData（见文档提交之commit（一））我们可以将索引信息恢复到任意一个提交状态；缺点在于索引目录需要保留大量的索引文件，特别是多线程下执行flush()操作较多的场景下，如果你看过文档的增删改的系列文章，索引文件的数量与DPWT对象的个数成正比。\n KeepOnlyLastCommitDeletionPolicy\n  该策略是Lucene7.5.0中默认的策略，它描述了当有新的提交，则删除上一个提交，即索引目录中最多只存在一个segment_N文件。\n SnapshotDeletionPolicy\n  SnapshotDeletionPolicy用来保留提交的快照，它封装了其他的索引删除策略，由于NoDeletionPolicy保留了每一次的提交，所以封装该策略没有什么意义，当封装了KeepOnlyLastCommitDeletionPolicy，那么可以通过主动调用SnapshotDeletionPolicy.snapshot()的方法来实现快照功能，使得新的提交产生后，上一个提交能以快照的方式保留在内存中，这种策略的缺点在于需要额外一份索引信息大小的内存。\n PersistentSnapshotDeletionPolicy\n  该策略跟SnapshotDeletionPolicy一样提供快照功能，区别在于SnapshotDeletionPolicy的快照信息保留在内存中，而该策略则持久化（persist）到磁盘，并且生成snapshot_N文件，该文件中描述了快照信息，不展开介绍：\n图5：\n\n  另外SnapshotDeletionPolicy跟PersistentSnapshotDeletionPolicy都可以设置多个快照，只需要在每次提交后，执行SnapshotDeletionPolicy.snapshot()方法即可。\n  使用了封装KeepOnlyLastCommitDeletionPolicy的SnapshotDeletionPolicy或者PersistentSnapshotDeletionPolicy策略，控制了KeepOnlyLastCommitDeletionPolicy处理上一个提交的方式，上文中我们说到，每当一个新的提交产生，根据KeepOnlyLastCommitDeletionPolicy，旧的提交就会删除，即引目录中最多只存在一个segment_N文件文件，通过封装后，在每次执行IndexWriter.commit()后，主动调用了SnapshotDeletionPolicy.snapshot()的方法后，能使得上一个提交不再被删除，即保留旧的segment_N文件，这既是快照的实现本质。\n 例子1\n  图6中，我们只使用KeepOnlyLastCommitDeletionPolicy策略，并且执行三次commit()操作：\n图6：\n\n  图7为图6中执行了三次commit()操作后索引目录中segment_N文件，由于使用了KeepOnlyLastCommitDeletionPolicy策略，故segment_1、segment_2文件分别被删除，只留下segments_3：\n图7：\n\n 例子2\n  图6中，我们使用了封装KeepOnlyLastCommitDeletionPolicy的PersistentSnapshotDeletionPolicy策略，并且执行三次commit()操作，并且当count == 2时不生成快照，也就是不对segment_2生成快照：\n图8：\n\n  图9为图8中执行了三次commit()操作后索引目录中segment_N文件，由于使用了PersistentSnapshotDeletionPolicy策略，故只有segment_2被删除，留下segments_1跟segments_3：\n图9：\n\n  通过上面的例子我们可以看出以下的内容：\n\n可以选择性的执行快照功能：图8中我们没有对segments_2执行快照功能\n快照功能的实现方式：通过保留segment_N文件的方式\n\n  另外我们也可以选择性的删除某个或者某些快照，主动调用SnapshotDeletionPolicy.release(long)或者SnapshotDeletionPolicy.release(IndexCommit)的方法，这个就不展开介绍了。\n  上述例子的demo看这里：https://github.com/LuXugang/Lucene-7.5.0/tree/master/LuceneDemo/src/main/java/lucene/index/IndexDeletePolicyTest.java。\n  最后，对于PersistentSnapshotDeletionPolicy策略，生成跟删除快照都有磁盘同步操作（见文档提交之commit（一）关于同步磁盘的介绍），这是需要注意的地方。\n 操作一：增加这次提交对应的索引文件的计数引用\n  在操作二中，我们了解到，有些索引删除策略会删除上一个提交，删除提交的过程实质是减少该提交对应的索引文件的计数引用，为了防止本次提交对应的索引文件被误删，所以需要增加这次提交对应的索引文件的计数引用。\n  在文档提交之commit（一）中，我们已经增加了新的提交对应的索引文件的计数引用（图1中，二阶段提交之第一阶段的更新索引文件的计数引用），为什么这里还要增加：\n\n二阶段提交之第一阶段的更新索引文件的计数引用目的是防止执行段合并的其他线程导致索引文件被删除，操作一中的更新操作对应的是操作二中的情况，两次的更新操作目的不一样。\n\n 操作三：减少被删除的提交对应的索引文件的计数引用\n  基于索引删除策略，当旧的提交被删除后，通过减少其对应的索引文件的计数引用来正确删除索引目录中的部分索引文件。\n 设置rollbackSegments\n图10：\n\n  这里对本次提交对应的索引信息进行备份，使得可以实现回滚操作，该操作在后面的文章中会介绍，rollbackSegments的定义如下\nprivate List&lt;SegmentCommitInfo&gt; rollbackSegments; \n  该流程将pendingCommit（见文档提交之commit（一））中最重要的信息备份到rollbackSegments中。\n 更新索引文件的计数引用\n图11：\n\n  从图1中的流程中我们知道，本次提交对应的索引文件一共增加了两次计数引用，到此流程点时，我们已经完成了commit的工作，故需要减少本次提交对应的索引文件的计数引用。\n  另外释放对象锁，使得其他线程可以执行commit()的操作，至此二阶段提交之第二阶段的流程都已完成。\n 尝试段合并\n  每一次索引发生变化，都会尝试判断是否需要执行段的合并操作，其判断条件依据不同的合并策略而有所不同，合并策略的文章可以看这里：LogMergePolicy、TieredMergePolicy。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（一）","url":"/Lucene/Index/2019/0716/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  阅读本文章需要前置知识：文档的增删改的系列文章，下文中出现的未展开介绍的变量说明已经这些文章中介绍，本文中不赘述。\n  触发flush的方式可以分为主动flush跟自动flush：\n\n主动flush：触发该方式的场景很多，本篇文章只介绍由IndexWriter.flush()方法触发的flush。其他的触发flush的场景包括执行段合并的操作IndexWriter.forceMerge(int)、Facet中IndexWriter.addIndexes(Directory)、IndexWriter.shutDown( )、IndexWriter.forceMergeDeletes( )等等\n自动flush：在文档的增删改的系列文章中，我们介绍了文档的增删改操作的具体流程，并且知道当满足某些条件后触发自动flush\n\n  无论哪种方式触发flush，目标都是将内存中新生成的索引信息写入到Directory中。\n 预备知识\n updateStallState\n  更新拖延状态updateStallState实际更新的是写入的健康度（indexing healthiness），由于添加/更新文档跟执行主动flush是并行操作，故可能会存在添加/更新的速度快于flush的情况，使得内存中堆积索引信息，那么很容易出现OOM的错误，所以某些操作后需要调用updateStallState方法来更新当前写入的健康度。\n  例如添加/更新文档的操作会降低健康度，当达到阈值时，会阻塞这些执行添加/更新操作的线程；flush操作会提高健康度，当执行完flush，如果健康度恢复到小于阈值，那么唤醒那些被阻塞的线程，由于阈值的判断公式中涉及了两个变量activeBytes跟flushBytes，故当这两个变量发生变化时就需要调用updateStallState。\n  阈值判断公式以及线程阻塞的方式已在文档的增删改（三）中介绍，不赘述。\n 文档提交之flush的流程图\n图1：\n\n点击查看大图\n 自动flush\n图2：\n\n  在文章文档的增删改（三）中，介绍了在自动flush，执行DWPT的doFlush( )的条件：\n\nmaxBufferedDocs：当某一个DWPT收集的文档号达到maxBufferedDocs，该DWPT会执行doFlush( )\nramBufferSizeMB：当内存中activeBytes与deleteRamByteUsed的和值达到ramBufferSizeMB，那么从DWPTP中找到一个持有DWPT，并且该DWPT收集的索引信息量最大的ThreadState，将其置为flushPending，并且ThreadState持有的DWPT执行doFlush( )\n\n  上面的两种情形可以看出，无论是哪种方式触发的自动flush，每次只会有一个DWPT执行doFlush( )，这是跟主动flush其中一个不同点，在下文中我们会了解到，触发了主动flush以后，所有收集了索引信息的DWPT都会被执行doFlush( )，各自生成一个段。\n  自动flush中的 执行DWPT的doFlush()、IndexWriter处理事件这两个流程点跟主动flush中的有相同的逻辑，故下文中只介绍主动flush的流程点\n 执行flush前的工作\n图3：\n\n  在执行主动flush前，Lucene提供了一个钩子方法（hook method），使用者可以自定义一些执行flush前的信息，继承IndexWriter类就可以实现该方法。\n  唯一需要注意的是该方法不在flush的临界区（下文会介绍）内执行，钩子方法的定义如下：\nprotected void doBeforeFlush() throws IOException &#123;&#125;\n 收集所有达到flush条件的DWPT\n图4：\n\n  该流程点展开后的流程图如下：\n图5：\n\n点击查看大图\n 同步\n  多线程下可能存在多个线程同时调用IndexWriter.flush( )的方法，通过fullFlushLock对象实现同步（临界区）。\n  fullFlushLock的定义如下：\nprivate final Object fullFlushLock = new Object();\n  多线程下需要同步执行主动flush( )，通过抢占该对象，利用对象锁实现同步：\nsynchronized (fullFlushLock) &#123;    ... ... &#125;\n 置全局变量fullFLush为true\n图6：\n\n  在临界区内就可以置fullFLush为true，表示当前线程正在执行主动flush操作，fullFLush的作用范围包括正在执行添加/更新、删除的其他线程，该值在文档的增删改的系列文章中反复被提及，即触发全局flush。\n 替换全局删除队列\n图7：\n\n  在文档的增删改（四）我们已经介绍了删除队列deleteQueue的概念及其用途，它用来存储所有的删除信息，而在主动flush中，它还有一个额外的功能：定义flush的作用域。\n  添加/更新文档跟执行主动flush是并行操作，当某个线程执行主动flush后，随后其他线程获得的新生成的DWPT 不能在这次的flush作用范围内，又因为DWPT的构造函数的其中一个参数就是deleteQueue，故可通过判断DWPT对象中持有的deleteQueue对象来判断它是否在此次的flush的作用范围内。\n  故当某个线程执行了主动flush后，保存当前的全局变量deleteQueue为flushingQueue，然后生成一个新的删除队列newQueue更新为全局变量deleteQueue，其他线程新生成的DWPT则会使用新的deleteQueue，所以任意时刻最多只能存在两个deleteQueue，一个是正在主动flush使用的用于区分flush作用域的flushingQueue，另一个则是下一次flush需要用到的newQueue。\n 从DWPTP中找出满足flush条件的DWPT\n图8：\n\n  上图中虽然流程点很多，但目的就是每次从DWPTP中找出同时满足以下两个条件的DWPT，并且添加到fullFlushBuffer中（flushQueue、fullFlushBuffer、blockedFlushes、flushingWriters的概念已在文档的增删改（五）中介绍）：\n\n持有flushingQueue：上文中我们介绍了持有flushingQueue对象的DWPT都在这次的flush作用范围内，如果不持有，说明DWPT是在其他线程在当前线程主动flush之后生成的\nnumDocsInRAM &gt; 0：在文档的增删改（四）介绍了numDocsInRAM的概念，numDocsInRAM的个数描述了DPWT处理的文档数，故收集（处理）过文档的DWPT都在这次的flush作用范围内，如果DWPT对应的ThreadState不是flushPending状态，那么设置其状态，如果没有收集过文档，那么重置ThreadState\n\n 取出当前ThreadState持有的DWPT做了些什么工作\n  该操作需要顺序执行以下的工作：\n\n获得bytes：DWPT收集的所有文档对应的索引总量\n获得DWPT：没什么好说的\n重置ThreadState：重置后的ThreadState等待新的添加/更新任务\n更新flushWriters：将DWPT作为key，bytes作为value添加到flushWriters中\n\n 设置ThreadState为flushPending状态\n  设置状态的过程就实际是更新flushBytes和activeBytes，每一个DWPT达到flush条件前，其收集的文档对应的索引量会被添加到activeBytes，当DWPT达到条件后或者其对应的ThreadState被强制置为flushPending后（主动flush），那么需要从activeBytes扣除索引量，并且把索引量添加到flushBytes，flushBytes和activeBytes的值描述了当前内存索引的占用量，同时也可以用来反映索引堆积情况（健康度）（见上文中updateStallState的介绍）。\n 更新拖延状态\n  由于可能有设置ThreadState为flushPending状态的流程，所以需要调用updateStallState方法，调整当前写入的健康度，上文中已经介绍，不赘述。\n 为什么从DWPTP中取出一个ThreadState后，需要获得ThreadState的锁\n  在文档的增删改（二）的文章中，我们介绍了DWPTP中通过一个freeList的链表来维护完成了添加/更新操作后的ThreadState，同时还有一个threadStates的链表，他记录了所有的ThreadState，即正在执行添加/更新操作（active threadState）以及完成添加/更新任务的ThreadState（inactive threadState，freeList中的ThreadState）。threadStates链表中的个数只会增加（执行添加/更新的任务如果没能从freeList中得到ThreadState对象则新生成一个）不会减少，不过总的ThreadState对象的个数（active threadState和inactive threadState的总个数）不会大于持有相同IndexWriter对象的线程的线程总数。\n  由于添加/更新文档跟执行主动flush是并行操作，可能该active threadState中的DWPT在这一次flush的作用范围内，那么当前执行主动flush的线程就可以通过ThreadState的锁等待其他线程完成添加/更新文档的任务，最后结合遍历threadStates链表，就可以收集到满足这次flush条件的所有DWPT（不包括blockedFlushes中的DWPT，下文会介绍）\n  从这个逻辑可以看出即使是多线程执行添加/更新操作，还是可能因为某个线程执行主动flush会出现同步等待的情况。\n 将blockedFlushes、fullFlushBuffer中的DWPT添加到flushQueue中\n图9：\n\n  blockedFlushes用来存放优先执行doFLush( )的DWPT，在文档的增删改（五）文章中介绍了为什么有些DWPT需要优先执行doFLush( )，而在这里文章里面我们介绍了为什么需要额外的blockedFlushes对象来存放该DWPT，原因就是添加/更新跟主动flush是同步操作，我们必须通过遍历存放所有ThreadState的threadStates链表才能找到所有满足flush要求的DWPT，如果不使用blockedFlushes，我们需要从这些DWPT挑选出一个优先执行doFlush( )的DWPT，相比较之下，使用blockedFlushes更简单高效。\n  上述填了文档的增删改（五）文章中提到的一号坑，而还有一个二号坑则还得在后面的文章中解释。\n  将blockedFlushes跟fullFlushBuffer中的DWPT都添加到flushQueue中之后，那么收集所有达到flush条件的DWPT的流程就完成了。\n 结语\n  由于篇幅原因，将在后面的文章中继续介绍文档提交之flush的其他流程。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（七）","url":"/Lucene/Index/2019/0807/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E4%B8%83%EF%BC%89/","content":"  本文承接文档提交之flush（六），继续依次介绍每一个流程点。\n 文档提交之flush的整体流程图\n图1：\n\n 预备知识\n Set&lt;FrozenBufferedUpdates&gt; update\n  update中存放的是FrozenBufferedUpdates的集合，在文档提交之flush（六）中FrozenBufferedUpdates获得nextGen的值之后，它会被添加到update容器中，在FrozenBufferedUpdates中的删除信息作用到段之后，从update中移除。\n IndexWriter处理事件\n  在文档提交之flush（六）中我们介绍了IndexWriter处理事件流程中的几种事件类型，下面仅介绍发布生成的段中的处理删除信息事件。\n 处理删除信息\n  根据作用（apply）的目标段，处理删除信息划分为两种处理方式：\n\n全局FrozenBufferedUpdates：根据全局FrozenBufferedUpdates内的nextGen（见文档提交之flush（六））值，其删除信息将要作用到所有比该nextGen值小的段\n段内FrozenBufferedUpdates：在文档提交之flush（三）中我们提到，在生成索引文件的过程中，我们只处理了部分满足删除信息，即只处理了满足删除信息TermArrayNode、TermNode（见文档的增删改（四））的段内部分文档，而如果段内FrozenBufferedUpdates还存在删除信息QueryArrayNode、DocValuesUpdatesNode，那么在当前流程中需要找出所有剩余的满足删除的文档\n\n 处理删除信息的流程图\n图2：\n\n点击查看大图\n  该流程在以下三种情况下会被调用：\n\n线程执行eventQueue中的发布生成的段中的处理删除信息事件\n线程执行段合并之前的初始化操作（IndexWriter.mergeInit(MergePolicy.OneMerge)方法）\n执行主动flush的线程等待其他线程执行完所有的发布生成的段中的处理删除信息事件（下文中会介绍）\n\n  上述的三种情况会发生并发执行同一个FrozenBufferedUpdates的作用（apply）删除信息工作。\n 是否已经处理过当前删除信息\n图3：\n\n  如上文中介绍，多线程可能同时执行同一个FrozenBufferedUpdates的作用（apply）删除信息的工作，即图2的流程，但只允许一个线程执行，否则会重复的处理删除信息，其他线程会被阻塞直到获得锁的线程执行完图2的流程。\n  通过什么方式判断其他线程已经处理过当前删除信息：\n\n初始化一个CountDownLatch，如下所示：\n\npublic final CountDownLatch applied = new CountDownLatch(1);\n\n当线程执行完成图2的流程，会将计数置为0，其他线程进入图2的流程后如果当前计数为0则直接退出\n\n 获得当前已经完成段合并的计数mergeGenStart\n图4：\n\n  执行段合并的线程在执行完一次段合并之后，会递增一个计数，即mergeGenStart，该值会在后面的流程介绍，这里只要知道获得mergeGenStart所在的流程点位置。\n 获得被作用的段集合infos\n图5：\n\n  在文档提交之flush（六）中我们了解到，描述新生成的段以及旧段的索引信息SegmentCommitInfo都存放在IndexWriter的全局变量SegmentInfos中，此流程从SegmentInfos找到那些将要被作用删除信息的段的集合，根据当前处理的FrozenBufferedUpdates是全局还是段内，获取的方式有点区别：\n\n全局FrozenBufferedUpdates：取出SegmentInfos中所有的SegmentCommitInfo\n段内FrozenBufferedUpdates：只取出段对应的SegmentCommitInfo\n\n  infos为空的后执行的流程在下文介绍。\n  为什么这里需要通过IndexWriter对象实现同步：\n\n由于其他线程可能正在执行段的合并，有可能会使得一些SegmentCommitInfo被合并掉（merge away），在这个临界区内我们需要保证所有的SegmentCommitInfo暂时不允许被修改（change），直到我们获得这些SegmentCommitInfo包含的索引信息，之后就可以离开临界区了。在后面的流程中我们会知道，当离开临界区之后，可能某些SegmentCommitInfo马上发生了更改（比如其他线程执行了段合并），那么我们会重新去获取索引目录中最新的段集合\n\n 获取infos中所有SegmentCommitInfo的信息\n图6：\n\n  获取的信息被保存到SegmentState[ ]数组中，在介绍数组元素包含的信息前，得先说明只有满足下列要求的SegmentCommitInfo才会将其信息保存到SegmentState[ ]数组中：\nbufferedDeletesGen &lt;= delGen &amp;&amp; alreadySeenSegments.contains(SegmentCommitInfo) == false\n\n\nbufferedDeletesGen：该值描述的是SegmentCommitInfo的delGen，delGen的值即nextGen，只是换了个名字而已\n\n\ndelGen：delGen即当前FrozenBufferedUpdates的nextGen\n\n\n  首先保证bufferedDeletesGen &lt;= delGen，上文中我们提到的，根据全局FrozenBufferedUpdates内的nextGen（见文档提交之flush（六））值，即delGen，其删除信息将要作用到所有比该nextGen（delGen）值小的段，其中等号&quot;=&quot;考虑是段内的FrozenBufferedUpdates的delGen跟此段的delGen是相等的情况。\n\nalreadySeenSegments：这个变量在下文会介绍，因为得结合下文中的内容，所以稍安勿躁\n\n  再满足了上面的条件之后，就可以获取infos中所有满足条件的SegmentCommitInfo的信息了。SegmentState[ ]数组中的数组元素至少（跟本篇文章相关的）包括了下面的信息：\n\nSegmentReader：该值不具体展开介绍，这里我们只需要知道它描述了段中的索引信息\nReadersAndUpdates：在本篇文章中我们只需要知道该类中有一个变量PendingDeletes，它用来记录段中被删除的文档号集合\n\n  为什么要增加段集合中的所有索引文件计数引用：\n\n索引目录中一个段对应的所有索引文件，在生成复合索引文件时，非复合索引文件会被删除，除了这个场景，比如在执行了段合并后，合并前的旧段对应的索引文件也需要被删除，其删除的机制即计数引用，一个段被其他对象有N次引用时，其索引文件对应的计数引用为N，当该段没有被任何对象引用后，那么就可以删除该段对应的索引文件\n在当前的流程点我们需要引用段，而此时有可能其他线程正在合并此段，为了防止合并后，段的引用计数为0，即其索引文件的引用计数为0，我们这里需要增加计数引用，防止索引文件被删除\n\n 处理删除信息\n图7：\n\n  图7中，处理TermDeletes即处理删除信息TermArrayNode、TermNode；处理QueryDeletes即处理删除信息QueryArrayNode；处理DocValuesUpdates即处理删除信息DocValuesUpdatesNode。其删除信息的介绍看文档的增删改（四）。\n  上文中我们获得了SegmentReader，该值使得我们能获得段中的索引信息，包括文档的信息：\n\n处理TermDeletes：该处理逻辑跟Lucene查询阶段，查找一个Term对应所有文档的逻辑是一样的，在这里我们暂时不介绍，在介绍Lucene的查询时会详细展开\n处理QueryDeletes：该处理逻辑跟Lucene查询阶段，通过一个Query查找出所有文档的逻辑是一样的，在这里我们暂时不介绍，在介绍Lucene的查询时会详细展开\n处理DocValuesUpdates：该处理在后面介绍软删除的文章会展开介绍\n\n  尽管我们没有对上述几个处理逻辑进行展开介绍，但这三个流程最终的目的就是找出满足删除要求的文档号，通过ReaderPool对象暂存TermDeletes、QueryDeletes生成的删除信息以及DocValuesUpdates生成的更新信息。最后在图1的流程点更新ReaderPool中将删除信息以及更新信息生成索引文件。\n  如果图2中处理的是段内FrozenBufferedUpdates，那么是不用处理处理TermDeletes的，因为删除信息TermArrayNode、TermNode在生成索引文件.tim、.tip、.doc、.pos、.pay阶段就被处理了（见文档提交之flush（三））。\n 处理完删除信息后的工作\n图8：\n\n  在处理完删除信息后，我们需要以下的工作：\n\n减少段集合中的所有索引文件计数引用：我们不再需要引用段，故减少段对应的索引文件的计数引用，如果计数值为0，那么删除这些索引文件，同时说明该段被合并了\n判断是否至少有一个段发生了变化：变化描述的是在上面的流程中，段中的一个或多个文档被标记为删除，那么我们需要另IndexWriter中的一个全局变量maybeMerge为true，maybeMerge描述的是需要进行尝试段合并操作，在执行完主动flush后，会尝试进行段合并，段的合并策略以及合并计划可以看LogMergePolicy、TieredMergePolicy、MergeScheduler\n判断是否有些段中的文档都被标记为删除：在上面的流程中，有可能一个或多个段中所有的文档都被值为删除，那么我们需要丢弃这些段\n\n 再次处理DocValuesUpdates\n图9：\n\n  该流程会在后面介绍软删除的文章中展开介绍，这里只要知道其流程所在位置即可。\n 是否为段内删除信息？\n图10：\n\n  图2如果到达此流程点，并且段内FrozenBufferedUpdates的流程，那么我们已经成功的处理了段内的删除信息，故可以直接进入其下一个流程，该流程点在下文会介绍。\n 没有并发的段合并操作\n图11：\n\n  在上文的获得当前已经完成段合并的计数mergeGenStart中，我们获得了mergeGenStart，并且在当前流程点再次去获得段合并的计数mergeGenCur，如果mergeGenCur 与 mergeGenStart相等，说明图2的流程从开始到现在这段期间，没有其他线程执行段合并 或者 某些段合并操作还未结束，否则我们需要重新执行图2的流程，原因是索引目录中的段已经发生了变化，我们需要重新将全局的FrozenBufferedUpdates作用到索引目录中的段。\n  从上面的流程可以看出，这种处理逻辑即乐观锁的一种实现方式。\n  由于执行段合并的段集合只是索引目录中的部分段，所以有些段并没有发生变化，并且这些段已经被作用（apply）了删除信息，故可以存放到上文中提到的alreadySeenSegments中，使得在下一轮的图2流程中，不会重复作用删除信息。\n 处理FrozenBufferedUpdates\n图12：\n\n  处理FrozenBufferedUpdates的流程点逻辑跟图11中的是一致的，在退出图2流程之前，我们总是需要处理FrozenBufferedUpdates。只有mergeGenCur 与 mergeGenStart相等后才属于正确的处理删除信息，在其他流程点进入该流程点都属于未能正确处理删除信息。\n  处理FrozenBufferedUpdates的工作如下，仅列出跟本篇文章有关的工作：\n\n另applied（见上文的是否已经处理过当前删除信息流程）的计数为0，使得并发执行图2流程的线程直接退出\n从update中移除当前FrozenBufferedUpdates\n\n 结语\n  基于篇幅，图1中剩余流程点留到下一篇文章介绍。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（三）","url":"/Lucene/Index/2019/0725/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E4%B8%89%EF%BC%89/","content":"  本文承接文档提交之flush（二），继续依次介绍每一个流程点，下面先给出在前面的文章中我们列出的流程图：\n图1：\n\n  图1是文档提价之flush的流程图。\n图2：\n\n  图2是图1中执行DWPT的doFlush()的流程图流程点的流程图，在文档提交之flush（二）中我们已经介绍了将DWPT中收集的索引信息生成一个段newSegment之前的流程点。\n点击查看大图\n 将DWPT中收集的索引信息生成一个段newSegment的流程图\n图3：\n\n  在上面流程图中，可以看出在流程点将DWPT中收集的索引信息生成一个段newSegment实现了所有索引文件的生成，DWPT中包含了生成这些索引文件需要的信息。\n 处理出错的文档\n图4：\n\n  在文档的增删改（四）中我们了解到，删除信息根据不同的删除方式会被记录到BufferedUpdates（全局BufferedUpdates或者DWPT的私有BufferedUpdates）不同的容器中：\n\nMap&lt;Term,Integer&gt; deleteTerms\nMap&lt;Query,Integer&gt; deleteQueries\nMap&lt;String,LinkedHashMap&lt;Term,NumericDocValuesUpdate&gt;&gt; numericUpdates\nMap&lt;String,LinkedHashMap&lt;Term,BinaryDocValuesUpdate&gt;&gt; binaryUpdate\n\n  而在两阶段生成索引文件之第一阶段的文章中我们介绍了添加一篇文档的过程，即DWPT收集该文档的索引信息的过程，如果在这个过程中发生任何错误，那么该文档的文档号会记录到DWPT的私有BufferedUpdates（见文档提交之flush（二））中，即存放到下面的容器中：\n\nList&lt;Integer&gt; deleteDocIDs = new ArrayList&lt;&gt;();\n\n  执行到该流程点，将deleteDocIDs中所有的文档号写入到FixedBitSet对象中，该对象描述了那些被删除的文档号，在后面的流程中，FixedBitSet中的文档信息会被写入到索引文件.liv中。\n 生成索引文件\n图5：\n\n  这里只简单介绍下生成各个索引文件的先后顺序，其生成过程没啥好写的，只要熟悉每个索引文件的数据结构就行啦。\n  不过生成索引文件的过程中，有两个知识点还是要说明下的，一个是Sorter.DocMap对象，另一个是找出部分删除文档的文档号。\n Sorter.DocMap sortMap\n  DocMap是类Sorter的内部类，而sortMap则是在源码中Sorter.DocMap类的一个对象名。\n  当我们在生成IndexWriter对象时，可以通过IndexWriterConfig.setIndexSort(Sort)的方法来定义一个排序规则，在生成索引文件的过程中，使得一个段内的所有索引文件中的文档根据该规则进行排序，当然并不是真正的排序，而是生成一个映射关系sortMap（见Collector（三）中的预备知识），sortMap描述了文档之间的顺序。至于为什么要对文档排序，sortMap如何实现映射，并不是本篇文章关心的，在后面的文章中会介绍。\n 找出部分删除文档的文档号\n  在上面的内容中我们知道，BufferedUpdates的多个容器中存放各种删除信息，其中Map&lt;Term,Integer&gt; deleteTerms中存放了根据Term进行删除的删除信息，根据该删除信息，在生成索引文件.tim、.tip、.doc、.pos、.pay的过程中会找到那些满足删除要求的文档号，随后将这些文档号添加到FixedBitSet（上文介绍了该对象的用途）对象中。\n  至于查找过程，这块的内容会跟后面介绍文档查询的文章重复，故这里先不做介绍。\n  为什么只找出部分删除文档的文档号，而不是根据BufferedUpdates中所有容器的删除信息找到所有满足删除要求的文档：\n\n原因一：先说为什么不根据Map&lt;Query,Integer&gt; deleteQueries容器找出满足删除要求的文档号，由于这是通过一个查询删除（跟在查询阶段，生成一个Query进行查询是一个操作），即查询一个段中文档的操作，而此时该段还没有完全生成结束，故无法实现该操作。至于为什么不根据numericUpdates、binaryUpdate容器找出满足删除要求的文档号，这块的话在后面介绍软删除的文章中会介绍。\n原因二： 在自动flush的操作中，允许并发的执行多个DWPT生成段，在当前阶段可以并发的找出被删除的文档号，如果不在此时执行找出部分删除文档的文档号的操作，尽管在后面的流程中也会执行，但是在那个过程中，是串行执行，所以提高了生成段的性能。至于为什么要串行执行，在下面的流程中会说明。\n\n 处理软删除文档\n图6：\n\n  在上一个流程中，我们知道，有些文档被标记为删除了，而这些文档有可能是软删除的文档，那么软删除文档的个数需要被更新。\n 清楚删除信息TermNode\n图7：\n\n  在前面的流程中，根据Term进行删除的删除信息已经作用（apply）到了当前段，所以需要清除TermNode的信息，以免在后面的流程中重复执行，TermNode描述的删除信息即容器Map&lt;Term,Integer&gt; deleteTerms中的删除信息。\n 记录所有生成的索引文件\n图8：\n\n  记录当前已经生成的索引文件的文件名，因为如果通过IndexWriterConfig.setUseCompoundFile(boolean)设置了使用复合索引文件cfs&amp;&amp;cfe存储文档的索引信息，那么在后面的流程中，在生成完复合索引文件后，需要删除这些索引文件，所以在索引文件之cfs&amp;&amp;cfe的文章的开头部分，我们提到了当使用了复合索引文件后，索引目录中最终保留.cfs、.cfe、.liv、.si的索引文件（执行commit()操作之前）。\n 生成FlushedSegment\n图9：\n\n  这里只是为了介绍生成FlushedSegment对象的时机，至于FlushedSegment是干嘛的，在本篇文章中并不重要，在后面的文章中会详细介绍。\n 生成复合索引文件\n图10：\n\n  如果通过IndexWriterConfig.setUseCompoundFile(boolean)设置了使用复合索引文件cfs&amp;&amp;cfe存储文档的索引信息，那么在当前流程点会正式的生成复合索引文件.cfs、.cfe，注意的是此时非复合索引文件还没有被删除，在后面的流程中才会被删除，在后面的文章中会介绍为什么不在这个流程点删除。\n 生成索引文件.si\n图11：\n\n  生成索引文件.si的过程不介绍了，没什么好讲的，了解.si文件的数据结构就行啦。\n 生成索引文件.liv\n图12：\n\n  生成索引文件.liv的过程不介绍了，没什么好讲的，了解.liv文件的数据结构就行啦。\n 结语\n  本篇文章主要介绍了每一种索引文件的生成顺序，强调的是，想要理解Lucene如何实现查询的原理，那么必须了解所有索引文件的数据结构，当然在以后的文章中会介绍其查询原理。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（二）","url":"/Lucene/Index/2019/0718/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  本文承接文档提交之flush（一），继续依次介绍每一个流程点。\n 文档提交之flush的流程图\n图1：\n\n点击查看大图\n 执行DWPT的doFlush()\n图2：\n\n  在文档提交之flush（一）的文章中，我们知道，执行主动flush并完成流程点收集所有达到flush条件的DWPT之后，具有相同删除队列deleteQueue的DWPT都被添加到了flushQueue中，而在当前流程点中，则是从flushQueue中依次去取出每一个DWPT，执行doFlush( )操作。\n  从图1中可以知道除了主动flush，自动flush的DWPT也会执行doFlush( )，由于两种情况使用同一个流程点，但实际流程点中的具体逻辑还是有所区别。\n 执行DWPT的doFlush()的流程图\n图3：\n\n点击查看大图\n 取出一个DWPT\n图4：\n\n  图4中，尽管主动flush跟自动flush都是执行DWPT的doFlush( )，但是DWPT的来源是不一样的。\n  在全局flush触发的情况下，如果此时自动flush处理的DWPT跟全局flush的DWPT类型不一致（只可能存在两种不同类型的DWPT，见文档提交之flush（一）），那么主动flush跟自动flush的线程不能同时进入执行DWPT的doFlush()的流程图（该流程点的入口对应源码中DocumentsWriter.doFlush(DocumentsWriterPerThread)），反之如果自动flush处理的DWPT跟全局flush的DWPT类型是一致的，那么允许并发的执行执行DWPT的doFlush()。\n  什么情况下自动flush处理的DWPT跟全局flush的DWPT类型会不一致：\n\n在文档提交之flush（一）中我们了解到，当主动flush触发后，会执行替换全局删除队列的流程，此时如果有新的添加/更新文档的操作，意味着会生成另一种类型的DWPT，即该DWPT持有不同全局删除队列，如果在执行完处理文档的任务后，DWPT满足自动flush的条件，那么此时就会出现自动flush处理的DWPT跟全局flush的DWPT类型会不一致的情况\n\n  什么情况下自动flush处理的DWPT跟全局flush的DWPT类型是一致的：\n\n全局flush未触发的情况下，线程A中的DWPT满足自动flush的条件，准备执行执行DWPT的doFlush()的流程，此时线程B执行主动flush，那么此时就会出现自动flush处理的DWPT跟全局flush的DWPT类型是一致的情况\n\n  通过什么方式防止同时进入执行DWPT的doFlush()的流程图这个流程点：\n\n在线程A触发了全局flush后，并且在flush期间（全局变量fullFlush为true）如果线程B（持有的DWPT跟当前正在flush的DWPT持有不一样的删除队列deleteQueue）中的DWPT执行了添加/更新的操作后，如果因为达到了maxBufferedDocs而触发自动flush，那么该DWPT会被添加到blockedFlushes中；如果因为达到ramBufferSizeMB而触发自动flush，并且该DWPT收集的索引量是DWPTP中所有跟它具有相同删除队列的DWPT集合中最大的，那么它也会被添加到blockedFlushes中，否则什么也不做。另外当主动flush在执行完后，会马上将blockedFlushes中的DWPT添加到flushQueue中\n上面的描述同时也填了我们在文档的增删改（五）中介绍blockedFlushes时挖的二号坑，即blockedFlushes的另一个作用是防止主动flush跟自动flush同时进入执行DWPT的doFlush()的流程图这个流程点\n\n  为什么不允许同时进入执行DWPT的doFlush()的流程图这个流程点：\n\n这里先暂时给出结论：为了能正确处理删除信息。在下面的流程点中会展开\n\n  为什么都是执行DWPT的doFlush( )，但是DWPT的来源是不一样的：\n\n主动flush：在文档提交之flush（一）的文章中我们介绍了，由于主动flush跟添加/更新文档是并行操作，此时的DWPTP中可能有最多两种包含不同的删除队列deleteQueue的DWPT类型，所以主动flush需要从DWPTP中挑选出这次flush的DWPT，并存放到flushQueue中，随后依次执行doFLush( )\n自动flush：全局flush没有被触发，那么DWPTP中的DWPT肯定都包含相同的deleteQueue（原因见文档提交之flush（一）），所以可以从DWPTP中把所有满足flush条件的DWPT都依次取出来执行doFLush( )，为什么还要先去flushQueue中找DWPT呢：\n\n自动flush处理的DWPT类型跟主动flush的不一致：上文中我们提到，由于线程A正在执行主动flush，线程B中的满足flush条件的DWPT被存到了blockedFlushes中，并且在主动flush结束后，这些DWPT被添加到了flushQueue中，所以先到要flushQueue中找DWPT，并且这些DWPT能被优先执行doFLush( )，毕竟它们的存在说明内存中堆积了不少的索引内存量\n自动flush处理的DWPT类型跟主动flush的是一致的：在后面的文章中我们将会知道，执行DWPT的doFlush()是开销较大的流程点，先去flushQueue中找DWPT可以的线程可以帮助正在执行主动flush的线程将DWPT生成一个段，提高主动flush的性能\n\n\n\n 更新拖延状态\n图5：\n\n  如果是从flushQueue中取出的DWPT，说明正在执行主动flush或者刚刚结束，那么需要更新拖延状态，其更新的原因在文档提交之flush（一）文章关于updateStallState的概念已经作了介绍，不赘述。\n 处理全局BufferedUpdates、更新DWPT的私有BufferedUpdates\n图6：\n\n  从图5中的流程点是否新增删除信息可以看出，这个流程点只针对于自动flush，因为在主动flush阶段，新增的删除信息会被添加到新的全局删除队列newQueue中（见文档提交之flush（一）），所以主动flush使用的删除队列flushingQueue不会新增删除信息。\n  首先回顾下之前讲的内容，在文档的增删改（四）的文章中我们提到，每个DWPT有一个私有的deleteSlice，该deleteSlice描述了作用于DWPT中的文档的删除信息，该删除信息最后会保存在DWPT的私有BufferedUpdates中，另外我们还介绍了一个全局的删除队列deleteQueue，它描述了所有的删除信息，该删除信息会被保存在全局BufferedUpdates（源码中的变量名是globalBufferedUpdates）中。\n 生成FrozenBufferedUpdates并清空全局BufferedUpdates\n  全局BufferedUpdates会被冻结（Frozen）为一个全局FrozenBufferedUpdates，它包含了删除信息，并且该删除信息在后面的流程中会作用于之前的所有段中的文档，之后清空全局BufferedUpdates。\n  为什么要清空全局BufferedUpdates：\n\n主动flush：如果不清空，那么每一个DWPT都会生成一个全局FrozenBufferedUpdates，并且都携带了相同的删除信息（flushingQueue不再改变），这是没有必要的\n自动flush：如果不清空，那么后生成的FrozenBufferedUpdates会携带之前生成的FrozenBufferedUpdates中的冗余删除信息\n\n FlushTicket添加到TicketQueue\n  FlushTicket类中主要的两个变量如下所示：\nstatic final class FlushTicket &#123;    private final FrozenBufferedUpdates frozenUpdates;    private FlushedSegment segment;    ... ....&#125;\n  其中frozenUpdates就是上文中包含删除信息且作用于其他段中的文档的全局FrozenBufferedUpdate，而segment则是DWPT对应生成的段，只是segment在随后的流程中才会被添加进来，最后FlushTicket对象被添加到TicketQueue中，TicketQueue的内容在后面的流程中会详细介绍。\n 更新DWPT的私有BufferedUpdates\n  不管是主动flush还是自动flush，DWPT在最后一次收集文档后，其他线程可能新增了deleteQueue中的删除信息，这些删除信息需要作用于该DWPT，故需要更新DWPT的私有deleteSlice，即让deleteSlice的sliceTail持有跟tail一样的对象引用（见文档的增删改（四）），最后将deleteSlice中的删除信息更新到私有BufferedUpdates中。\n  在后面的将DWPT中收集的索引信息生成一个段newSegment流程中，我们会了解到，DWPT的私有BufferedUpdates也会被冻结为其私有FrozenBufferedUpdates，其包含的删除信息只作用于DWPT自身收集的文档，或者说只作用于DWPT对应生成的段，而全局的FrozenBufferedUpdates则是作用于其他的段。\n  为什么这里需要使用synchronized同步：\n\n源码中给出了解释，为了避免可能翻译造成的不准确描述，所以这里直接原文：Since with DWPT the flush process is concurrent and several DWPT could flush at the same time we must maintain the order of the flushes before we can apply the flushed segment and the frozen global deletes it is buffering. The reason for this is that the global deletes mark a certain point in time where we took a DWPT out of rotation and freeze the global deletes\nExample: A flush ‘A’ starts and freezes the global deletes, then flush ‘B’ starts and freezes all deletes occurred since ‘A’ has started. if ‘B’ finishes before ‘A’ we need to wait until ‘A’ is done otherwise the deletes frozen by ‘B’ are not applied to ‘A’ and we might miss to deletes documents in ‘A’\n\n  下面用一个例子来说明上文中描述的内容。\n 例子\n  有两个线程ThreadA、ThreadB，他们都是执行添加文档的操作，并且假设他们在执行完添加操作后两个线程中的DWPT都会生成一个段，并且全局删除队列deleteQueue为空。\n  ThreadA在执行完添加操作之后，其他线程新增了一个删除信息（对应图6中的TermNode）到全局删除队列deleteQueue中，当ThreadA中的DWPT执行doFlush时，更新DWPT私有的deleteSlice（即更新deleteSlice中的sliceTail，deleteSlice的概念见文档的增删改（四）），并且全局的删除信息会被冻结到全局FrozenBufferedUpdates（包含TermNode对应的删除信息），我们称之为F1，即删除信息TermNode将要作用于已有的段中的文档。\n图7：\n\nThreadB在执行完添加操作之后，其他线程新增了一个删除信息（对应图7中的QueryArrayNode）到全局删除队列deleteQueue中，当ThreadB中的DWPT执行doFlush时，更新DWPT私有的deleteSlice，并且全局的删除信息会被冻结到全局FrozenBufferedUpdates（包含QueryArrayNode、TermNode对应的删除信息），我们称之为F2，即删除信息QueryArrayNode、TermNode将要作用于已有的段中的文档。\n图8：\n\n  如果此时ThreadB优先ThreadA先生成一个段，那么F2中的删除信息，即QueryNode无法作用于ThreadA中的DWPT对应的段中的文档，并且在上面的例子中，F1中的删除信息还会错误的作用到ThreadB中的DWPT对应的段中的文档。\n  如果能同步两个线程的操作，那么在上面的例子中ThreadA中的DWPT会先生成一个段，并且F1中的删除信息TermNode作用于已有的段中的文档，然后清空全局BufferedUpdates，接着ThreadB中的DWPT会先生成一个段，并且F2中的删除信息QueryArrayNode（注意F2包含的删除信息只有一个QueryArrayNode）能作用于ThreadA中的DWPT对应生成的段以及其他已有的段中的文档，最后清空全局BufferedUpdates。\n  上面的描述同时解释了为什么不允许主动flush跟自动flush同时进入执行DWPT的doFlush()的流程图这个流程点，自动flush中的删除信息可能会无法作用于主动flush中所有的DWPT对应的段中的文档。\n 结语\n  下一个流程点将DWPT中收集的索引信息生成一个段newSegment会利用一篇文章的篇幅来介绍，同时也是填了之前的一个坑，即迟迟未写的两阶段生成索引文件之第一阶段系列的第二阶段，在这个阶段，会生成除了.fdx、.fdt和.tvx、.tvd之外其他所有的索引文件。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（五）","url":"/Lucene/Index/2019/0801/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E4%BA%94%EF%BC%89/","content":"  本文承接文档提交之flush（四），继续依次介绍每一个流程点。\n 文档提交之flush的整体流程图\n图1：\n\n 更新删除信息\n图2：\n\n  在执行更新删除信息的流程点之前，我们需要等待所有执行DWPT的doFlush()的线程执行完毕。\n  为什么会有多线程执行执行DWPT的doFlush()的流程：\n\n在文档提交之flush（二）中我们了解到，如果主动flush跟自动flush的DWPT是相同的类型（持有相同全局删除队列deleteQueue，deleteQueue的概念见文档的增删改（四）），那么会存在多线程执行执行DWPT的doFlush()的流程的情况\n\n  为什么需要等待所有执行DWPT的doFlush()的线程执行完毕：\n  可能会导致触发主动flush的线程已经执行完flush的工作，但其他线程中的DWPT还未生成一个段，无法保证线程A执行主动flush后应有的结果完整性。\n  为什么还要更新删除信息：\n\n先给出FlushTicket类：\n\nstatic final class FlushTicket &#123;    private final FrozenBufferedUpdates frozenUpdates;    private FlushedSegment segment;    ... ....&#125; \n\n在文档提交之flush（二）中我们了解到，在执行DWPT的doFlush()流程中需要生成一个全局的删除信息FrozenBufferedUpdates，它将作用（apply）到索引目录中已有的段，但是在执行DWPT的doFlush()的流程中，需要通过FrozenBufferedUpdates跟第一个生成FlushedSegment的DWPT作为一个FlushTicket（见文档提交之flush（四））来携带删除信息，如果此次的主动flush没有可用的DWPT可处理（即上次主动flush到这次主动flush之间没有添加/更新的操作），那么上次主动flush到这次主动flush之间的删除操作在执行DWPT的doFlush()的流程中无法生成对应的删除信息。所以在执行更新删除信息的目的就是为了处理 上次主动flush到这次主动flush之间只有删除操作的情况\n需要强调的是，在文档提交之flush（四）中的执行DWPT的doFlush()流程图中的有一个是否处理删除信息的流程点，在主动flush中，该流程点处理的是新的全局删除队列newQueue（见文档提交之flush（一））中的删除信息\n\n  在此流程点，删除信息同样被封装到一个FlushTicket中，跟之前文章中所有提及的FlushTicket不同的是，它其中的FlushedSegment对象是个null值，这个null很重要，在后面执行发布生成的段的流程中，根据FlushedSegment是否为null来区分两种不同作用的FlushTicket：\n\nFlushedSegment不为空：FlushTicket在发布生成的段的流程中需要执行将删除信息（如果有的话，见下文介绍）作用（apply）到其他段以及更新生成的段的任务\nFlushedSegment为空：FlushTicket在发布生成的段的流程中仅仅需要执行将删除信息（如果有的话，见下文介绍）作用到其他段的任务\n\n  我们结合文档提交之flush（四）中提到的生成出错的FlushTicket的情况，根据FlushedSegment跟FrozenBufferedUpdates不同可以归纳出在主动flush下FlushTicket的四种状态：\n\n状态一：FrozenBufferedUpdates != null &amp;&amp; FlushedSegment != null，FlushedSegment正确生成，FlushedSegment对应的DWPT是主动flush处理的第一个DWPT\n状态二：FrozenBufferedUpdates == null &amp;&amp; FlushedSegment != null，FlushedSegment正确生成，FlushedSegment对应的DWPT不是主动flush处理的第一个DWPT\n状态三：FrozenBufferedUpdates == null &amp;&amp; FlushedSegment == null，FlushedSegment未正确生成，FlushedSegment对应的DWPT不是主动flush处理的第一个DWPT\n状态四：FrozenBufferedUpdates !=  null &amp;&amp; FlushedSegment == null，这种还可细分为两种子状态\n\n子状态一：FlushedSegment未正确生成，FlushedSegment对应的DWPT不是主动flush处理的第一个DWPT\n子状态二：上文中在更新删除信息中的情况\n\n\n\n 强制发布生成的段\n图3：\n\n  发布生成的段分为强制跟尝试两种情况，其区别在文档提交之flush（四）已介绍，不赘述，在本篇文章中主要介绍发布生成的段的流程。\n  为什么在这个阶段要强制执行发布生成的段：\n\n原因在于当前是触发主动flush的线程，它必须保证完成主动flush的操作时，所有的DWPT已经生成对应的段，即保证线程A执行主动flush后应有的结果完整性。\n\n 发布生成的段的流程图\n图4：\n\n 强制、尝试发布生成的段\n图5：\n\n  图5中，是否强制执行的是与否分别对应了强制发布生成的段以及尝试发布生成的段\n\n强制发布生成的段：多线程执行发布生成的段，会等待获得锁\n尝试发布生成的段：多线程执行发布生成的段，如果锁已被占用，那么直接退出\n\n  为什么要区分强制跟尝试：\n\n在文档提交之flush（四）中已介绍，不赘述\n\n 取出允许发布（publish）的FlushTicket\n图6：\n\n  图6中，从队列中取出一个FlushTicket，队列即Queue&lt;FlushTicket&gt; queue（见文档提交之flush（四））\n  源码中判断FlushTicket是否能发布的条件如下：\nboolean canPublish() &#123;    return hasSegment == false || segment != null || failed;&#125;\n\nhasSegment：通过DWPT生成FlushTicket的过程中会将该值值为true\nsegment：FlushTicket中的FlushedSegment不为null\nfailed：FlushTicket未能正常生成也允许发布，因为该FlushTicket中的FrozenBufferedUpdates可能包含删除信息（为什么是可能包含，见文档提交之flush（二））\n\n 发布FrozenBufferedUpdates、FlushedSegment\n图7：\n\n  图7中FlushedSegment和FrozenBufferedUpdates同时为空的条件的条件即上文中FlushTicket的状态三。\n  由于篇幅原因，发布FrozenBufferedUpdates、FlushedSegment的流程将在下篇文章中展开介绍。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（八）","url":"/Lucene/Index/2019/0812/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E5%85%AB%EF%BC%89/","content":"  本文承接文档提交之flush（七），继续依次介绍每一个流程点，本篇文章是介绍文档提交之flush流程的最后一篇文章。\n 文档提交之flush的整体流程图\n图1：\n\n 更新ReaderPool\n图2：\n\n  在执行更新ReaderPool流程之前，我们需要等待所有发布生成的段中的处理删除信息事件执行结束。\n  为什么需要等待：\n\n在文档提交之flush（二）中我们了解到，如果自动flush处理的DWPT跟全局flush的DWPT类型是一致的（有相同的全局删除队列deleteQueue，见文档的增删改（四）），那么允许并发的执行执行DWPT的doFlush()。根据图1的流程图，即存在多线程执行流程点IndexWriter处理事件，自动fulsh的线程可能会从eventQueue中取出发布生成的段中的处理删除信息事件，那主动flush的线程只能等待所有的事件执行完成才能继续往下执行，否则会造成主动flush的不完整性，不完整性描述的是，调用主动flush的方法（IndexWriter.flush()）已经完成，但删除信息可能还没有完成\n\n  等待的逻辑是什么：\n\n根据前面的文章我们可以知道，到达此流程点，说明主动flush中的所有删除信息已经作为FrozenBufferedUpdates添加到update中，故只需要获得当前的update的状态，等待update容器中的元素为空（见上文中update的介绍），说明至少主动flush必须（must be）要处理的删除信息都已经完成了\n\n  为什么是至少主动flush必须（must be）要处理的删除信息都已经完成了，至少主动flush必须（must be）要处理的删除信息这句话需要拆分为两部分来介绍：\n\n主动flush必须（must be）要处理的删除信息：主动flush对应的全局删除队列中的删除信息\n至少：主动flush对应的全局删除队列中的删除信息肯定都能被处理，同时有可能newQueue中的删除信息会被处理，由于此时fullFlush已经被置为false，newQueue中的FrozenBufferedUpdates可能会被添加到update中\n\n  主动flush的线程如何获得当前update状态：\n\n先给出源码：\n\nSet&lt;FrozenBufferedUpdates&gt; waitFor;synchronized (this) &#123;    waitFor = new HashSet&lt;&gt;(updates);&#125;\n\n从上面的代码可以看出，通过new的方式仅获得线程运行到此处时的当前updates中已有的FrozenBufferedUpdates对象引用，描述为当前update状态（current status），updates的状态可能还会被其他线程更新，如果主动flush直接判断updates，那么可能因为newQueue一直生产新的FrozenBufferedUpdates被添加到update中，导致无法或等待长时间后才能退出\n\n 更新ReaderPool的流程图\n图3：\n\n  ReaderPool是什么：\n\nReaderPool描述了所有SegmentCommitInfo的信息，在本篇文章中我们只需要知道，ReaderPool类中包含了一个容器，其定义如下，其中ReaderAndUpdates的介绍见文档提交之flush（七）\n\nprivate final Map&lt;SegmentCommitInfo,ReadersAndUpdates&gt; readerMap = new HashMap&lt;&gt;();\n  故更新ReaderPool的过程就是更新每一个SegmentCommitInfo对应的ReadersAndUpdates的过程。\n 取出一个SegmentCommitInfo\n图4：\n\n  从IndexWriter的全局变量segmentInfos中依次取出每一个SegmentCommitInfo：\nprivate final SegmentInfos segmentInfos;\n 处理每一个SegmentCommitInfo\n图5：\n\n  在文档提交之flush（七）中我们了解到全局FrozenBufferedUpdates中的删除信息会作用到segmentInfos中的每一个SegmentCommitInfo中，同时段内FrozenBufferedUpdates中的删除信息会作用到本段的SegmentCommitInfo，即当作用（apply）了删除信息后，每一个段中的删除信息可能会发生变化。如果一个段中有新的被标记为删除的文档产生，那么被删除的文档会被记录到.liv索引文件中，如果该段已经存在.liv索引文件，那么先生成一个新的.liv索引文件，然后删除旧的.liv索引文件（通过索引文件计数引用来判断是否能删除该索引文件，见文档提交之flush（七））。\n  如果存在更改DocValues域的操作，那么需要更新DocValues域，这部分在后面介绍软删除的文章中会展开介绍。\n 处理需要被丢弃的段\n图6：\n\n  在上面的流程中，通过.liv索引文件确定了每一个段中的被删除的文档集合，如果一个段中所有文档都被标记为删除的，那么需要丢弃该段。\n  在执行完更新ReaderPool流程之后，Lucene还提供了一个钩子函数，用户可以根据具体业务来实现这个接口，这个接口同文档提交之flush（六）中的图3中执行flush后的工作流程是同一个钩子函数。\n 尝试段合并\n  每一次索引发生变化，都会尝试判断是否需要执行段的合并操作，其判断条件依据不同的合并策略而有所不同，合并策略的文章可以看这里：LogMergePolicy、TieredMergePolicy。\n 结语\n  至此，除了跟DocValues相关的知识点，我们通过八篇文章详细的介绍了执行了IndexWriter.flush()的所有流程。\n  在后面的文章中，我们会介绍软删除的内容，填补未展开的坑。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（六）","url":"/Lucene/Index/2019/0805/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E5%85%AD%EF%BC%89/","content":"  本文承接文档提交之flush（五），继续依次介绍每一个流程点。\n 文档提交之flush的整体流程图\n图1：\n\n 强制发布生成的段\n图2：\n\n  在文档提交之flush（五）中我们还剩余发布FlushedSegment跟发布FlushedSegment两个流程点未介绍。\n 发布FlushedSegment的流程图\n  图3的流程图描述的是一个FlushTicket执行发布FlushedSegment流程所涉及的处理过程。\n图3：\n\n 处理全局删除信息\n图4：\n\n  首先给出FlushTicket类，类中包含的主要变量如下：\nstatic final class FlushTicket &#123;    private final FrozenBufferedUpdates frozenUpdates;    private FlushedSegment segment;    ... ....&#125;\n  从前面的文章（见文档提交之flush（二））中我们了解到，不是每一个FlushTicket都有全局删除信息，即FrozenBufferedUpdates可能为null。如果不为空，我们首先要获得一个nextGen。\n  nextGen有什么作用：\n\nnextGen是一个从1开始递增的值，每一个FlushTicket中的FrozenBufferedUpdates都会同步获得一个唯一的nextGen，某个FrozenBufferedUpdates中的删除信息会作用（apply）到所有比它持有的nextGen小的段\n在文档提交之flush（二）中我们提到，我们生成FlushTicket后，将FlushTicket添加到Queue&lt;FlushTicket&gt; queue中是一个同步的过程，这是一个使得多个删除信息能正确的获得唯一的nextGen的方法之一\n\n  由于根据nextGen能保证正确的处理删除信息，所以真正的从别的段中处理删除信息的操作，即处理删除信息，就可以作为一个事件添加到eventQueue（见文档提交之flush（四）），使得可以多线程并发执行，在下篇文章中我们将会了解到，处理删除信息的过程是开销较大的工作。\n 处理段内的删除信息\n图5：\n\n  在文档提交之flush（四）中我们简单介绍了FlushTicket 中的FlushedSegment中包含的几个信息，至少包含了一个DWPT处理的文档对应的索引信息（SegmentCommitInfo）、段中被删除的文档信息（FixedBitSet对象）、未处理的删除信息FrozenBufferedUpdates（见文档提交之flush（三））、Sorter.DocMap对象，以上内容在文档提交之flush（三）的文章中已介绍。在当前流程点，FlushedSegment中的FrozenBufferedUpdates（非FlushTicket中的FrozenBufferedUpdates）中包含了段内的删除信息。\n  另外，在文档提交之flush（三）中我们介绍了，在生成索引文件的过程中，我们只处理了部分满足删除信息（见下文处理丢弃的段的介绍）的文档，到此流程点，我们需要处理剩余的删除信息。\n  无论当前FlushTicket中是否还有段内删除信息，当前段都需要获得一个nextGen，之后任何大于nextGen值的删除信息都需要作用到当前段。\n 将当前段的SegmentCommitInfo添加到SegmentInfos中\n图6：\n\n  将FlushedSegment中的SegmentCommitInfo，即索引信息添加到SegmentInfos中。SegmentInfos即所有段的索引信息的集合， 在commit( )阶段，SegmentInfos中的信息对应生成Segment_N的索引文件。\n 处理丢弃的段\n图7：\n\n  如果当前段中的文档的总个数maxDoc与被标记为删除的（deleted）文档的个数相同，那么该段需要被丢弃，判断条件如下：\ndelCount + softDelCount == maxDoc\n\ndelCount：该值描述的是满足删除信息TermArrayNode、TermNode（见文档的增删改（四））的文档的个数，在生成索引文件.tim、.tip、.doc、.pos、.pay的过程中会找到那些满足删除要求的文档号，随后将这些文档号添加到FixedBitSet（上文介绍了该对象的用途）对象中，随后FixedBitSet中的文档信息在写入到索引文件.liv过程中，将被删除的文档的个数统计到SegmentCommitInfo的delCount中\nsoftDelCount：该值描述的是包含软删除信息的文档，软删除的具体介绍会单独开一篇文章介绍，这里只要简单的知道，包含软删除信息的文档也是被标记被删除的（deleted）。在文档提交之flush（三）中介绍了在处理软删除文档的流程中，计算出了softDelCount的值，不赘述\nmaxDoc：该值描述了当前段中的文档总数，即DWPT收集的文档（见文档的增删改（二））的文档个数\n\n  从上文介绍中我们知道，当前段的SegmentCommitInfo已经被添加到SegmentInfos中，由于段的合并跟flush是异步操作，故运行到此流程点时，当前段可能正在执行段的合并。如果正在合并当前段，那么就不处理，在合并结束后，会自动丢弃该段，否则将此段对应SegmentCommitInfo从SegmentInfos中移除。\n 执行flush后的工作\n图8：\n\n  同图1中的执行flush前的工作的流程点,Lucene在此流程点预留了一个钩子函数（hook function），使用者可以实现在此实现自己的方法。\n 结束\n  至此，强制发布生成的段中的发布FlushedSegment流程介绍结束，另外发布FrozenBufferedUpdates的流程逻辑即上文中的处理全局删除信息。\n IndexWriter处理事件\n  在执行该流程之前，需要先执行下面的几个收尾工作，即执行源码中DocumentsWriterFlushControl.finishFullFlush( )的方法：\n\n从blockedFlushes（见文档的增删改（五））中将newQueue（见文档提交之flush（一））对应的DWPT添加到flushQueue（见文档的增删改（五））中，如果在主动flush期间，其他线程的添加/更新文档操作满足自动flush的要求，那么对应的DWPT会暂时被存放在blockedFlushes中，至于原因已在前面的文章中介绍，不赘述\nfullFlush（见文档提交之flush（二））置为false：该值置为false，表示此次主动flush已经执行结束，自动flush的DWPT（跟主动flush中的DWPT具有不同的全局删除队列deleteSlice，见文档提交之flush（二））可以开始执行图1中执行DWPT的doFlush()的流程，注意的是当前线程还未释放用来同步主动flush的fullFlushLock对象（见文档提交之flush（一））\n调用updateStallState：更新拖延状态，即调整当前索引写入的健康度，见文档提交之flush（一）中的详细介绍\n\n  上面的收尾工作结束后，接着还需要尝试处理newQueue中的删除信息。\n  为什么此时需要尝试处理newQueue中的删除信息：\n\n在文档提交之flush（四）中其实我们在图1的执行DWPT的doFlush()流程中已经处理过一次newQueue中的删除信息，条件是内存中的删除信息如果超过阈值的一半，那么需要处理删除信息，阈值即通过IndexWriterConfig setRAMBufferSizeMB设置允许缓存在内存的索引量（包括删除信息）的最大值，目的是为了防止产生过多的小段。\n而在此流程点，判断的条件是内存中的删除信息是否超过阈值，如果超过阈值并且此时不处理删除信息，那么其他线程的添加/更新文档的操作会被阻塞（见文档的增删改（三））\n\n  为什么此时能尝试处理newQueue中的删除信息：\n\n主动flush对应的FrozenBufferedUpdates已经获得了nextGen，即能保证正确的作用（apply）删除信息的顺序，故处理自动flush的删除信息是没有问题的\n\n  执行完收尾工作后，当前线程从eventQueue队列中逐个取出所有的事件，即执行事件对应的函数调用，关于事件的概念见文档提交之flush（四）。\n  总结前几篇文章中的内容，eventQueue中包含以下几种事件类型：\n\n删除文件事件：当使用复合索引文件时，我们需要删除非复合索引文件，见文档提交之flush（四）\nflush失败事件：未能正确生成FlushedSegment产生的事件，在后面的文章介绍IndexWriter的异常处理时会展开介绍\n处理删除信息事件：当段中的删除信息超过阈值时，需要生成该事件，见文档提交之flush（四）\n堆积（backlog）处理事件：flush（主动或者自动flush）的速度慢于添加/更新文档的操作时会发生堆积问题，那么生成一个事件来缓解堆积情况，见文档提交之flush（四）\n发布生成的段中的处理删除信息事件：即上文中的内容\n\n 结语\n  在下篇文章中，将会介绍图1中剩下的几个流程点以及发布生成的段中生成的处理删除信息事件的逻辑。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档提交之flush（四）","url":"/Lucene/Index/2019/0730/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  本文承接文档提交之flush（三），继续依次介绍每一个流程点。\n  先给出文档提交之flush的整体流程图：\n图1：\n\n图2是文档提交之flush中的执行DWPT的doFlush()的流程图，在前面的文章中，我们介绍到了此流程图中将DWPT中收集的索引信息生成一个段newSegment的流程点，在本篇文章中会将执行DWPT的doFlush()中剩余的流程点介绍完毕：\n图2：\n\n 预备知识\n FlushTicket\n  在文档提交之flush（二）中，我们简单的介绍了FlushTicket类，类中包含的主要变量如下：\nstatic final class FlushTicket &#123;    private final FrozenBufferedUpdates frozenUpdates;    private FlushedSegment segment;    ... ....&#125;\n  frozenUpdates是一个包含删除信息且作用于其他段中的文档的全局FrozenBufferedUpdate对象（见文档提交之flush（二）），而segment则是图2中的流程点将DWPT中收集的索引信息生成一个段newSegment执行结束后生成的FlushedSegment对象，它至少包含了一个DWPT处理的文档对应的索引信息（SegmentCommitInfo）、段中被删除的文档信息（FixedBitSet对象）、未处理的删除信息FrozenBufferedUpdates（见文档提交之flush（三））、Sorter.DocMap对象，以上内容在文档提交之flush（三）的文章中已介绍。\n  为什么FlushTicket中生成FrozenBufferedUpdates跟FlushedSegment是两个有先后关系的流程\n\n如果FrozenBufferedUpdates未能正确生成，那么FlushedSegment也不会生成，即全局的删除信息跟DWPT收集的文档就无法生成索引文件，但是如果FrozenBufferedUpdates正确生成（只有第一个DWPT会生成不为null的FrozenBufferedUpdates，见文档提交之flush（二）），而FlushedSegment没有生成，那删除信息还能正确的作用（apply）到索引目录中的所有段，即只丢失添加的文档的信息，不会丢失删除信息\n\n  如何处理DWPT未能正确的生成一个FlushedSegment对象的情况：\n\n在文档提交之flush（三）中我们了解到，如果DWPT在收集文档索引信息阶段（见两阶段生成索引文件之第一阶段），那么出错的文档的文档号会被标记在索引文件.liv中，而如果DWPT因某种原因（在介绍IndexWriter的异常处理时会展开）导致没有完成图2中的将DWPT中收集的索引信息生成一个段newSegment流程点，那么需要删除该DWPT中的对应的所有索引数据（如果已经生成的索引文件的话）\n\n Queue&lt;FlushTicket&gt; queue\n  该队列用来存放FlushTicket对象，每一个DWPT执行doFlush后，都会生成一个FlushTicket对象，并同步的添加到Queue&lt;FlushTicket&gt; queue中。\n Queue&lt;Event&gt; eventQueue\n  eventQueue队列用来存放事件（Event），Event类是一个添加了@FunctionalInterface注解的类，每一个Event对象用来描述一个函数调用，通过函数调用实现一个事件的执行。\n  在多线程下，每个线程同步的从eventQueue队列中取出一个事件，即执行该事件对应的函数调用。\n  在eventQueue队列中，事件间（Between Event）执行结束的先后顺序是无法保证的，不过可以根据事件内（Inner Event）的同步机制实现某些事件间的同步。\n  什么时候会添加事件到eventQueue：\n\n在文档提交之flush（三）中我们提到，如果通过IndexWriterConfig.setUseCompoundFile(boolean)设置了使用复合索引文件cfs&amp;&amp;cfe存储文档的索引信息，那么在生成完复合索引文件后，需要删除那些非复合索引文件，而删除操作就会作为一个事件会添加到eventQueue中\n上文中提到DWPT可能无法正确的生成一个FlushedSegment对象，那么需要删除该DWPT中的对应的所有索引数据（如果已经生成的索引文件的话），而删除操作就会作为一个事件会添加到eventQueue中\n从Queue&lt;FlushTicket&gt; queue中取出每一个FlushTicket去执行某个操作，该操作也会作为一个事件添加到eventQueue中，该操作会在后面的流程中介绍\n还有一些其他的需要添加到eventQueue会在后面的流程中提及\n\n  为什么要将事件添加到eventQueue中处理：\n\n在后面的介绍中我们将会了解到，文档提交之flush的整个流程中部分逻辑（包括异常）是可以并行执行的，如果将逻辑拆分成多个事件，那么可以充分利用多线程来并行的执行这些事件，其中一个作用可以使得主动flush的操作能优先、尽快完成，因为在文档提交之flush（一）中我们知道，如果flush的速度慢于添加/更新文档的操作，那么会阻塞添加/更新文档的操作。比如说在主动flush期间，如果此时其他线程触发了自动flush时，那么该线程执行完自动flush后，会去执行eventQueue中的事件（自动flush的有些操作也会作为事件添加到eventQueue中），基于队列FIFO的性质，如果此时队列中还有主动flush时添加的事件，那么就可以&quot;帮助&quot;主动flush先尽快完成\n如果事件之间（Between Event）需要同步，可以通过事件内部（Inner Event）的同步机制来实现，比如说当一个类中的两个方法作为两个事件时，可以通过对象锁synchronized实现同步，不同类中的两个方法可以使用ReentrantLock可重入锁实现同步。在上文中删除非复合索引文件跟从Queue&lt;FlushTicket&gt; queue中取出每一个FlushTicket去执行某个操作就属于可以并行的两个事件（其实两个方法中的部分代码块还是需要同步的）\n\n  什么时候会执行eventQueue中的事件：\n\n执行了添加事件到eventQueue中的操作的线程最终都会执行eventQueue中的事件，并且直到eventQueue中不存在事件才会退出\n\n 发布（publish）生成的段\n  发布生成的段的过程描述的是依次从Queue&lt;FlushTicket&gt; queue中取出FlushTicket，将其包含全局删除信息的FrozenBufferedUpdates对象作用到当前索引目录中已有的段的过程，同时还是对FlushedSegment对象进行最终处理的过程，比如找出未处理的删除信息（在文档提交之flush（三）中我们只找出了部分删除的文档）等一些操作，这里先简单的提一下，因为发布生成的段的逻辑篇幅较长，会在下一篇的文章中展开介绍。\n  发布生成的段还可以划分成两种类型，即强制发布生成的段和尝试发布生成的段，图1跟图2中均有该流程点：\n\n在多线程下，多个线程可能同时执行发布生成的段的逻辑，如果线程调用的是尝试发布生成的段，那么当发现有其他线程正在执行发布生成的段的操作，当前线程就不等待，继续执行后面的流程，否则等待其他线程执行结束，即等待Queue&lt;FlushTicket&gt; queue中的所有FlushTicket都被处理结束\n\n  源码中是如何实现的：\n\n通过tryLock()跟lock()分别实现尝试发布生成的段跟强制发布生成的段\n\n  为什么要划分强制发布生成的段和尝试发布生成的段：\n\n为了缓解堆积（backlog）问题。下文中会具体介绍\n\n 执行DWPT的doFlush( )\n  继续介绍执行DWPT的doFlush()中的剩余流程点。\n 添加删除文件事件到eventQueue\n图3：\n\n  该流程即将删除非复合索引文件的操作作为一个事件添到eventQueue中，如果此时有其他线程正在处理eventQueue中的事件，那么删除非复合索引文件的操作可能会被马上执行。\n 堆积（backlog）处理\n图4：\n\n  发生堆积情况即flush（主动或者自动flush）的速度慢于添加/更新文档的操作，判断是否堆积的条件如下：\nticketQueue.getTicketCount() &gt;= perThreadPool.getActiveThreadStateCount()\n  其中**ticketQueue.getTicketCount( )描述的是Queue&lt;FlushTicket&gt; queue中FlushTicket的个数，该个数即当前等待flush为一个段的段的个数（可能包含出错的FlushTicket，见下文介绍），而perThreadPool.getActiveThreadStateCount( )**描述的是线程池DWPTP中ThreadState的个数（见文档的增删改（二））\n  为什么通过上面的方式能判断是否发生堆积：\n\n上文中我们知道了FlushTicket的个数即DWPT的个数，又因为在文档的增删改（二）中，我们了解到，一个ThreadState中持有一个DWPT的引用之后，才去执行文档的添加/更新操作，当DWPT中收集的索引量满足自动flush的条件（见文档的增删改（五））后，DWPT进入图1中自动flush的流程点，开始生成一个段，最后释放DWPT对象，而ThreadState对象则是回到DWPTP中，故在单线程下，DWPT的个数总是小于等于DWPTP中ThreadState的个数。在多线程下，ThreadState对象回到DWPTP之后，又有新的线程执行添加/更新的操作，那么ThreadState会再次持有新的DWPT对象去执行任务，如果再次出发自动flush，当flush的速度（即DWPT生成一个段）较慢时，就会满足ticketQueue.getTicketCount() &gt;= perThreadPool.getActiveThreadStateCount()的条件，即发生了堆积\n上一条中，DWPT满足自动flush后进入生成一个段与ThreadState回到DWPTP体现了flush跟添加/更新 文件是并行操作，另外主动flush的情况也是一样的，见文档提交之flush（一）\n\n  上文中为什么说Queue&lt;FlushTicket&gt; queue中可能包含出错的FlushTicket：\n\n在文档提交之flush（二）中我们知道，FlushTicket在成功生成了FrozenBufferedUpdates对象frozenUpdates之后，FlushTicket就添加到了Queue&lt;FlushTicket&gt; queue中，此时的FlushTicket中还没有生成FlushedSegment对象segment，如果在后续流程中未能正确生成一个FlushedSegment对象，那么FlushTicket被认为是未能正确生成的，即出错的FlushTicket，当然了如果未能生成FrozenBufferedUpdates对象，FlushTicket就不会被添加到Queue&lt;FlushTicket&gt; queue中\n\n 添加强制发布生成的段的事件到eventQueue中\n  发生堆积后，通过该流程点使得所有执行flush的线程必须等待Queue&lt;FlushTicket&gt; queue中所有的FlushTicket处理结束后才能去执行新的添加/更新文档的任务来处理堆积问题。\n 执行完doFLush后的工作\n图5：\n\n  到此流程点，我们需要更新几个全局的信息，以下的内容在前面的文章中已经介绍，不详细展开：\n\nflushingWriters：见文档的增删改（五）\nflushBytes：见文档的增删改（三）\n执行updateStallState( )方法：见文档提交之flush（一）\n\n 尝试发布生成的段\n图6：\n\n  尝试发布生成的段的概念在上文关于发布（publish）生成的段的内容中已作介绍，不赘述，其详细的发布过程在下一篇文章会展开。\n\n主动flush：当flushQueue中的DWPT都处理结束，就可以执行下一个流程点尝试发布生成的段\n自动flush：除了满足图6中的条件，另外如果出现堆积问题，那么该线程就不再取新的DWPT执行，目的很明确，即缓解堆积问题\n\n 处理删除信息\n图7：\n\n  当前内存中的删除信息如果超过阈值的一半，那么需要处理删除信息，阈值即通过IndexWriterConfig setRAMBufferSizeMB设置允许缓存在内存的索引量（包括删除信息）的最大值，当超过该阈值，会触发自动flush（见文档提交之flush（一））\n  为什么内存中的删除信息如果超过阈值的一半，需要处理删除信息：\n\n触发自动flush的其中一个条件如下所示：\n\nactiveBytes + deleteBytesUsed &gt;= ramBufferSizeMB\n\n其中activeBytes描述的是当前内存中所有DWPT收集的索引总量，deleteBytesUsed描述的是当前内存中删除信息的总量，ramBufferSizeMB描述的是即允许缓存在内存的索引量的最大值。\n根据公式可以看出如果不处理删除信息，那么使得触发自动flush的频率更高，这样可能会产生很多的小段（Tiny Segment），即处理删除信息的目的\n\n  如何处理删除信息：\n\n如图7中所示，执行添加处理删除信息事件到eventQueue即可，处理删除信息事件实际是将强制发布生成的段作为一个事件添加到eventQueue中。\n\n  为什么通过强制发布生成的段能用来处理删除信息\n\n使得所有执行flush的线程必须等待Queue&lt;FlushTicket&gt; queue中所有的FlushTicket处理结束后才能去执行新的添加/更新文档、删除的任务，等待删除信息从内存写到磁盘（.liv索引文件），同时放慢触发自动flush的速度。\n\n 结语\n  至此我们介绍完了图1中逻辑相对最复杂的执行DWPT的doFlush()的流程。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["flush","commit"]},{"title":"文档的增删改（一）","url":"/Lucene/Index/2019/0626/%E6%96%87%E6%A1%A3%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  在Lucene中，可以对文档（Document）进行添加（增）、删除（删）、更新（改）的操作，而每一种操作各自又有多个功能的扩展。\n 文档的增删改应用\n 添加文档\n  该小结介绍增删改的简单使用方法。\n图1：\n\n图2：\n\n  Lucene允许通过IndexWriter对象添加一篇文档或者多篇文档。\n\n添加一篇文档：图1中，我们添加了一篇文档，该文档中包含三个域，分别是DocValues域&quot;author&quot;、存储域&quot;content&quot;、点数据域&quot;coordinate&quot;\n添加多篇文档：图2中，我们添加了两篇文档\n\n 删除文档\n图3：\n\n  删除文档的操作可以根据Term、Query进行删除：\n\n根据Term删除文档：图3的第99行，包含域名为&quot;content&quot;，域值为&quot;abc&quot;的文档都满足删除要求\n根据Query删除文档：图3的第101行，使用TermQuery进行查询，满足查询要求的文档都会被删除\n删除所有文档：删除所有的文档\n\n 更新文档\n图4：\n\n图5：\n\n  允许更新一篇或多篇文档，更新文档是一个先删除、后添加的过程。\n\n更新一篇文档：图4中，先删除所有包含域名为&quot;content&quot;，域值为&quot;a&quot;的文档，并且添加一篇新的文档，该文档包含一个存储域&quot;newField&quot;，域值为&quot;newFieldValue&quot;\n更新多篇文档：图5中，先删除所有包含域名为&quot;author&quot;，域值为&quot;Shakespeare&quot;的文档，并且添加两篇新的文档\n\n图6：\n\n  Lucene7.5.0版本中提供了三种更新文档的DocValues域的方法。\n\n更新NumericDocValueField：图6的124行，包含域名&quot;author&quot;，域值为&quot;ShakeSpare&quot;的所有文档的域名为“age”的NumericDocValueField的域值更新为23。注意的是，索引中必须存在域名为&quot;age&quot;的NumericDocValueField，在更新过程前，会通过一个全局的globalFieldNumberMap的Map对象检查是否存在该域名，不存在则抛出异常。\n更新BinaryDocValueField：图6的126行，包含域名&quot;subject&quot;，域值为&quot;Calculus&quot;的所有文档，这些文档的域名为“inventor”的BinaryDocValueField的域值更新为&quot;Leibniz&quot;。注意的是，索引中必须存在域名为&quot;inventor&quot;的BinaryDocValueField，在更新过程前，会通过一个全局的globalFieldNumberMap的Map对象检查是否存在该域名，不存在则抛出异常。\n批量更新DocValues域：批量的更新DocValues域，但是目前版本只能允许更新NumericDocValueField或BinaryDocValueField，跟单独更新DocValues域不同的是，该方法不会检查当前索引中是否存在待更新的域名，如果没有则直接添加，但如果待更新的域名已经存在并且对应的DocValues域的类型不是BinaryDocValueField或者NumericDocValueField，那么抛出异常\n\n图7：\n\n  软删除(softDelete)也属于文档的更新，支持添加一篇或多篇文档，图4及图5中的更新文档是 先删除、后添加的过程，而软删除则是先标记、后添加。\n  软删除的概念会在后面的文章中详细介绍。\n\n添加一篇文档：图7的140行，先标记所有包含域名为&quot;content&quot;，域值为&quot;a&quot;的文档，使得这些文档添加一个域名&quot;softDelete&quot;，域值&quot;1&quot;的NumericDocValuesField的域，然后添加一篇新的文档，该文档包含一个存储域&quot;author&quot;，域值为&quot;a&quot;\n添加多篇文档：图7中150行，先标记所有包含域名为&quot;content&quot;，域值为&quot;a&quot;的文档，使得这些文档添加一个域名&quot;softDelete&quot;，域值&quot;3&quot;的NumericDocValuesField的域，然后添加两篇新的文档\n\n 文档的增删改原理\n  上一节中介绍文档的增删改的方法即：\n  添加/更新一篇文档操作：\n\n添加一篇文档：addDocument( )\n更新一篇文档：updateDocument( )\n软删除中的添加一篇文档：softUpdateDocument( )\n\n  添加/更新多篇文档操作：\n\n添加多篇文档：addDocuments( )\n更新多篇文档：updateDocuments( )\n软删除中的添加多篇文档：softUpdateDocuments( )\n\n  删除文档操作：\n\n按照Term进行删除：deleteDocuments(Terms)\n按照Query进行删除：deleteDocuments(Querys)\n删除所有文档：deleteAll( )\n\n  更新DocValues域操作：\n\n更新BinaryDocValues：updateBinaryDocValue( )\n更新NumericDocValues：updateNumericDocValue( )\n更新多个DocValues：updateDocValues( )\n\n 文档的增删改流程图\n  单文档跟多文档的添加/更新操作的流程图略有不同：\n 单文档的增删改流程图\n图8：\n\n点击查看大图\n 多文档的增删改流程图\n图9：\n\n点击查看大图\n  从图8跟图9的流程图中可以看出，尽管有多种增删改的操作，但其相同的逻辑部分重合度是很高的，另外没有列出删除出所有文档的操作，即deleteAll( )。该操作暂时不作介绍，原因是一方面这个操作在实际业务中几乎不会使用，另一方该操作的流程完全不同于其他的增删改操作，展开介绍则需要另外开一章节，感觉没这个必要。\n 结语\n  本章节介绍了文档增删改的应用及其流程图，由于流程图的每一个步骤展开介绍会使得本篇文章篇幅过大，故在下一篇文章中介绍。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["document"]},{"title":"文档的增删改（三）","url":"/Lucene/Index/2019/0701/%E6%96%87%E6%A1%A3%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%EF%BC%88%E4%B8%89%EF%BC%89/","content":"  本文承接文档的增删改（一）、文档的增删改（二）、继续介绍文档的增删改，为了能深入理解，还是得先介绍下几个预备知识。\n 预备知识\n DocumentsWriterStallControl\n  在文档的增删改（二）我们知道，多线程（持有相同的IndexWriter对象的引用）执行添加/更新操作时，每个线程都会获取一个ThreadState，每次执行完一次添加/更新的后，如果持有的DWPT对象收集的索引信息没有达到flush的要求，该索引信息的大小会被累加到activeBytes，否则会被累加到flushBytes中，并且执行flush操作，这种方式即 添加/更新和flush为并行操作。\n  并行操作即某些ThreadState执行添加/更新，而其他ThreadState执行flush，故可能会存在 添加/更新的速度一直快于flush的情况，导致内存中堆积索引信息，那么很容 易出现OOM的错误。所以DocumentsWriterStallControl类就是用来通过阻塞添加/更新的操作来保证写入(indexing)的健康度(This class used to block incoming indexing threads if flushing significantly slower than indexing to ensure the healthiness)\n  满足下面的条件时需要阻塞添加/更新的操作：\n(activeBytes + flushBytes) &gt; limit &amp;&amp; activeBytes &lt; limit\n  其中limit的值为 2*ramBufferSizeMB，ramBufferSizeMB描述了索引信息被写入到磁盘前暂时缓存在内存中允许的最大使用内存值。\n  如何阻塞执行添加/更新操作的线程：\n\nwait(1000)：等待1秒的方法来实现阻塞，源码中的注释给出了使用这种方式的原因： Defensive, in case we have a concurrency bug that fails to .notify/All our thread, just wait for up to 1 second here, and let caller re-stall if it’s still needed\nstall：该值是一个对所有线程可见的变量，当stall的值为true，那么需要执行阻塞\n\n 文档的增删改流程图\n  在文档的增删改（一）的文章中，我们给出了文档的增删改流程图，这篇文章对该流程图的每一个流程点进行介绍，由于当文档跟多文档的增删改流程是雷同的，故下文中只介绍单文档的增删改流程图：\n图1：\n\n[点击](http://www.amazingkoala.com.cn/uploads/lucene/index/文档的增删改/文档的增删改（下）（part 1）/单文档操作.html)查看大图\n 添加/更新文档\n图2：\n\n  在文档的增删改（一）中我们知道，updateDocument( )、softUpdateDocument( )的操作分为删除跟添加，其中添加的逻辑跟addDocument( )是一致，故这三种方法使用相同的流程先处理文档的添加操作，而不同点在于处理删除的操作，下文会详细介绍。\n 处理文档前的工作\n图3：\n\n 处理文档前的工作的流程图\n图4：\n\n[点击](http://www.amazingkoala.com.cn/uploads/lucene/index/文档的增删改/文档的增删改（下）（part 1）/处理文档前的工作.html)查看大图\n  这个在源码中即调用DocumentWriter的preUpdate( )方法。在处理文当前，如果满足下面的两种条件之一，需要执行上图的流程：\n\nstall为true：该值即上文中提到的DocumentsWriterStallControl中的变量stall，如果该值为true，说明当前flush的速度赶不上添加/更新\ncheckPendingFlushOnUpdate为true &amp;&amp; flushQueue.size &gt; 0：checkPendingFlushOnUpdate默认值为true，可以通过LiveIndexWriterConfig自定义设置，它描述了是否需要在每一次的添加/更新操作时去检查flushQueue中是否有待flush的DWPT。flushQueue的概念以及在后面的介绍flush的文章中会展开介绍，这里只要知道flushQueue中存放了待执行doFlush操作的DWPT集合\n\n 是否执行flush？\n图5：\n\n  是否执行flush的条件即上文提到的两种条件。\n 处理flushQueue中的DWPT\n图6：\n\n  该流程点从flushQueue中依次取出每一个DWPT，依次执行以下两个操作：\n\n更新stall：重新计算(activeBytes + flushBytes) &gt; limit &amp;&amp; activeBytes &lt; limit，更新stall的值\n执行doFlush( )：将DWPT中收集的索引信息生成索引文件，doFlush( )的流程会在介绍flush时候详细展开\n\n 处理DWPTP中状态为flushPending的ThreadState\n图7：\n\n  该流程点从DWPTP（DocumentsWriterPerThreadPool）中取出每一个被标记为flushPending的ThreadState，将其持有的DWPT对象引用执行doFlush的操作。\n  为什么要处理DWPTP中状态为flushPending的ThreadState：\n\n这种情况主要考虑的是多线程的情况下，因为ThreadState被置为flushPing和被添加到flushQueue这两个操作不是一个事务操作，目的是尽量让所有达到flush条件的ThreadState中的DWPT执行doFlush( )操作\n\n 阻塞添加/更新操作\n图8：\n\n  由于上面执行了更新stall的操作，所以在该流程点，stall的值可能被置为了false，就不用阻塞等待，反之需要等待1秒。\n 再次判断判断flushQueue中是否又有了新的DWPT\n图9：\n\n  同样地，由于没有事务操作，当执行到该流程点可能又存在新的待flush的DWPT，这个流程点说明了当需要进行flush(满足flush的两个条件之一)时，Lucene会尽可能先将待flush的DWPT执行doFlush( )操作。\n 处理文档（Document）\n图10：\n\n  在该流程点，我们先获得一个ThreadState，然后开始处理文档的操作，获得ThreadState的流程在文档的增删改（二）已经介绍，不赘述。而处理文档的逻辑则在文章两阶段生成索引文件之第一阶段中已经介绍。在这篇文档中，详细的介绍了收集并生成.fdx、fdt、.tvd、tvm的过程，其他索引文件信息的收集流程会在随后介绍两阶段生成索引文件之第二阶段中详细展开，总之当处理文档（Document）的流程结束后，DWPT就收集到了所有的索引信息，基于这些信息生成索引文件。\n 结语\n  由于处理删除信息跟处理文档后的工作这两个流程点涉及的知识点较多，会造成本篇文章篇幅过长，故在挪到下篇文章中介绍\n[点击下载](http://www.amazingkoala.com.cn/attachment/Lucene/Index/文档的增删改/文档的增删改（下）（part 1）/文档的增删改（下）（part 1）.zip)附件\n","categories":["Lucene","Index"],"tags":["document"]},{"title":"文档的增删改（二）","url":"/Lucene/Index/2019/0628/%E6%96%87%E6%A1%A3%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%EF%BC%88%E4%BA%8C%EF%BC%89/","content":" 文档的增删改（中）\n  在文档的增删改（一）中，我们介绍了应用示例并给出了流程图，本篇文章承接上文，就流程图的每个流程点展开介绍。\n 预备知识\n  在介绍流程点前需要了解以下几个知识点，下文使用到的名称都是源码中的类名。\n DocumentsWriterPerThread\n  DocumentsWriterPerThread在处理完文档后，会收集到以下的数据，注意的这只是部分数据，并且是跟本篇文章相关的，而其他数据以及DocumentsWriterPerThread提供的其他方法(功能)则会在后面介绍flush的文章中进一步展开：\n\nnumDocs：DocumentsWriterPerThread处理的文档个数，上文中我们知道，IndexWriter可以一次添加一篇或多篇文档，而这一次的添加操作实际由DocumentsWriterPerThread完成，用numDocs来记录处理的文档个数\nIndexByteUsed（索引信息总量）：DocumentsWriterPerThread处理的文档转化为索引文件后占用的内存大小，这里的IndexByteUsed至少包含了生成索引文件需要的内存大小\n\n  DocumentsWriterPerThread在下文中简称DWPT。\n DocumentsWriterPerThreadPool、ThreadState\n  DocumentsWriterPerThreadPool是一个逻辑上的线程池，它实现了类似Java线程池的功能，在Java的线程池中，新来的一个任务可以从ExecutorService中获得一个线程去处理该任务，而在DocumentsWriterPerThreadPool中，每当IndexWriter要添加文档，会从DocumentsWriterPerThreadPool中获得一个ThreadState去执行，故在多线程（持有相同的IndexWriter对象引用）执行添加文档操作时，每个线程都会获得一个ThreadState对象。\n  每一个ThreadState对象中都持有一个DWPT的引用，所以正如上文中所述，实际的添加文档操作还是DWPT。\n  当ThreadState执行完添加文档的任务后，它会回到DocumentsWriterPerThreadPool中，等待下次的文档添加操作，通过一个名为freeList的链表来存储。\nprivate final List&lt;ThreadState&gt; freeList = new ArrayList&lt;&gt;();\n  ThreadState在两种情况下不持有一个DWPT的引用：\n\n情况一：当一个新的添加文档任务来时，DocumentsWriterPerThreadPool中没有可用的ThreadState对象，那么会生成一个新的ThreadState对象，此时新生成的ThreadState对象没有DWPT的引用(个人理解：从源码结构上看ThreadState的构造函数所在的类DocumentsWriterPerThreadPool没有可用的生成一个DWPT对象所需要的参数)\n情况二：上文中提到DWPT在执行完添加文档操作后，会收集numDocs跟IndexByteUsed的值，其中IndexByteUsed的值会被累加到一个全局的变量activeBytes（线程共享）中，另外还有一个全局变量deleteRamByteUsed，它描述了被删除文档的信息占用的内存大小（在后面介绍flush的文章中会展开），如果activeBytes与deleteRamByteUsed的和值，以及numDocs 分别超过下面两个变量，那么持有DWPT的ThreadState会被标记为flushPending状态，并且失去该DWPT的引用，随后DWPT执行doFlush操作，将收集到的索引信息生成索引文件：\n\nramBufferSizeMB：该值描述了索引信息被写入到磁盘前暂时缓存在内存中允许的最大使用内存值\nmaxBufferedDocs：该值描述了索引信息被写入到磁盘前暂时缓存在内存中允许的文档最大数量，这里注意的是这里指的是一个DWPT允许添加的最大文档数量，在多线程下，可以同时存在多个DWPT，而maxBufferedDocs并不是所有线程的DWPT中添加的文档数量和值\n\n\n\n  用户可以通过LiveIndexWriterConfig对象自定义配置ramBufferSizeMB跟maxBufferedDocs的值，这两个变量的概念在后面介绍flush的文章中会详细展开。\n 获取ThreadState的流程图\n图1：\n\n点击查看大图\n 从DocumentsWriterPerThreadPool(DWPTP)中获取一个ThreadState\n图2：\n\n  当IndexWriter执行添加文档操作，比如说IndexWriter.addDocument()的操作，那么进入开始的流程点，由于可以多线程(持有相同的IndexWriter对象引用)添加文档，故使用synchronized(IndexWriter对象)关键字从DWPTP中获取ThreadState。\n  如果DWPTP中没有ThreadState对象，那么直接生成一个新的ThreadState，如果存在，那么从freeList中的链表尾部取出一个ThreadState，因为当一个ThreadState完成添加文档的任务后，会重新回到DWPTP，即添加到freeList中，所以从链表尾部获取一个ThreadState即取出最近完成添加文档任务的ThreadState，即LIFO（Last In First Out）。\n 优先取出持有DWPT对象引用的ThreadState\n图3：\n\n  从freeList的链表尾部获得一个ThreadState对象后，我们这里称之为threadState1，如果threadState1中不持有DWPT对象的引用，那么需要从freeList的首部开始遍历每一个ThreadState，找到第一个持有DWPT对象引用的ThreadState对象，并且将threadState1重新添加到freeList中，当然也有可能链表中所有的ThreadState都不持有DWPT对象的引用。\n  为什么要优先取出持有DWPT对象引用的ThreadState：\n\n前面提到当一个ThreadState完成添加文档的任务后，如果其引用的DWPT对象收集的numDocs、全局变量activeBytes分别达不到maxBufferedDocs、ramBufferSizeMB，即达不到flush的要求，那么ThreadState会重新回到DWPTP中，如果新的添加文档任务到来，引用一个新的DWPT对象，特别是文档（Document）大小较小（更难达到ramBufferSizeMB）且数量较多（每个DWPT中收集的numDocs达到maxBufferedDocs才会flush）的时候，那么内存中会相对堆积更多的索引数据（更多的持有DWPT对象引用的ThreadState对象），如果此时添加文档的频率降低了，那么更会加剧堆积的问题，会导致不可确定的长期占用内存(leave docs indefinitely buffered，tying up RAM)的情况。所以优先取出持有DWPT对象引用的ThreadState可以使得该ThreadState中的DWPT收集的numDocs、全局变量activeBytes尽快达到flush的要求，释放内存，并且生成一个较大的段，BTW：段越大，commit的时候性能更高\n\n  为什么可能会取到不持有DWPT对象引用的ThreadState：\n\n当一个ThreadState完成添加文档的任务后，其引用的DWPT对象收集的numDocs或者全局变量activeBytes达到flush的要求，ThreadState会被标记为flushPending为true状态，那么失去该DWPT的引用，DWPT执行doFlush操作（该操作目前只需要知道它会将内存索引信息转化为生成索引文件，减少内存，在后面介绍flush的文章中会详细介绍），将收集到的索引信息生成索引文件，该ThreadState重新添加到freeList中，并且重新标记flushPending为false\n\n 全局flush被触发\n图4：\n\n  当前的ThreadState持有DWPT对象引用，说明在上一步的流程中，从freeList中取到了一个刚刚完成文档添加任务的ThreadState，如果此时全局flush被触发，那么ThreadState会失去该DWPT的引用，而该DWPT被加入到flush队列，其包含的索引信息等待被写入到磁盘，并且ThreadState被重置（恢复到刚生成ThreadState对象的状态）\n  这里简单提下常见的几个会触发全局flush的场景，其他场景以及具体的逻辑会在后面介绍flush的文章时展开：\n\n调用IndexWriter.commit( )方法\n调用IndexWriter.flush( )方法\n调用IndexWriter.openIfChanged(DirectoryReader )方法，即使用NRT（near real-time）搜索功能。\n\n 让ThreadState持有一个DWPT对象的引用\n图5：\n\n  从上面的流程可以看出，到了这个流程点，从freeList中获取的ThreadState不一定持有DWPT对象的引用，故需要提供一个新的DWPT对象。当然新生成的hreadState在这个流程点也需要持有一个新的DWPT对象。\n 结束\n图6：\n\n  至此，获取ThreadState的流程已经结束，并且ThreadState必定是持有DWPT对象的引用。\n FlushPolicy\n  FlushPolicy即flush策略，准确的说应该称为 自动flush策略，因为flush分为自动flush跟主动flush，即显示的调用IndexWriter.flush( )方法，flushPolicy描述了IndexWriter执行了增删改的操作后，将修改后的索引信息写入磁盘的时机。\n  Lucene7.5.0版本中，有且仅有一个flushPolicy：FlushByRamOrCountsPolicy\n FlushByRamOrCountsPolicy\n  FlushByRamOrCountsPolicy还定义了IndexWriter执行完增删改的操作后的后续工作：\n\n添加操作：先判断numDocs是否maxBufferedDocs，再判断activeBytes与deleteRamByteUsed的和值是否达到ramBufferSizeMB\n删除操作：判断deleteRamByteUsed是否达到ramBufferSizeMB\n更新操作：更新操作实际是执行删除跟添加的操作\n\n DocumentsWriterFlushControl\n  DocumentsWriterFlushControl类中定义了ThreadState在添加/更新文档过程中的各种行为，上文中提及的例如ThreadState失去持有DWPT的行为、activeBytes、flush队列等等都是在该类中定义，下面列出几个跟本篇文章内容相关的DocumentsWriterFlushControl类中的方法、变量：\n\nactiveBytes（long类型）：多线程（持有相同的IndexWriter对象的引用）执行添加/更新操作时，每一个DWPT收集到的IndexByteUsed都会被累加到activeBytes中\nflushBytes（long类型）：待写入到磁盘的索引数据量，如果全局的flush被触发，即使某个ThreadState中的DWPT达不到flush的要求，DWPT中的索引信息也会被累加到flushBytes中(没有触发全局flush的话，则是被累加到activeBytes中)\nnumPending（int类型）：描述了被标记为flushPending的ThreadState的个数\nfullFlush（boolean 类型）：全局flush是否被触发的标志\nflushQueue（Queue类型）：存放DWPT的队列，即flush队列，在此队列中的DWPT等待执行doFlush操作\nflushingWriters（Map类型）:该Map的key为DWPT，value为DWPT收集的索引信息的大小，当一个ThreadState被标记为flushPending，那么它持有的DWPT对象收集到的索引信息的大小会被添加到当flushingWriters中，同样地一个DWPT执行完doFlush，那么该DWPT对应的索引大小就可以从flushBytes扣除，故它用来维护flushBytes的值\ncommitPerThreadBytes( ) (方法)：该方法描述了刚刚完成添加/更新的DWPT收集到的索引信息应该被添加到activeBytes还是flushBytes中，取决于ThreadState的flushPending状态\nsetFlushPending( )(方法)：该方法用来设置一个ThreadState为flushPending状态\n\n 结语\n  要完全的深入了解文档增删改的过程，这些预备知识必须都先了解，果不其然，篇幅又超了，所以文档增删改的流程图的介绍又得拖到下一篇文章啦。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["document"]},{"title":"文档的增删改（五）","url":"/Lucene/Index/2019/0709/%E6%96%87%E6%A1%A3%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%EF%BC%88%E4%BA%94%EF%BC%89/","content":"  本文承接文档的增删改（一）、文档的增删改（二）、文档的增删改（三）、文档的增删改（四）继续介绍文档的增删改，是文档的增删改系列的最后一篇文章，另外下文中如果出现未展开介绍的变量名，说明在先前的文章中已经给出介绍，不赘述。\n 预备知识\n 三个重要的容器对象\n  以下介绍的几个容器对理解 添加/更新 跟 flush是并行操作、索引占用内存大小控制起关键的作用。\n fullFlushBuffer\n  定义如下：\nprivate final List&lt;DocumentsWriterPerThread&gt; fullFlushBuffer = new ArrayList&lt;&gt;();\n  当全局flush触发时，意味着该flush之前所有添加的文档都在该flush的作用范围内，那么需要将DWPTP中所有持有DWPT对象引用，并且DWPT的私有变量numDocsInRAM 需要大于 0（numDocsInRAM 的概念在文档的增删改（四）已介绍），即该DWPT处理过文档的所有ThreadState置为flushPending，他们所持有的DWPT随后将其收集到的索引信息各自生成一个段，在此之前DWPT先存放到fullFlushBuffer链表中。\n blockedFlushes\n  定义如下：\nprivate final Queue&lt;BlockedFlush&gt; blockedFlushes = new LinkedList&lt;&gt;();\n  在介绍blockedFlushes前，先补充一个ThreadState的知识点，ThreadState类实际是继承了可重入锁ReentrantLock类的子类：\nfinal static class ThreadState extends ReentrantLock &#123;    ... ...&#125;\n  在文档的增删改（二）的文章中我们知道，一个线程总是从DWPTP中获得一个ThreadState，然后执行添加/更新文档的操作，当成功获得ThreadState，该线程就获得了ThreadState的锁，直到处理完文档后才会释放锁。如果在处理文档期间（未释放锁），有其他线程触发了全局的flush，并且ThreadState中持有的DWPT对象达到了flush的条件（见文档的增删改（二）中介绍ThreadState失去DWPT引用的章节），那么该DWPT会被添加到blockedFlushes中，并且在blockedFlushes中的DWPT优先fullFlushBuffer中的所有DWPT去执行doFlush( )(每个DWPT中收集的索引信息生成索引文件，或者说生成一个段)。\n  为什么blockedFlushes中的DWPT优先执行doFlush( )，这个问题可以拆分两个小问题：\n\n为什么要优先执行doFlush( )：该ThreadState被置为flushingPending状态，其中一种情况是因为某个DWPT添加/更新文档数量达到阈值maxBufferedDocs（用户设定通过indexWriterConfig设定文档个数作为flush的条件），由于仅仅是通过文档个数来控制flush，而不考虑这些文档对应的索引信息总量，所以可能会出现及时文档个数很小，但是占用的内存很大的情况，无法估计实际的内存占用，为了安全起见，所以该DWPT必须优先flush，正如源码中的注释说道：only for safety reasons if a DWPT is close to the RAM limit，而如果通过indexWriterConfig设定通过ramBufferSizeMB作为flush的条件时，由于能掌握索引占用的内存量，无需通过这种方式来优先执行doFlush( )\n为什么使用一个额外的队列blockedFlushes存放该DWPT：为了能保证blockedFlushes中的DWPT能优先添加到flushQueue中，这里先简单的提一句，具体的原因在介绍flush时候会展开，这里留个坑，称为一号坑\n\n  blockedFlushes除了上述描述的作用外，在另一种情况下的也需要暂时存放DWPT，但并不是因为安全问题，这里先不展开，在介绍flush时会解释，同样地留个坑，称为二号坑。\n flushingWriters\n  定义如下：\nprivate final IdentityHashMap&lt;DocumentsWriterPerThread, Long&gt; flushingWriters = new IdentityHashMap&lt;&gt;();\n  flushingWriters容器的key为DWPT的引用，value为DWPT收集的索引信息占用的内存大小，flushingWriters中元素的个数描述了当前待flush的DWPT的个数，所有元素的value的和值描述了当前内存中的flushBytes（flushBytes的概念在文档的增删改（三）已经介绍）。\n flushQueue\n  flushQueue的定义如下：\nprivate final Queue&lt;DocumentsWriterPerThread&gt; flushQueue = new LinkedList&lt;&gt;();\n  flushQueue中存放了待执行doFlush( )的DWPT集合。\n fullFlushBuffer、blockedFlushes、flushingWriters、flushQueue之间的关联\n  当全局flush触发，fullFlushBuffer跟blockedFlushes中DWPT都会被添加进flushQueue，触发全局flush的线程总是只从flushQueue中依次取出每一个DWPT，当执行完doFlush( )的操作后，将该DWPT占用的内存大小从flushingWriters中移除，这里只是简单的概述下，详细的过程在介绍flush时展开。\n flushDeletes\n  flushDeletes的定义如下：\nfinal AtomicBoolean flushDeletes = new AtomicBoolean(false);\n  每当将删除信息添加到DeleteQueue后，如果DeleteQueue中的删除信息使用的内存量超过ramBufferSizeMB，flushDeletes会被置为true。\n 文档的增删改流程图\n  我们继续介绍最后的 删除文档的处理删除信息与添加/更新文档的处理文档后的工作的流程点，添加/更新文档的处理删除信息的内容已经在文档的增删改（四）介绍了。\n图1：\n\n 删除文档之处理删除信息\n  删除文档即deleteDocuments(Querys)、deleteDocuments(Terms)，不同于添加/更新文档，删除文档没有DWPT的概念，即没有私有DeleteSlice的概念。\n图2：\n\n 添加删除信息到DeleteQueue中\n图3：\n\n  在文档的增删改（四）中我们已经介绍了如何将一个删除信息添加到DeleteQueue中，故这里不赘述。\n 执行flush策略，设置flushDeletes\n图4：\n\n  如果更新文档、删除文档的操作较多，堆积的删除信息占用的内存也不能被忽视，所以在添加到DeleteQueue后，需要判断DeleteQueue中的删除信息大小是否超过ramBufferSizeMB（ramBufferSizeMB描述了索引信息被写入到磁盘前暂时缓存在内存中允许的最大使用内存值，可以通过LiveIndexWriterConfig自定义配置），如果超过那么设置flushDeletes为True。\n 记录待处理的删除信息\n图5：\n\n  如果此时其他线程已经触发了全局flush，那么该线程会处理删除信息，故当前线程就不用重复的处理删除信息了，否则将删除信息添加到待处理队列中，生成一个事件，之后IndexWriter会处理该事件，待处理队列跟事件的概念在介绍flush时候会展开。\n 添加/更新之处理文档后的工作\n图6：\n\n[点击](http://www.amazingkoala.com.cn/uploads/lucene/index/文档的增删改/文档的增删改（下）（part 3）/添加_更新之处理文档后的工作.html)查看大图\n 统计处理文档占用的内存使用量\n图7：\n\n  DWPT处理完一篇文档后，需要将这篇文档对应的索引信息占用的内存添加到全局的flushBytes或activeBytes，如果ThreadState被置为flushPending了，那么添加到flushBytes，否则添加到activeBytes。\n  为什么在这个流程点，ThreadState可能是flushPending的状态：\n这种情况只发生在索引占用的内存(包括删除信息)达到ramBufferSizeMB的情况，如果线程1中的ThreadState处理完一篇文档后，索引占用的内存超过阈值，那么会从DWPTP中找到一个持有DWPT，并且该DWPT收集的索引信息量最大的ThreadState，将其置为flushPending，如果此时正好线程2开始执行添加/更新的操作，那么可能会从DWPTP中取出该ThreadState。\n 执行flush策略\n图8：\n\n  如果当前的ThreadState没有被置为flushPending(内存索引未达到ramBufferSizeMB前ThreadState都是flushPending为false的状态)，那么需要执行flush策略，目前Lucene7.5.0中有且仅有一种flush策略：FlushByRamOrCountsPolicy。用户可以实现自己的flush策略并通过IndexWriterConfig自定义设置。\n  flush策略用来判断执行完增删改的操作后，是否要执行flush，这里的介绍是对文档的增删改（二）的补充：\n\n添加操作：如果设置了flushOnDocCount，那么判断DWPT处理的文档个数是否达到maxBufferedDocs，满足条件则将ThreadState置为flushPending，否则判断activeBytes与deleteRamByteUsed的和值是否达到ramBufferSizeMB，如果达到阈值，那么从DWPTP中找到一个持有DWPT，并且该DWPT收集的索引信息量最大的ThreadState，将其置为flushPending。\n删除操作：判断deleteRamByteUsed是否达到ramBufferSizeMB，满足条件则另flushDeletes为true\n更新操作：更新操作实际是执行删除跟添加的操作\n\n  另外每一个DWPT允许收集的索引信息量最大值为perThreadHardLimitMB （默认值为1945MB），当达到该阈值时，ThreadState会被置为flushPending，同样的可以通过IndexWriterConfig自定义设置，所以即使ramBufferSizeMB设置的很大时，也有可能因为达到了perThreadHardLimitMB而自动触发flush。\n 将当前ThreadState持有的DWPT添加到blockedFlushes中\n图9：\n\n  此时其他线程触发了全局flush，那么当前ThreadState如果没有被置为flushPending，说明其持有的DWPT没有达到flush的条件，ThreadState在随后释放锁后，其DWPT会被其他线程添加到fullFlushBuffer中，等待执行doFlush( )，否则该DWPT被添加到blockedFlushes中，然后从flushQueue中取出一个DWPT，目的就是尽可能先让flushQueue中的DWPT执行doFlush( )，只有这样，blockedFlushes中的DWPT才能随着尽快执行doFlush( )。\n 取出当前ThreadState持有的DWPT\n图10：\n\n  全局flush未被触发，如果当前ThreadState被置为flushPending，那么取出其持有的DWPT，在随后执行doFlush( )操作。\n 处理删除信息\n图11：\n\n  添加/更新操在处理完文档后，需要跟删除操作之处理删除信息一样，因为更新操作、其他线程的删除操作都会改变DeleteQueue。这一段逻辑在上文中已介绍，不赘述。\n DWPT为空的情况\n图12：\n\n  这里的描述不太好，在上面的流程中DWPT收集的索引信息未达到flush要求，故不需要处理该DWPT，故描述为DWPT为空，下图中的红线是DWPT为空的流程线。\n图13：\n\n  如果DWPT为空，那么我们这个线程需要&quot;帮助&quot;(help out)那些已经可以生成一个段的DWPT执行doFlush( )。帮助的方式是先尝试从flushQueue中找到一个DWPT，如果没有，那么从DWPTP中找，前提是其他线程没有触发全局flush，因为在全局flush的情况下，执行全局flush的线程会去DWPTP中找到所有已经收集过文档的DWPT，无论DWPT对应的ThreadState是否被置为flushPending，所以如果此时能从DWPTP中得到一个被置为flushPending的ThreadState，说明在其他线程中的ThreadState持有的DWPT刚刚执行完添加文档的任务，就被当前线程”抢夺“了DWPT，让DWPT提前执行了doFlush()。在DocumentsWriterFlushControl.java的tryCheckoutForFlush()方法抢夺了&quot;DWPT&quot;。方法定义如下：\nsynchronized DocumentsWriterPerThread tryCheckoutForFlush(    ThreadState perThread) &#123;    return perThread.flushPending ? internalTryCheckOutForFlush(perThread) : null;&#125;\n DWPT不为空的情况\n图14：\n\n  DWPT不为空也就是下图中的两种情况，那么随后执行DWPT的doFlush( )：\n图15：\n\n 完整的单文档操作流程图\n结合前几篇文章，单文档操作的流程图如下：\n图16：\n\n[点击](http://www.amazingkoala.com.cn/uploads/lucene/index/文档的增删改/文档的增删改（下）（part 3）/单文档操作全图.html)查看大图\n 结语\n  文档的增删改系列正式介绍完毕，如果你准备深入源码理解，那么必须结合全局flush的源码一起看，否则有些流程点很难理解，比如说上文中提高的三个容器，还有为什么ThreadState设计成继承可重入锁的类等等。\n  在下一篇介绍flush的文章中，当前系列的内容还会被反复提及，所以关心flush流程的朋友，需要先了解文档的增删改的流程。\n[点击下载](http://www.amazingkoala.com.cn/attachment/Lucene/Index/文档的增删改/文档的增删改（下）（part 3）/文档的增删改（下）（part 3）.zip)附件\n","categories":["Lucene","Index"],"tags":["document"]},{"title":"文档的增删改（四）","url":"/Lucene/Index/2019/0704/%E6%96%87%E6%A1%A3%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  本文承接文档的增删改（一）、文档的增删改（二）、文档的增删改（三）继续介绍文档的增删改，为了能深入理解，还是得先介绍下几个预备知识。\n 预备知识\n Node类\n  下面是Node类中仅有的两个成员变量：\nstatic class Node&lt;T&gt; &#123;    volatile Node&lt;?&gt; next;    final T item;&#125;\n  多个Node对象通过next实现了队列结构，其中item为队列中某个结点(Node)的删除信息，每当DWPT处理一个删除信息，就会将该删除信息作为一个item加入到队列中，即deleteQueue。\n  在文档的增删改（一）我们已经介绍了删除的几种方式，其删除信息会生成不同的Node子类：\n图1：\n\n\nQueryArrayNode：deleteDocuments(Querys)\nTermArrayNode：deleteDocuments(Terms)\nDocValuesUpdatesNode：updateBinaryDocValue( )、updateNumericDocValue( )、updateDocValues( )、softUpdateDocument( )、softUpdateDocuments( )\nTermNode：updateDocument(Term, Document)、updateDocuments( Term, Documents)\n\n deleteQueue\n  在多线程（持有相同IndexWriter对象的引用）执行删除操作时，多个DWPT将自己携带的删除信息生成Node对象，并添加到deleteQueue中，deleteQueue是一个全局的变量：\n图2：\n\n  为什么上图的队列有 &quot;多线程&quot;的前提：因为单线程的话每当添加一个Node到队列中，该Node的信息会马上被添加到BufferedUpdates（下文中会介绍）中，故在单线程下，不会出现结点数量大于1个的队列。\n tail\n  该变量的定义如下：\nprivate volatile Node&lt;?&gt; tail;\n  tail用来指向deleteQueue中最后一个Node结点，也就是最新的一个删除操作(latest delete operation)。\n DeleteSlice类\n  下面是DeleteSlice仅有的两个成员变量：\nstatic class DeleteSlice &#123;    Node&lt;?&gt; sliceHead;    Node&lt;?&gt; sliceTail;&#125;\n  DeleteSlice中的sliceHead、sliceTail指向deleteQueue中的Node结点。\n  在文档的增删改（二）中我们知道，每一个执行添加/更新操作的线程会先从DWPTP获得一个ThreadState，如果ThreadState中没有持有DWPT的对象引用，那么需要生成一个新的DWPT对象让其持有，并且每一个DWPT对象中都拥有一个私有的DeleteSlice对象，并且在初始化DeleteSlice对象，会让DeleteSlice对象的sliceHead、sliceTail同时指向tail指向的deleteQueue对象中的Node结点，即最新的一个删除操作，DeleteSlice类的构造函数如下，其中参数currentTail为tail：\nDeleteSlice(Node&lt;?&gt; currentTail) &#123;    // 另sliceHead、sliceTail与currentTail有相同的对象引用    sliceHead = sliceTail = currentTail;&#125;\n  为什么sliceHead、sliceTail指向最后一个删除操作：因为最后一个删除操作只能对该删除操作执行前的添加的所有文档（满足删除要求的）进行删除，并且每一个DWPT中的索引信息最终会flush为一个段(在后面介绍flush时会详细介绍)，而DWPT的私有DeleteSlice对象记录了DWPT添加的第一篇文档（最后一个删除操作不能作用于该文档及后面的所有文档）到生成一个段这段时间内所有的删除操作，该操作包括自己跟其他线程的删除操作，这些删除信息在flush时需要作用于(apply)该DWPT对应的段中的文档，即删除段中满足删除要求的所有文档。\n  上面的解释有点绕。。。自己看着都有些变扭。\n  所以简单的表述就是sliceHead、sliceTail必须指向最后一个删除操作的目的就是不让该操作及之前的删除操作添加到DWPT私有的DeleteSlice对象中，因为这些操作不能作用于后生成的新的文档。\n 例子1\n  有两个线程ThreadA、ThreadB，他们分别调用了updateDocument(Term, Document)和deleteDocuments(Querys)，即分别会生成TermNode和QueryArrayNode。我们这里假设两个线程从DWPTP中获得的ThreadState对象持有的DWPT引用都是新生成的，并且当前DeleteQueue为空，并且假设（实际添加到deleteQueue顺序是不可确定的，下文会介绍）ThreadA先往deleteQueue中添加。\n ThreadA添加结点\n\n步骤一：获得一个ThreadState，新生成一个DWPT对象，初始化私有的DeleteSlice，由于deleteQueue未添加任何的删除结点，所以此时的deleteQueue的状态如下图, 其中红圈 tail、sliceHead (ThreadA)、sliceTail (ThreadA) 三个具有相同的对象引用：\n\n图3：\n\n\n步骤二：添加TermNode结点到deleteQueue中，sliceTail (ThreadA) 跟tail 的对象引用更新为TermNode对象\n\n图4：\n\n ThreadB添加结点\n\n获得一个ThreadState，新生成一个DWPT对象，初始化私有的DeleteSlice，即让sliceHead(ThreadB)、sliceTail(ThreadB)跟tail指向相同的对象。\n\n图5：\n\n\n添加QueryArrayNode结点到deleteQueue中,sliceTail (ThreadB) 跟tail 的对象引用更新为QueryArrayNode的对象\n\n图6：\n\n  图6中，deleteQueue反映了两个情况：\n\n情况1：ThreadA的删除操作，即TermNode不会作用于ThreadB添加的文档\n情况2：ThreadB的删除操作，即QueryArrayNode会作用于ThreadA添加的文档（在介绍flush时会展开介绍作用（apply）的过程）\n\n  情况1是因为删除操作不能作用于后新增的文档，情况2是因为删除操作要作用于该删除操作前已添加的所有文档。\n 例子2：\n  由于DWPT初始化私有的DeleteSlice跟添加到删除结点到deleteQueue 这两个操作不是一个临界区内的两个操作，只有添加到deleteQueue是线程同步操作（synchronized），所以根据CPU调度的不同，同样是例子1中两个线程的删除操作，生成的deleteQueue不总是一样，下面例子的一个假设前提是deleteQueue为空。\n  例如图7跟图8是ThreadA跟ThreadB初始化各自私有的DeleteSlice，并且DeleteQueue还没有删除结点，ThreadA和ThreadB分别先将删除结点添加到DeleteQueue中：\n图7：\n\n图8：\n\n BufferedUpdates类\n  上文中提到一个DWPT从生成到flush到一个段，在这段期间其私有的DeleteSlice会记录所有的删除操作，这些删除操作会作用于(apply)DWPT添加的文档，即删除那些满足删除要求的文档，然而对于某一个删除操作来说，它只能作用于该删除操作前DWPT已添加的文档，其随后新添加的文档不能被apply，BufferedUpdates类就是通过以下几个Map对象来描述 某一个删除操作的作用范围(apply scope)：\n\nMap&lt;Term,Integer&gt; deleteTerms\nMap&lt;Query,Integer&gt; deleteQueries\nMap&lt;String,LinkedHashMap&lt;Term,NumericDocValuesUpdate&gt;&gt; numericUpdates：暂不作介绍\nMap&lt;String,LinkedHashMap&lt;Term,BinaryDocValuesUpdate&gt;&gt; binaryUpdate：暂不作介绍\n\n  numericUpdates、binaryUpdate的介绍 见文章软删除softDeletes（一）。\n  在deleteTerms中，该Map的key为Term，表示包含该Term的文档都会被删除，value为一个哨兵值，描述了该删除操作的作用范围，即只能作用于文档号小于哨兵值的文档，这里需要补充一个概念：\n\nnumDocsInRAM：该值是每一个DWPT的私有变量。DWPT每添加一个文档，会为该文档赋予一个文档号，文档号是从0开始递增的值，numDocsInRAM描述了当前添加的文档数量\n\n  在deleteQueries中，该Map的key为Query，表示满足该查询要求的文档都会被删除，value的概念同deleteTerms。\n  哨兵值怎么用：\n\n在deleteTerms中：在DWPT将索引信息生成索引文件期间，利用倒排表中的信息找到包含该Term的所有文档的文档号，文档号小于哨兵值的文档都会删除，当然所谓的&quot;“删除”&quot;其实是将被删除的文档号从文档号集合（docId Set，0 ~ (numDocsInRAM - 1)的文档号集合）中剔除，在处理完所有的删除(比如下文中的deleteQueries)后，该集合会生成索引文件.liv\n在deleteQueries中：在DWPT将索引信息生成索引文件之后，通过查询的方式找出满足删除要求的文档号，然后从文档号集合中剔除这些文档号\n\n  处理被删除的文档号的详细过程在介绍flush时会详细展开。\n 例子\n  下面的例子介绍了当出现多个更新操作，并且存在相同删除操作的情况下，deleteTerms如何处理：\n图9：\n\n\n第一个更新操作：第87行，需要删除包含term1的文档，此时deleteTerms中key为term1，value为1,因为当前添加了两篇文档（注意：更新操作是 先添加 后删除 的操作，故此时文档1已经被添加），value的值为1，表示该删除作用范围是文档号在[0, 1)左闭右开的区间内的文档\n第二个更新操作：第97行，需要删除包含term2的文档，此时deleteTerms中key为还是term1（term1跟term2的hash值是一样的），value更新为3，表示该删除作用范围是文档号在[0, 3)左闭右开的区间内的文档\n第三个更新操作：第102行，需要删除包含term3的文档，此时deleteTerms相比较前两次更新操作，新增了一个key为term3，value为4的元素，表示该删除作用范围是文档号在[0, 4)左闭右开的区间内的文档，当然了我们用肉眼看一下，这个操作不会删除任何文档，因为[0, 4)的文档没有一篇文档满足删除要求。\n\n  故最终deleteTerms中包含了两条删除信息：\n图10：\n\n 文档的增删改流程图\n  在文档的增删改（一）的文章中，我们给出了文档的增删改流程图，我们紧接文档的增删改（三），继续介绍剩余的两个流程点：处理删除信息、处理文档后的工作。\n图11：\n\n 处理删除信息\n图12：\n\n  文档的增删改都需要处理删除信息。\n 添加文档\n 添加文档处理删除信息的流程图\n图13：\n\n 更新DeleteSlice\n图14：\n\n  检查其他线程是否新增了删除操作，这些删除操作要作用于DWPT已经添加的文档，判断方式通过判断DWPT的私有DeleteSlice对象中的sliceTail是否跟tail有相同的对象引用，如果不是，那么更新私有DeleteSlice，即另sliceTail引用跟tail相同的对象。\n 重置DeleteSlice\n图15：\n\n  通过DWPT的numDocsInRAM是否大于0来判断是否已经添加过文档：\n\n没有添加过文档：只有DWPT第一次执行添加/更新的操作，该值才会为0，由于上一个流程点，该DWPT的私有DeleteSlice可能被更新了，但由于DWPT中没有添加过任何的文档，所以私有DeleteSlice中不需要删除信息，故需要重置DeleteSlice，重置的方法即将另私有DeleteSlice的sliceHead引用跟sliceTail相同的对象，当sliceHead跟sliceTail引用相同的对象时，表示DWPT的私有DeleteSlice没有删除信息\n添加过文档：如果上一个流程点没有更新私有DeleteSlice，那么就不要更新BufferedUpdates，否则需要将新增的删除信息添加到BufferedUpdates中，这些新增的删除信息作用于DWPT已经收集的所有文档（不包含本次添加的文档）更新BufferedUpdates的过程已在上文BufferedUpdate类中介绍，不赘述\n\n 更新numDocsInRAM\n图16：\n\n  到达此流程点说明我们添加了一篇文档，故更新numDocsInRAM的值，即执行numDocsInRAM++的操作。\n 更新文档\n 添加文档处理删除信息的流程图\n图17：\n\n  将DWPT自己的删除信息添加到DeleteQueue中，然后更新BufferedUpdates，注意的是，在上文中我们知道，当前DWPT添加到DeleteQueue时可能有其他的线程先添加了新的删除信息，故可能会更新自己以及其他线程的删除信息到BufferedUpdates。\n 删除文档\n  由于删除文档的处理删除信息的流程与添加/更新文档的处理文档后的工作的流程有很多的相似点，又因为剩余的内容会导致本篇文章的篇幅过长，故在下一篇文章中展开。\n 结语\n  无\n[点击下载](http://www.amazingkoala.com.cn/attachment/Lucene/Index/文档的增删改/文档的增删改（下）（part 2）/文档的增删改（下）（part 2）.zip)附件\n","categories":["Lucene","Index"],"tags":["document"]},{"title":"构造IndexWriter对象（一）","url":"/Lucene/Index/2019/1111/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  该系列文章将会介绍构造一个IndexWriter对象的流程，该流程总体分为下面三个部分：\n\n设置索引目录Directory\n设置IndexWriter的配置信息IndexWriterConfig\n调用IndexWriter的构造函数\n\n 设置索引目录Directory\n  Directory用来维护索引目录中的索引文件，定义了创建、打开、删除、读取、重命名、同步(持久化索引文件至磁盘)、校验和（checksum computing）等抽象方法，索引目录中不存在多级目录，即不存在子文件夹的层次结构(no sub-folder hierarchy)，另外Directory的具体内容已经在Directory系列文章中介绍，这里不赘述。\n 设置IndexWriter的配置信息IndexWriterConfig\n  在调用IndexWriter的构造函数之前，我们需要先初始化IndexWriter的配置信息IndexWriterConfig，IndexWriterConfig中的配置信息按照可以分为两类：\n\n不可变配置（unmodifiable configuration）：在实例化IndexWriter对象后，这些配置不可更改，即使更改了，也不会生效，因为仅在IndexWriter的构造函数中应用一次这些配置\n可变配置（modifiable configuration）：在实例化IndexWriter对象后，这些配置可以随时更改\n\n 不可变配置\n  不可变配置包含的内容有：OpenMode、IndexDeletionPolicy、IndexCommit、Similarity、MergeScheduler、Codec、DocumentsWriterPerThreadPool、ReaderPooling、FlushPolicy、RAMPerThreadHardLimitMB、InfoStream、IndexSort、SoftDeletesField，下面我们一一介绍这些不可变配置。\n OpenMode\n  OpenMode描述了在IndexWriter的初始化阶段，如何处理索引目录中的已有的索引文件，这里称之为旧的索引，OpenMode一共定义了三种模式，即：CREATE、APPEND、CREATE_OR_APPEND。\n\nCREATE：如果索引目录中已经有旧的索引（根据Segment_N文件读取旧的索引信息），那么会覆盖（Overwrite）这些旧的索引，但注意的是新的提交（commit）生成的Segment_N的N值是旧索引中最后一次提交生成的Segment_N的N值加一后的值，如下所示：\n\n图1：\n\n  图1为索引目录中的旧的索引，并且有三个Segment_N文件，即segments_1、segments_2、segments_3。\n图2：\n\n  接着我们通过CREATE打开图1的索引目录，并且执行了一次commit操作后，可以看出旧的索引信息被删除了（_0.cfe、_0.cfs、_0.si、_1.cfe、_1.cfs、_1.si、_2.cfe、_2.cfs、_2.si被删除了），并且新的提交（commit）生成的Segment_N（segment_4）的N值是旧索引中最后一次提交生成的Segment_N（segment_3）的N值加一后的值。\n\nAPPEND：该打开模式打开索引目录会先读取索引目录中的旧索引，新的提交操作不会删除旧的索引，注意的是如果索引目录没有旧的索引（找不到任何的Segment_N文件），并且使用当前模式打开则会报错，报错信息如下：\n\nthrow new IndexNotFoundException(&quot;no segments* file found in &quot; + directory + &quot;: files: &quot; + Arrays.toString(files));\n  上述的异常中，directory即上文提到的索引目录Directory，而Arrays.toString(files)用来输出索引目录中的所有文件。\n\nCREATE_OR_APPEND：该打开模式会先判断索引目录中是否有旧的索引，如果存在旧的索引，那么相当于APPEND模式，否则相当于CREATE模式\n\n  OpenMode可以通过IndexWriterConfig.setOpenMode(OpenMode openMode)方法设置，默认值为CREATE_OR_APPEND。\n IndexDeletionPolicy\n  IndexDeletionPolicy是索引删除策略，该策略用来描述当一个新的提交生成后，如何处理上一个提交，在文档提交之commit（二）的文章中详细介绍了几种索引删除策略，这里不赘述。\n IndexCommit\n  执行一次提交操作（执行commit方法）后，这次提交包含的所有的段的信息用IndexCommit来描述，其中至少包含了两个信息，分别是segment_N文件跟Directory，在文档提交之commit（二）的文章中，我们提到了一种索引删除策略SnapshotDeletionPolicy，在每次执行提交操作后，我们可以通过主动调用SnapshotDeletionPolicy.snapshot()来实现快照功能，而该方法的返回值就是IndexCommit。\n  如果设置了IndexCommit，那么在构造IndexWriter对象期间，会先读取IndexCommit中的索引信息，IndexCommit可以通过IndexWriterConfig.setIndexCommit(IndexCommit commit)方法设置，默认值为null。\n  另外IndexCommit的更多的用法见近实时搜索NRT系列文章。\n Similarity\n  Similarity描述了Lucene打分的组成部分，在查询原理系列文章中详细介绍了Lucene如何使用BM25算法实现对文档的打分，这里不赘述。\n  Similarity可以通过IndexWriterConfig.setSimilarity(Similarity similarity)方法设置，默认使用BM25。\n MergeScheduler\n  MergeScheduler即段的合并调度策略，用来定义如何执行一个或多个段的合并，比如并发执行多个段的合并任务时的执行先后顺序，磁盘IO限制，Lucene7.5.0中提供了三种可选的段的合并调度策略，见文章MergeScheduler。\n  MergeScheduler可以通过IndexWriterConfig.setMergeScheduler(MergeScheduler mergeScheduler)方法设置，默认使用ConcurrentMergeScheduler。\n Codec\n  Codec定义了索引文件的数据结构，即描述了每一种索引文件需要记录哪些信息，以及如何存储这些信息，在索引文件的专栏中介绍了所有索引文件的数据结构，这里不赘述。\n  Codec可以通过IndexWriterConfig.setCodec(Codec codec)方法设置，在Lucene7.5.0版本中，默认使用Lucene70Codec。\n DocumentsWriterPerThreadPool\n  DocumentsWriterPerThreadPool是一个逻辑上的线程池，它实现了类似Java线程池的功能，在Java的线程池中，新来的一个任务可以从ExecutorService中获得一个线程去处理该任务，而在DocumentsWriterPerThreadPool中，每当IndexWriter要添加文档，会从DocumentsWriterPerThreadPool中获得一个ThreadState去执行，故在多线程（持有相同的IndexWriter对象引用）执行添加文档操作时，每个线程都会获得一个ThreadState对象，DocumentsWriterPerThreadPool以及ThreadState的具体介绍可以看文章文档的增删改（二），这里不赘述。\n  如果不是深度使用Lucene，应该不会去调整这个配置吧。。。\n ReaderPooling\n  ReaderPooling该值是一个布尔值，用来描述是否允许共用（pool）SegmentReader，共用（pool）可以理解为缓存，在第一次读取一个段的信息时，即获得该段对应的SegmentReader，并且使用ReaderPool（见执行段的合并（二）中关于ReaderPool的介绍）来缓存这些SegmentReader，使得在处理删除信息（删除操作涉及多个段时效果更突出）、NRT搜索时可以提供更好的性能，至于为什么共用/缓存SegmentReader能提高性能见文章SegmentReader（一）。\n  ReaderPooling可以通过IndexWriterConfig.setReaderPooling(boolean readerPooling)方法设置，默认值为true。\n FlushPolicy\n  FlushPolicy即flush策略，准确的说应该称为 自动flush策略，因为flush分为自动flush跟主动flush（见文档提交之flush（一）），即显式调用IndexWriter.flush( )方法，FlushPolicy描述了IndexWriter执行了增删改的操作后，将修改后的索引信息写入磁盘的时机（实际是存入磁盘缓存，见文档提交之commit（一）中关于执行同步磁盘工作的介绍）。\n  Lucene7.5.0版本中，有且仅有一个FlushPolicy：FlushByRamOrCountsPolicy，可以看文章文档的增删改（二）中关于FlushByRamOrCountsPolicy的详细介绍。\n RAMPerThreadHardLimitMB\n  该配置在后面介绍可变配置中的MaxBufferedDocs、RAMBufferSizeMB时一起介绍。\n InfoStream\n  InfoStream用来在对Lucene进行调试时实现debug输出信息，在业务中打印debug信息会降低Lucene的性能，故在业务中使用默认值就行，即不输出debug信息。\n  InfoStream可以通过IndexWriterConfig.setInfoStream(InfoStream infoStream)方法设置，默认值为NoOutput。\n IndexSort\n  IndexSort描述了在索引阶段如何对segment内的文档进行排序，排序的好处及其实现方式见文章Collector（三）中的预备知识及文章文档提交之flush（三）中的sortMap。\n  IndexSort可以通过IndexWriterConfig.setIndexSort(Sort sort)方法设置，默认值为null。\n SoftDeletesField\n  SoftDeletesField用来定义哪些域为软删除的域，关于软删除的概念在后面的文章中会用多篇文章的篇幅介绍，这里暂不展开。\n  IndexSort可以通过IndexWriterConfig.setSoftDeletesField(String softDeletesField)方法设置，默认值为null。\n 可变配置\n  可变配置包含的内容有：MergePolicy、MaxBufferedDocs、RAMBufferSizeMB、MergedSegmentWarmer、UseCompoundFile、CommitOnClose、CheckPendingFlushUpdate。\n 结语\n  基于篇幅，我们将在下一篇文章中介绍可变配置的内容。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（七）","url":"/Lucene/Index/2019/1202/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E4%B8%83%EF%BC%89/","content":"  本文承接构造IndexWriter对象（六），继续介绍调用IndexWriter的构造函数的流程。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 生成对象IndexFileDeleter\n  在文章构造IndexWriter对象（六）中，我们简单介绍了IndexFileDeleter作用，即用来删除索引目录中的索引文件，本文根据IndexFileDeleter的构造函数的实现来介绍关于计数引用、删除无效（Invalid）索引文件的内容。\n IndexFileDeleter的构造函数流程图\n图2：\n\n点击查看大图\n SegmentInfos\n图3：\n\n  构造 IndexFileDeleter最重要的一个对象就是SegmentInfos，它是在图1的生成对象IndexFileDeleter流程点之前通过用户配置的IndexCommit或者索引目录中的旧索引生成的SegmentInfos对象（见构造IndexWriter对象（三）、构造IndexWriter对象（四）、构造IndexWriter对象（五）中关于生成SegmentInfos的介绍），并作为 IndexFileDeleter的构造函数的参数之一。\n 是否能获得索引文件segments_N的文件名？\n图4：\n\n  构造IndexWriter对象里面调用 IndexFileDeleter的构造函数时，总是能通过SegmentInfos获得一个segments_N的文件名，并且通过下面的方法来获得segments_N的文件名，那么当前流程点的判断结果总是为true，由于该方法可能会返回null，所以在这里会有一个判断。\nSegmentInfos.getSegmentsFileName()\n 判断索引文件是否满足要求\n图5：\n\n  接着就是从索引目录依次取出索引文件，然后判断是否满足某个要求。\n需要满足什么要求：\n  先给出该要求对应的代码：\nif (!fileName.endsWith(&quot;write.lock&quot;) &amp;&amp; (m.matches() || fileName.startsWith(IndexFileNames.SEGMENTS) || fileName.startsWith(IndexFileNames.PENDING_SEGMENTS))&#123;    ... ...&#125;\n  其中fileName就是待处理的索引文件的文件名、m.matches()描述的是fileName是否满足下面的正则表达式\n_[a-z0-9]+(_.*)?\\\\..*\n  上述的代码进行拆分后即需要满足下面三个字要求之一：\n\n子要求一：!fileName.endsWith(“write.lock”) &amp;&amp; (m.matches()\n\nwriter.lock即索引目录的索引文件锁，用来同步不同的IndexWriter对象，只允许一个IndexWriter可以操作同一个索引目录，占用了索引文件锁的IndexWriter可以通过调用Inde.close()方法来释放该锁，wrtier.lock不满足要求\nm.matches()则是根据正则表达式来匹配命名方式为下图中的文件名，满足正则表达式即满足要求：\n\n\n\n图6：\n\n\n子要求二：fileName.startsWith(IndexFileNames.SEGMENTS)\n\n满足以&quot;segment&quot;开头的文件名即满足子要求二，比如segments_N文件\n\n\n子要求三：fileName.startsWith(IndexFileNames.PENDING_SEGMENTS)\n\n满足以“pending_segments”开头的文件名即满足子要求三，比如执行commit()操作时，在两阶段提交之第一阶段会生成该命名方式的文件，详细见文件文档提交之commit（二）\n\n\n\n 初始化索引文件的计数引用\n图7：\n\n  如果索引文件满足上文中的要求，那么我们初始化这些索引文件的计数引用。\nLucene中如何实现对索引文件的计数引用：\n  通过一个HashMap对象refCounts以及一个RefCount类实现，他们的定义如下所示：\nfinal private static class RefCount &#123;    final String fileName; // 索引文件名    int count; // 计数值      public int IncRef() &#123;        // 增加计数    &#125;        public int DecRef() &#123;        // 减少计数    &#125;&#125;// key为索引文件名，value为RefCount对象private Map&lt;String, RefCount&gt; refCounts = new HashMap&lt;&gt;();\n  如果我们要增加某个索引文件的计数引用，那么根据refCounts找到该文件对应的RefCount对象，接着通过对象的IncRef()方法来增加计数。\n  上述的RefCount类的具体内容可以查看这个链接：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java 。\n 索引文件是否为segments_N？\n图8：\n\n  如果是segments_N文件，那么需要对该文件进行额外的处理，判断方法同上文中的子要求二。\n 处理索引目录中所有的segments_N\n图9：\n\n  索引目录中可能存在多个segments_N文件，那么这些文件都需要被处理，其中只有一个segments_N对应的SegmentInfos为构造IndexFileDeleter对象的参数，即图2中SegmentInfos流程点的segmentInfos。\n为什么索引目录中会存在多个segments_N文件：\n  原因主要取决于上一个IndexWriter对象使用了哪种索引删除策略IndexDeletionPolicy（见文档提交之commit（二）关于IndexDeletionPolicy的介绍），比如使用了索引删除策略NoDeletionPolicy，那么每次提交都会保留，又比如使用了默认的索引删除策略KeepOnlyLastCommitDeletionPolicy，那么只会保留最后一次提交。\n图10：\n\n  在图10中，使用不同的索引删除策略对相同的数据进行索引，在执行了4次commit提交后，对于NODeletetionPolicy来说，它会保留所有的提交，而对于KeepOnlyLastCommitDeletionPolicy，当生成segments_2时，会删除segments_1，生成segments_3时，会删除segments_2，以此类推，即只保留最后一次提交。\n为什么要根据每一个segments_N对应的SegmentInfos生成CommitPoint，并且添加到CommitPoint集合commits中：\n\n\n原因是我们需要根据正在构造的IndexWriter对象中的索引删除策略来处理这些提交，而CommitPoint对象为索引删除策略作用的对象。\n\n\n这里将所有的CommitPoint添加到commits中是为了在下面的流程中作为索引删除策略的输入数据进行统一处理。\n\n\n  接着如果某个segments_N对应的SegmentInfos为构造IndexFileDeleter的参数，即图2中SegmentInfos流程点的SegmentInfos，那么它对应生成的CommitPoint被设置为当前提交点currentCommitPoint，该对象在后面的流程点是否出现异常会作为条件进行异常判断。\n  最后我们需要增加每一个segments_N对应的SegmentInfos中对应的索引文件的计数引用，其原因是在后面的流程中能判断能否能删除索引文件。\n 是否处理异常\n图11：\n\n  这里考虑的异常是使用NFS（Network File System）网络文件系统的场景，使用该文件系统可能会导致下面的情况：我们能通过构造IndexFileDeleter的参数获得SegmentInfos对象，并且通过SegmentInfos获得segments_N的文件名（注意只是文件名），那么segments_N文件必定是在索引目录中（见构造IndexWriter对象（五）/构造IndexWriter对象（四）/构造IndexWriter对象（三）不同的打开模式OpenMode的内容），但是我们在图9的流程中，如果没有读取到segments_N文件（通过是否获得currentCommitPoint对象来判断），那么有可能就是NFS导致的，例如目录缓存机制的影响，那么可以根据segment_N文件名，通过重试机制来获得它对应的SegmentInfos对象，并且生成currentCommitPoint对象，如果还是读取不到，那么就会抛出下面的异常：\nthrow new CorruptIndexException(&quot;unable to read current segments_N file&quot;, currentSegmentsFile, e);\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（三）","url":"/Lucene/Index/2019/1118/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E4%B8%89%EF%BC%89/","content":"  构造一个IndexWriter对象的流程总体分为下面三个部分：\n\n设置索引目录Directory\n设置IndexWriter的配置信息IndexWriterConfig\n调用IndexWriter的构造函数\n\n  大家可以查看文章构造IndexWriter对象（一）、构造IndexWriter对象（二）来了解前两部分的内容，我们接着继续介绍最后一个部分，即调用IndexWriter的构造函数。\n  IndexWriter类有且仅有一个有参构造函数，如下所示：\npublic IndexWriter(Directory d, IndexWriterConfig conf) throws IOException &#123;    ... ...&#125;\n  其中参数d以及conf正是分别由设置索引目录Directory、设置IndexWriter的配置信息IndexWriterConfig两部分获得。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 获取索引目录的索引文件锁\n图2：\n\n  该流程为Lucene使用索引文件锁对索引文件所在的目录进行加锁，使得同一时间总是只有一个IndexWriter对象可以更改索引文件，即保证单进程内(single in-process)多个不同IndexWriter对象互斥更改（多线程持有相同引用的IndexWriter对象视为一个IndexWriter不会受制于LockFactory，而是受制于对象锁（synchronized(IndexWriter)）、多进程内(multi-processes)多个对象互斥更改。\n  更多关于索引文件锁的介绍可以看文章索引文件锁LockFactory。\n 获取封装后的Directory\n图3：\n\n  该流程中我们需要对Directory通过LockValidatingDirectoryWrapper对象进行再次封装， 使得在对索引目录中的文件进行任意形式的具有&quot;破坏性&quot;（destructive）的文件系统操作（filesystem operation）前尽可能（best-effort）确保索引文件锁是有效的（valid）。\n  索引目录中的&quot;破坏性&quot;的文件系统操作包含下面几个内容：\n\ndeleteFile(String name)方法：删除索引目录中的文件\ncreateOutput(String name, IOContext context)方法：在索引目录中创建新的文件\ncopyFrom(Directory from, String src, String dest, IOContext context)方法：在索引目录中，将一个文件中的内容src复制到同一个索引目录中的另外一个不存在的文件dest\nrename(String source, String dest)方法：重命名索引目录中的文件\nsyncMetaData()方法：磁盘同步操作\nsync(Collection&lt;String&gt; names)方法：磁盘同步操作\n\n 获取IndexCommit对应的StandardDirectoryReader\n图4：\n\n  如果IndexWriter的配置信息IndexWriterConfig设置了IndexCommit配置，那么我们需要获得描述IndexCommit中包含的信息的对象，即StandardDirectoryReader，生成StandardDirectoryReader的目的在后面的流程中会展开介绍，这里只要知道它的生成时机即可。\n  IndexCommit的介绍可以查看文章构造IndexWriter对象（一），而StandardDirectoryReader的介绍可以查看近实时搜索NRT、SegmentReader系列文章，这里不赘述。\n 根据不同的OpenMode执行对应的工作\n图5：\n\n  从图5中可以看出，尽管Lucene提供了三种索引目录的打开模式，但实际上只有CREATE跟APPEND两种打开模式的逻辑，三种模式的介绍可以看文章构造IndexWriter对象（一），这里不赘述。\n  在源码中，使用一个布尔值indexExists来描述图5中的流程点索引目录中是否已经存在旧的索引？，如果存在，那么indexExists的值为true，反之为false。indexExists在后面的流程中会被用到。\n  下面我们分别介绍执行CREATE模式下的工作、执行APPEND模式下的工作这两个流程。\n 执行CREATE模式下的工作的流程图\n图6：\n\n 配置检查1\n图7：\n\n  该流程会检查用户是否正确设置了IndexCommit跟OpenMode两个配置，由于代码比较简单，故直接给出：\nif (config.getIndexCommit() != null) &#123;    // 条件一    if (mode == OpenMode.CREATE) &#123;        throw new IllegalArgumentException(&quot;cannot use IndexWriterConfig.setIndexCommit() with OpenMode.CREATE&quot;);    // 条件二    &#125; else &#123;        throw new IllegalArgumentException(&quot;cannot use IndexWriterConfig.setIndexCommit() when index has no commit&quot;);    &#125;&#125;\n  上面的代码描述的是在设置了配置IndexCommit之后对OpenMode进行配置检查，其中config指的是IndexWriter的配置信息IndexWriterConfig对象：\n\n条件一：如果用户设置的OpenMode为CREATE，由于该模式的含义是生成新的索引或覆盖旧的索引，而设置IndexCommit的目的是读取已经有的索引信息，故这两种是相互冲突的逻辑，Lucene通过抛出异常的方法来告知用户不能这么配置\n条件二：如果用户设置的OpenMode为CREATE_OR_APPEND，由于通过图5中的流程点索引目录中是否已经存在旧的索引？判断出indexExists的值为false，即索引目录中没有任何的提交，但用户又配置了IndexCommit，这说明用户配置的IndexCommit跟IndexWriter类的有参构造函数中的参数d必须为同一个索引目录\n\n 初始化一个新的SegmentInfos对象\n图8：\n\n  该流程只是描述了生成SegmentInfos对象的时机点，没其他多余的内容。\n  SegmentInfos是什么：\n\nSegmentInfos对象是索引文件segments_N以及索引文件.si在内存中的描述，可以看文章近实时搜索NRT（一）中关于流程点获得所有段的信息集合SegmentInfos的介绍，这里不赘述\n\n 同步SegmentInfos的部分信息\n图9：\n\n  如果索引目录中已经存在旧的索引，那么indexExists的值为true，那么我们先需要获得旧的索引中的最后一次提交commit中的SegmentInfos中的三个信息，即version、counter、generation：\n\nversion：该值用来描述SegmentInfos发生改变的次数，即索引信息发生改变的次数\ncounter：它跟下划线“_”作为一个组合值，用来描述下一次生成（commit、flush操作）的新段对应的索引文件的前缀值，下图中&quot;_4&quot;、&quot;_5&quot;的4、5即为counter值，该值为一个从0开始的递增值\n\n图10：\n\n\ngeneration：用来描述执行提交操作后生成的segments_N文件的N值，图10中，generation的值为2\n\n  上述三个信息在索引文件segments_N中的位置如下所示：\n图11：\n\n  图11中，generation的值通过索引文件segments_N的文件名来获得。\n  接着将version、counter、generation同步到刚刚初始化的新的SegmentInfos对象中。\n  为什么执行同步这三个信息的操作：\n\n使得新生成的索引文件不会跟旧的索引文件有一样的名字，即不会覆盖旧的索引文件，那么其他线程可以正常通过IndexCommit读取旧索引执行搜索。\n\n 设置回滚\n图12：\n\n  该流程为回滚的初始化，初始化一个叫做rollbackSegments的链表，该链表的定义如下：\nprivate List&lt;SegmentCommitInfo&gt; rollbackSegments;  \n  如果索引目录中存在旧的索引，那么另旧的索引对应的SegmentInfos对象中的segments对象赋值给回滚内容rrollbackSegments，否则rollbackSegments为null。在执行commit()的过程中，rollbackSegments会被更新为这次提交对应的segments对象。segments对象即图11中所有SegmentCommitInfo在内存中的描述。\n 更新SegmentInfos的version\n图13：\n\n  由于SegmentInfos被同步了version、counter、generation三个信息，说明SegmentInfos发生了变化，那么需要通过更新SegmentInfos的version来描述这次的变化。\n  为什么要记录SegmentInfos的变化：\n\n通过version判断SegmentInfos如果没有发生变化，那么在复用StandardDirectoryReader时可以极大的提高性能，至于为什么能提高性能，以及如何提高性能，在近实时搜索NRT（一）的系列文章中已经介绍，不赘述\n\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（九）","url":"/Lucene/Index/2019/1205/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E4%B9%9D%EF%BC%89/","content":"  本文承接构造IndexWriter对象（八），继续介绍调用IndexWriter的构造函数的流程。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 生成对象IndexFileDeleter\n  我们紧接上一篇文章，继续介绍剩余的流程点，下面先给出IndexFileDeleter的构造函数流程图：\n IndexFileDeleter的构造函数流程图\n图2：\n\n点击查看大图\n 更新SegmentInfos的metaData\n图3：\n\n  我们先介绍下需要更新SegmentInfos的哪些metaData：\n\ngeneration：该值该值是一个迭代编号（generation number），用来命名下一次commit()生成的segments_N的N值\nnextWriteDelGen：该值是一个迭代编号，用来命名一个段的下一次生成的索引文件.liv\nnextWriteFieldInfosGen：该值是一个迭代编号，用来命名一个段的下一次生成的索引文件.fnm\nnextWriteDocValuesGen：该值是一个迭代编号，用来命名一个段的下一次生成的索引文件.dvm&amp;&amp;.dvd\n\n  我们通过下面的例子来介绍上面的metaData的用途，图4的例子使用的索引删除策略为NoDeletionPolicy，完整的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/InflateGenerationTest.java 。\n图4：\n\n  图4中的例子描述的是，在代码第67行执行IndexWriter.commitI()之后生成一个段，该段的索引信息对应为索引目录中以_0为前缀的索引文件，如下所示：\n图5：\n\n  随后执行了图4中的第70行的DocValues的更新操作，由于以_0为前缀的段中的文档满足该更新条件，即该段中包含域名为&quot;author&quot;、域值为&quot;Lily&quot;的文档，故在执行完第72行的IndexWriter.commit()操作后，索引目录中的索引文件如下所示：\n图6：\n\n  图6中的三个索引文件_0_1.fnm、_0_1.Lucene70_0.dvd、_0_1.Lucene70_0.dvm描述了第一次的DocValue更新后以_0为前缀的段中的DocValues信息，随后继续执行图4中第75行的DocValues的更新操作，同样的以_0为为前缀的段中的文档仍然满足该更新条件，故在执行完第77行的IndexWriter.commit()操作后，索引目录中的索引文件如下所示：\n图7：\n\n  图7中的三个索引文件_0_2.fnm、_0_2.Lucene70_0.dvd、_0_2.Lucene70_0.dvm描述了第二次的DocValue更新后以_0为前缀的段中的DocValues信息。\n图8：\n\n  我们以两次生成索引文件.fnm为例，nextWriteFieldInfosGen用来命名下次生成索引文件.fnm，图8中红框标注的两个值即迭代编号（generation number）通过nextWriteFieldInfosGen来命名，在生成图8的两个索引文件.fnm后，此时nextWriteFieldInfosGen的值变为3，为下一次生成索引文件.fnm作准备，nextWriteDocValuesGen是同样的意思，用来命名索引文件.dvd、.dvm。\n  我们再给出下面的例子来介绍下nextWriteDelGen，完整demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/MultiDeleteTest.java 。：\n图9：\n\n图10：\n\n  图10为图9的例子执行结束后的索引目录的内容，可以发现它跟nextWriteFieldInfosGen、nextWriteDocValuesGen是一样的用法。\n  我们换一个例子来介绍segments_N文件跟迭代编号之间的关系，该例子的完整demo见 https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/MultiDeleteUpdateTest.java ，本文就不贴出来了，直接给出运行结束后索引目录中的内容：\n图11：\n\n  我们以图11中的segments_3为例，介绍生成的索引文件.fnm、.dvd、.dvm的迭代编号在segments_N文件中的描述：\n图12：\n\n  图12中，DelGen即图11中索引文件_0_2.liv的迭代编号，FieldInfosGen即图11中索引文件_0_2.fnm的迭代编号，DocValuesGen即图11中索引文件  _0_2_Lucene70_0.dvd、  _0_2_Lucene70_0.dvm的迭代编号，而FieldInfosFiles中描述的是以_0为前缀的段的域信息对应的索引文件名，UpdatesFiles中描述的是以_0为前缀的段的DocValues的信息对应的索引文件名。\n  继续给出segment_2的例子，不作多余的介绍：\n图13：\n\n接着我们继续介绍为什么要执行更新SegmentInfos的metaData的操作：\n  先给出源码中的注释，该流程点对应在源码中的方法为：IndexFileDeleter.inflateGens(…) 。\nSet all gens beyond what we currently see in the directory, to avoid double-write in cases where the previous IndexWriter did not gracefully close/rollback (e.g. os/machine crashed or lost power)\n  上述的注释中，directory即索引目录、gens就是上文中我们提到的4个迭代编号，该注释的大意描述的是，之前的IndexWriter没有“优雅”的退出（操作系统/物理机 崩溃或者断电），导致索引目录中生成了一些“不优雅”的文件，为了避免新的IndexWriter生成的索引文件的文件名与索引目录中的相同（即double-write）可能会引起一些问题，不如先根据索引目录中索引文件找到gens的各自的最大值N，使得随后生成的索引文件的迭代编号从N+1开始。\n  看完上文中的注释不然引申出下面的三个问题：\n问题一：“不优雅”的文件有哪些：\n  作者无法列出所有的不优雅的文件，只介绍某些我们可以通过代码演示的文件，并且会给出对应的demo：\n\npending_segments_N文件：在文档提交之commit系列文章中，我们知道执行IndexWriter.commit()是一个两阶段提交的过程，如果在二阶段提交的第一阶段成功执行后，即生成了pending_segments_N文件，IndexWriter无法执行二阶段提交的第二阶段，比如操作系统/物理机 崩溃或者断电，那么在索引目录中就会存在pending_segments_N文件，我们可以通过这个demo来演示：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/UnGracefulIndexFilesTest1.java\n\n  该demo运行结束后，索引目录中的文件如下所示：\n图14：\n\n  从图14可以看出，IndexWriter成功的执行了一次commit()操作，即生成了segments_1文件，当再次执行commit()操作时，只成功执行了二阶段提交的第一阶段，即只生成了pending_segments_2文件。\n\n修改了索引目录中的内容，但是没有commit：执行了文档的增删改之后，但是没有执行commit()操作就异常退出了，那么上一次commit之后的生成的索引文件都是“不优雅”的\n无法删除的索引文件：由于本人对文件系统没有深入的理解，这方面的内容不敢妄言，故直接给出Lucene在源码中的解释\n\nOn windows, a file delete can fail because there is still an open file handle against it.  We record this in pendingDeletes and try again later.\n问题二：为什么新的IndexWriter生成的索引文件的文件名可能会与索引目录中的相同：\n  我们先给出SegmentCommitInfo对象的构造函数：\n图15：\n\n  我们以图11为例，如果我们另segments_2中的内容作为IndexCommit来构造一个新的IndexWriter，此时以_0为前缀的段的delGen、fieldInfosGen、docValuesGen都为1，那么根据图15的构造函数，nextWriteDelGen、nextWriteFieldInfosGen、nextWriteDocValuesGen都会被初始化为2，也就说如果以_0为前缀的段在后续的操作中满足删除或者DocValues的更新操作，新生成的.fnm、.dvd、.dvm的迭代编号就是2，那么就会出现与图11中的索引文件有相同的文件名\n问题三：新的IndexWriter生成的索引文件的文件名与索引目录中的相同（即double-write）可能会引起哪些问题：\n  其中一个问题是即将生成的某个索引文件的文件名与索引目录中某个无法删除的索引文件的文件名是一致的，那必然会出问题，另外在Linux平台，如果挂载的文件系统是CIFS（Common Internet File System），也是有可能出现文件无法删除的情况。\n  综上所述，能避免这些问题的最好方式就是根据索引目录中索引文件找到gens的各自的最大值N，使得随后生成的索引文件的迭代编号从N+1开始，使得新生成的索引文件不可能与索引目录中的任何索引文件重名。\n 删除计数为0的索引文件\n图16：\n\n  该流程用来删除索引目录中那些计数为0的索引文件，那么问题就来了：\n索引目录中哪些索引文件的计数会为0呢：：\n  我们在文章构造IndexWriter对象（八）中说道，在图17的流程中，会根据索引目录中的segments_N文件，找到对应的所有索引文件，然后增加了这些索引文件的计数，所以他们是不会被删除的，即下面用红框标注的流程点：\n图17：\n\n  除去segments_N对应的索引文件，那么此时索引目录中还剩下两种类型的索引文件：\n\n“不优雅”的索引文件：这些文件的计数肯定为0\n通过NRT生成的索引文件（见文章构造IndexWriter对象（八））：这些索引文件是有效的索引信息，不能被删除，这也是解释为什么我们需要执行图17中用蓝色标注的流程点，在这两个流程点中，通过NRT生成的索引文件会被增加计数，故不会被删除\n\n 执行索引删除策略\n图18：\n\n  仍旧以图11中的索引目录为例，如果上一个IndexWriter执行了close()方法后，索引目录中的内容如图11所示，如果我们用segments_2对应的IndexCommit作为新的IndexWriter的配置，那么当执行到当前流程点后，实际是让用户来选择如何处理segments_1跟segments_3这两次的提交，这两次都是有效的提交。根据新的IndexWriter使用的索引删除策略来处理这两个提交。\n 执行检查点（checkPoint）工作\n图19：\n\n  这里的流程点的逻辑跟图17中蓝色标注的流程点是一致的，具体的执行流程在构造IndexWriter对象（八）已介绍，不赘述。\n为什么这里还要执行一次checkPoint的工作：\n  介绍这个问题需要介绍执行索引删除策略的完整流程才能理解，故基于篇幅，剩余的内容将在下一篇文章中展开。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（二）","url":"/Lucene/Index/2019/1114/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  构造一个IndexWriter对象的流程总体分为下面三个部分：\n\n设置索引目录Directory\n设置IndexWriter的配置信息IndexWriterConfig\n调用IndexWriter的构造函数\n\n  在文章构造IndexWriter对象（一）中我们讲到了设置IndexWriter的配置信息IndexWriterConfig中不可配置的内容，接着我们继续介绍可配置的内容。\n 设置IndexWriter的配置信息IndexWriterConfig\n 可变配置\n  可变配置包含的内容有：MergePolicy、MaxBufferedDocs、RAMBufferSizeMB、MergedSegmentWarmer、UseCompoundFile、CommitOnClose、CheckPendingFlushUpdate。\n  可变配置指的是在构造完IndexWriter对象后，在运行过程也可以随时调整的配置。\n MergePolicy\n  MergePolicy是段的合并策略，它用来描述如何从索引目录中找到满足合并要求的段集合（segment set），在前面的文章了已经介绍了LogMergePolicy、TieredMergePolicy两种合并策略，这里不赘述。\n  MergePolicy可以通过IndexWriterConfig.setMergePolicy(MergePolicy mergePolicy)方法设置，在版本Lucene7.5.0中默认值使用TieredMergePolicy，如果修改了MergePolicy，那么下一次的段的合并会使用新的合并策略。\n MaxBufferedDocs、RAMBufferSizeMB\n  RAMBufferSizeMB描述了索引信息被写入到磁盘前暂时缓存在内存中允许的最大使用内存值，而MaxBufferedDocs则是描述了索引信息被写入到磁盘前暂时缓存在内存中允许的文档最大数量，这里注意的是，MaxBufferedDocs指的是一个DWPT允许添加的最大文档数量，在多线程下，可以同时存在多个DWPT（DWPT的概念见文档的增删改（二）），而MaxBufferedDocs并不是所有线程的DWPT中添加的文档数量和值。\n  每次执行文档的增删改后，会调用FlushPolicy（flush策略）判断是否需要执行自动flush（见文档提交之flush（一）），在Lucene7.5.0版本中，仅提供一个flush策略，即FlushByRamOrCountsPolicy，该策略正是依据MaxBufferedDocs、RAMBufferSizeMB来判断是否需要执行自动flush。\n  在文档的增删改系列文章中，详细介绍了自动flush，以及FlushByRamOrCountsPolicy的概念，这里不赘述。\n  另外在文章中构造IndexWriter对象（一）中我们说到一个不可配置值，即RAMPerThreadHardLimitMB，该值被允许设置的值域为0~2048M，它用来描述每一个DWPT允许缓存的最大的索引量。\n图1：\n\n  如果你没有看过文档的增删改系列文章，那么可以简单的将DWPT理解为一个容器，存放每一篇文档对应转化后的索引信息，在多线程下执行文档的添加操作时，每个线程都会持有一个DWPT，然后将一篇文档的信息转化为索引信息（DocumentIndexData），并添加到DWPT中。\n  如果每一个DWPT中的DocumentIndexData的个数超过MaxBufferedDocs时，那么就会触发自动flush，将DWPT中的索引信息生成为一个段，如图1所示，MaxBufferedDocs影响的是一个DWPT。\n  如果每一个DWPT中的所有DocumentIndexData的索引内存占用量超过RAMPerThreadHardLimitMB，那么就会触发自动flush，将DWPT中的索引信息生成为一个段，如图1所示，RAMPerThreadHardLimitMB影响的是一个DWPT。\n  如果所有DWPT（例如图1中的三个DWPT）中的DocumentIndexData的索引内存占用量超过RAMBufferSizeMB，那么就会触发自动flush，将DWPT中的索引信息生成为一个段，如图1所示，RAMPerThreadHardLimitMB影响的是所有的DWPT。\n  为什么要提供不可配置RAMPerThreadHardLimitMB：\n\n为避免翻译歧义，直接给出源码中的英文注释\n\nSets the maximum memory consumption per thread triggering a forced flush if exceeded. A DocumentsWriterPerThread(DWPT) is forcefully flushed once it exceeds this limit even if the RAMBufferSizeMB has not been exceeded. This is a safety limit to prevent a DocumentsWriterPerThread from address space exhaustion due to its internal 32 bit signed integer based memory addressing. The given value must be less that 2GB (2048MB)\n\n上文中的forcefully flushed即自动flush\n\n  MaxBufferedDocs、RAMBufferSizeMB分别可以通过IndexWriterConfig.setMaxBufferedDocs(int maxBufferedDocs)、IndexWriterConfig.setRAMBufferSizeMB(double ramBufferSizeMB)方法设置，其中MaxBufferedDocs默认值为-1，表示在flush策略中不依据该值，RAMBufferSizeMB默认值为16M。\n MergedSegmentWarmer\n  MergedSegmentWarmer即预热合并后的新段，它描述的是在执行段的合并期间，提前获得合并后生成的新段的信息，由于段的合并和文档的增删改是并发操作，所以使用该配置可以提高性能，至于为什么能提高性能，以及提高了什么性能可以看文章执行段的合并（四）关于生成IndexReaderWarmer的介绍。\n  MergedSegmentWarmer可以通过IndexWriterConfig.setMergedSegmentWarmer(IndexReaderWarmer mergeSegmentWarmer)方法设置，MergedSegmentWarmer默认为null。\n UseCompoundFile\n  UseCompoundFile是布尔值，当该值为true，那么通过flush、commit的操作生成索引使用的数据结构都是复合索引文件，即索引文件.cfs、.cfe。\n  UseCompoundFile可以通过IndexWriterConfig.setUseCompoundFile(boolean useCompoundFile)方法设置，UseCompoundFile默认为true。\n  注意的是执行段的合并后生成的新段对应的索引文件，即使通过上述方法另UseCompoundFile为true，但还是有可能生成非复合索引文件，其原因可以看文章执行段的合并（三）中生成复合索引文件的流程介绍。\n CommitOnClose\n  该值为布尔值，它会影响IndexWriter.close()的执行逻辑，如果设置为true，那么会先应用（apply）所有的更改，即执行commit操作，否则上一次commit操作后的所有更改都不会保存，直接退出。\n  CommitOnClose可以通过IndexWriterConfig.setCommitOnClose(boolean commitOnClose)方法设置，CommitOnClose默认为true。\n CheckPendingFlushUpdate\n  该值为布尔值，如果设置为true，那么当一个执行添加或更新文档操作的线程完成处理文档的工作后，会尝试去帮助待flush的DWPT，其执行的时机点见下图中红框标注的两个流程点，图2为文档的增删改的完整流程图：\n图2：\n\n点击查看大图\n 结语\n  在下一篇文章中，我们继续介绍构造一个IndexWriter对象的流程的剩余部分，即调用IndexWriter的构造函数。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（五）","url":"/Lucene/Index/2019/1126/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E4%BA%94%EF%BC%89/","content":"  本文承接构造IndexWriter对象（四），继续介绍调用IndexWriter的构造函数的流程。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 根据不同的OpenMode执行对应的工作\n  在上一篇文章中，我们介绍了执行APPEND模式下的工作的部分流程点，故继续对剩余的流程进行介绍。\n 执行APPEND模式下的工作的流程图\n图2：\n\n 用索引目录中最新的提交初始化一个新SegmentInfos对象\n图3：\n\n  由于StandardDirectoryReader为空，那么就从索引目录中初始化一个新SegmentInfos对象（见构造IndexWriter对象（三）），即通过找到索引目录中的segments_N文件读取索引信息。\n  当索引目录中有多个segments_N文件时该如何选择：\n\nLucene设定为读取最新的一次提交，即选取segments_N的N值最大的那个，因为N越大意味着更新的提交（commit()操作）\n\n IndexCommit是否为空？\n图4：\n\n  在构造IndexWriter对象（四）文章中我们说到，图2中StandardDirectoryReader为空的情况分为下面两种：\n\n用户没有设置IndexCommit\n用户设置了IndexCommit，但是IndexCommit中没有StandardDirectoryReader对象的信息\n\n  如果是第一种情况的进入到当前流程点，那么当前流程点的出口为是，那么以APPEND模式打开的IndexWriter追加的索引信息为索引目录中最新的一次提交。\n 配置检查3\n图5：\n\n  如果IndexCommit不为空，那么IndexCommit必定是CommitPoint或者SnapshotCommitPoint对象，接着就需要执行下面的配置检查：\nif (commit.getDirectory() != directoryOrig) &#123;            throw new IllegalArgumentException(&quot;IndexCommit&#x27;s directory doesn&#x27;t match my directory, expected=&quot; + directoryOrig + &quot;, got=&quot; + commit.getDirectory());          &#125;\n  其中commit即IndexCommit对象、directoryOrg为IndexWriter的工作目录，这个配置检查意味着要求当前构造的IndexWriter的工作目录必须和IndexCommit对应的索引信息所在的目录必须一致\n 用IndexCommit更新SegmentInfos对象\n图6：\n\n  通过IndexCommit读取对应的索引信息，然后更新到上文中已经完成初始化的SegmentInfos对象中。\n 更新SegmentInfos的version\n图7：\n\n  对一个已经初始化的SegmentInfos进行更新操作必然需要更新version，version的概念在构造IndexWriter对象（三）的文章中介绍，这里不赘述。\n  至此，我们介绍完了分别在CREATE、APPEND、CREATE_AND_APPEND模式下的执行流程，接着我们根据图1介绍剩余的流程点。\n 检查IndexSort合法性\n图8：\n\n  如果设置了IndexSort，那么在生成一个段的过程中，Lucene会根据用户提供的排序规则对段内的文档进行排序，关于IndexSort的详细介绍见文章构造IndexWriter对象（一）， 如果用户通过IndexWriterConfig.setIndexSort(Sort sort)设置了IndexSort配置，那么需要对参数Sort进行合法性检查，检查逻辑如下所示：\n 检查IndexSort合法性的流程图\n图9：\n\n SegmentInfos对象\n图10：\n\n为什么检查IndexSort合法性的准备数据是SegmentInfos对象：\n  SegmentInfos对象是索引文件segments_N跟.si文件在内存中的描述，如下图所示：\n图11：\n\n  由图11可以看出，我们只能通过SegmentInfos找到每一个段（图11中的SegmentCommitInfo）的段内排序规则IndexSort（图11总红色标注）。\n 是否还有未处理的SegmentCommitInfo？\n图12：\n\n  对每一个SegmentCommitInfo（见图11）进行IndexSort合法性检查，只要有一个段判断为非法，那么就抛出异常，即构造IndexWriter对象失败。\n 比较Lucene版本\n图13：\n\n  如果通过图11的索引文件.si中的IndexSort字段来判断出段中没有排序规则，那么需要判断生成该段的Lucene版本号，代码如下：\nif (segmentIndexSort == null &amp;&amp; info.info.getVersion().onOrAfter(Version.LUCENE_6_5_0))&#123;    throw new CorruptIndexException(&quot;segment not sorted with indexSort=&quot; + segmentIndexSort, info.info.toString());&#125;\n  上述代码中，segmentIndexSort为段中的排序规则，info.info.getVersion( )中，第一个info是SegmentCommitInfo，第二个info为segmentInfo对象（即索引文件.si在内存中的描述），getVersion( )获得值即图11中SegVersion。\n  图13的流程描述的是，如果正在构造的IndexWriter对象设置了IndexSort配置，并且旧索引（旧索引指的是图1中执行三种打开模式流程获得的索引）中一个或多个段中没有排序规则，并且生成这些段的版本号大于等于6.5.0，那么就不能通过调用IndexWriter构造函数来读取旧的索引。\n如何读取没有排序规则的段，并且生成这些段的Lucene版本号大于等于6.5.0的旧索引：\n\n如果旧索引的版本号是Lucene7以上，那么通过IndexWriter.addIndexes(Directory… dirs)方法读取旧索引，该方法必须要求旧索引跟当前读取索引的Lucene主版本（即图11中索引文件segments_N的IndexCreatedVersionMajor字段的值）是一致的。下面的这个demo演示了如何添加上述旧索引：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/ValidateIndexSort.java 。\n如果旧索引的版本号是Lucene7以下并且是Lucene6以上，可以通过DirectoryReader.open(Directory directory)的方式读取\n如果旧索引的版本号是Lucene6以下，那么无法读取\n\n 比较排序规则\n图14：\n\n  如果旧索引中的段包含排序规则，那么需要判断是否与正在构造中的IndexWriter设置的排序规则一致，不一致则抛出异常，如下所示：\nif (segmentIndexSort != null &amp;&amp; indexSort.equals(segmentIndexSort) == false) &#123;    throw new IllegalArgumentException(&quot;cannot change previous indexSort=&quot; + segmentIndexSort + &quot; (from segment=&quot; + info + &quot;) to new indexSort=&quot; + indexSort);&#125;\n  其中segmentIndexSort为段中的排序规则，indexSort为IndexWriter配置的排序规则。\n 结语\n  基于篇幅，剩余的流程点将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（八）","url":"/Lucene/Index/2019/1203/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E5%85%AB%EF%BC%89/","content":"  本文承接构造IndexWriter对象（七），继续介绍调用IndexWriter的构造函数的流程。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 生成对象IndexFileDeleter\n  我们紧接上一篇文章，继续介绍剩余的流程点，下面先给出IndexFileDeleter的构造函数流程图：\n IndexFileDeleter的构造函数流程图\n图2：\n\n点击查看大图\n 执行检查点（checkPoint）工作\n图3：\n\n  在介绍当前流程点之前，我们先通过下面的流程图来介绍执行检查点的工作这个流程点做了哪些事情。\n  另外执行检查点（checkPoint）工作在源码中对应的方法方法定义：\npublic void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOException &#123;    ... ...&#125;\n 执行检查点（checkPoint）工作的流程图\n图4：\n\n 准备数据\n图5：\n\n  图4流程图的准备数据就是checkPoint方法的两个参数SegmentInfos以及boolean。\n 增加SegmentInfos对应的索引文件的计数引用\n图6：\n\n  在后面的流程中，可能会执行索引文件的删除，如果某些索引文件被SegmentInfos引用，那么这些索引文件不应该被删除，防止被删除的方法就是增加SegmentInfos对应的索引文件的计数引用。\n 当isCommit为true时的处理流程\n图7：\n\n  在执行commit()操作时，也会执行checkPoint的操作，那么此时的isCommit是位true的，在文章文档提交之commit（二）中介绍了这个流程，不赘述。\n 当isCommit为false时的处理流程\n图8：\n\n  lastFiles是一个IndexFileDeleter类的成员变量，它用来存放上次执行checkPoint的SegmentInfos中对应的索引文件定义如下：\nprivate final List&lt;String&gt; lastFiles = new ArrayList&lt;&gt;();\n  所以当isCommit为false时，先尝试删除lastFiles中的索引文件，删除的方式就是减少每一个索引文件的计数引用，如果计数值为0，那么该索引文件就要被删除，否则不删除，最后清空lastFiles中的索引文件后，将这次SegmentInfos对应的索引文件添加到lastFiles中。\n  结合图6跟图8我们可以发现这种流程的逻辑设计可以使得，即使对同一个SegmentInfos对象执行多次checkPoint的操作时，如果该对象中的段没有发生变化，那么段对应索引文件的计数是不会变的（先增加计数，再减少计数），同样地，如果SegmentInfos中如果增加了段，能通过图6的流程点对该段中的索引文件执行正确的+1计数，如果删除了某个段，能通过图8的流程点尝试删除lastFiles中的索引文件对该段中的索引文件执行正确的-1计数。\n为什么要通过checkPoint来实现索引文件的删除：\n  Lucene通过IndexWriter对象中的成员变量SegmentInfos来描述当前IndexWriter对应的索引信息，索引信息的变化通过SegmentInfos对象来反应，但是SegmentInfos对象并不真正的拥有索引文件的数据，它拥有只是这些索引文件的文件名，所以当SegmentInfos对应的信息发生变化时，例如某个段中包含的所有文档满足某个删除操作，该段的信息会从SegmentInfos中移除（段的信息即SegmentCommitInfo，见文章构造IndexWriter对象（四）关于流程点获得回滚（rollback）信息的介绍），那么这个段对应的索引文件也应该要删除（如果索引文件的计数引用为0），当然如果其他段同时引用这些索引文件，那么就不会被删除（索引文件的计数引用不为0），也就是为什么图7的流程点尝试删除lastFiles中的索引文件为什么要带有尝试两个字。\n我们回到当前流程点，介绍下为什么获得StandardDirectoryReader后需要执行执行检查点（checkPoint）工作：\n  根据图2的流程，我们是在构造IndexFileDeleter对象的流程中第一次调用checkPoint()方法，那么lastFiles中必定不包含任何的数据，并且在源码中调用checkPoint()方法的参数如下所示：\ncheckpoint(segmentInfos, false);\n  segmentInfos即StandardDirectoryReader中对应的索引信息（见文章构造IndexWriter对象（四）中用StandardDirectoryReader初始化一个新的SegmentInfos对象流程点的介绍），同时isCommit的值为false，也就说在当前流程点调用checkPoint()方法的目的就是仅仅为了增加segmentInfos对应的索引文件的计数，那么就转变为如下的问题：\n为什么获得StandardDirectoryReader后，需要增加segmentInfos对应的索引文件的计数：\n  首先给出源码中的解释：\n// Incoming SegmentInfos may have NRT changes not yet visible in the latest commit, so we have to protect its files from deletion too:\n  我们用一个例子来介绍上文的注释所描述的场景，完整代码见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/CheckPointInIndexFileDeleter.java 。\n图9：\n\n  图9中当第62行的oldIndexWriter.commit()操作执行结束后，索引目录中的索引文件如下所示：\n图10：\n\n  接着执行完第64行的代码后，我们获得了一个NRT的Reader（见文章近实时搜索NRT（三）的介绍），接着第70行代码结束后，oldIndexWriter新增了一篇文档2，注意的是并没有执行commit()操作（即没有生成新的segments_N文件），随后通过第72行的openIfChange()方法获得一个包含最新索引信息的reader（即StandardDirectoryReader），通过该reader获得一个IndexCommit，IndexCommit中包含了第70行代码增加的索引信息，即图11中以_1为前缀的索引文件，并且在第76行通过IndexWriterConfig.setIndexommit()方法将IndexCommit成为newIndexWriter的配置，在执行完第78行代码后，索引目录中的索引文件如下所示：\n图11：\n\n  从图11可以看出，以_1为前缀的索引文件无法被最后的一次commit()可见，即Incoming SegmentInfos may have NRT changes not yet visible in the latest commit，那么在下面的用红色标注的流程中，就无法通过segments_N文件来增加_1.cfe、_1.cfs、_1.si的计数，这些索引文件就是通过NRT changes获得的，在后面的流程中，我们将会知道计数为0的文件都会被删除：\n图12：\n\n 对commits集合进行排序\n图13：\n\n  在文章构造IndexWriter对象（七）中，根据索引目录中的segments_N文件，将对应的segments_N对应的SegmentInfos生成CommitPoint，并且添加到CommitPoint集合commits中，添加的过程是无序的，如果构造中的IndexWriter对象使用的是默认的索引删除策略KeepOnlyLastCommitDeletionPolicy，那么就无法正确的处理 了，所以需要按照从旧到新的提交顺序来排序。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（六）","url":"/Lucene/Index/2019/1127/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E5%85%AD%EF%BC%89/","content":"  本文承接构造IndexWriter对象（五），继续介绍调用IndexWriter的构造函数的流程。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 生成对象BufferedUpdatesStream\n图2：\n\n  介绍该对象会涉及很多在文档提交之flush系列文章中的知识点，故如果没有看过或者不熟悉flush流程的同学可以跳过下面的内容，只需要知道该对象的生成时机就行了。\n  BufferedUpdatesStream用来追踪（track）FrozenBufferedUpdates，主要负责执行FrozenBufferedUpdates的两个工作：\n\n\n获得nextGen：它用来描述FrozenBufferedUpdates中的删除信息应该作用哪些段，见文档提交之flush（六）文章中的介绍\n\n\n作用（apply）删除信息：FrozenBufferedUpdates中存放了删除信息以及更新信息（DocValues相关），为了方便描述，在下文中 删除信息、更新信息统称为删除信息。删除信息被作用到每一个段称为处理删除信息，根据作用（apply）的目标段，处理删除信息划分为两种处理方式：\n\n全局FrozenBufferedUpdates：根据全局FrozenBufferedUpdates内的nextGen（见文档提交之flush（六））值，其删除信息将要作用到所有比该nextGen值小的段\n段内FrozenBufferedUpdates：在文档提交之flush（三）中我们提到，在生成索引文件的过程中，我们只处理了部分满足删除信息，即只处理了满足删除信息TermArrayNode、TermNode（见文档的增删改（四））的段内部分文档，而如果段内FrozenBufferedUpdates还存在删除信息QueryArrayNode、DocValuesUpdatesNode，那么根据段内FrozenBufferedUpdates就可以找出所有剩余的满足删除的文档\n\n\n\n  获得nextGen的执行时机点在flush的流程中的位置如下所示，用红框标注：\n图3：\n\n  图3的流程图的每个流程点的详细介绍见文档提交之flush（六）。\n  作用（apply）删除信息的执行时机点在flush的流程中的位置如下所示，用红框标注：\n图4：\n\n  从图3的流程中可以知道，在FrozenBufferedUpdates获得nextGen之后就被添加到了eventQueue（见文档提交之flush（四）中的介绍）中，故该作用（apply）删除信息的执行时机点在图4的IndexWriter处理事件的流程中。\n 生成对象DocumentsWriter\n图5：\n\n  DocumentsWriter对象主要负责下面的三个工作：\n\n文档的增删改：用户通过IndexWriter对象执行文档的增删改的任务，实际都是IndexWriter通过调用DocumentsWriter对象来实现的，文档的增删改的详细过程可以看文档的增删改的系列文章\n将DWPT生成（flush）为一个段：该工作即图4中的流程执行DWPT的doFlush()\n执行主动flush以后的收尾工作：该内容见文档提交之flush（六）中关于DocumentsWriterFlushControl.finishFullFlush( )的方法的介绍\n\n 生成对象ReaderPool\n  跟BufferedUpdatesStream一样，由于个人表达能力有限，无法通过有限的语句来描述ReaderPool，故阅读下面的内容需要很多前置的内容，这些内容会以链接的方式给出，不会作详细的介绍，见谅。\n  ReaderPool的命名方式就能完美描述该对象的作用，字面意思就是 存放reader的池子（pool），在源码注释中只用了一句话来描述该对象的作用，如下所示：\nHolds shared SegmentReader instances\n  ReaderPool就是用来缓存SegmentReader对象（SegmentReader用来描述一个段的索引信息，详细介绍可以看SegmentReader系列文章），使得Lucene在执行下面的操作时都会尝试先去ReaderPool取出SegmentReader：\n\n作用（apply）删除信息、更新DocValues信息\n执行段的合并\n分发（handing out）一个实时的Reader\n\n 作用（apply）删除信息、更新DocValues信息\n  对于索引目录中的某一个段，由于后续有新的删除/更新操作，如果该段中的文档满足删除/更新的条件，那么该段对应的SegmentReader中的索引信息也需要发生更改，那么根据索引信息是否会被更改可以分为下面两类：\n\n不会发生变更的索引信息：该索引信息即我们在文章SegmentReader（一）中介绍的SegmentCoreReaders\n会发生变更的索引信息：该索引信息即描述删除信息的索引文件.liv、描述域信息的索引文件.fnm、以及描述DocValues的索引文件.dvd&amp;&amp;.dvm\n\n  生成一个SegmentReader对象的开销是极大的，原因是读取索引信息为磁盘I/O操作，故使用ReaderPool来缓存SegmentReader，当需要作用（apply）删除信息、更新DocValues信息时，只需要从ReaderPool中取出该段对应的SegmentReader（如果不存在则先添加到ReaderPool），并且只修改SegmentReader中会发生变更的索引信息。\n  在flush()阶段，DWPT（见文章文档的增删改（二））被flush为一个段后，并不会马上被添加到ReaderPool中（lazy init机制），而是当该段需要被作用（apply）删除信息、更新DocValues信息时，被添加到ReaderPool的时机点在下图中用红框标注：\n图6：\n\n  图6的流程图在文章文档提交之flush（七）中做了详细介绍，感兴趣的同学可以看一看。\n 执行合并\n  执行段的合并的过程是通过每个段对应的SegmentReader中包含的索引信息进行合并（见执行段的合并（三）），故在合并期间需要获取待合并段的SegmentReader，而获取的方式就是从ReaderPool获取。\n  当然也有可能一个或多个待合并的段对应的SegmentReader并不在ReaderPool（原因是没有 作用（apply）删除信息、更新DocValues信息），那么此时就需要生成新的SegmentReader对象，并添加到ReaderPool中。\n 分发（handing out）一个实时（real time）的Reader\n  在文章近实时搜索NRT（一）中我们说到，有下面的四种方法可以获得StandardDirectoryReader：\n\n方法一：DirectoryReader.open(final Directory directory)\n方法二：DirectoryReader.open(final IndexCommit indexCommit)\n方法三：DirectoryReader.open(final IndexWriter indexWriter)\n方法四：DirectoryReader.open(final IndexWriter indexWriter, boolean applyAllDeletes, boolean writeAllDeletes)\n\n  其中通过方法三，方法四能获得具有NRT功能的StandardDirectoryReader（见文章近实时搜索NRT（三）），并且在这两个方法的实现过程中，会将StandardDirectoryReader中的SegmentReader缓存到ReaderPool中，这样的做法使得当再次通过方法三、方法四或者性能更高的OpenIfChange()方法（近实时搜索NRT（四））获得StandardDirectoryReader时，能先从ReaderPool中获得缓存的SegmentReader，即所谓的&quot;分发&quot;。\n  实时（real time）的StandardDirectoryReader缓存到ReaderPool的时机点如下红框标注所示所示：\n图7：\n\n  图7的流程图的详细介绍见文章近实时搜索NRT（二）。\n ReaderPool的构造函数\n  另外还要说下的是，在ReaderPool的构造函数中，会将图1中流程点获取IndexCommit对应的StandardDirectoryReader获得的StandardDirectoryReader中的SegmentReader缓存到ReaderPool中。\n 生成对象IndexFileDeleter\n图8：\n\n  IndexFileDeleter用来追踪SegmentInfos是否还&quot;活着（live）&quot;，在文章构造IndexWriter对象（四）中我们介绍了SegmentInfos对象跟索引文件segments_N的关系，简单的概括就是SegmentInfos对象是索引文件segments_N和索引文件.si在内存中的表示。\n  当执行索引删除策略时，例如默认的索引删除策略KeepOnlyLastCommitDeletionPolicy，新的提交生成后（即生成新的segments_N文件）需要删除上一次提交，即需要删除上一次提交对应的所有索引信息，而用来描述所有索引信息的正是SegmentInfos，删除SegmentInfos的真正目的是为了删除对应在索引目录中的索引文件，但这些索引文件如果正在被其他SegmentInfos引用，那么就不能被删除，IndexFileDeleter真正的工作就是判断索引目录中的索引文件是否允许被删除。\nIndexFileDeleter如何判断索引目录中的索引文件是否允许被删除\n  使用引用计数的方式。\n IndexFileDeleter的构造函数\n  基于篇幅，将在下一篇文章中介绍IndexFileDeleter的构造函数\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（十）","url":"/Lucene/Index/2019/1210/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E5%8D%81%EF%BC%89/","content":"  本文承接构造IndexWriter对象（九），继续介绍调用IndexWriter的构造函数的流程。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 生成对象IndexFileDeleter\n  我们紧接上一篇文章，继续介绍剩余的流程点，下面先给出IndexFileDeleter的构造函数流程图：\n IndexFileDeleter的构造函数流程图\n图2：\n\n点击查看大图\n 执行检查点（checkPoint）工作\n  在上一篇文章中，我们简单提了一下该流程点，其中checkPoint的作用及其逻辑在文章构造IndexWriter对象（八）已经介绍，不赘述，我们关注问题是为什么在当前流程点还要执行checkPoint的工作，这也是上一篇文章遗留的问题。\n为什么这里还要执行一次checkPoint的工作：\n  先给出源码中的注释：\nAlways protect the incoming segmentInfos since sometime it may not be the most recent commit\n  上述注释中的segmentInfos即图2流程图中的准备数据SegmentInfos，该段注释展开后的具体内容描述的是如果最后一次commit（索引目录segments_N中N值最大的那次提交）中不包含该SegmentInfos信息，那么为了防止SegmentInfos对应的索引信息因为某些索引删除策略IndexDeletionPolicy被删除，故需要执行checkPoint的工作。\n  我们以一个例子来描述对应的场景，该例子中的oldIndexWriter使用的索引删除策略是NoDeletionPolicy，完整demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/PersistentSnapshotDeletionPolicyTest.java 。\n图3：\n\n  图3中，在执行了第56行以及第62行的代码的oldIndexWriter.commit()方法后，索引目录中生成了两个段，如下所示：\n图4：\n\n  接着在执行了第63行的代码后，我们通过索引删除策略PersistentSnapshotDeletionPolicy对索引目录中最新的一次提交生成一个快照，该提交即segments_2，并通过PersistentSnapshotDeletionPolicy.snapshot()方法获得一个IndexCommit对象，并且在第75行我们将这个IndexCommit对象作为构造newIndexWriter的配置，此时索引目录中的内容如下所示：\n图5：\n\n  图5中snapshots_0即生成的快照。\n  IndexCommit对象中的索引信息即快照对应的索引信息，即segments_2对应的索引信息，对应的索引文件即_0.cfe、_0.cfs、_0.si、_1.cfe、_1.cfs、_1.si，如下图所示：\n图6：\n\n点击查看大图\n  图6中，根据SegmentCommitInfo的SegName字段获得对应的索引文件.si。\n  我们顺便给出segments_1包含的索引信息，在后面的流程中会用到：\n图7：\n\n点击查看大图\n  接着在执行了第70行的删除文档操作后，由于文档0跟文档1都满足该删除条件，即文档0跟文档1中都包含域名为&quot;author&quot;，域值为&quot;Lucy&quot;的信息，那么在执行了第71行的oldIndexWriter.commit()后，生成的第三个段中就不会包含文档1以及文档2的信息，即不会包含以_0为前缀和以_1为前缀的段的信息，索引目录中的内容如下所示：\n图8：\n\n  segments_3包含的索引信息如下所示：\n图9：\n\n点击查看大图\n  图9中以_2为前缀的段的索引信息即在图3中文档2对应的内容。\n  接着执行图3中第77行代码构造newIndexWriter，执行完下图中红色框标注的流程后，会根据索引目录中的segments_N文件对其对应的索引文件执行计数+1的操作：\n图10：\n\n  根据图8的索引目录中的内容，有segments_1、segments_2、segments_3共三个segments_N文件，他们对应的索引文件的计数如下所示：\n表1：\n\n\n\n段名/索引文件\n_0.cfs\n_0.cfe\n_0.si\n_1.cfs\n_1.cfe\n_1.si\n_2.cfs\n_2.cfe\n_2.si\n\n\n\n\nsegments_1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\nsegments_2\n1\n1\n1\n1\n1\n1\n0\n0\n0\n\n\nsegments_3\n0\n0\n0\n0\n0\n0\n1\n1\n1\n\n\n计数和值\n2\n2\n2\n1\n1\n1\n1\n1\n1\n\n\n\n  根据图6、图7、图9，每个segments_N文件对应的索引文件计数和如上所示，它描述了这些索引文件被引用的次数，故称为计数引用。\n  在继续介绍之前，我们先介绍下图10中绿色标注的流程点执行CommitPoint的删除工作：\n\n根据索引删除策略，那些应该被删除的提交在蓝色标注的流程点执行索引删除策略执行结束后，这些提交只是被添加到待删除队列中，并没有正在被删除，真正的删除工作是在流程点执行CommitPoint的删除工作完成的\n\n如何执行删除工作：\n  其过程就是对所有待删除的提交对应的索引文件执行计数-1的操作，如果计数值为0，那么索引文件就会被删除。\n  我们回到图3的例子，当执行到图10中的蓝色标注的流程时，如果图3中的newIndexWriter使用的是KeepOnlyLastCommitDeletionPolicy，该索引删除策略描述的是只保留最新的提交，即只保留segments_3对应的索引信息，segments_1跟segments_2对应的索引信息都需要被删除，故这两个段在流程点执行索引删除策略中被添加到待删除队列，如果不执行执行检查点(checkPoint)工作，而是直接执行绿色标注的流程点，那么根据表1中的内容，索引文件_1.cfs、_1.cfe、_1.si由于在执行计数-1的操作后，计数值都变为0而被删除，但由于构造当前newIndexWriter对象使用了IndexCommit（快照snapshot的索引信息）配置，该对象对应的索引信息是segments_2，而segments_2中包含索引文件_1.cfs、_1.cfe、_1.si的索引信息，如果被删除，那么索引信息就会被破坏，所以我们必须在执行CommitPoint的删除工作之前先执行执行检查点（checkPoint）来增加segments_2对应的索引文件的计数值，而这就是注释所谓的&quot;Always protect the incoming segmentInfos since sometime it may not be the most recent commit&quot;。\n  在执行完执行检查点（checkPoint）工作工作后，索引文件的计数值如下所示：\n表2：\n\n\n\n计数值/索引文件\n_0.cfs\n_0.cfe\n_0.si\n_1.cfs\n_1.cfe\n_1.si\n_2.cfs\n_2.cfe\n_2.si\n\n\n\n\ncheckPoint前的计数值\n2\n2\n2\n1\n1\n1\n1\n1\n1\n\n\ncheckPoint后的计数值\n3\n3\n3\n2\n2\n2\n1\n1\n1\n\n\n\n  由表2可以看出，只有segments_2对应的索引文件的计数值都被+1，而segments_3对应的索引文件则不变。\n  最后在执行执行CommitPoint的删除工作的流程点之后，索引文件的计数值如下所示：\n表3：\n\n\n\n计数值/索引文件\n_0.cfs\n_0.cfe\n_0.si\n_1.cfs\n_1.cfe\n_1.si\n_2.cfs\n_2.cfe\n_2.si\n\n\n\n\ncheckPoint前的计数值\n3\n3\n3\n2\n2\n2\n1\n1\n1\n\n\ncheckPoint后的计数值\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n  由表3可以看出，由于索引文件_0.cfs、_0.cfe、_0.si被segments_1跟segments_2计数引用，所以他们被执行了两次计数-1操作，而索引文件_1.cfs、_1.cfe、_1.si只被segments_2计数引用，故只执行了一次计数-1操作，那么快照信息，即segments_2对应的索引信息就能被正确的保留了下来。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"构造IndexWriter对象（四）","url":"/Lucene/Index/2019/1125/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  本文承接构造IndexWriter对象（三），继续介绍调用IndexWriter的构造函数的流程。\n 调用IndexWriter的构造函数的流程图\n图1：\n\n 根据不同的OpenMode执行对应的工作\n  在上一篇文章中，我们介绍了执行CREATE模式下的工作，故继续介绍执行APPEND模式下的工作。\n 执行APPEND模式下的工作的流程图\n图2：\n\n StandardDirectoryReader是否为空？\n图3：\n\n  在图1的流程点获取IndexCommit对应的StandardDirectoryReader，如果用户通过IndexWriterConfig.setIndexCommit(IndexCommit commit)设置了IndexCommit，那么Lucene会尝试根据该IndexCommit获得一个StandardDirectoryReader，它描述了IndexCommit中包含的索引信息（主要是SegmentInfos对象的信息，见构造IndexWriter对象（三）关于SegmentInfos对象的介绍）。\n  StandardDirectoryReader为空的情况有两种：\n\n用户没有设置IndexCommit\n用户设置了IndexCommit，但是IndexCommit中没有StandardDirectoryReader对象的信息\n\n为什么IndexCommit中可能会没有StandardDirectoryReader对象的信息：\n  IndexCommit类其实是一个抽象类，它的类图关系如下所示：\n图4：\n\n ReaderCommit、CommitPoint\n  ReaderCommit类跟CommitPoint类中包含的内容如下图所示\n图5：\n\n  故如果是ReaderCommit对象，那么就可以获得StandardDirectoryReader对象，而ReaderCommit对象则是通过StandardDirectoryReader.getIndexCommit()方法获得，由于该方法的实现很简单，故直接给出：\n图6：\n\n  图6为StandardDirectoryReader类中的方法，可以看出，StandardDirectoryReader对象通过调用getIndexCommit()方法，构造了一个新的ReaderCommit对象，并且将自己（this指针）作为ReaderCommit的成员变量之一，即图5中红框标注。\n  尽管无法通过CommitPoint对象中获得StandardDirectoryReader对象，但这里仍然要说下CommitPoint对象是什么生成的。\n  CommitPoint对象生成点有两处：\n\n生成IndexFileDeleter对象期间：这个时机点即图1中的流程点生成对IndexFileDeleter，故我们这里先暂时不展开\n执行commit()操作期间：图7是执行IndexWriter.commit()的两阶段提交的流程图，并且红框标注的流程点为CommitPoint对象生成的时机，该流程点的具体介绍可以看文档提交之commit（二）\n\n图7：\n\n SnapshotCommitPoint\n  SnapshotCommitPoint实际是采用装饰者模式来实现额外的功能，该类中的成员变量很少，故直接给出：\n图8：\n\n  根据图8可以看出，它封装了另一个IndexCommit对象（见图7的红色标注的流程在文章文档提交之commit（二）中的介绍），实际上该对象就是 CommitPoint。\n  至于它实现了哪些额外的功能，在后面的流程中会展开介绍，这里我们只需要知道它同CommitPoint一样，无法提供StandardDirectoryReader对象。\n  由于SnapshotCommitPoint封装了CommitPoint对象，所以它的生成时机同样在图7中的红色标注流程点。\n 配置检查2\n图9：\n\n  我们先介绍下能获得StandardDirectoryReader对象的情况，需要对用户的配置信息进行三项检查，如下所示\n图10：\n\n  图10中，reader即StandardDirectoryReader对象、commit即IndexCommit对象、这里没什么好介绍的。\n 用StandardDirectoryReader初始化一个新的SegmentInfos对象\n图11：\n\n  在文章构造IndexWriter对象（三）中我们介绍执行CREATE模式下的工作时说到，该模式下初始化一个新的SegmentInfos对象时，它不包含任何的索引信息，而在APPEND模式下，则是用StandardDirectoryReader中的索引信息来初始化一个新的SegmentInfos对象，即所谓的&quot;追加&quot;。\n 获得回滚（rollback）信息\n图12：\n\n  上文中根据IndexCommit获得的StandardDirectoryReader，它包含的SegmentInfos在后面的流程中将会作为回滚内容，而在这个流程中，最重要的一步是检查SegmentInfos中包含的索引信息对应的索引文件是否还在索引目录中。\n为什么要执行这个检查：\n  我们首先根据下面两张图来介绍下SegmentInfos对象跟索引文件segments_N、索引文件.si以及其他索引文件的关系：\n图14：\n\n  图14为索引目录中的索引文件，并且存在两个段。\n图15：\n\n  SegmentInfos对象是索引文件segments_N和索引文件.si在内存中的表示，图14中的提交中包含了两个段，即以_0跟_1开头的两个段，所以索引文件segments_1中有两个SegmentCommitInfo字段，接着根据SegmentCommitInfo中的SegName字段，该字段的值描述的是该段对应的所有索引文件的前缀值（见文章索引文件.si中的介绍），即_0，那么就可以在索引目录中找到 索引文件_0.si，而在 索引文件_0.si的Files字段（图15中红框标注）中存储了其他索引文件的名字，同样地根据这些索引文件的名字在索引目录中读取到所有的索引信息。\n  另外图15中SegmentCommitInfo中的两个字段FieldInfosFies、UpdatesFiles也是存储了索引文件的名字，当一个段中的DocValues发生变更时，变更的信息也用索引文件描述，并且索引文件的名字存储在这两个字段里。\n  从上文的描述可以看出，尽管我们通过IndexCommit可以获得SegmentInfos信息，但是该对象只是描述了它对应的索引文件有哪些，并不具有这些索引文件真正的数据，故可能在获得IndexCommit之后，索引又发生了变化，例如又出现了新的提交，那么根据默认的索引删除策略（见文档提交之commit（二）中关于执行检查点(checkPoint)工作d的介绍），segments_1文件就会被删除，当执行回滚操作时就无法获得真正的索引数据。如果出现在这个情况，那么在当前流程点会抛出如下的异常：\nthrow new IllegalArgumentException(&quot;the provided reader is stale: its prior commit file \\&quot;&quot; + segmentInfos.getSegmentsFileName() + &quot;\\&quot; is missing from index&quot;);\n  其中segmentInfos.getSegmentsFileName()指的就是segment_1文件。\n  感兴趣的同学可以执行这个demo：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/index/StaleIndexCommit.java ，该demo模拟了上述异常的抛出。\n 同步SegmentInfos以及回滚信息中SegmentInfos中的部分信息\n图16：\n\n为什么StandardDirectoryReader中可能没有IndexWriter对象：\n  在近实时搜索NRT系列文章中我们说到可以通过下面四种open()方法获得一个StandardDirectoryReader，其中：\n\n方法一：DirectoryReader.open(final Directory directory)\n方法二：DirectoryReader.open(final IndexCommit indexCommit)\n方法三：DirectoryReader.open(final IndexWriter indexWriter)\n方法四：DirectoryReader.open(final IndexWriter indexWriter, boolean applyAllDeletes, boolean writeAllDeletes)\n\n  其中通过方法一根方法二获得的StandardDirectoryReader对象中是没有IndexWriter对象的，即使方法二的参数indexCommit对象中有IndexWriter对象。\n为什么持有（引用）IndexWriter对象的StandardDirectoryReader需要执行图16中的两个同步操作：\n  源码中是这么说的：\n// In case the old writer wrote further segments (which we are now dropping)\n  其中old Writer指的就是StandardDirectoryReader中的IndexWriter对象，上述注释的意思是为了能处理old writer可能生成的新提交（一个或多个），并且该提交是需要丢弃的。\n  该注释的详细意思就是：我们使用的IndexCommit参数对应的索引信息可能不是old writer最新的提交对应的索引信息，那么比IndexCommit更加新的的提交（一个或多个）都应该丢弃（dropping），为了能正确的处理那些应该被丢弃的段，我们需要上面图16中的两个更新操作。\n为什么执行图16的两个更新操作就能正确的处理那些应该被丢弃的段：\n  处理的时机点在后面的流程中，到时候再介绍。\n  另外图16中两个同步的内容即version、counter、generation，在构造IndexWriter对象（三）文章中介绍了这三个值，这里不赘述。\n 设置回滚\n  该流程跟构造IndexWriter对象（三）文章中的中的设置回滚是相同的意思，将流程点获得回滚（rollback）信息的信息更新到rollbackSegments。\n  在设置回滚后，下一次commit前出现任何的错误，都可以回到当前设置的回滚状态，如果某次提交成功了，那么rollbackSegment会被重新设置该次提交。\n 结语\n  基于篇幅，剩余的内容将在下篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["indexWriter"]},{"title":"查询TopN的优化之NumericDocValues（二）（Lucene 8.9.0）","url":"/Lucene/Search/2021/0629/%E6%9F%A5%E8%AF%A2TopN%E7%9A%84%E4%BC%98%E5%8C%96%E4%B9%8BNumericDocValues%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  在上一篇文章的结尾，我们总结了使用NumericDocValues优化查询TopN的原理：假设查询TopN的排序规则为按照正排值从小大小的顺序，即正排值越小，优先级越高。故在开启优化后，当收集器收到一个文档号，先根据文档号从正排索引中拿到正排值，在满足某些条件后，根据正排值，通过查询BKD树获取所有小于该正排值的文档集合，该文档集合用于生成一个新的迭代器。随后每次传入到收集器的文档号将会从新的迭代器中获取，达到所谓的skip non-competitive documents的效果。\n  上文的描述中，我们需要对两个问题进一步展开介绍：\n\n问题一：满足什么条件后会生成一个新的迭代器\n问题二：如何生成一个新的迭代器\n\n 问题一：满足什么条件后会生成一个新的迭代器\n  满足的条件很苛刻，本文中只挑选出部分条件，且这些条件必须同时满足。完整的内容可以阅读源码：NumericComparator类中的updateCompetitiveIterator()方法。\n 条件一：Collector已经收集了N个文档号\n  只有在收集器收集了N个文档号后才会考虑是否需要生成一个新的迭代器。对应代码中通过判断用于收集文档信息的优先级队列queue是否full来判断。\n 条件二：是否允许使用PointValues来优化\n  在上一文章中说过，开启优化的其中一点是需要显示指定开启优化。在显示指定后，对应代码中一个布尔类型的enableSkipping会被置为true。\n 条件三：Collector是否已经处理了totalHitsThreshold个文档号\n  totalHitsThreshold是创建实现TopN的Collector（TopFieldCollector类的create方法）时，允许用户指定的一个int类型参数。totalHitsThreshold描述的是Collector至少处理了totalHitsThreshold个文档号后才会开启优化。本文介绍的优化是其中一种。另外还有基于IndexSort优化会在后续的文章中介绍。\n  在Lucene 7.2.0之后，查询TopN首次引入了允许提前结束Collector收集文档号的优化（见文章Collector（三）），即在已经收集到了全局最具competitive的N个文档号，Collector不用再处理剩余的文档号。这个优化会导致一个用户体验的问题，有些用户使用的场景需要记录hit count ，即命中的文档数量（满足用户设置的查询条件的文档数量），提前退出会导致没法将所有满足查询条件的文档号传入到Collector，使得Collector中的totalHits（传入到Collector的文档号数量）的值总是小于等于hit count的，最终使得用户无法通过Collector获得精确的（accurate）hit count。\n  所以在这次优化中同时增加了一个用户可以配置的布尔参数trackTotalHits，如果参数为true，那么当Collector已经收集到了TopN的文档号，并且即使这N个文档号已经是全局最具competitive的集合，Collector仍然继续收集其他的文档号（只统计totalHits），最终使得totalHits的数量能等于hit count。\n  随后在LUCENE-8060讨论下，最终在Lucene8.0.0之后，用int类型的参数totalHitsThreshold替换了trackTotalHits，使得既能让用户获得想要的hit count，又能在开启优化后，减少一定的Collector中处理的文档号数量。当totalHitsThreshold的值大于等于满足查询条件的文档数量时，其相当于trackTotalHits置为true。\n 条件四：是否超过迭代器的更新次数\n  在Collector收集文档号期间，当达到条件三或者达到条件一并且当前需要更新queue中堆顶元素时，Collector会尝试更新迭代器。每次尝试更新迭代器会使用一个int类型的updateCounter统计尝试更新的次数。如果满足下列的条件，那么不会生成一个新的迭代器：\nupdateCounter &gt; 256 &amp;&amp; (updateCounter &amp; 0x1f) != 0x1f\n 条件五：估算新的迭代器中的文档号数量是否低于阈值\n  在上文四个条件都满足的情况下，才需要考虑最后一个条件。从条件五中可知我们需要了解两个内容：如何估算新的迭代器中的文档号数量、如何设定阈值。\n 如何估算新的迭代器中的文档号数量\n  如果当前的排序规则是从小到大的升序，那么条件一中提到的queue中的堆顶元素，即堆中竞争力最低的（weakest competitive ）的正排值，它就是堆中的最大值，我们称之为maxValueAsBytes。估算的逻辑为从BKD树中统计出比maxValueAsBytes小的正排值的数量estimatedNumberOfMatches，注意的是estimatedNumberOfMatches是一个估算值。\n  统计estimatedNumberOfMatches的逻辑就是深度遍历BKD树，其详细遍历过程见文章索引文件的读取（一）之dim&amp;&amp;dii的介绍，我们通过一个例子简单的概述下。\n 例子\n  BKD树中存放了[1, 100]共100个正排值，其中maxValueAsBytes的值为60。\n图1：\n\n 访问根节点\n  maxValueAsBytes与根节点的关系是CELL_CROSSES_QUERY（见文章索引文件的读取（一）之dim&amp;&amp;dii的介绍），那么依次访问根节点的左右子节点：节点一、节点八。\n 访问节点一\n  由于maxValueAsBytes比节点一的最大值还要大，即maxValueAsBytes与节点一的关系是CELL_INSIDE_QUERY。此时可以累加计算estimatedNumberOfMatches的值，该值为节点三、节点四、节点六、节点七四个叶子节点中点数据的数量总和。在源码中，默认每个子节点中的点数据数量最大值为512，故计算方式为：512 * 叶子节点数量。\n 访问节点八\n  由于节点一以及它所有子节点都处理结束，故下一个访问节点为节点八。\n  maxValueAsBytes与节点八的关系是CELL_CROSSES_QUERY，那么将依次访问节点八的左右子节点：节点九、节点十二。\n 访问节点九\n  maxValueAsBytes与节点九的关系是CELL_CROSSES_QUERY，那么将以此访问节点十跟节点十一。\n 访问节点十\n  maxValueAsBytes与节点十的关系是CELL_CROSSES_QUERY，注意的是由于节点十是叶子节点，在源码中，不会通过遍历叶子节点中的点数据来获得一个准确的estimatedNumberOfMatches，其计算方式为叶子节点中的默认点数据数量最大值的一半，即(512 + 1) / 2。\n 访问节点十一、十二\n  maxValueAsBytes与节点十一、节点十二的关系是CELL_OUTSIDE_QUERY，即maxValueAsBytes比节点十一、节点十二的最小值还要小，故直接返回。\n  最终，累加在各个节点获得的estimatedNumberOfMatches作为新的迭代器中的文档号数量的估算值。\n 如何设定阈值\n  阈值threshold的计算基于当前迭代器的开销值iteratorCost（见文章迭代器中关于开销cost的介绍），如果获取了新的迭代器，那么iteratorCost会被更新为新的迭代器的开销值：\nfinal long threshold = iteratorCost &gt;&gt;&gt; 3;\n  如果estimatedNumberOfMatches的值大于等于，那么将不会更新迭代器。\n 问题二：如何生成一个新的迭代器\n  当问题一中所有条件都满足后，那么随后将根据maxValueAsBytes再次遍历BDK树，这次的遍历将精确的获取所有大于maxValueAsBytes的正排值对应的文档号。在遍历的过程中，使用文档号收集器获取一个文档集合，并用这个集合生成一个新的迭代器。随后下一次传给Collector收集器的文档号将会从新的迭代器中获取。\n 一些其他细节\n  另外使用了一个int类型的maxDocVisited记录了Collector目前处理过的最大文档号，使得新的迭代器不会收集Collector已经处理过的文档号。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","optimization","topN","NumericDocValues"]},{"title":"查询TopN的优化之NumericDocValues（一）（Lucene 8.9.0）","url":"/Lucene/Search/2021/0621/%E6%9F%A5%E8%AF%A2TopN%E7%9A%84%E4%BC%98%E5%8C%96%E4%B9%8BNumericDocValues%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  在索引阶段，我们可以在每一篇文档中添加一条或多条DocValues信息（正排），使得在查询阶段，当收集器Collector收集到满足查询条件的文档号后，可以根据文档号找到对应的正排信息，并依据正排信息对查询结果进行排序。\n图1：\n\n  图1中添加了3篇文档，红框标注即正排信息，我们在查询条件中设置一个排序规则：\n图2：\n\n  在图2的排序规则下，返回的结果为：\n图3：\n\n 查询TopN的性能（优化前）\n  基于目前Lucene的查询模块设计，整个查询过程可以按照获取迭代器和收集器处理文档的这两个大的功能块进行划分。\n 获取迭代器\n  获取迭代器描述的是从倒排信息中或者存储数值类型的BKD树中找到满足查询条件的文档号集合，该集合用于生一个迭代器。随后迭代器会按照文档号从小到大的顺序（在设置了段的排序IndexSort后，顺序可能会发生变化，见文章Collector（三）中的预备知识的介绍）依次取出每一个文档号，并将文档号送到收集器Collector中。\n 收集器处理文档\n  收集器Collector会从迭代器中依次的获得有序的文档号，执行排序、过滤等操作。\n 问题\n  上文介绍的查询模块设计会存在这个问题（为了便于介绍，我们暂时不考虑设置了段排序的情况）：假设我们有10000篇文档号，排序方式为按照正排值升序，并且文档0中的正排值为0，文档1中的正排值为1，剩余的9998篇文档的正排值都大于1。如果我们只要Top2的结果，那么很明显，最好的期望的处理方式应该是收集器Collector在处理完文档0跟文档1后就不再处理剩余的其他文档了。然而在优化之前，由于依次传入到收集器Collector的文档号是从小到大是有序的，但是文档号对应的正排值是无法保证有序的，意味着收集器Collector只有处理完所有的文档号才能实现正确的Top2。在文章Collector（三）中详细介绍了优化前利用NumericDocValues获取TopN的过程，可以先行阅读下。\n 查询TopN的性能（优化后）\n  在Lucene8.6.0后，当使用NumericDocValues实现TopN的查询时，可以跳过那些没有竞争力（uncompetitive）的文档集合，快速实现TopN的查询。我们先给出一个例子，比较下优化前后的性能差别，demo的地址：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.9.0/src/main/java/index/NumericDocValuesTopNOptimization.java。\n图4：\n\n图5：\n\n 开启优化\n  开启优化需要满足以下两点：定义相同域名的PointValues、显示指定开启优化\n\n定义相同域名的PointValues：图4中，我们在文档中定义了一个域名为&quot;sortField&quot;的NumericDocValues，那么需要同时定义一个具有相同域名的PointValues，即图4中红框标注\n显示指定开启优化：在图4中的第64行，该方法描述的是允许使用排序优化，具体内容看图6中的注释\n\n图6：\n\n  图6的注释说到：使用域名相同的doc values跟points存储相同的数值，就可以使用排序优化。图4中的第53、54行的代码IntPoint跟NumericDocValuesField存储了相同的数值sortValue。\n 优化\n/**  * Informs the comparator that the skipping of documents should be disabled. This function is  * called by TopFieldCollector in cases when the skipping functionality should not be applied or  * not necessary. An example could be when search sort is a part of the index sort, and can be  * already efficiently handled by TopFieldCollector, and doing extra work for skipping in the  * comparator is redundant.  */ public void disableSkipping() &#123;&#125;\n  从图5的比较中可以看到，大大的减少了在收集器中处理的文档数量，意味着查询性能得到了很大的提高。\n  其优化过程可以用一句话来概括：利用PointValues实现正排信息的排序，通过PointValues更新迭代器，最新的迭代器中的文档对应的正排信息都是有竞争力的（competitive）。\n  正如上文开启优化中介绍的那样，需要在文档中增加一个与NumericDocValues相同域名的PointValues。因为从存储NumericDocValues的数据结构中可以看出，正排信息的值以及对应的文档号在索引文件.dvd、.dvm中是按照文档号从小到大存储的，而在PointValues中，在生成一棵BKD树过程期间，节点划分前先会对点数据进行排序，然后根据中位数，将点数据集合划分为两块，用于构建左右子节点（见文章索引文件的生成（九）的介绍），故最终生成的BKD树的所有叶子节点从左到右是按照点数据有序存储的。\n  我们假设查询TopN的排序规则为按照正排值从小大小的顺序，即正排值越小，优先级越高。故在开启优化后，当收集器收到一个文档号，先根据文档号从正排索引中拿到正排值，在满足某些条件后，根据正排值，通过查询BKD树获取所有小于该正排值的文档集合，该文档集合用于生成一个新的迭代器。随后每次传入到收集器的文档号将会从新的迭代器中获取，达到所谓的skip non-competitive documents的效果。\n 结语\n  基于篇幅，我们将在下一篇文章中，继续介绍如何更新迭代器，在哪些情况下触发迭代器的更新等内容。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","optimization","topN","NumericDocValues"]},{"title":"查询原理（一）","url":"/Lucene/Search/2019/0820/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  从本篇文章开始介绍Lucene查询阶段的内容，由于Lucene提供了几十种不同方式的查询，但其核心的查询逻辑是一致的，该系列的文章通过Query的其中的一个子类BooleanQuery，同时也是作者在实际业务中最常使用的，来介绍Lucene的查询原理。\n 查询方式\n  下文中先介绍几种常用的查询方式的简单应用：\n\nTermQuery\nBooleanQuery\nWildcardQuery\nPrefixQuery\nFuzzyQuery\nRegexpQuery\nPhraseQuery\nTermRangeQuery\nPointRangeQuery\n\n TermQuery\n图1：\n\n  图1中的TermQuery描述的是，我们想要找出包含**域名（FieldName）为“content”，域值（FieldValue）中包含“a”的域（Field） **的文档。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/TermQueryTest.java。\n BooleanQuery\n图2：\n\n  BooleanQuery为组合查询，图2中给出了最简单的多个TermQuery的组合（允许其他查询方式的组合），上图中描述的是，我们期望的文档必须至少（根据BooleanClause.Occur.SHOULD）满足两个TermQuery中的一个，如果都满足，那么打分更高。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/BooleanQueryTest.java。\n WildcardQuery\n  该查询方式为通配符查询，支持两种通配符：\n// 星号通配符 *public static final char WILDCARD_STRING = &#x27;*&#x27;;// 问号通配符 ?public static final char WILDCARD_CHAR = &#x27;?&#x27;;// 转义符号（escape character）public static final char WILDCARD_ESCAPE = &#x27;\\\\&#x27;;\n  星号通配符描述的是匹配零个或多个字符，问号通配符描述的是匹配一个字符，转义符号用来对星号跟问号进行转移，表示这两个作为字符使用，而不是通配符。\n图3：\n\n  问号通配符的查询：\n图4：\n\n  图4中的查询会匹配文档3，文档1。\n  星号通配符的查询：\n图5：\n\n  图4中的查询会匹配文档0、文档1、文档2、文档3。\n  转义符号的使用：\n图6：\n\n  图4中的查询会匹配文档3。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/WildcardQueryTest.java。\n PrefixQuery\n  该查询方式为前缀查询：\n图7：\n\n  图7中的PrefixQuery描述的是，我们想要找出包含域名为“content”，域值的前缀值为&quot;go&quot;的域的文档。\n  以图3为例子，图7的查询会匹配文档0、文档1。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/PrefixQueryTest.java。\n FuzzyQuery\n  该查询方式为模糊查询，使用编辑距离来实现模糊匹配，下面的查询都是以图3作为例子：\n图8：\n\n  图8中的各个参数介绍如下：\n\nmaxEdits：编辑距离的最大编辑值\nprefixLength：模糊匹配到的term的至少跟图8中的域值&quot;god&quot;有两个相同的前缀值，即term的前缀要以&quot;go&quot;开头\nmaxExpansions：在maxEidts跟prefixLength条件下，可能匹配到很多个term，但是只允许处理最多20个term\ntranspositions：该值在本篇文档中不做介绍，需要了解确定型有穷自动机的知识\n\n  图8中的查询会匹配文档0、文档1。\n图9：\n\n  图9中的方法最终会调用图8的构造方法，即maxExpansions跟transpositions的值会使用默认值：\n\nmaxExpansions：默认值为50\ntranspositions：默认值为true\n\n  图9中的查询会匹配文档0、文档1。\n图10：\n\n  图10中的方法最终会调用图8的构造方法，即prefixLength、maxExpansions跟transpositions的值会使用默认值：\n\nprefixLength：默认值为0\nmaxExpansions：默认值为50\ntranspositions：默认值为true\n\n  图10中的查询会匹配文档0、文档1、文档2、文档3。\n图11：\n\n  图11中的方法最终会调用图8的构造方法，即maxEdits、maxEprefixLength、maxExpansions跟transpositions的值会使用默认值：\n\nmaxEdits：默认值为2\nprefixLength：默认值为0\nmaxExpansions：默认值为50\ntranspositions：默认值为true\n\n  图10中的查询会匹配文档0、文档1、文档2、文档3。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/FuzzyQueryTest.java。\n RegexpQuery\n  该查询方式为正则表达式查询，使用正则表达式来匹配域的域值：\n图12：\n\n  图12中的RegexpQuery描述的是，我们想要找出包含**域名（FieldName）为“content”，域值（FieldValue）中包含以&quot;g&quot;开头，以&quot;d&quot;结尾，中间包含零个或多个&quot;o&quot;的域（Field）**的文档。\n  图12中的查询会匹配文档0、文档1、文档2。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/RegexpQueryTest.java。\n PhraseQuery\n图13：\n\n  该查询方式为短语查询：\n图14：\n\n  图14中，我们定义了两个Term，域值分别为&quot;quick&quot;、“fox”，期望获得的这样文档：文档中必须包含这两个term，同时两个term之间的相对位置为2 （3 - 1），并且允许编辑距离最大为1，编辑距离用来调整两个term的相对位置（必须满足）。\n  故根据图13的例子，图14中的查询会匹配文档0、文档1、文档2。\n图15：\n\n  图15中，我们另编辑距离为0，那么改查询只会匹配文档0、文档1。\n图16：\n\n  图16中，我们另编辑距离为4，此时查询会匹配文档0、文档1、文档2、文档3。\n  这里简单说下短语查询的匹配逻辑：\n\n步骤一：找出同时包含&quot;quick&quot;跟&quot;fox&quot;的文档\n步骤二：计算&quot;quick&quot;跟&quot;fox&quot;之间的相对位置能否在经过编辑距离调整后达到查询的条件\n\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/PhraseQueryTest.java。\n TermRangeQuery\n图17：\n\n  该查询方式为范围查询：\n图18：\n\n  图18中的查询会匹配文档1、文档2、文档3。\n  在后面的文章中会详细介绍TermRangeQuery，对这个查询方法感兴趣的同学可以先看Automaton，它通过确定型有穷自动机的机制来找到查询条件范围内的所有term。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/TermRangeQueryTest.java。\n PointRangeQuery\n图19：\n\n  该查询方式为域值是数值类型的范围查询（多维度查询）：\n图20：\n\n  PointRangeQuery用来实现多维度查询，在图19中，文档0中域名为&quot;coordinate&quot;，域值为&quot;2, 8&quot;的IntPoint域，可以把该域的域值看做是直角坐标系中一个x轴值为2，y轴值为8的一个坐标点。\n  故文档1中域名为&quot;coordinate&quot;的域，它的域值的个数描述的是维度的维数值。\n  在图20中，lowValue描述的是x轴的值在[1, 5]的区间，upValue描述的y轴的值在[4, 7]的区间，我们期望找出由lowValue和upValue组成的一个矩形内的点对应的文档。\n图21：\n\n  图21中红框描述的是lowValue跟upValue组成的矩形。\n  故图20中的查询会匹配文档1。\n  在后面的文章中会详细介绍PointRangeQuery的查询过程，对这个查询方法感兴趣的同学可以先看Bkd-Tree以及索引文件之dim&amp;&amp;dii，这两篇文章介绍了在索引阶段如何存储数值类型的索引信息。\n  该查询方式的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/PointRangeQueryTest.java。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","search"]},{"title":"查询原理（三）","url":"/Lucene/Search/2019/0823/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E4%B8%89%EF%BC%89/","content":"  本文承接查询原理（二），继续介绍查询原理。\n 查询原理流程图\n图1：\n\n点击查看大图\n 是否使用多线程\n图2：\n\n  在生成IndexSearcher对象时，用户可以提供参数ExecutorService，调用下面的构造函数，来实现多线程搜索：\npublic IndexSearcher(IndexReader r, ExecutorService executor) &#123;    ... ...&#125;\n  当索引目录中有多个段时，把对一个段的搜索（search by segment）作为一个任务（task）交给线程池，最后在合并（reduce）所有线程的查询结果后退出；如果不使用多线程，那么单线程顺序的对每一个段进行搜索，在遍历完所有的段后退出。\n  无论是否使用多线程，其核心流程是对一个段进行处理，故下文中我们只介绍单线程的流程，并且在最后介绍图1中的流程点合并查询结果。\n 是否还有未查询的段\n图3：\n\n  当索引目录中有多个段，那么每次从一个段中查询数据，直到所有的段都处理结束。\n 生成BulkScorer\n图4：\n\n  该流程点将会获得查询对应的文档和词频，这些信息被封装在BulkScorer中。\n  如图5所示，不同的查询方式（见查询原理（一））有不一样BulkScorer对象（通过装饰者模式层层包装BulkScorer来实现多样的BulkScorer），在本篇文章中只根据图6、图7的中的BooleanQuery对应的BulkScorer展开介绍，其生成的BulkScorer对象为ReqExclBulkScorer。\n  图5中是BulkScorer的类图：\n图5\n\n图6，索引阶段的内容：\n\n图7，查询阶段的内容：\n\n  注意的是本文中的例子跟查询原理（一）中的BooleanQuery例子是不同的。\n ReqExclBulkScorer\n  图8中描述的是ReqExclBulkScorer中包含的两个对象，其中BooleanScorer描述了BooleanClause.Occure.SHOULD的TermQuery的信息，BlockDocsEnum描述了BooleanClause.Occure.MUST_NOT的TermQuery的信息。\n图8\n\n BooleanScorer\n图9\n\n  图9中描述的是BooleanScorer中的主要信息，其包含的BulkScorerAndDoc的个数跟图7中BooleanClause.Occure.SHOULD的TermQuery的个数一致。\n图10\n\n  图10中描述的是BulkScorerAndDoc中包含的主要信息。\n cost、next\n  这两个值在这里我先暂时不给出解释，因为在下一篇文章中才会使用到这两个值。\n docDeltaBuffer、freqBuffer\n  docDeltaBuffer、freqBuffer都是int数组类型，其中docDeltaBuffer中存放的是文档号，freqBuffer中存放的是词频\n  按照图7中的例子，4个子查询对应的BlockDocsEnum中包含的两个数组docDeltaBuffer、freqBuffer如下所示：\n 子查询1\n图11\n\n 子查询2\n图12\n\n 子查询3\n图13\n\n 子查询4\n图14\n\n  图14中我们没有列出该查询对应的freqBuffer，原因是图7中子查询4属于BooleanClause.Occure.MUST_NOT，我们只关心这个查询对应的文档号。\n docDeltaBuffer、freqBuffer的关系\n  两个数组的同一个数组下标值对应的数组元素描述的是域值在文档号中的词频，如图15所示：\n图15\n\n  在图15中，结合图6，描述的是域值&quot;h&quot;在文档3中的词频是2。\n  docDeltaBuffer、freqBuffer数组的信息是通过读取索引文件.doc获得的。\n图16\n\n  docDeltaBuffer、freqBuffer数组的信息要么分别从图16中红框标注的PackedDocDeltaBlock跟PackedFreqBlock中获取，要么分别从DocDelta跟Freq获得，其选择方式和读取索引文件.doc的过程不在本篇文章中介绍\n  在索引文件之doc文章中我们提到，文档号使用差值存储，所以实际在搜索阶段，我们获得的docDeltaBuffer数组中的数组元素都是差值，还是以图7中的子查询1为例，它获得真正的docDeltaBuffer数组如下所示：\n图17\n\n  注意的是图17中使用差值存储的docDeltaBuffer数组，它的第一个数组元素是原始值。\n  为什么要使用差值存储：\n\n压缩数据，减少内存/磁盘占用\n\n  去重编码(dedupAndEncode)的文章中详细介绍了差值存储，感兴趣的可以了解下。\n 关于BulkScorer\n  在查询原理（二）的文章中我们提到，在生成Weight的阶段，除了文档号跟term在文档中的词频这两个参数，我们已经获得了计算文档打分的其他条件，而在生成BulkScorer的过程中，我们获得了每一个子查询对应的文档号跟词频，所以在图1的生成Weight跟生成BulkScorer两个流程后，我们获得了一个段中文档打分需要的所有条件。\n  最后给出完整ReqExclBulkScorer包含的主要信息：\n图18\n\n 结语\n  本篇文章中我们只介绍了在生成BulkScorer的过程中，我们获得了每一个子查询对应的文档号跟词频，实际上还有几个重要的内容没有讲述，这些内容用来描述在Collector处理查询结果的流程中如何对每一个子查询的文档号进行遍历筛选（即上文中为介绍的 next、cost的信息），基于篇幅，这些内容在下一篇文章中展开介绍。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","search"]},{"title":"查询原理（二）","url":"/Lucene/Search/2019/0821/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  在查询原理（一）的文章中，我们介绍了几种常用查询方式的使用方法，从本篇文章开始，通过BooleanQuery来介绍查询原理。\n 查询原理流程图\n图1：\n\n点击查看大图\n 执行IndexSearcher的search()方法\n  根据用户提供的不同参数IndexSearcher类提供了多种search( )方法：\n\n分页参数：当用户提供了上一次查询的ScoreDoc对象就可以实现分页查询的功能，该内容的实现方式已经在Collector（二）中介绍，不赘述\n排序参数：用户通过提供Sort参数使得查询结果按照自定义的规则进行排序，默认使用TopFieldCollector对满足查询条件的结果进行排序。\nCollector参数：用户通过提供Collector收集器来自定义处理那些满足查询条件的文档的方法，如果不提供，那么默认使用TopFieldCollector或者TopScoreDocCollector，如果用户提供了Sort参数，那么使用前者，反之使用后者\nTopN参数：用户通过提供该参数用来描述期望获得的查询结果的最大个数\n\n  至于使用上述不同的参数对应哪些不同的search( )就不详细展开了，感兴趣的同学可以结合上述的参数介绍及源码中的几个search( )方法，相信很容易能理解。\n 重写Query\n  每一种Query都需要重写（rewrite）才能使得较为友好的接口层面（api level）的Query完善成一个&quot;最终的（final）“Query，如果这个Query已经是&quot;最终的”，就不需要重写，这个最终的Query在源码注释中被称为primitive query。\n  图2中定义了一个TermQuery（接口层面（api level）的Query），它**显示的（explicit）**描述了满足查询条件的文档必须包含一个域名（FieldName）为&quot;content&quot;，域值（FIeldValue）中包含&quot;a&quot;的term（对域值分词后的每一个结果称为term）的域，我们称这种TermQuery是&quot;最终&quot;的Query。在TermQuery中，我们能显示的知道，查询的term为&quot;a&quot;。\n图2：\n\n  图3中定义了一个PrefixQuery（前缀查询，见查询原理（一）），它描述了满足条件的文档必须包含一个域名为&quot;content&quot;，域值中包含前缀为&quot;go&quot;的term的域，相比较TermQuery，这个查询没有显示的在用户使用的接口层面（api level）描述我们要查询具体哪个term，我们称之为这不是一个“最终”的Query，故需要通过重写Query来完善成一个新的Query，先找到以&quot;go&quot;为前缀的所有term集合，然后根据这些term重新生成一个Query对象，具体过程在下文中展开。\n图3：\n\n  注意的是上述的介绍只是描述了重写Query的其中一个目的。\n  根据查询原理（一）中介绍的9种Query，我们分别来讲解这些Query的重写过程。\n TermQuery\n  TermQuery不需要重写。\n PointRangeQuery\n  数值类型的查询，它没有重写的必要。\n BooleanQuery\n  BooleanQuery的重写过程在BooleanQuery的文章中介绍，不赘述。\n PhraseQuery\n  PhraseQuery的重写会生成以下两种新的Query：\n\nTermQuery：图4中的PhraseQuery，它只有一个域名为“content”，域值为&quot;quick&quot;的term，这种PhraseQuery可以被重写为TermQuery，TermQuery是所有的查询性能最好的查询方式（性能好到Lucene认为这种查询方式都不需要使用缓存机制，见LRUQueryCache），可见这次的重写是一个完善的过程\n\n图4：\n\n\nPhraseQuery：图5中的PhraseQuery跟图6中的PhraseQuery，他们的查询结果实际是一致的，因为对于图5的PhraseQuery，它会在重写PhraseQuery后变成图6中的PhraseQuery，也就是这种查询方式只关心term之间的相对位置，对于图5中的PhraseQuery，在重写的过程中，&quot;quick&quot;的position参数会被改为0，&quot;fox&quot;的position参数会被改为2，由于本篇文章只是描述PhraseQuery的重写过程，对于为什么要做出这样的重写逻辑，在后面的文章中会展开介绍\n\n图5：\n\n图6：\n\n FuzzyQuery、WildcardQuery、PrefixQuery、RegexpQuery、TermRangeQuery\n  这几种Query的重写逻辑是一致的，在重写的过程中，找到所有的term，每一个生成对应的TermQuery，并用BooleanQuery封装。\n  他们的差异在于不同的Query还会对BooleanQuery进行再次封装，不过这不是我们本篇文章关心的。\n  下面用一个例子来说明上面的描述：\n图7：\n\n图8：\n\n  图8中我们使用TermRangeQuery对图7中的内容进行查询。\n图9：\n\n  图9中我们使用BooleanQuery对图7中的内容进行查询。\n  图8中TermRangeQuery在重写的过程中，会先找到&quot;bc&quot; ~ “gc&quot;之间的所有term（查找方式见Automaton），这些term即&quot;bcd”、“ga”、“gc”，然后将他们生成对应的TermQuery，并用BooleanQuery进行封装，所以图8中的TermRangeQuery相当于图9中的BooleanQuery。\n  不得不提的是，TermRangeQuery最终重写后的Query对象不仅仅如此，生成BooleanQuery只是其中最重要，最关键的一步，本篇文章中我们只需要了解到这个程度即可，因为在后面的文章会详细介绍TermRangeQuery。\n  所有的Query在查询的过程中都会执行该流程点，但不是重写Query唯一执行的地方，在构建Weight的过程中，可能还会执行重写Query的操作。\n 生成Weight\n  不同的Query生成Weight的逻辑各不相同，由于无法介绍所有的情况，故挑选了最最常用的一个查询BooleanQuery来作介绍。\n图10：\n\n图11：\n\n  图10跟图11分别是索引阶段跟查询阶段的内容，我们在查询阶段定义了一个BooleanQuery，封装了3个TermQuery，该查询条件描述的是：我们期望获得的文档中至少包含三个term，“h”、“f”、&quot;a&quot;中的一个。\n BooleanWeight\n  对于上述的例子中，该BooleanQuery生成的Weight对象如下所示：\n图12：\n\n  BooleanQuery生成的Weight对象即BooleanWeight对象，它由三个TermWeight对象组合而成，TermWeight即图11中封装的三个TermQuery对应生成的Weight对象。\n TermWeight\n  图13中列出了TermWeight中至少包含的几个关键对象：\n图13：\n\n Similarity\n  Similarity描述的是当前查询使用的文档打分规则，当前Lucene7.5.0中默认使用BM25Similarity。用户可以使用自定义的打分规则，可以在构造IndexSearcher后，执行IndexSearcher的search()方法前，调用IndexSearcher.setSimilarity(Similarity)的方法设置。Lucene的文档打分规则在后面的文章中会展开介绍。\n SimWeight\n图14：\n\n  图14中描述的是SimWeight中包含的几个重要的信息，这些信息在后面的流程中用来作为文档打分的参数，由于SimWeight是一个抽象类，在使用BM25Similarity的情况下，SimWeight类的具体实现是BM25Stats类。\n  我们以下图中红框标识的TermQuery为例子来介绍SimWeight中的各个信息\n图15：\n\n field\n  该值描述的是TermQuery中的域名，在图15中，field的值即“content”。\n idf\n  idf即逆文档频率因子，它的计算公式如下：\n(float) Math.log(1 + (docCount - docFreq + 0.5D)/(docFreq + 0.5D))\n\ndocCount：该值描述是包含域名为“content”的域的文档的数量，从图10中可以看出，文档0~文档9都包含，故docCount的值为10\ndocFreq：该值描述的是包含域值&quot;h&quot;的文档的数量，从图10中可以看出，只有文档0、文档8包含，故docFreq的值为2\n0.5D：平滑值\n\n boost\n  该值描述的是查询权值（即图17中打分公式的第三部分），boost值越高，通过该查询获得的文档的打分会更高。\n  默认情况下boost的值为1，如果我们期望查询返回的文档尽量是通过某个查询获得的，那么我们就可以在查询（搜索）阶段指定这个查询的权重，如下图所示：\n图16：\n\n  相比较图15，在图16中，我们使用BoostQuery封装了TermQuery，并显示的指定这个查询的boost值为100。\n  图16中的查询条件表达了这么一个意愿：我们更期待在执行搜索后，能获得包含&quot;h&quot;的文档。\n avgdl\n  avgdl（average document length，即图17中打分公式第二部分中参数K中的avgdl变量）描述的是平均每篇文档（一个段中的文档）的长度，并且使用平均每篇文档中包含域名为&quot;content&quot;的term的数量作为平均每篇文档的长度。\n  例如图7中的文档3，在使用空格分词器（WhitespaceAnalyzer）的情况下，域名为&quot;content&quot;，域值为&quot;a c e&quot;的域，在分词后，文档3中就包含了3个域名为&quot;content&quot;的term，这三个term分别是&quot;a&quot;、“c”、“e”。\n  avgdl的计算公式如下：\n(float) (sumTotalTermFreq / (double) docCount)\n\nsumTotalTermFreq：域名为&quot;content&quot;的term的总数，图7中，文档0中有1个、文档1中有1个，文档2中有2个，文档3中有3个，文档4中有1个，文档5中有2个，文档6中有3个，文档7中有1个，文档8中有8个，文档9中有6个，故sumTotalTermFreq的值为（1 + 1 + 2 + 3 + 1 + 2 + 3 + 1 + 8 + 6）= 28\ndocCount：同idf中的docCount，不赘述，该值为10\n\n cache\n  cache是一个数组，数组中的元素会作为BM25Similarity打分公式中的一个参数K（图17打分公式第二部分的参数K），具体cache的含义会在介绍BM25Similarity的文章中展开，在这里我们只需要了解cache这个数组是在生成Weight时生成的。\n weight\n  该值计算公式如下：\nidf * boost\n  图17是BM25Similarity的打分公式，它由三部分组成，在Lucene的实现中，第一部分即idf，第三部分即boost，至此我们发现，在生成Weight的阶段，除了文档号跟term在文档中的词频这两个参数，我们已经获得了计算文档分数的其他条件，至于为什么需要文档号，不是本篇文章关心的部分，再介绍打分公式的文章中会展开介绍，另外 idf跟boost即SimWeight中的信息，不赘述。\n图17：\n\n  图17源自于&lt;&lt;这就是搜索引擎&gt;&gt;，作者：张俊林。\n TermContext\n图18：\n\n  图18中描述的是TermContext中包含的几个重要的信息，其中红框标注表示生成Weight阶段需要用到的值，这些信息通过读取索引文件.tip、tim中的内容获得，其读取过程不再这里赘述（因为太复杂~），不过会在以后的文章中介绍，而每个变量的含义都在.tip、tim中详细介绍了，不赘述。\n  图19中是索引文件.tim的数据结构：\n图19：\n\n点击查看大图\n  上文中，计算idf（DocCount、DocFreq）跟avgdl（SumTotalTemFreq、DocCount）需要用到的信息在图19中用红框标注。\n  最后给出完整的BooleanWeight包含的主要信息：\n图20：\n\n 关于Weight的一些其他介绍\n  生成Weight的目的是为了不更改Query的属性，使得Query可以复用。\n  从Weight包含的主要信息可以看出，生成这些信息的目的就是为了文档打分，那如果我们不关心文档的打分，生成Weight的过程又是如何呢？\n  这个问题包含了两个子问题：\n  问题一：如何设置不对文档进行打分：\n\n我们在执行IndexSearcher的search()方法时，需要提供自定义的Collector，并且通过Collector.needsScores( )来设置为不对文档进行打分\n\n  问题二：生成的Weight有什么不同：\n\n由于不需要对文档进行打分，所以不需要用到TermContext，即TermContext为null，同时也不需要SimWeight，这两个信息都是为文档打分准备的\n如果设置了查询缓存（queryCache，默认开启），那么在不对文档打分的前提下，我们还可以使用查询缓存机制，当然使用缓存机制的前提是有要求的，感兴趣的同学可以看LRUQueryCache\n\n 结语\n  基于篇幅，本篇只介绍了图1中的三个流程点执行IndexSearcher的search()方法、重写Query、生成Weight，从本文的内容可以看出，想要深入了解查询逻辑的前提索引文件的数据结构。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","search"]},{"title":"查询原理（五）终","url":"/Lucene/Search/2019/0820/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E4%BA%94%EF%BC%89/","content":"  本文承接查询原理（四），继续介绍查询原理。\n 查询原理流程图\n图1：\n\n点击查看大图\n 合并查询结果\n  该流程点遍历所有子收集器的结果，对这些进行结果进行合并，合并过程比较简单，即利用优先级队列，由于太过简单，故不详细展开了。\n 遗留问题\n  在介绍这个遗留问题前，我们先说下在查询原理（三）的文章中，我们在介绍ReqExclBulkScorer时，有两个信息没有介绍，即cost、next，这两个信息用来选择哪些子查询进行处理。\n图2：\n\n图3：\n\n\ncost：该值描述的是满足子查询的文档个数，例如图3中的子查询3，因为docDeltaBuffer数组有7个数组元素（数组元素为满足子查询的文档号），故它的cost值为7\nnext：该值描述的是下一次处理的文档号，每当处理一篇文档，即更新到Bucket数组（见查询原理（四）），那么next被更新为下一个要处理的文档号，next的值是 一个递增值\n\n  在查询原理（四）的文章中，我们介绍了单线程下的查询原理的所有流程点，但还有一个很重要的逻辑没有介绍，那就是我们并没有介绍在还有未处理的子查询的情况下，如何选择哪个子查询进行处理，这个逻辑实际是个优化的过程，可能可以减少遍历区间（见查询原理（四））的处理，下面将填补这个坑。\n  上文的描述可以拆分两个问题，以图3为例：\n\n问题一：我们从子查询1、子查询2、子查询3中的哪个docDeltaBuffer开始遍历（选择子查询）\n问题二：是不是所有的docDeltaBuffer的每一篇文档号都要遍历（减少遍历区间）？\n\n  这两个问题可以转化为一道面试算法题，来了解面试者对Lucene的熟悉程度：有N个int类型数组，其中所有数组的数组元素都是有序（升序）的，同一个数组内的数组元素都是不重复的，设计一种方法，从这N个数组中找出所有重复（minShouldMatch 大于等于2）的数组元素。\n  对于上述的算法题，以图3为例，对于子查询1、子查询2、子查询3，总的时间复杂度至少为3个子查询的开销的和（子查询的开销即上文中的cost），即我们需要遍历每一个子查询对应的文档号。\n  Lucene是如何处理的：\n\n下图给出Lucene中处理方式的注释，原注释可以看这里：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/search/MinShouldMatchSumScorer.java中cost( )方法的注释\n\n图4：\n\n\n图4中描述了这么一个结论：如果BooleanQuery有n个子查询，它们之间为BooleanClause.Occur.SHOULD的关系，并且minShouldMatch为m，那么BooleanQuery的开销最少可以是( numScores - minShouldMatch + 1)个子查询的开销和，也就说在某些情况下我们不用遍历所有子查询对应的文档集合\n\nnumScores：子查询的个数n\nminShouldMatch：文档必须同时满足BooleanQuery中的至少m个子查询的查询条件\n\n\n\n  BooleanQuery的开销最少可以是( numScores - minShouldMatch + 1)个子查询的开销和是怎么推算出来的：\n\n包含n个子查询c1，c2，… cn且minShouldMatch为m的BooleanQuery，它可以转化为\n\n(c1 AND (c2..cn | msm = m - 1)) OR (!c1 AND (c2..cn | msm = m))，两块部分通过&quot;或的关系&quot;（OR）组合而成：\n\n(c1 AND (c2…cn|msm=m-1)) ：第一块部分描述了满足BooleanQuery查询要求的文档，如果满足子查询c1，那么必须至少满足c2…cn中任意m-1个子查询\n(!c1 AND (c2…cn|msm=m))：第二块部分描述了满足BooleanQuery查询要求的文档，如果不满足子查询c1，那么必须至少满足c2…cn中任意m个子查询\n\n根据两块部分的组合关系，BooleanQuery的开销是这两部分的开销和\n\n\n假设子查询c1，c2，… cn是按照cost（上文中已经介绍）升序排序的，那么对于第一块部分(c1 AND (c2…cn|msm=m-1)) ，由于c1的cost最小，并且必须满足c1的查询条件，所以第一块部分的开销就是c1的开销\n对于第二块部分(!c1 AND (c2…cn|msm=m))，它相当于一个包含 n -1 个子查询c2，… cn且minShouldMatch为m的子BooleanQuery，所以它又可以转化为(c2 AND (c3…cn|msm=m-1)) OR (!c2 AND (c3…cn|msm=m))\n以此类推如下所示\n\n图5：\n\n点击查看大图\n  在图5中，最后推导出BooleanQuery的总开销为 n-m+1个查询的开销，所以在Lucene中，它使用优先级队列head（大小为n-m+1）、tail（大小为m - 1）来存放子查询的信息（即查询原理（三）中的BulkScorerAndDoc），优先级队列的排序规则如下：\n\nhead：按照cost升序\ntail：按照next升序\n\n  当head中优先级最低的BulkScorerAndDoc的文档号不在遍历区间内，那么就可以跳过这个遍历区间，即使此时tail中还有其他的BulkScorerAndDoc。\n  这里提供一个demo：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/BooleanQuerySHOULDNOTTEST.java，这个demo对应图3的内容，根据子查询4，我们会获得3个遍历区间（见[查询原理（四）](https://www.amazingkoala.com.cn/Lucene/Search/2019/0827/89.html)）， 即[0，3)、[4，8)、[9，2147483647)，但是实际只需要遍历[0，3)、[4，8)，因为子查询1、子查询2会被放到head中，而这满足这两个查询的最大文档号为8，故不用处理[9，2147483647)的遍历区间，所以能降低时间复杂度，并且m的值越大，查询开销越小。\n 结语\n  至此，BooleanQuery的其中一种组合模式介绍完毕，其他的组合方式在后面不会详细展开，只介绍文档合并的逻辑，比如文档号合并（SHOULD）、文档号合并（MUST）。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","search"]},{"title":"查询原理（四）","url":"/Lucene/Search/2019/0827/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  本文承接查询原理（三），继续介绍查询原理。\n 查询原理流程图\n图1：\n\n点击查看大图\n  图2、图3是BooleanQuery的查询实例，在查询原理（三）中我们根据这个例子介绍了生成BulkScorer的流程点，本篇文章根据这个例子，继续介绍图1中剩余的流程点。\n图2：\n\n图3：\n\n Collector处理查询结果\n  在查询原理（三）文章中的生成BulkScorer流程点，我们获得了每一个子查询对应的文档号跟词频，见图4，结合查询原理（二）文章中的生成Weight流程点，我们就可以在当前流程点获得满足查询条件（图3）的文档集合及对应的文档打分值。\n图4：\n\n Collector处理查询结果的流程图\n图5：\n\n 是否还有遍历区间？\n图6：\n\n  图4中的子查询4，该查询对应的文档号集合不是用户期望返回的，那么可以根据这些文档号划分出多个左闭右开的遍历区间。\n  满足子查询4条件的文档号集合为3、8，故可以划分出3个遍历区间：\n\n[0，3)\n[4，8)\n[9，2147483647)：2147483647即 Integer.MAX_VALUE\n\n  随后从每个遍历区间中找到满足子查询1、子查询2、子查询3且minShouldMatch为2的文档号，minShouldMatch通过图3中的builder.setMinimumNumberShouldMatch方法设置，描述的是用户期望的文档必须至少满足子查询1、子查询2、子查询3中的任意两个（minShouldMatch）的查询条件。\n  为什么要使用遍历区间：\n\n降低时间复杂度：通过左闭右开实现过滤子查询4中的文档号（时间复杂度O(n)），否则当我们找出根据子查询1、子查询2、子查询3且minShouldMatch为2文档号集合后，每一个文档号都要判断是否满足子查询4的条件（时间复杂度O(n*m)），其中n跟m都为满足每一个子查询条件的文档数量，在图4中的例子中，n值为17（6 + 4 + 7，3个docDeltaBuffer数组的元素个数的和值），m值为2\n\n 处理子查询的所有文档号\n图7：\n\n  该流程处理子查询的所有文档号，先看下Bucket数组，该数组的数组下标用来描述文档号，数组元素是Bucket对象，它用来记录该文档的打分值跟满足子查询条件的查询个数，Bucket类如下所示：\nstatic class Bucket &#123;    double score;    int freq;&#125;\n\nscore：文档打分值\nfreq：满足子查询条件的查询个数\n\n freq\n  我们先说下freq，freq描述的是满足子查询条件的查询个数，例如图2中的文档8（文档号为8），因为文档号8中包含了&quot;h&quot;、“f”、“a”，所以它满足子查询1、子查询2、子查询3的三个查询条件，故文档号8对应的Bucket对象的freq值为3。\n score\n  图8为BM25模型的理论打分公式：\n图8：\n\n  图17源自于&lt;&lt;这就是搜索引擎&gt;&gt;，作者：张俊林。\n  图9为在Lucene7.5.0版本中BM25模型的具体实现BM25Similarity的公式：\n图9：\n\n  从图9的公式可以看出，一篇文档的打分值是一个累加值，累加的过程即更新Bucket数组的流程，如果一篇文档满足多个子查询的条件，那么该文档的打分值是每个子查询对这篇文档打分的和值。\n  例如图2中的文档0，该文档包含了两种term，分别是 “a”，“h”，故文档0满足图3中的两个子查询的条件，分别是子查询1、子查询3，所以文档0的打分值是两个查询对这篇文档打分的和值，最后将这个和值添加到Bucket数组的数组下标为0（因为文档0的文档号是0）的数组元素Bucket对象中，该对象的freq的值同理会被赋值为2。\n BM25Similarity打分公式\n图10：\n\n\nIdf、boost、avgdl、docCount、docFreq：这些值在查询原理（二）中计算SimWeight时获得，不赘述\nfreq：子查询条件中的域值在文档（正在计算打分的文档）中的词频，即图4中的freqBuffer数组的数组元素\nk1k_1k1​、b：BM25模型的两个调节因子，这两个值都是经验参数，默认值为k1k_1k1​ = 1.2、b = 0.75。k1k_1k1​值用来控制非线性的词频标准化（non-linear term frequency normalization）对打分的影响，b值用来控制文档长度对打分的影响\nnorm：该值描述的是文档长度对打分的影响，满足同一种查询的多篇文档， 会因为norm值的不同而影响打分值\n\ncache数组：在查询原理（二）文章中，我们简单的提了一下cache生成的时机是在生成Weight的流程中，下面详细介绍该数组。\n\ncache数组的数组下标normValue描述的是文档长度值，这是一个标准化后的值（下文会介绍），在Lucene中，用域值的个数来描述文档长度，例如图3中的子查询1，它查询的条件是域名为&quot;content&quot;，域值为&quot;h&quot;的文档，那么对于文档0，文档长度值为域名为&quot;content&quot;，term为&quot;h&quot;在文档0中的个数，即2；cache数组的数组元素即norm值\n上文说到域值的个数来描述文档长度，但是他们两个的值不总是相等，域值的个数通过标准化（normalization）后来描述文档长度，标准化的过程是将文档的长度控制在[1，255]的区间中，跟归一化的目的是类似的，为了平衡小文档相跟大文档的对打分公式的影响，标准化的计算方式不在本文中介绍，感兴趣的可以看https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/SmallFloat.java中的 intToByte4(int)方法，该方法的返回值与0XFF执行与操作后就得到标准化后的文档长度值\n根据标准化后的文档长度值（取值范围为[1，255]）就可以计算出norm中dl的值，dl为文档长度值对应的打分值，同样两者之间的计算方法不在本文中介绍，感兴趣可以看https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/SmallFloat.java中的byte4ToInt(byte)方法，图11给出了文档长度值跟dl之间的映射关系\n\n\n\n\n\n图11：\n\n  图11中，横坐标为文档长度值，纵坐标为dl，由于数据跨度太大，无法看出文档长度值较小的区间的趋势图，故图12给出的是文档长度值在[1，100]区间的映射图\n图12：\n\n  图13中，文档长度值在[1，50]区间的映射图，能进一步看出文档长度值小于等于40时，dl正比于文档长度值\n图13：\n\n  图10中的normValue根据文档号从索引文件.nvd中获得，图14中用红框标识了一篇文档的文档号及其对应的normValue。\n图14：\n\n 处理Bucket数组\n图15：\n\n  处理Buck数组的过程就是找出所有满足图3中minShouldMatch的文档，然后分别将文档号交给Collector收集器处理\n  某个遍历区间内的生成Bucket数组的过程在文档号合并（SHOULD）的文章中已经介绍，不过注意的是，在那篇文档中，没有考虑文档的打分值，故Bucket数组只介绍了freq。由于那篇中没有类似图3中的子查询4，所以遍历区间为[0，2147483647]。\n  对于本篇文章中图2、图3的例子，在遍历区间为[0，3)对应生成的Bucket数组如下所示，相比较文档号合并（SHOULD）中的内容，我们增加每篇文档的打分值，列出遍历区间为[0，3)的Bucket数组：\n 遍历区间[0，3)\n图16：\n\n  在图16中，文档0跟文档2的freq 大于等于minShouldMatch（2），故这两篇文档满足图3中的查询要求。\n 结语\n  至此，我们介绍了单线程下的查询原理的所有流程点，但还有一个很重要的逻辑没有介绍，那就是在图5的是否还有未处理的子查询流程点，我们并没有介绍在还有未处理的子查询的情况下，如何选择哪个子查询进行处理，这个逻辑实际是个优化的过程，可能可以减少遍历区间的处理，以图2、图3为例，尽管根据子查询4，我们得出3个遍历区间，实际上我们只要处理[0，3)、[4，8)这两个逻辑区间，至于原因会在下一篇文档中展开。\n  图2.、图3的demo点击这里：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/query/BooleanQuerySHOULDNOTTEST.java。\n  另外对于多线程的情况，图1中的合并查询结果流程也留到下一篇文章中介绍。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["query","search"]},{"title":"段的多线程查询（一）（Lucene 9.6.0）","url":"/Lucene/Search/2023/0626/%E6%AE%B5%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%9F%A5%E8%AF%A2%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  前段时间有个朋友问到我：对多个段进行查询时，为什么在定义IndexSearcher时使用了Executor后，相比较单个线程轮询方式查询相同的多个段，查询速度并没有提升，有时候还下降了？本篇文章会介绍多线程查询中的一些知识点，给与大家在解决性能问题时提供一些思路。\n 查询流程图\n图1：\n\n  图1中的查询流程图中，如果未使用多线程，那么Lucene将依次处理每一个段，否则每个线程会负责查询某个段，并在所有线程执行结束后对结果进行合并。\n Slice\n  图1中的描述说到每个线程会负责一个段的查询工作，这其实只是一种特例（下文会介绍）。实际上，在初始化IndexSearcher对象阶段会所有的段进行划分，一个或者多个段根据划分规则被分配不同的Slice中，随后每一个Slice会被分配一个线程进行查询。\n 划分规则\n\n排序：首先将所有段根据段中包含的文档数量降序排序。\n分配：从包含文档数量最大的段开始，依次处理每一个段，将段分配到Slice中。每一个Slice需要同时满足下面的条件：\n\n如果段中的文档数量大于250000（该值为源码中的MAX_DOCS_PER_SLICE），则分配到一个新的Slice中，并且不再将更多的段分配到这个Slice中\n如果某个段分配到一个Slice后，Slice中的所有段的的文档总数大于等于250000（该值为源码中的MAX_DOCS_PER_SLICE），那么不再将更多的段分配到这个Slice中\nSlice中段的数量必须小于等于5（该值为源码中的MAX_SEGMENTS_PER_SLICE）\n\n\n\n 例子\n  如果索引文件中有以下9个段，即图3中的demo：\n图2：\n\n  图2中，段0中的文档数量超过了250000（MAX_DOCS_PER_SLICE），所以Slice 0中只有这一个段；段2中的文档数量小于250000，所以它被分配到Slice 1后，Slice 1还可以被分配其他的段，由于段1被分配到Slice 1后，Slice 1中的文档数量超过了250000，所以不再分配更多的段到Slice 1中；Slice 2中被分配段4~段8这5个段后，尽管此时Slice 2中的文档数量未超过250000，但是Slice 2中段的数量已经达到了5（MAX_DOCS_PER_SLICE），所以不再分配更多的段到Slice 2中。\n  上文中说到，每个线程负责一个Slice的查询工作。因此每个线程负责的Slice实际上处理的段的数量是不一样的。\n 查询性能差异\n  接下来介绍下一个多线程查询不及单线程的例子\n图3：\n\n  这个例子中，索引文件内有9个段，一共658000篇文档。查询条件是获取Top 1000的文档号，分别使用多线程跟单线程，查看Collector中分别处理的文档数量，如下所示：\n图4：\n\n  可见使用多线程查询需要处理的文档数量（totalHits）反而比单线程多，那性能当然是不及单线程的。\n early termination机制\n  Lucene中，会使用totalHits（图3中第65、69行代码）来统计Collector处理的文档数量，注意的是，由于在查询条件中定义了TopN，所以在Collector的处理逻辑中，收集完TopN篇文档后，如果能确定剩余满足查询条件的文档相比较已收集的TopN中的文档都不具备竞争力（competitive），那么就可以提前退出Collector，即early termination机制，直接返回TopN中的文档即可。\n  图3中使用单线程的查询条件是MatchAllDocsQuery，那么在Collector中，文档号越小的文档越具备竞争力（基于MatchAllDocsQuery对应的ConstantScoreWeight，这里不展开介绍），所以Collector中收集完文档号区间为0~999的文档后，就可以提前结束查询，而不需要全量处理索引中的658000篇文档。\n  对于使用多线程查询，根据图2的介绍，会对4个Slice进行并发查询。early termination机制只能作用在每一个Slice中，这个例子中每个Slice都分别实现了early termination，也就是每个Slice的Collector在收集完TopN后就提前退出，因此对于4个Slice，总的totalHits为4000（4 * topN）。\n early termination的实现原理\n  因为篇幅原因就不在本文中展开了，简单的提一下。尽管每个Collector的实现原理各不相同，但early termination的核心内容都是通过调整DocIdSetIterator来减少后续待处理的文档集合的大小，比如在TopScoreDocCollector中，当收集了TopN篇文档后并且确定剩余的文档不具备竞争力后，就会将DISI调整为空的DISI，即DocIdSetIterator.empty()。\n 合并Slice的查询结果\n  这个过程可以简单的描述为将多个Slice中的TopN根据排序规则，以及比较规则来获得最终的TopN。\n\n排序规则：例如我们在查询阶段定义了Sort对象，那么在合并查询结果时会使用该Sort对象，如果没有排序规则，或者该排序规则无法用于比较出优先级，则接着使用比较规则。\n比较规则：Lucene提供了两个内置的比较规则，对应源码中的SHARD_INDEX_TIE_BREAKER和DOC_ID_TIE_BREAKER，以及这两个比较规则的组合使用，即默认的比较规则DEFAULT_TIE_BREAKER，直接给出源码：\n\n/** Internal comparator with shardIndex */private static final Comparator&lt;ScoreDoc&gt; SHARD_INDEX_TIE_BREAKER =  Comparator.comparingInt(d -&gt; d.shardIndex);/** Internal comparator with docID */private static final Comparator&lt;ScoreDoc&gt; DOC_ID_TIE_BREAKER =  Comparator.comparingInt(d -&gt; d.doc);/** Default comparator */private static final Comparator&lt;ScoreDoc&gt; DEFAULT_TIE_BREAKER =  SHARD_INDEX_TIE_BREAKER.thenComparing(DOC_ID_TIE_BREAKER);\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开介绍。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["search","multiThread"]},{"title":"索引文件之cfs&&cfe","url":"/Lucene/suoyinwenjian/2019/0710/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bcfs&&cfe/","content":"  索引文件.cfs、.cfe被称为复合（compound）索引文件，在IndexWriterConfig可以配置是否生成复合索引文件，默认开启。\n  在前面的文章中，我们介绍了其他的索引文件，而复合索引文件则是将这些索引文件的数据组合到一个文件中，这种设计的目的是为了减少文件描述符的使用。\n  正如上文中描述的那样，复合文件实质是索引文件的组合，意思是无论是否设置了使用复合文件，总是先生成非复合索引文件，随后在flush阶段，才将这些文件生成.cfs、.cfe文件，其中.liv、.si索引文件不会被组合到.cfs、.cfe中。\n cfs文件的数据结构\n图1：\n\n FieldData\n图2：\n\n  FieldData为非复合索引文件的数据。 原非复合索引文件的Header、Footer会被重新计算，只保留有效数据区域IndexData，计算过程不展开介绍（暂时不感兴趣~）。\n 例子\n  如果一个段中有以下的非复合索引文件：\n图3：\n\n  生成符合索引文件后 .cfs的数据结构如下：\n图4：\n\n  遍历一个Set容器，容器的key为非复合索引文件的文件名，根据文件名将其索引信息添加到.cfs文件中，故非复合索引文件在.cfs文件的排列为遍历Set的顺序。\n  顺序并不重要，因为在读取阶段，总是一次性的读取.cfs文件中的所有内容。\n cfe文件的数据结构\n图5：\n\n FileCount\n  该值描述了复合索引文件中包含的非符合索引文件的种类数量，在图3的例子中，该值就是10。\n FileDataIndex\n图6：\n\n  FileDataIndex描述了某个非复合索引文件在.cfs文件中的数据区域。\n FileName、DataOffset、DataLength\n  FileName是非复合索引文件的部分文件名：\n在图3中，_0.tvd跟_0_Lucene50_0.tim对应的FileName分别是 .tvd、_Lucene50_0.tim。\n  DataOffset为在.cfs文件中的偏移位置，DataLength为非复合索引文件的数据长度，DataOffset跟DataLength就能确定非复合索引文件在.cfs文件中的数据区域。\n cfe、cfs文件的映射关系\n  根据图3中的例子给出以下的映射关系。\n图7：\n\n 结语\n  复合文件的数据结构过于简单，写这篇文档的目的是作为在后面介绍flush文章时的一个预备知识。\n点击下载附件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile"]},{"title":"索引文件之dim&&dii","url":"/Lucene/suoyinwenjian/2019/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdim&&dii/","content":"  从Lucene6.0开始出现点数据（Point Value）的概念，通过将多维度的点数据生成KD-tree结构，来实现快速的单维度的范围查询（比如 IntPoint.newRangeQuery）以及N dimesional shape intersection filtering。\n  索引文件.dim中的数据结构由一系列的block组成，在内存中展现为一颗满二叉树(单维度可能不是，这块内容会在介绍数值类型的范围查询时候介绍)，并且叶子节点描述了所有的点数据。\n  阅读本文章之前，必须先了解多维度的点数据是如何生成一个满二叉树，否则难以理解文章中的一些名词概念，而在本文中不会赘述这些名词。在Bkd-Tree中介绍了生成树的过程，请务必先看这篇文章。\n  索引结构的主要逻辑在下面的Java文件中，关键逻辑都加以了注释：https://github.com/luxugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter.java。\n  另外索引文件.dim、dii的生成过程见系列文章索引文件的生成（八）之dim&amp;&amp;dii。\n dim文件的数据结构\n图1：\n\n  上图中是单个域的.dim文件数据结构，下图为存在多个域的数据结构图。\n图2：\n\n LeafNodeData\n  当所有的点数据生成一颗满二叉树后，点数据会被分布到不同的叶子节点上，而LeafNodeData正是描述了一个叶子节点上的点数据的相关信息。\n图3：\n\n Count\n  当前叶子节点中有多少个点数据。\n DocIds\n  当前叶子节点中的点数据所属文档（document）的文档号。\n  根据文档号的大小以及在叶子节点中的排序，其数据结构也各有不同，其最终目的还是尽可能的减少空间存储。\n 文档号有序\n  叶子节点中的点数据是按照某个维度排序的，如果排序后的点数据对应的文档号正好也是有序的，那么会使用下面的数据结构。\n图4：\n\n 固定值：0\n  这是一个标志位，在读取阶段可以知道当前叶子节点中的点数据对应的文档号是有序的。\n DeltaDocId\n  由于文档号是有序的，所以实际存储是当前文档号都是与前一个文档号的差值，并且使用VInt存储，使得最好情况下只要1个字节就可以存储一个文档号。\n 文档号无序\n  在文档号无序的前提下，先计算出最大的文档号DocId maxDocId~maxDocId max可以最少用多少个字节表示。如果DocId maxDocId~maxDocId max ≤ 0xffffff，那么使用3个字节存储每一个文档号（图5），否则使用4个字节存储（图6）。\n图5：\n\n 固定值：24\n  这是一个标志位，在读取阶段可以知道当前叶子节点中的点数据对应的文档号是无序的，并且每一个文档号占用固定3个字节。\n DocId高2个字节、DocId低1个字节\n  由于使用固定3个字节存储一个文档号，所以高2个字节用short类型存储，低1个字节用byte字节存储。\n图6：\n\n 固定值：32\n  这是一个标志位，在读取阶段可以知道当前叶子节点中的点数据对应的文档号是无序的，并且每一个文档号占用4个字节。\n DocId\n  注意这里与图4中的DocId的区别，图6中使用的是int类型存储，而图4中使用的是VInt。\n PointValues\n  当前叶子节点中的点数据。\n图7：\n\n commonPrefixes\n图8：\n\n Length\n  计算出所有点数据的某个维度值的相同前缀的长度。\n Value\n  前缀值。\n BlockPackedValues\n  BlockPackedValues中存储了叶子节点中每个点数据的每个维度的值。\n  如果叶子节点中的点数据都是一样的，那么数据结构如下：\n图9：\n\n 固定值 -1\n  这是一个标志位，在读取阶段可以知道当前叶子节点中的点数据都是一样的， 并且点数据的值可以根据commonPrefixes的数据获得。\n图10：\n\n ActualBounds\n图11：\n\n  ActualBounds保存了当前叶子节点中每一种维度的最大值跟最小值，并且只保存后缀值，在读取阶段通过跟commonPrefixes就可以拼出原始值。\n SortedDim\n  叶子节点中的点数据是有序，选取其中一个维度作为排序规则，这个维度就是SortedDim。\n PackedValues\n图12：\n\n  由于叶子节点中的是按照点数据的SortedDim维度排序的，源码中采取了一种方式，目的是为了能尽可能的减少空间存储。这种方式就是：遍历SortedDim维度的值，比如当前遍历到第n个维度值，然后跟第n + 1个维度值比较一个字节，判断这两个字节是否相同，如果是相同的，那么这两个维度值是属于同一个PackedValuesRange(图12)的，而这个字节就是SortedDim维度的所有维度值的相同前缀的下一个字节，并且这个字节就是图12中的PrefixByte。\n PackedValuesRange、PrefixByte、RunLen、PackedValue\n  同一个PackedValuesRange中的点数据，它们不但拥有commonPrefixes中的SortedDim维度的长度为Length的相同前缀Value，而且还拥有一个额外的字节PrefixByte也是相同的，并且这个PrefixByte是SortedDim维度的所有维度值的相同前缀的下一个字节。\n图13：\n\n  在图13中，假设一个叶子节点只有4个点数据，并且维度0为SortedDim，那么4个点数据按照SortedDim进行排序，并且生成3个PackedValuesRange：\n\n点数据(2,4)为第一个PackedValuesRange\n点数据(3，8)和点数据(3，2)为第二个PackedValuesRange\n点数据(4，7)为第三个PackedValuesRange。\n\n  RunLen的值描述了PackedValuesRange中的点数据个数，对于第二个PackedValuesRange，RunLen的值为2，同时PrefixByte的值为3。\n  PackedValue即存储了某个PackedValuesRange中的所有点数据的所有维度值，当然维度值只存储后缀值。\n BDK\n图14：\n\n NumDims\n  点数据的维度个数。\n CountPerLeaf\n  每个叶子节点中的点数据数量。\n BytesPerDim\n  数值类型转化为字节的数量。\n NumLeaves\n  满二叉树中的叶子节点的数量。\n MinPackedValue\n  MinPackedValue中的每个维度的值都是所在维度的最小值。\n MaxPackedValue\n  MinPackedValue中的每个维度的值都是所在维度的最大值。\n PointCount\n  当前域中的点数据的数量。\n DocCount\n  包含当前域中的点数据域的文档数量。\n一篇文档中可以包含多个相同域名的点数据域，但是DocCount的计数为1。\n PackedIndex\n  PackedIndex存放了非叶节点的信息。\n图15：\n\n Length\n  用来描述PackedIndexValue的长度，在读取阶段用来确定从.dim文件读取的数据区间。\n PackedIndexValue\n  PackedIndexValue中保存了所有非叶节点的信息。\n  非叶节点的信息根据它的子树是否为叶子节点有着不同的数据结构，为了便于描述，根据不用的类型给予对应的字段名：\n\n非叶节点的子树是叶子节点，并且它是父节点的左子树：LeftSubtreeHasLeafChild\n非叶节点的子树是叶子节点，并且它是父节点的右子树：RightSubtreeHasLeafChild\n非叶节点的子树不是叶子节点，并且它是父节点的左子树：LeftSubtreeHasNotLeafChild\n非叶节点的子树不是叶子节点，并且它是父节点的右子树：RightSubtreeHasNotLeafChild\n\n图16：\n\n  上图中对这颗满二叉树进行前序遍历，将所有的非叶节点信息的写入到PackedIndexValue中。\n RightSubtreeHasLeafChild\n图17：\n\n  当前非叶节点的左右子树是叶子节点，同时它是父节点的右子树：\n LeftLeafBlockFP\n  当前非叶节点的左子树是叶子节点，LeftLeafBlockFP为叶子节点的的信息在.dim文件中的偏移值。\n Code\n  Code值是一个int类型的值，它由多个值的组成，公式如下：\nint code = (firstDiffByteDelta * (1+bytesPerDim) + prefix) * numDims + splitDim\n\nprefix：当前非叶节点的划分值与上一个非叶节点的划分值相同前缀的字节数\nsplitDim：非叶节点的划分维度\nfirstDiffByteDelta：当前非叶节点的划分值与上一个非叶节点的划分值第一个不相同的字节位置偏移\nbytesPerDim：表示一个维度值需要的字节数\nnumDims：点数据中的维度数量\n\n SplitValue\n  当前非叶节点划分值，前缀存储。\n RightLeafBlockFP\n  当前非叶节点的右子树是叶子节点，LeftLeafBlockFP为叶子节点的的信息在.dim文件中的偏移值。\n LeftSubtreeHasLeafChild\n图18：\n\n  当前非叶节点的左右子树是叶子节点，同时它是父节点的左子树：\n  字段含义同上。\n LeftSubtreeHasNotLeafChild\n图19：\n\n  当前非叶节点的左右子树不是叶子节点，同时它是父节点的左子树：\n  Code、SPlitValue字段含义同上。\n LeftNumBytes\n  当前非叶节点的所有子树的信息大小，在读取阶段，通过该字段从.dim中读取一个数据区间。\n RightSubtreeHasNotLeafChild\n图20：\n\n  当前非叶节点的左右子树不是叶子节点，同时它是父节点的右子树：\n  Code、SPlitValue、LeftNumBytes字段含义同上。\n LeftLeafBlockFP\n  当前非叶节点的最左叶子节点的信息在.dim文件中的偏移值。\n dim文件总数据结构\n  下面的数据结构不是唯一的.dim文件结构，我们选取了其中一种：\n图21：\n\n点击查看大图。\n dii文件的数据结构\n图22：\n\n Count\n  这次的IndexWriter写入的点数据域的种类数量。\n FieldNumber\n  域的编号。\n IndexFP\n  当前域的非叶节点的信息在.dim文件中的偏移位置（图1中的BKD）。\n 结语\n  无\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","dim","dii","indexFile"]},{"title":"索引文件之doc","url":"/Lucene/suoyinwenjian/2019/0324/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdoc/","content":"  索引文件.doc中按块（block）的方式存放了每一个term的文档号、词频，并且保存skip data来实现块之间的快速跳转，本篇只介绍.doc文件的数据结构，其生成过程见文章索引文件的生成（一）。\n doc文件的数据结构\n图1：\n\n  TermFreqs保存了term的所有文档号、词频信息，TermFreqs中按块存储，使用SkipData实现这些块之间的快速跳转。\n TermFreqs\n图2：\n\n PackedBlock\n每处理包含term的128篇文档，就将这些文档的信息处理为一个PackedBlock。\n PackedDocDeltaBlock\nPackedDocDeltaBlock存放了128篇文档的文档号，计算相邻两个文档号的差值后，利用PackedInts压缩存储。\n PackedFreqBlock\nPackedFreqBlock存放了term分别在128文档中的词频，利用PackedInts压缩存储。\n  这里注意是由于在每篇文档中的词频值无法保证递增，使用PackedInts只能压缩原始的词频值。\n VIntBlocks &amp;&amp; VIntBlock\n如果包含term的文档号不足128个，那么将这些文档的信息处理为一个VIntBlocks。(比如包含term的文档数量有200，那么前128篇文档的信息被处理为一个PackedBlock，剩余的72篇文档处理为72个VIntBlock，72个VIntBlock为一个VIntBlocks)\n DocDelta\n当前文档号跟上一个文档号的差值。\n Freq\nterm在当前文档中的词频。\n  在介绍SkipData前先介绍下跳表（SkipList）的概念，注意的是下图只是跳表的概述，并不是Lucene中的实现：\n图3：\n\n  在每一层中，每3个数据块就会在上一层中添加一个索引，实现了对数级别的时间复杂度。\n  关于跳表的详细介绍可以看文章索引文件的生成（三）之跳表SkipList。\n SkipData\n图5：\n\n SkipLevelLength\n当前层的跳表（skipList）数据长度，在读取的时候用来确定往后读取的一段数据区间。\n SkipLevel\nSkipLevel描述了当前层中的所有跳表真实数据\n SkipDatum\n当前层中每一个跳表信息按块处理为一个SkipDatum。\n DocSkip\n描述了当前SkipDatum指向的文档号，不过DocSkip的实际值是当前文档号与上一个SkipDatum的文档号差值，还是使用了差值存储。\n DocFPSkip\n每当处理128篇文档，在level = 0的跳表中就会生成一个SkipDatum，而DocFPSkip指向的就是存储这128篇文档的PackedBlock的起始位置。\n PosFPSkip\nPosFPSkip指向了.pos文件中一个位置。这个位置是PackedPosBlock(每128个position信息处理为一个PackedPosBlock)的起始位置\n PosBlockOffset\nPosBlockOffset描述的是上一条说的PackedPosBlock中的一个偏移位置。\n PayLength\nPayLength描述的是在.pay文件中的payload的信息，这段payload的信息跟上一条中位置信息是对应的。\n PayFPSkip\nPayFPSkip指向了.pay文件中一个位置。这个位置是PackedPayBlock(每128个offset信息处理为一个PackedPayBlock)的起始位置。\n SkipChildLevelPointer\n如果当前SkipDatum属于大于level = 0的较高层，那么SkipChildLevelPointer指向了下一层的某个位置。\n 多个域的doc文件的数据结构\n图6：\n 结语\n  .pos、.pay、.doc、.tim、.tip文件都是通过读取内存倒排表的过程中一起生成的，在处理完每个term的信息并写入.pos、.pay、.doc文件后，开始生成.tim、.tip文件，在最后的文章中会更新这部分内容。\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","doc","indexFile"]},{"title":"索引文件之fdx&&fdt","url":"/Lucene/suoyinwenjian/2019/0301/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bfdx&&fdt/","content":"  当STORE.YES的域生成了倒排表以后，将文档的域值信息写入到.fdt（field data）、.fdx（field index）文件中。\n 数据结构\n .fdt\n图1：\n\n### ChunkSize\n  ChunkSize作为一个参数，用来判断是否要生成一个Chunk，以及用来描述压缩存储域值信息的方式，后面会详细介绍。\n PackedIntsVersion\n  PackedIntsVersion描述了压缩使用的方式，当前版本中是VERSION_MONOTONIC_WITHOUT_ZIGZAG。\n Chunk\n图2：\n\n  在处理每一篇文档的过程中，如果满足以下任意一个条件，那么就将已处理的文档的域值信息生成一个chunk：\n\n已处理的文档数量达到128\n已处理的所有域值的总长度达到ChunkSize。\n\n DocBase\n  当前chunk中第一个文档的文档号（该文档号为段内文档号），因为根据这个文档号来差值存储，在读取的阶段需要根据该值恢复其他文档号。\n ChunkDocs\n图3：\n\n  ChunkDocs是一个numBufferedDocs跟slicedBit的组合值。ChunkDocs = (numBufferedDocs |slicedBit )。\n numBufferedDocs\n  numBufferedDocs描述了当前chunk中的文档数量。numBufferedDocs是一个 ≤ 128的值。\n slicedBit\n  如果待处理的域值信息的长度超过2倍的chunkSize（默认值 16384），那么需要分块压缩，下文会具体介绍。\n DocFieldCounts\n  根据chunk中包含的文档个数numBufferedDocs、每篇文档包含的存储域的数量numStoredFields分为不同的情况。\n numBufferedDocs的个数为1\n图4：\n\n##### numBufferedDocs的个数＞ 1 并且每篇文档中的numStoredFields都是相同的\n图5：\n\n  只要存储一个numStoredFields的值就行啦。\n numBufferedDocs的个数＞ 1 并且每篇文档中的numStoredFields不都相同的\n图6：\n\n  使用PackedInt来存储所有的numStoredFields，这里不赘述了，点击这里可以看其中的一种压缩方式。\n DocLengths\n  同DocFieldCounts类似，据chunk中包含的文档个数numBufferedDocs、每篇文档中域值信息的长度分为不同的情况。\n numBufferedDocs的个数为1\n图7：\n\n numBufferedDocs的个数＞ 1 并且每篇文档中的域值信息长度都是相同的\n图8：\n\n##### numBufferedDocs的个数＞ 1 并且每篇文档中的域值信息长度不都是相同的\n图9：\n\n  使用PackedInt来存储所有的域值信息长度，这里不赘述了，点击这里可以看其中的一种压缩方式。\n CompressedDocs\n图10：\n\n  CompressedDocs中使用LZ4算法将域值信息压缩存储。域值信息包含如下内容，字段Doc的数量对应为一个chunk中包含的文档数量：\n\n域的编号\n域值的类型：String、BinaryValue、Int、Float、Long、Double\n域值的编号跟域值的类型组合存储为FieldNumAndType\nValue：域值\n\n .fdt整体数据结构\n图11：\n\n  上图中是其中一种 .fdt文件数据结构。\n图12：\n\n Block\n图13：\n\n  在.fdt中，每当chunk的个数达到1024(blockSzie)，在.fdx文件中就会生成一个block，block中的信息作为索引来映射.fdt中的数据区间。\n BlockChunks\n  block中包含的chunk的个数，即1024个。\n DocBases\n图14：\n\n  DocBases中描述了文档号的信息。\n DocBase\n  block中第一个文档的文档号。用来在读取阶段，恢复所有chunk中其他被编码的文档号。\n AvgChunkDocs\n  AvgChunkDocs描述了block中平均一个chunk中包含的文档数。\n BitsPerDocBaseDelta\n  BitsPerDocBaseDelta描述了存储文档号的需要固定bit个数。\n DocBaseDeltas\n  一个block中用docBaseDeltas[]数组来存放每个chunk中的文档个数，而每一个chunk中的文档个数是不一样的，出于最大化优化空间存储，不直接对文档数量值进行存储，而是存储差值docDelta。又因为docBaseDeltas[]数组又不能保证数组元素递增，所以不能使用相邻数组元素的差值来作为docDelta，Lucene提供的方法就是计算docBaseDeltas[]中数组元素平均值avgChunkDocs，对每一个数组元素存储一个docDelta的值，docDelta的计算公式为：docDelta = ( docBase - avgChunkDocs * i), 其中i为数组的下标值，docBase是下标值为i的数组元素前所有的数组元素之和，然后对所有docDelta使用PackedInts进行压缩编码，即DocBaseDeltas。\n StartPointers\n图15：\n\n  StartPointers中描述了.fdt文件中每一个chunk的索引映射信息。\n StartPointerBase\n  当前block中第一个chunk的索引值。\n AvgChunkSize\n  block中平均每一个chunk的大小。\n BitsPerStartPointerDelta\n  存储每一个chunk大小需要固定bit个数。\n StartPointerDeltas\n  逻辑跟DocBaseDeltas一样，不赘述。\n ChunkCount\nchunk的个数。\n DirtyChunkCount\n在索引阶段，如果还有一些文档未被写入到索引文件中，那么在flush阶段会强制写入，并用该字段记录。\n .fdx整体数据结构\n图16：\n\n 结语\n  没啥要讲的。\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","fdx","fdt"]},{"title":"索引文件之fnm","url":"/Lucene/suoyinwenjian/2019/0606/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bfnm/","content":"  索引文件.fnm用来描述域信息（FieldInfo）\n 例子\n  为了便于介绍.fnm中的各个字段，给出下面的例子\n图1：\n\n fnm文件的数据结构\n图2：\n\n FieldsCount\n  FieldsCount描述的是.fnm中域的种类。\n Field\n图3：\n\n FieldName\n  该字段描述的是域名，例如图1中的&quot;author&quot;、“content”、&quot;abc&quot;都是FieldName。\n FieldNumber\n  域的编号，根据处理域的先后顺序，每个域都会获得一个从0开始递增的域的编号。\n FieldBits\n  该字段是一个组合值，它用来描述当前域是否有以下的属性：\n\n是否存储词向量(termVector)：0x1，词向量的介绍可以看这里索引文件之tvx&amp;&amp;tvd\n是否忽略域的norm值：0x2，用于域的打分的norm值的介绍可以看这里索引文件之nvd&amp;&amp;nvm\n是否带有payload：0x4，payload的介绍可以看这里索引文件之pos&amp;&amp;pay\n该域是否为软删除域(soft delete field)：0x8，这个概念在后面的文档中会介绍\n\n  在图1中，域&quot;content&quot;的FieldBits的值为 (0x1 | 0x2 | 0x4) = 0x0111。\n IndexOptions\n  该字段描述了当前域的索引选项(IndexOptions)，哪些倒排信息会被写入到索引文件中。IndexOptions有以下值：\n\n0：NONE\n1：DOCS\n2：DOCS_AND_FREQS\n3：DOCS_AND_FREQS_AND_POSITIONS\n4：DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS\n\n DocValuesBits\n  该字段占用一个字节，其中高4个bit用来描述是否记录norm，低4个bit用来描述DocValues类型，DocValues的类型包括以下类型，在DocValues对每一种都已介绍，不赘述：\n\n0：NONE\n1：NUMERIC\n2：BINARY\n3：SORTED\n4：SORTED_SET\n5：SORTED_NUMERIC\n\n DocValuesGen\n  该字段描述了DocValues类型的域的更新状态，比如我们调用IndexWriter.updateDocValues(…)方法后，那么DocValuesGen的值会变更，这里不展开介绍，在介绍IndexWriter时会详细介绍。\n Attributes\n  该字段描述了存储当前域的索引文件的格式(format)，比如说当前是一个DocValues的域，那么Attributes的字段会有下面的值：\n\nPerFieldDocValuesFormat.format：Lucene70\n\n  表示使用Lucene70这种格式来生成索引文件.dvd、.dvm。\n DimensionCount\n  该字段描述的是如果域为点数据类型，那么DimensionCount的值为点数据的维度，点数据以及维度的概念在Bkd-Tree以及索引文件之dim&amp;&amp;dii介绍不赘述，在图1中，IntPoint域即为点数据域，DimensionCount的值为3，因为有3，5，9共三个值。\n DimensionNumBytes\n  该字段描述的是每一个维度占用的字节个数（数值类型被编码为多个字节），同样已经在前面的文章中介绍了。\n fnm文件的总数据结构\n图4：\n\n点击下载Markdown文档\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","fnm"]},{"title":"索引文件之kdd&kdi&kdm（Lucene 8.6.0）","url":"/Lucene/suoyinwenjian/2020/1027/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bkdd&kdi&kdm/","content":"  从Lucene8.6.0开始，用于存储点数据（point value）的索引文件由原先的两个索引文件dim&amp;&amp;dii，改为三个索引文件kdd&amp;kdi&amp;kdm。由于生成kdd&amp;kdi&amp;kdm的过程基本上没有太大的变动，并且索引文件的数据结构中的字段也变化不大。故本文不会再详细介绍每一个字段的含义，即阅读本章前，最好先看下文章索引文件dim&amp;&amp;dii的数据结构，以及索引文件dim&amp;&amp;dii的生成过程以及索引文件dim&amp;&amp;dii的读取过程的系列文章，使得能理解优化的目的。当然在下文中，会结合一些issues来简单的叙述下优化的目的。\n  再次强调下，先阅读索引文件dim&amp;&amp;dii的生成过程以及索引文件dim&amp;&amp;dii的读取过程的系列文章，因为开始下笔写这篇文章的时候，我几乎忘光了之前写的这些东东，也就是相当于一片空白的重新复习了下这些系列文章， 发现看完后很容易的理解了（看来我的写作表达能力还阔以），哈哈😁。\n 索引文件的数据结构\n  我们先直接分别给出单个点数据域、多个点数据域的数据结构：\n 单个点数据域的数据结构\n图1：\n\n 多个点数据域的数据结构\n图2：\n\n  源码中对于这三个索引文件的简单描述：\n图3：\n\n  图3中红框标注.kdm应该是.kdd文件，可惜的是直到目前最新版本的Lucene 8.6.3，这里的书写错误依旧未被修正。\n\n索引文件.kdm（meta）中存储的是元数据，即描述点数据域的数据的信息，例如点数据的维度、每个维度占用的字节数等等，这些元数据存储在图1的Index字段中，下文中会进一步介绍该字段。\n索引文件.kdi（index）中存储的是内部节点的数据\n索引文件.kdd（data）描述的是叶子节点的数据\n\n  点数据的信息使用了Bkd-tree的树形结构存储，可以阅读文章Bkd-Tree简单了解下bkd的概念。\n  为了便于介绍，我们以单个点数据域的数据结构来展开介绍。\n 索引文件.kdd\n图4：\n\n  索引文件.kdd中存储了叶子节点的数据，其中字段LeafNodeData中包含的内容跟索引文件.dim中的LeafNodeData是相同的，如下图所示，详细的字段介绍见文章索引文件dim&amp;&amp;dii，这里不赘述。\n图5：\n\n 索引文件.kdi\n图6：\n\n  索引文件.kdi中存储了内部节点的数据，其中字段PackedIndexValue中包含的内容跟索引文件.dim中的PackedIndexValue是相同的，如下图所示，详细的字段介绍见文章索引文件dim&amp;&amp;dii，这里不赘述。\n图7：\n\n 索引文件.kdm\n图8：\n\n  索引文件.kdm中存储的是元数据，即描述点数据域的数据的信息，图8中，除了红框标注的几个字段，其他字段跟索引文件.dim中的BKD中的字段是相同的，如下图所示，详细的字段介绍见文章索引文件dim&amp;&amp;dii，这里不赘述。\n图9：\n\n  我们接着介绍图9中未被标注红框的字段。\n numDataDims、numIndexDims\n  这两个字段分别描述了叶子节点、内部节点的点数据维度数。\n FieldNumber\n  该字段描述的是域的编号，在Lucene 8.6.0之前，该字段存储在索引文件.dii中，如下所示：\n图10：\n\n  图10中，IndexFP字段用来指向图9中BKD的起始读取位置，该字段的作用相当于在Lucene8.6.0中的DataStartFP、IndexStartFP，见下文介绍。\n -1\n  该字段会写入一个数值类型的固定值-1，在读取索引文件.kdm期间，它作为一个结束标志位用来描述所有域的FieldNumber跟Index字段信息已经被读取结束。\n  结合图2中多个点数据域的索引文件.kdm，由于每个域的FieldNumber、Index字段占用的字节数量是相同的，所以在读取阶段，只要按照固定的长度读取字节流即可，当读取到值为-1时，说明读取结束。这块内容在类Lucene86PointsReader初始化Reader时读取，由于代码比较简单，故直接给出：\n图11：\n\n  图11中，第87行代码会读取索引文件.kdm的FieldNumber字段；第93行代码将会读取Index字段，字段中的数据将用于生成BKDReader，下文中还会进一步展开；第88行代码会判断是否读取了-1字段，如果满足那么就跳出while循环，然后继续执行第96、97行的代码，即分别读取索引文件.kdm中的KdiFileLength、KddFileLength字段。\n KdiFileLength、KddFileLength\n  这两个字段分别描述了索引文件.kdi、索引文件.kdd的文件长度，在读取阶段通过长度来检查这两个文件是否为合法的。同样的，检查的代码也相对简单，故直接给出：\n图12：\n\n  KdiFileLength、KddFileLength的值将作为参数expectedLength传入到图12的方法中，其中in.length()方法描述了某个索引文件的长度，通过in.length()跟expectedLength的长度比较，判断索引文件是否合法的。\n DataStartFP、IndexStartFP\n  这两个字段分别描述了某个点数据域的叶子节点、内部节点数据在索引文件.kdd、kdi中起始读取位置：\n图13：\n\n 优化目的\n  通过上文介绍大家可能会发现，用于存储点数据的索引文件在Lucene8.6.0中的主要变动就是将内部节点跟叶子节点的信息从索引文件.dim中进行了分离，这里做的目的是出于什么考虑呢？作者的详细解释见原文地址：https://issues.apache.org/jira/browse/LUCENE-9148 。\n  其优化的目的主要有以下两点：\n\n可以让用户充分利用MmapDirectory的preload功能，使得可以将叶子节点、内部节点的数据提前载入到内存，提高读取性能，在 Lucene8.4.0~Lucene 8.6.0这个版本区间，使用了off-heap机制载入图7中的PackedIndexValue字段的值，使得在搜索阶段才根据某个点数据域载入所属PackedIndexValue字段，并且内部节点跟叶子节点的数据都在同一个索引文件.dim中。\n更好的检查索引文件的合法性，在优化之前只能通过检查Footer来判断，关于索引文件的合法性的检查在后面的文章会展开。\n\n off-heap\n  对于off-heap在点数据域中的使用，可以分为三个阶段，另外下文中代码读取的索引文件为.dim或者.kdm：\n 不使用off-heap\n  在Lucene 8.4.0之前，不使用off-heap机制，即在生成DirectoryReader阶段，会把所有段中的所有点数据域的PackedIndexValue(图7中索引文件.dim的PackedIndexValue)读取到内存中。\n图14：\n\n图15：\n\n  在图14中，第86、87、89行代码分别读取了索引文件.dim中的pointCount、DocCount、Length字段，随后根据Length字段的值，往后读取Length个字节的数据，即PackedIndexValue字段的全量数据，并且最后写入到字节数组packedIndex。\n off-heap参数化\n  在Lucene 8.4.0~Lucene 8.6.0的版本期间，参数化选择是否使用off-heap：\n图16：\n\n  图16中，如果使用off-heap，那么会把文件指针指向图14的PackedIndexValue字段的起始读取位置，随后在搜索阶段，根据搜索条件中的点数据域，再读取出该域对应的PackedIndexValue的数据，否则在生成DirectoryReader阶段就将所有段中的所有点数据域的PackedIndexValue读取到内存中。\n  图16中第220行，使用offHeap取决于使用哪种Directory，例如使用MMapDirectory则会使用off-heap。\n 只使用off-heap\n  在Lucene 8.6.0的版本之后，直到目前最新版本的Lucene 8.6.3，总是使用off-heap：\n图17：\n\n  图17中，代码第93行的if语句判断的是当前读取的点数据的索引文件的版本号，如果是Lucene 8.6.0以以上，那么按照图8的索引文件.kdm读取，否则按照图14的索引文件.dim读取，这里说明Lucene 8.6.0兼容低版本的点数据域对应的索引文件。\n  图17中，代码第97、99行读取的是Lucene8.6.0之前的索引文件，可以看出把文件指针指向了PackedIndexValue字段，但并没有读取。\n  由于PackedIndexValue的数据不在索引文件.kdm中，所以只能使用off-heap。\n  另外版本号通过Header字段获取，其包含的其他内容不是很重要，故省略，如下图红框标注：\n图18：\n\n 结语\n  无\n点击下载附件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","kdd","kdi","kdm"]},{"title":"索引文件之liv","url":"/Lucene/suoyinwenjian/2019/0425/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bliv/","content":"索引文件.liv只有在一个segment中包含被删除的文档时才会生成，它记录了当前段中没有被删除的文档号。这里不会讨论一个segment是如何获得被删除的文档号，在后面的文章中，介绍IndexWriter.flush()时会详细介绍，本篇文章只介绍那些被删除的文档号生成的索引文件的数据结构。\n 预备知识\n介绍.liv文件的数据结构前，大家必须得了解Lucene的一个工具类FixedBitSet，这个类在源码中有大量的应用，是必须熟悉的一个工具。\n 数据结构\n图1：\n\n CurrentBits\nCurrentBits占固定8个字节，即写入的是一个long类型的值，每一个CurrentBits分别表示了FixedBitSet对象中的bits[]数组的元素。\n 例子\n图2：\n\n上图中添加了10篇文档，对应文档号0~9，然后在第84、85行执行了删除操作，即满足域名为&quot;content&quot;，域值为&quot;h&quot;或者&quot;f&quot;  的文档都会被删除，即文档号0、4、7会被删除。在删除操作以后，在查询阶段实际可以获得的文档号只有1、2、3、5、6、8、9。\n由于一共只有10篇文档，所以只要一个long类型的值就可以表示这些文档号，即FixedBitSet对象中的long bit[]数组只有一个元素，数组下标代表了文档号。\n图3：\n\n 结语\n.liv索引文件非常的简单，只要熟悉FixedBitSet的用法，相信其数据结构也一目了然。\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","liv","delete","softDeletes"]},{"title":"索引文件之nvd&&nvm","url":"/Lucene/suoyinwenjian/2019/0305/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bnvd&&nvm/","content":"nvd&amp;&amp;nvm用来存储域的标准化值(normalization values)，这两个索引文件记录了每一篇文档中每一种域的标准化值跟索引信息。在Lucene 7.5.0中，标准化值的计算实际就是统计一篇文档中某个域的域值，这个域值经过分词器处理后生成的term的个数，最后将term的个数通过intToByte4()的方法编码为一个 byte类型的值。标准化的过程在本篇文章中不作介绍，可以查看看BM25Similarity中的computeNorm()方法。我们可以自定义自己的Similarity来实现定制化的计算域的标准化值的逻辑。\n我们通过下面的例子来介绍nvd&amp;&amp;nvm文件的数据结构\n图1：\n\n 数据结构\n nvd\n图2：\n\n FieldData\nnvd文件在索引阶段按照添加域的顺序地分块地存储每一种域的信息，FieldData描述了一种域的所有信息，在例子中，共有三个FieldData块，分别描述了“content”、“author”、“attachment”三种域的信息。\n DocsWithFieldData\nDocsWithFieldData的数据结构根据包含当前域的文档个数numDocsWithValue分为三种情况\n numDocsWithValue == 0\nDocsWithFieldData无数据，通过nvm文件中存放固定值来描述当前情况。\n numDocsWithValue == maxDoc\nmaxDoc表示当前处理的文档总个数，这种情况说明所有文档都含有当前域，DocsWithFieldData无数据，通过在nvm文件中同样存放固定值来描述当前情况。在本文的例子中，域”author“属于当前情况。\n numDocsWithValue &lt; maxDoc\n这种情况说明并不是所有的文档都包含当前域，所以需要存储所有包含当前域的文档号。这里使用IndexedDISI类来实现对文档号docID的存储，IndexedDISI类会在随后的博客中介绍，在这里想说明的是，在索引阶段不同的添加document的方式会有不同数据结构的nvd文件产生，导致生成索引跟查询的性能各不相同。在本文的例子中，域&quot;content&quot;、”attachment“的DocsWithFieldData属于当前情况，会用当前的数据结构。\n图3：\n\n NormsData\n图4：\n\n Value\nValue为当前域在每一篇文档中的标准化值。\n nvm\n图5：\n\n FieldNumber\n域的编号，用来唯一标识一种域。\n DocsWithFieldAddress &amp;&amp; DocsWithFieldLength\n如果numDocsWithValue == maxDoc 或者numDocsWithValue == 0，那么DocsWithFieldAddress跟DocsWithFieldLength会被置为固定值。否则这两个值作为索引来映射nvd中的一块数据区域。\n图6：\n\n NumDocsWithField\nNumDocsWithField描述了包含当前域的文档个数。\n BytesPerNorm\n找出当前域在所有所属文档中的最大跟最小的两个标准化值来判断存储一个标准化值最大需要的字节数，这里还是出于优化索引空间的目的。由于最小值有可能是负数，所以不能仅仅靠最大值来判断存储一个标准化值需要的字节数。比如说最小值min为 -130(需要2个字节)，最大值max为5(需要1个字节)，那么此时需要根据min来决定字节个数。\n NormsAddress\nNormsAddress作为索引映射nvd中一块数据区域，这块数据区域即当前域在所有文档中的标准化值。\n图7：\n\n nvm&amp;&amp;nvd映射关系图\n图8：\n\n 集合图\n放一起 更直观~~\n图9：\n\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","nvd","nvm"]},{"title":"索引文件之fdx&&fdt&&fdm（Lucene 8.6.0）","url":"/Lucene/suoyinwenjian/2020/1013/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bfdx&&fdt&&fdm/","content":"  在索引阶段，如果某个域的属性中包含store，意味着该域的域值信息将被写入到索引文件fdx&amp;&amp;fdt&amp;&amp;fdm中，域的属性可以通过FieldType来设置，如下所示：\n图1：\n\n  图1中，域&quot;content&quot;跟域&quot;title&quot;的域值将被存储，即写入到索引文件fdx&amp;&amp;fdt&amp;&amp;fdm中，而域”attachment“则不会。\n  在文章索引文件之fdx&amp;&amp;fdt中介绍了Lucene 7.5.0版本存储域值对应的索引文件，该版本中使用了两个索引文件.fdx、.fdt存储域值信息，而从Lucene 8.5.0版本开始进行了优化，最终用三个索引文件.fdx、fdt、fdm三个索引文件来存储域值信息，其优化的目的以及方式不会在本文中提及，随后在介绍生成这三个索引文件的生成过程的文章中再详细展开，并会跟Lucene 7.5.0版本进行对比，本文只对索引文件中的字段作介绍。\n 数据结构\n 索引文件.fdt\n图2：\n\n ChunkSize\n  ChunkSize作为一个参数，用来判断是否要生成一个Chunk，以及用来描述压缩存储域值信息的方式，后面会详细介绍。\n PackedIntsVersion\n  PackedIntsVersion描述了压缩使用的方式，当前版本中是VERSION_MONOTONIC_WITHOUT_ZIGZAG。\n Chunk\n图3：\n\n  在处理文档的过程中，如果满足以下任意一个条件，那么将已处理的文档的域值信息生成一个chunk：\n\n已处理的文档数量达到128\n已处理的所有域值的总长度达到ChunkSize。\n\n NumDoc\n  当前chunk中第一个文档的文档号（该文档号为段内文档号），因为根据这个文档号来差值存储，在读取的阶段需要根据该值恢复其他文档号。\n ChunkDocs\n图4：\n\n  ChunkDocs是一个numBufferedDocs跟slicedBit的组合值。ChunkDocs = (numBufferedDocs |slicedBit )。\n numBufferedDocs\n  numBufferedDocs描述了当前chunk中的文档数量。numBufferedDocs是一个 ≤ 128的值。\n slicedBit\n  如果待处理的域值信息的长度超过2倍的chunkSize（默认值 16384），那么需要分块压缩，下文会具体介绍。\n DocFieldCounts\n  根据chunk中包含的文档个数numBufferedDocs、每篇文档包含的存储域的数量numStoredFields分为不同的情况。\n numBufferedDocs的个数为1\n图5：\n\n numBufferedDocs的个数＞ 1 并且每篇文档中的numStoredFields都是相同的\n图6：\n\n  只要存储一个numStoredFields的值就行啦。\n numBufferedDocs的个数＞ 1 并且每篇文档中的numStoredFields不都相同的\n图7：\n\n  使用PackedInt来存储所有的numStoredFields，这里不赘述了，点击这里可以看其中的一种压缩方式。\n DocLengths\n  同DocFieldCounts类似，据chunk中包含的文档个数numBufferedDocs、每篇文档中域值信息的长度分为不同的情况。\n numBufferedDocs的个数为1\n图8：\n\n numBufferedDocs的个数＞ 1 并且每篇文档中的域值信息长度都是相同的\n图9：\n\n numBufferedDocs的个数＞ 1 并且每篇文档中的域值信息长度不都是相同的\n图10：\n\n  使用PackedInt来存储所有的域值信息长度，这里不赘述了，点击这里可以看其中的一种压缩方式。\n CompressedDocs\n图11：\n\n  CompressedDocs中使用LZ4算法将域值信息压缩存储。域值信息包含如下内容，字段Doc的数量对应为一个chunk中包含的文档数量：\n\n域的编号\n域值的类型：String、BinaryValue、Int、Float、Long、Double\n域值的编号跟域值的类型组合存储为FieldNumAndType\nValue：域值\n\n ChunkCount\n  chunk的个数。\n DirtyChunkCount\n  在索引阶段，如果还有一些文档未被写入到索引文件中（未满足生成chunk的条件的文档），那么在flush阶段会强制写入，并用该字段记录。\n .fdt整体数据结构\n图12：\n\n 索引文件.fdx\n图13：\n\n  在索引文件.fdx中，每处理1024（blockShitf参数，下文会介绍）个图3中的chunk，就将chunk中的信息就生成一个NumDocsBlock和StartPointBlock，多个NumDocsBlock和多个StartPointBlock分别组成NumDocs字段以及StartPoints字段，注意的是最后一个NumDocsBlock和StartPoints中的chunk的信息数量可能不足1024个。\n NumDocsBlock\n  该字段中的数据使用了PackedInts进行压缩。\n图14：\n\n NumDoc\n  NumDoc描述了chunk中的文档数量。\n StartPointBlock\n  该字段中的数据使用了PackedInts进行压缩。\n图15：\n\n StartPoint\n  StartPoint描述了图2中每个chunk的信息在索引文件.fdt中的起始位置。\n .fdx整体数据结构\n图16：\n\n 索引文件.fdm\n图17：\n\n  索引文件.fdm中的信息用于在读取阶段映射索引文件.fdx中的信息。\n NumDocs\n  该字段描述了包含文档数量（无论文档是否包含存储域）。\n BlockShift\n  该字段用来描述在索引文件.fdx中，当处理2 &lt;&lt; BlockShitf个chunk后就生成一个NumDocsBlock或StartPointBlock，默认是10。\n TotalChunks\n  该字段描述了图2中Chunk的数量，执行+1操作后写入到索引文件\n NumDocsIndex\n  该字段描述了图16中NumDocs字段的信息在索引文件.fdx的起始读取位置\n NumDocsMeta\n  该字段由多个NumDocMeta组成，其数量最多是1024（2 &lt;&lt; BlockShitf），由于图14中的NumDocsBlock中的信息在存储过程中执行了先编码后压缩的操作（后面的文章中会展开介绍），NumDocMeta中存储了一些参数，使得在读取阶段能读取到NumDoc的原始值。\n图18：\n\n  图18中Min、AvgInc、Offset、BitRequired字段的值作为参数，将会在读取阶段用于解码图14中的NumDocsBlock中的信息，本文中不展开对这些参数的介绍。\n StartPointsMeta\n  该字段由多个StartPointMeta组成，其数量最多是1024（2 &lt;&lt; BlockShitf），由于图15中的StartPointBlock中的信息在存储过程中执行了先编码后压缩的操作（后面的文章中会展开介绍），StartPointMeta中存储了一些参数，使得在读取阶段能读取到StartPoint的原始值。\n图19：\n\n  图18中Min、AvgInc、Offset、BitRequired字段的值作为参数，将会在读取阶段用于解码图15中的StartPointBlock中的信息，本文中不展开对这些参数的介绍。\n SPEndPointer\n  该字段描述的是图16中StartPoints字段对应的数据块的在索引文件.fdx中的结束位置。\n maxPointer\n  该字段描述的是图2中的最后一个chunk字段对应的数据块在索引文件.fdt中的结束位置。\n .fdm整体数据结构\n图20：\n\n 结语\n  看完这篇文章后，如果感到一脸懵逼， 木有关系，在随后的文章将会详细介绍索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的生成过程。\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","fdx","fdt","fdm"]},{"title":"索引文件之pos&&pay（Lucene 8.4.0）","url":"/Lucene/suoyinwenjian/2019/0324/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bpos&&pay/","content":"  position在Lucene中描述的是一个term在一篇文档中的位置，并且存在一个或多个position。\n  payload是一个自定义的元数据(mete data)来描述term的某个属性，term在一篇文章中的多个位置可以一一对应多个payload，也可以只有部分位置带有payload。这里提供了一个简单的demo来介绍payload的使用：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/AnalyzerTest/PayloadAnalyzer.java。\n  offset是一对整数值(a pair of integers)，即startOffset跟endOffset，它们分别描述了term的第一个字符跟最后一个在文档中的位置。\n  每一个term在所有文档中的position、payload、offset信息在IndexWriter.addDocument()的过程中计算出来，在内存中生成一张倒排表，在flush阶段，通过读取倒排表，将position信息写入到.pos文件中，将payload、offset信息写入到.pay文件中。\n  本文介绍的是索引文件.pos、.pay的数据结构，其生成过程见文章索引文件的生成（二）。\n pay文件的数据结构\n图1：\n\n  图1中是仅有一个域的.pay文件的数据结构。\n TermPayload\n图2：\n\n PackedPayBlock\n  每次处理一个term的128个position信息，就会将对应的128个payload信息（不一定每个position都对应一个payload）处理为一个PackedPayBlock。即除了最后一个PackedPayBlock，其他PackedPayBlock中都包含了当前term的128个payload信息。\n PackedPayLengthBlock\n  PackedPayLengthBlock存放了128个payload的长度数据，并且使用了PackedInts进行了压缩存储，在读取阶段能根据长度数据从PayData中获得对应长度的数据（不明白？请阅读系列文章索引文件的读取（十二）之doc&amp;&amp;pos&amp;&amp;pay）。\n这里注意是由于每一个payload的长度无法保证递增，只能使用PackedInts存储原始数据。\n SumPayLength\n  SumPayLength存放了这128个payload的数据长度(字节数)，在读取.pay文件时用来确定128个payload的真实数据在.pay中的数据区间。\n PayData\n  PayData中存放了128个payload的真实数据。\n TermOffset\n图3：\n\n PackedOffsetBlock\n  跟TermPayload一样的是，都是每次处理一个term的128个position信息后，就会将对应的128个offset信息处理为一个block。\n PackedOffsetStartDeltaBlock\n  offset是一对整数值(a pair of integers)，startOffset跟endOffset分别描述了term的第一个字符跟最后一个在文档中的位置。PackedOffsetStartDeltaBlock存放了128个offset的startOffset值，并且使用了PackedInts进行压缩存储，由于这128个startOffset是个递增的值，所以实际存放了相邻两个offset的startOffset的差值。\n PackedOffsetLengthBlock\n  PackedOffsetLengthBlock存放了128个offset的startOffset跟endOffset差值，同样使用PackedInts进行压缩存储。\n pos文件的数据结构\n图4：\n\n  在.pos文件中，TermPosition记录一个term的position信息。\n TermPosition\n图5：\n\n PackedPosBlock\n  每次处理一个term的128个position信息，就会将这些position处理为一个PackedPosBlock。\n PackedPosDeltaBlock\n  PackedPosDeltaBlock存放了128个位置信息，计算相邻两个position的差值后，利用PackedInts压缩存储。\n VIntBlocks &amp;&amp; VIntBlock\n  如果position的个数不足128个，那么将每一个position处理为一个VIntBlock。(比如说某个term有200个position，那么前128个position处理为一个PackedPosBlock，剩余的72个position处理为72个VIntBlock，72个VIntBlock为一个VIntBlocks)。\n PositionDelta\n  term的position信息，这是一个差值。PositionDelta的最后一位用来标识当前position是否有payload信息。\n PayloadLength\n  当前position对应的payload信息的长度，在读取.pos时，用来确定往后读取的一个字节区间。\n PayloadData\n  当前position对应的payload真实数据。\n OffsetDelta\n  当前position对应的offset的startOffset值，同样是个差值\n OffsetLength\n  当前position对应的offset的endOffset与startOffset的差值。\n 多个域的pay文件的数据结构\n图6：\n\n 多个域的pos文件的数据结构\n图7：\n\n 结语\n  .pos、.pay、.doc、.tim、.tip文件都是在flush()阶段通过读取倒排表一起生成的，另外.doc跟.pos、.pay文件还有映射关系，在后面介绍.doc文件时候会涉及。\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","pos","pay"]},{"title":"索引文件之segments_N","url":"/Lucene/suoyinwenjian/2019/0610/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bsegments_N/","content":"  当IndexWriter执行commit()操作后，会生成一个segments_N文件，该文件描述了当前索引目录中所有有效的段信息文件(active segment info)，即之前文章介绍的segmentInfo文件，仅仅通过flush()生成的段成为无效的段信息文件。\n  索引目录中可能存在多个Segments_N文件，每个Segment_N文件代表某次commit()时的索引状态，其中N值最大的Segments_N文件代表最新的一次提交，它包含当前索引目录中所有的索引信息。\n  图1中最新的一次提交生成了Segments_5文件。\n图1：\n\n  图1中Segments_N文件包含的索引信息关系如下图：\n图2：\n\n  一个索引目录中存在多个segments_N文件的原因大体分为两点：\n\n旧的segments_N暂时不能被删除：原因很多，在后面介绍IndexWriter的文章中会提及\n使用了非默认的IndexDeletionPolicy：IndexDeletionPolicy提供了一个策略，该策略描述了当一个新的commit()提交后，如果处理旧的提交，Lucene7.5.0中默认使用的是KeepOnlyLastCommitDeletionPolicy，它是IndexDeletionPolicy的其中一个实现，即当有新的提交时，删除前面的提交，比如在图1中，就只会保留segments_5文件；例如同样作为IndexDeletionPolicy的另一个实现，NoDeletionPolicy，使用该策略就会保留每次的commit()，这么做的好处就相当于设置了每一个commit()检查点，配合CommitUserData(下文会介绍)，我们可以将索引信息恢复到任意一个检查点，缺点是很明显的，如图2中，每一个segments_N都包含了以往所有的信息，索引目录的大小因此会很大。\n\n segments_N文件的数据结构\n图3：\n\n LuceneVersion\n图4：\n\n  LuceneVersio描述了当前运行的Lucene版本，比如本文基于Lucene7.5.0写的，那么LuceneVersion的值如下：\n\nLATEST.major：7\nLATEST.minor：5\nLATEST.bugfix：0\n\n IndexCreatedVersionMajor\n  IndexCreatedVersionMajor描述的是创建该segment_N文件的Lucene的major值，在读取阶段，该segment_N文件可能被更高版本的Lucene读取，用来检查兼容性。\n Version\n  Version描述的是segmentInfos对象发生更改的次数。\n  segmentInfos对象的概念见文章近实时搜索NRT（一）中流程点获得所有段的信息集合SegmentInfos的介绍。\n NameCounter\n  NameCounter用来给新的segmentInfo文件提供名字的前缀值，例如下图中 _8 即为前缀值。\n图5：\n\n SegCount\n  该字段描述了当前索引目录中的有效的段信息文件(active segment info)。\n MinSegmentLuceneVersion\n图6：\n\n  索引目录中的.si文件的版本可能各不相同，MinSegmentLuceneVersion记录版本最小的，不详细展开，同图4。\n SegmentCommitInfo\n图7：\n\n  该字段描述了一个segmentInfo文件(.si文件)的信息。\n SegName\n  该字段描述了segmentInfo文件及对应的其他索引文件的名字前缀，图8中，下面所有的文件属于同一个segment，segName的值为&quot;_1&quot;\n  在读取segment_N文件阶段，通过SegName找到.si索引文件，结合SegmentCommitInfo就可以获得一个段的完整的信息\n图8：\n\n SegID\n  该字段描述了segmentInfo文件的一个唯一标示。\n SegCodec\n  该字段描述了segmentInfo文件编码值，例如&quot;Lucene70&quot;。\n DelGen\n  该字段描述了属于同一个segment的.liv文件的迭代编号（generation number），它用来命名下一次生成的索引文件.liv，该字段的详细介绍见构造IndexWriter对象（九）。\n DeletionCount\n  该字段描述了segmentInfo文件中被删除文档的个数。\n FieldInfosGen\n  该字段描述了属于同一个segment的.fnm文件的迭代编号（generation number），它用来命名下一次生成的索引文件.fnm，该字段的详细介绍见构造IndexWriter对象（九）。\n DocValuesGen\n  该字段描述了属于同一个segment的.dvm、.dvd文件的迭代编号（generation number）， 它用来命名下一次生成的索引文件.dvd、.dvm，该字段的详细介绍见构造IndexWriter对象（九）\n SoftDelCount\n  该字段记录软删除的文档个数，软删除的概念后面介绍文档的添加、删除、更新时会给出详细含义。\n FieldInfosFiles\n  如果域的信息发生了变化（更新），那么会记录最新生成的.fnm文件。\n UpdatesFiles\n  记录发生变化的索引文件，比如调用了IndexWriter.updateDocValues(…)的方法后，会生成新的.dvd、.dvm文件，那么域值跟索引文件名字的信息。\n图9：\n\n&emsp;&emsp;上图中，先找出包含域名为\"content\"，域值为\"c\"的文档，然后更新该文档中的NumericDocValuesField域，更新域名跟域值。此操作后，会生成新的.dvd、dvm文件。\n CommitUserData\n  该字段可以通过调用IndexWriter.setLiveCommitData(…)来在commit()时记录自定义的信息，上文中提到，如果使用了NoDeletionPolicy，那么Lucene会保留每一次commit()时的索引文件信息作为检查点，这样我们可以通过CommitUserData跟Segment_N来回退到任意的检查点。\n segments_N文件的总数据结构\n图10：\n\n 结语\n  无\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","segment_n"]},{"title":"索引文件之si","url":"/Lucene/suoyinwenjian/2019/0605/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bsi/","content":"  当生成一个新的segment时（执行flush、commit、merge、addIndexes(facet)），会生成一个描述段文件信息（segmentInfo）的.si索引文件。\n si文件的数据结构\n图1：\n\n SegVersion\n  SegVersion描述了该segment的版本信息。\n图2：\n\n Created.major、Created.minor、Created.bugfix\n  Created描述的是segment创建版本。major、minor、bugfix三者组成了一个版本号，比如本文介绍的就是Lucene7.5.0版本，所以major = 7、minor = 5、bugfix = 0。\n 标志位\n  在读取.si文件时的读取该标志位，如果该值为1，表示.si文件中带有Min.major、Min.minor、Min.bugfix的信息需要读取，否则标志位的值为0。\n Min.major、Min.minor、Min.bugfix\n  上文中提到生成一个新的segment可能由flush、commit、merge、addIndexes(facet)触发，那么当merge触发时，意味着多个segment会合并为一个新的segment，那么将某个最小创建版本的segment作为Min.major、Min.minor、Min.bugfix，可以用来判断是否兼容该最小版本的索引文件。\n SegSize\n  该字段描述了segment中的文档（Document）个数。\n IsCompoundFile\n  该字段描述了segment对应的索引文件是否使用组合文件，在索引文件中生成不同的文件\n  不使用组合文件会生成.fdx、.fdt、.tvd、tvx、.liv、.dim、.dii、tim、.tip、.doc、.pos、.pay、nvd、.nvm、.dvm、,dvd：\n图3：\n\n  使用组合文件：\n图4：\n\n Diagnostics\n  该字段描述了以下信息：\n\nos：运行Lucene所在的操作系统，版本号，架构，比如操作系统为Mac OS X，版本号为10.14.3，架构为x86_64\njava：java的发行商，版本号，JVM的版本号\nversion：Lucene的版本号，比如7.5.0\nsource：生成当前segment是由什么触发的，flush、commit、merge、addIndexes(facet)\ntimestamp：生成当前segment的时间戳\n\n Files\n  该字段描述了segment对应的索引文件的名字，索引文件即图3或者图4。\n Attributes\n  该字段描述了记录存储域的索引文件，即.fdx、.fdt使用的索引模式，索引模式有两种: BEST_SPEED 或 BEST_COMPRESSION，Attributes记录其索引模式的名称。\n IndexSort\n  该字段用来对segment内的文档进行排序（用法见Collector（三）中的预备知识及文档提交之flush（三）中的sortMap），该值在IndexWriterConfig对象中设置排序规则，可以提供多个Sort对象。\n图5：\n\n NumSortFields\n  该字段描述了排序规则的个数。\n FieldName\n  SortField的域名。\n SortTypeID\n  在前面的文章中介绍了FieldComparator，它同IndexSort一样使用Sort对象来实现排序，而IndexSort中的排序类型（SortField的域值类型）只是FieldComparator中的部分排序类型，每一种排序类型对应一个SortTypeID：\n\n0：STRING\n1：LONG\n2：INT\n3：DOUBLE\n4：FLOAT\n5：SortedSetSortField\n6：SortedNumericSortField\n\n Selector\n  只有SortTypeID = 5时才会有该字段，因为SortedSetSortField允许有多个域值(String类型)，我们必须确定其中一个域值来排序，Selector的值可以有以下几种：\n\n0：取最小域值\n1：取最大域值\n2：取中间的域值，如果域值个数为偶数个，那么中间的域值就有两个，取较小值，比如有4个域值，“a”，“c”，“d”，“e”，中间域值为&quot;c&quot;，, “d”，那么取&quot;c&quot;\n3：取中间的域值，如果域值个数为偶数个，那么中间的域值就有两个，取较大值\n\n NumericType、Selector\n  只有SortTypeID = 6时才会有该字段，因为SortedNumericSortField域值类型NumericType有多个，我们确定域值类型NumericType：\n\n0：LONG\n1：INT\n2：DOUBLE\n3：FLOAT\n\n  另外因为SortedNumericSortField域值个数可以是多个，所以我们必须确定其中一个域值来排序，Selector的值可以有以下几种：\n\n0：取最小域值\n1：取最大域值\n\n Reverse\n  该字段为0表示正序，1位倒序。\n 缺失值标志位\n  如果有些文档没有排序规则，即需要给该文档添加一个缺失值，那么标志位为1，为0则不添加缺失值。\n 缺失值\n  当缺失值标志位为1，那么需要记录缺失值。这里需要特别说明的是，如果域值是String类型，那么它的缺失值只能是固定的 “SortField.STRING_LAST&quot;或者&quot;SortField.STRING_FIRST”，表示没有排序规则的文档要么在序列最后面，要么在序列最前面，其他域值类型需要提供一个明确的缺失值。\n si文件总数据结构\n图6：\n\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","si"]},{"title":"索引文件之tim&&tip","url":"/Lucene/suoyinwenjian/2019/0401/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Btim&&tip/","content":"  .tim（TermDictionary）文件中存放了每一个term的TermStats，TermStats记录了包含该term的文档数量，term在这些文档中的词频总和；另外还存放了term的TermMetadata，TermMetadata记录了该term在.doc、.pos、.pay文件中的信息，这些信息即term在这些文件中的起始位置，即保存了指向这些文档的索引；还存放了term的Suffix，对于有部分相同前缀值的term，只需存放这些term不相同的后缀值，即Suffix。另外还存放了term所在域的信息等其他信息，下文中会详细介绍。\n  .tip文件中存放了指向tim文件的索引来实现随机访问tim文件中的信息，并且.tip文件还能用来快速判断某个term是否存在。\n  本文直接介绍了索引文件中每个字段的含义，其具体的生成过程见系列文章索引文件的生成（一）。\n tim文件的数据结构\n图1：\n\n  在tim文件中NodeBlock中包含了至少25个entries，每一个entries中包含了一个term（或者有相同前缀的term集合）的相关数据，FieldSummary中记录了域的一些信息。\n图2：\n\n  NodeBlock有两种类型，如上图所示，第一种是 OuterNode，第二种是 InnerNode。这两种类型的NodeBlock在数据结构上有细微的差别（详情见索引文件的生成（六）），我们先介绍OuterNode的数据，然后再介绍他们之间的差别以及为什么NodeBlock为什么需要两种类型。\n图3：\n\n OuterNode\n  OuterNode中包含了所有term的一些信息(后面会详细介绍包含哪些信息)，在Lucene7.5.0版本的源码中，按照term的大小顺序处理，并且用3个RAMOutputStream对象，即suffixWriter、statsWriter、bytesWriter来记录每一个term的Suffix信息、TermStats信息、TermMetadata信息。在所有的term处理结束后，将3个RAMOutputStream对象中的内容合并写入到.tim文件中。\n EntryCount\n  EntryCount描述了当前的OuterNode中包含多个entries，即包含了多少个term的信息（或者有相同前缀的term集合），该值是一个组合值，EntryCount的最低的一个bit位描述用来描述当前NodeBlock是否为该域的最后一个block。\n SuffixLength、StatsLength、MetaLength\n  这三个值分别描述了所有term的Suffix、TermStats、TermMetadata在.tim文件中的数据长度，在读取.tim时用来确定读取Suffix、TermStats、TermMetadata的范围区间。\n  不过SuffixLength是一个组合值，该值的最低的一个bit位描述了当前NodeBlock是否包含sub block（OuterNode即sub block，下文会介绍），即当前NodeBlock为OuterNode还是InnerNode。\n Suffix\n图4：\n\n Length\n  term的后缀长度。\n SuffixValue\n  term的后缀值，之前提到按照term的大小顺序进行处理的，如果一批term具有相同的前缀并且这批term的个数超过25个，那么这批term会被处理为一个NodeBlock，并且SuffixValue只存储除去相同前缀的后缀部分。\n  注意的是：下文中InnerNode中的Suffix字段将会额外多出一个index字段，该字段的介绍见下文。\n TermStats\n图5：\n\n DocFreq\n  DocFreq描述了包含当前term的文档个数。\n TotalTermFreq\n  TotalTermFreq描述了term在文档中出现的总数，实际存储了与DocFreq的差值，目的是尽可能压缩存储，即使用差值存储。\n TermMetadata\n图6：\n\n SingletonDocID\n  如果只有一篇文档包含当前term，那么SingletonDocID被赋值这篇文档号，如果不止一篇文档包含当前term，那么SingletonDocID不会写入到.tim文件中。\n LastPosBlockOffset\n  如果term的词频大于BLOC_SIZE,即大于128个，那么在.pos文件中就会生成一个block，LastPosBlockOffset记录最后一个block结束位置，通过这个位置就能快速定位到term的剩余的position信息，并且这些position信息的个数肯定是不满128个，可以看Lucene50PostingsWriter.java中finishTerm()的方法。\n SkipOffset\n  SkipOffset用来描述当前term信息在.doc文件中 跳表信息的起始位置。\n DocStartFP\n  DocStartFP是当前term信息在.doc文件中的起始位置。\n PosStartFP\n  PosStartFP是当前term信息在.pos文件中的起始位置。\n PayStartFP\n  payStartFP是当前term信息在.pos文件中的起始位置。\n InnerNode\n  介绍InnerNode具体的数据结构之前，我们先给出一个场景，如果我们需要处理的term值为下面的情况：\n图7：\n\n  遍历每一个term，将具有相同前缀&quot;ab&quot;的，并且个数超过25个的term先处理为一个OuterNode，接着前缀值&quot;ab&quot;作为一个term与剩余的term处理为一个InnerNode。\n图8：\n\n  由于InnerNode中的前缀为&quot;ab&quot;的所有term的Suffix、TermStats、TermMetadata已经存放在.tim文件中，所以在InnerNode只要记录在.tim文件中的偏移位置，即上图中的红色标注的index。所以通过图8与图3就可以看出InnerNode跟OuterNode的数据结构的差异。\n FieldSummary\n  介绍FieldSummary之前，先要声明的是在图1中，只是单个域的.tim文件数据结构，下面是多个域的.tim数据结构。\n图9：\n\n  下面是FieldSummary的数据结构。\n图10：\n\n  图10 中的Field的个数与位置图9中的Field一一对应。\n NumFields\n  NumFields描述了.tim文件中的有多少种域。\n FieldNumber\n  FieldNumber记录了当前域的编号，这个编号是唯一的，同时它是从0开始的递增值。数值越小，说明该域更早的被添加进了索引。\n NumTerms\n  NumTerms记录了当前域中有多少种term。\n RootCodeLength\n  RootCodeLength描述了当前域中的term的FST数据的长度。\n RootCodeValue\n  RootCodeValue描述了当前域中的term的FST数据。\n SumTotalTermFreq\n  sumTotalTermFreq描述了当前域中所有term在文档中的总词频。\n SumDocFreq\n  SumDocFreq描述了包含当前域中的所有term的文档数量。\n DocCount\n  DocCount描述了有多少篇文档包含了当前域。\n LongsSize\n  longsSize的值只能是1，2，3三种，1说明了当前域只存储了doc、frequency，2说明了存储了doc、frequency，positions，3说明存储了doc、frequency，positions、offset。\n MinTerm\n  当前域中的最小的term。\n MaxTerm\n  当前域中的最大的term。\n DirOffset\n  DirOffset记录了FieldSummary的信息在.tim文件中的起始位置。\n tip文件的数据结构\n图11：\n\n FSTIndex\n  FSTIndex记录了NodeBlock在.tim文件中一些信息，比如说fp为NodeBlock在.tim文件中的起始位置，hasTerms描述NodeBlock中是否包含pendingTerm对象, isFloor表示是否为floor block，然后将这些信息用FST算法存储，在前面的博客中有介绍FST的存储过程。\n IndexStartFP\n  IndexStartFP描述了当前的FSTIndex信息在.tip中的起始位置。\n DirOffset\n  DirOffset描述了第一个IndexStartFP在.tip中的位置。\n 结语\n  tim、tip文件是索引文件中最复杂的实现，加上工作较忙，看了蛮久。如果朋友们想要阅读这部分源码，必须先熟悉FST算法，并且源码中BlockTreeTermsWriter.java中pushTerm(BytesRef text)方法在刚开始看时，始终不明白这段代码的意思，尴尬，遇到同样情况的朋友可以简单的理解为它就是为了统计相同前缀的term的个数是否达到25(minItemsInBlock)，另外tip文件的数据结构没有详细介绍，因为这一块跟FST算法紧紧关联，理解了FST算法就自然知道了FSTIndex，而在前面的文章中已经介绍了这个算法，大家可以先去了解下。\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","tim","tip"]},{"title":"索引文件之tvd&&tvx&&tvm（Lucene 8.7.0）","url":"/Lucene/suoyinwenjian/2020/1117/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Btvd&&tvx&&tvm/","content":"  在索引（Indexing）阶段，当某个域被设置为需要记录词向量（term vector）信息后，那么随后在flush阶段，该域对应的词向量将被写入到索引文件.tvd&amp;&amp;tvx&amp;&amp;tvm三个文件中。\n图1：\n\n  图1中，域名&quot;content&quot;跟&quot;title&quot;都被设置为需要记录词向量信息，而域名&quot;author&quot;则没有。\n 数据结构\n 索引文件.tvd\n图2：\n\n PackedIntsVersion\n  PackedIntsVersion描述了压缩使用的方式，当前版本中是VERSION_MONOTONIC_WITHOUT_ZIGZAG。\n Chunk\n图3：\n\n  在索引阶段，每当处理128篇文档或者已经处理的域值的总长度达到4096，就生成一个chunk。\n DocBase\n  该字段描述的是chunk中第一篇文档的文档号。\n ChunkDocs\n  该字段描述的是Chunk中的文档数量。\n NumFields\n  该字段描述的是Chunk中每篇文档中记录词向量的域的数量。例如图1中只有一篇文档，这篇文档中的就包含了2个记录词向量的域。\n  根据Chunk中包含的文档数量，NumFields字段的数据结构各不相同\n Chunk中只包含一篇文档\n图4：\n\n  如果图1所示，那么NumFields的值为2，并且不会使用压缩存储。\n Chunk中包含多篇文档\n图5：\n\n  当包含多篇文档，那么需要记录每一篇文档中记录词向量的域的数量，然后使用PackedInts存储。\n FieldNums\n  该字段描述的是Chunk中记录词向量的域的种类，根据域的编号来获得域的种类。\n (域的种类 - 1) ≤ 7\n图6：\n\n token\n  token是一个组合值，并且大小是一个字节：\n\nnumDistinctFields：Chunk中记录词向量的域的种类\nbitsRequired：存储每个域的编号（因为使用了固定位数按位存储）需要的bit数量\n左移5位描述了bitsRequired的值最多可以是31\n由于一个字节的低五位被用来描述bitsRequired，所以还剩余3个bit可以用来表示numDistinctFields，所以numDistinctFields的值小于等于7时可以跟bitsRequired使用一个字节存储。\n\n FieldNum\n  FieldNum即域的编号， 用PackedInts存储。\n (域的种类 - 1) &gt; 7\n图7：\n\n token\n  token是一个组合值，并且大小是一个字节：\n\nbitsRequired：存储每一个域的编号需要最少bit位个数\n由于numDistinctFields的值大于7，那么在token的高三位用来描述numDistinctFields一部分值，即固定值7，低5位用来描述bitsRequired\nnumDistinctFields - 0x07：存储剩余的差值，例如假设numDistinctFields的值为13，那么7存储在token中、6存储在当前字段\n\n FieldNumOffs\n图8：\n\n  FieldNumOffs中存放了chunk中每一篇文档包含的所有域的编号的索引值FieldNumIndex，并且使用PackedInts存储。该索引其实就是fieldNums[ ]数组的下标值，fieldNums[ ]数组的数组元素是Chunk中的域的编号，数组长度是域的种类数。通过这种方式使得不直接存储域的编号，因为域的编号可能跨度很大，由于使用固定位数按位存储，每个域的编号占用的bit数量取决编号最大的，那会导致较大的存储空间，而存储下标值就缓解这个问题。\n图9：\n\n  图9中有4个域的编号，如果直接存储域的编号，其中域的编号最大值为255，占用8个bit，故需要 8 *4 = 32个bit。若只存储索引值，那么需要 1 (0) + 1 (1) + 2 (2) + 2 (3) = 6个bit位。\n Flags\n  Flags用来描述记录词向量的域具体记录哪些信息，这些信息包括：位置position、偏移offset、负载payload信息。flag的值可以是下面3个值的组合：\n\n0x01：包含位置position信息\n0x02：包含偏移offset信息\n0x04：包含负载payload信息\n\n  比如说 flag = 3，二进制即0b00000011，即该域会记录位置跟偏移信息。\n  根据同一个域名在不同的文档中是否有相同的Flag分为不同的情况：\n图10：\n\n  图10中，域名&quot;content&quot;、&quot;title&quot;在文档0中会记录位置position、偏移offset、负载payload信息，然而在文档1中只记录位置position、负载payload信息。那么这种情况就称为相同的域名有不相同的flag，反之如果所有记录词向量的域在所有文档中对应的flag都是相同的，那么这种情况称为相同的域名有相同的flag。\n 相同的域名有相同的flag\n图11：\n\n  对于某个记录词向量的域名来说，无论它在哪个文档中都记录相同的flag信息，所以只要只要记录一次即可，并且用PackedInts存储，固定值0为标志位，在读取阶段用来区分Flags的不同数据结构。图11中每个flag字段对应一种域，即flag的数量等于记录词向量的域的种类数量。\n 相同的域名有不相同的flag\n图12：\n\n  对于一个域名来说，它在不同文档中的flag可能不一样（例如当前文档中，某个记录词向量的域名只记录位置信息，而在下一篇文档中，该域名记录了位置信息跟偏移信息），那么只能所有文档中的所有域的flag，并且用PackedInts存储，固定值1为标志位，在读取阶段用来区分Flags的不同数据结构。图12中，每个Flag对应为某篇文档中的某个记录词向量的域。\n TermData\n  TermData记录了域值以及Payload信息。\n图13：\n\n NumTerms\n  NumTerms描述了每一篇文档的每一个域包含的term个数，使用PackedInts存储。\n TermLengths\n  TermLengths描述了每一篇文档的每一个域中的每一个term的长度，使用PackedInts存储。\n TermFreqs\n  TermFreqs描述了每一篇文档的每一个域中的每一个term在当前文档中的词频，使用PackedInts存储。\n Positions\n  Positions描述了每一篇文档的每一个域中的每一个term在当前文档中的所有位置position信息，使用PackedInts存储。\n StartOffset\n  StartOffset描述了每一篇文档的每一个域中的每一个term的startOffset，使用PackedInts存储。\n Lengths\n  Lengths描述了每一篇文档的每一个域中的每一个term的偏移长度，使用PackedInts存储。\n TermAndPayloads\n  使用LZ4算法存储每一篇文档的每一个域中的每一个term值跟payload(如果有的话)。\n 索引文件.tvd整体数据结构\n图14：\n\n 索引文件.tvx\n图15：\n\n  索引文件.tvx中的字段含义同索引文件.fdx， 不赘述。\n 索引文件.tvm\n图16：\n\n  索引文件.tvm中的字段含义同索引文件.fdm， 不赘述。\n 结语\n  看完这篇文章后，如果感到一脸懵逼， 木有关系，在随后的文章将会详细介绍索引文件tvd&amp;&amp;tvx&amp;&amp;tvm的生成过程。\n点击下载Markdown文件\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","tvd","tvx","tvm"]},{"title":"索引文件之tvx&&tvd","url":"/Lucene/suoyinwenjian/2019/0429/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Btvx&&tvd/","content":"当设置了TermVector的域生成了倒排表以后，将文档的词向量信息写到.tvx（vector_index）跟.tvd（vector_data）文件中。\n 数据结构\n .tvd\n图1：\n\n PackedIntsVersion\nPackedIntsVersion描述了压缩使用的方式，当前版本中是VERSION_MONOTONIC_WITHOUT_ZIGZAG。\n ChunkSize\nChunkSize用来在压缩存储域值信息的方式，后面会详细介绍。\n Chunk\n图2：\n\n在文档数达到128篇之前，如果这些文档中的所有存储域的域值长度达到4096，或者文档数达到128篇，那么就使得这些文档的信息生成一个Chunk。\n DocBase\nDocBase是chunk中第一个文档的文档号。\n ChunkDocs\nchunk中包含的文档个数。\n NumFields\nNumFields记录了每篇文档中需要记录词向量的域的数量，根据chunk中的个文档数chunkDocs，分为不同的情况：\n chunkDocs == 1\n图3：\n\n直接记录该篇文档中需要记录词向量的域的数量。\n chunkDocs ≥ 1\n图4：\n\n记录chunk中的每一篇文档中需要记录词向量的域的数量，使用[PackedInts](https://www.amazingkoala.com.cn/Lucene/yasuocunchu/2019/1217/PackedInts（一）存储。\n FieldNums\nFieldNums中存储当前chunk中需要记录词向量的域的种类，根据域的编号来获得域的种类，根据域的种类个数分为不用的情况：\n (域的种类 - 1) ≤ 7\n图5：\n\n token\ntoken是一个组合值，并且大小是一个字节：\n\nnumDistinctFields：当前chunk中需要记录词向量的域的种类\nbitsRequired：存储每个域的编号（最大值，因为使用了固定位数按位存储）需要的bit数量\n左移5位描述了bitsRequired最多可以是31\n由于一个字节的低五位被用来描述bitsRequired，所以还剩余3个bit可以用来表示numDistinctFields，所以numDistinctFields的值小于等于7时可以跟bitsRequired使用一个字节存储。\n\n PackedIntsValue\n把所有的域的编号用PackedInts存储。\n (域的种类 - 1) &gt; 7\n图6：\n\n token\ntoken是一个组合值，并且大小是一个字节：\n\nbitsRequired：存储每一个域的编号需要最少bit位个数\n由于numDistinctFields的值大于7，那么在token的高三位用来描述numDistinctFields一部分值，即7，低5位用来描述bitsRequired\nnumDistinctFields - 0x07：存储剩余的差值，例如假设numDistinctFields的值为13，那么7存储在token中、6存储在当前字段\n\n FieldNumOffs\n图7：\n\nFieldNumOffs中存放了chunk中每一篇文档包含的所有域的编号的索引，并且使用PackedInts存储，该索引其实就是fieldNums[]数组的下标值，fieldNums[]数组的数组元素是chunk中的域的编号，数组元素个数是域的种类。通过这种方式使得不直接存储域的编号，因为域的编号可能跨度很大，由于使用固定位数按位存储，每个域的编号占用的bit数量取决编号最大的，那会导致较大的存储空间，而存储下标值就缓解这个问题。\n图8：\n\n上图中有4个域的编号，如果直接存储域的编号，其中域的编号最大值为255，即需要8个bit，由于存在4个域的编号，故需要 8 *4 = 32个bit。若值存储索引值，那么需要 1 (0) + 1 (1) + 2 (2) + 2 (3) = 6个bit位。\n Flags\nFlags用来描述域是否存放位置position、偏移offset、负载payload信息，flag的值可以是下面3个值的组合：\n\n0x01：包含位置position信息\n0x02：包含偏移offset信息\n0x04：包含负载payload信息\n\n比如说 flag = 3，二进制即0b00000011，即包含位置跟偏移信息。\n根据相同域名在不同的文档中是否有相同的Flag分为不同的情况：\n 相同的域名有相同的flag\n图9：\n\n对于某个记录词向量的域名来说，无论它在哪个文档中都记录相同的flag信息，所以只要每种域名记录一次即可，并且用PackedInts存储，固定值0为标志位，在读取阶段用来区分Flags的不同数据结构。\n 相同的域名有不相同的flag\n图10：\n\n对于一个域名来说，它在不同文档中的flag可能不一样（例如当前文档中，某个记录词向量的域名只记录位置信息，而在下一篇文档中，该域名记录了位置信息跟偏移信息），那么只能所有文档中的所有域的flag，并且用PackedInts存储，固定值1为标志位，在读取阶段用来区分Flags的不同数据结构。\n TermData\nTermData记录了域值信息，下文中提及的term是指域值通过分词器获得一个或多个token。\n图11：\n\n NumTerms\nNumTerms描述了每一篇文档的每一个域包含的term个数，使用PackedInts存储。\n TermLengths\nTermLengths描述了每一篇文档的每一个域中的每一个term的长度，使用PackedInts存储。\n TermFreqs\nTermFreqs描述了每一篇文档的每一个域中的每一个term在当前文档中的词频，使用PackedInts存储。\n Positions\nPositions描述了每一篇文档的每一个域中的每一个term在当前文档中的所有位置position信息，使用PackedInts存储。\n StartOffset\nStartOffset描述了每一篇文档的每一个域中的每一个term的startOffset，使用PackedInts存储。\n Lengths\nLengths描述了每一篇文档的每一个域中的每一个term的偏移长度，使用PackedInts存储。\n TermAndPayloads\n使用LZ4算法存储每一篇文档的每一个域中的每一个term值跟payload(如果有的话)。\n ChunkCount\n.tvd文件中chunk的个数。\n .tvd整体数据结构\n图12：\n\n .tvx\n.tvx跟.fdx的生成代码在源码中是一模一样的，所以不赘述了。\n点击下载Markdown文档\n","categories":["Lucene","suoyinwenjian"],"tags":["index","indexFile","tvd","tvx"]},{"title":"索引文件的合并（一）之fdx&&fdt&&fdm（Lucene 8.7.0）","url":"/Lucene/Index/2020/1130/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  从本篇文章开始介绍索引文件合并的过程，其中合并策略、合并调度在之前的文章中已经介绍，没有阅读过这些文章并不会影响对本篇文章的理解。\n  由于本篇文章是索引文件的合并的开篇文章，故我们先给出各类索引文件合并的先后顺序，如下所示：\n图1：\n\n  图1中第96行的merge( )方法在被调用后，就开始依次对各种类型的索引文件进行合并，该方法被调用的时机点如下图所示：\n图2：\n\n  图2中的流程图是执行段的合并的完整流程，其介绍可以阅读系列文章执行段的合并。在红框标注的流程点执行索引文件的合并中会调用图1中的第96行的merge( )方法，开始索引文件的合并。我们先介绍索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并。\n 索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并流程图\n图3：\n\n 待合并集合\n图4：\n\n  无论执行哪种索引文件的合并，其输入参数都是同一个MergeState对象，该对象中包含了不同索引文件对应的’‘reader’'数组，数组中的某一个&quot;reader&quot;描述了某个段的索引文件信息，如下图所示：\n图5：\n\n  所以对于索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并，会使用到图5中的StoredFieldsReader[ ]数组，它即图4中的待合并集合。\n  MergeState对象是何时以及如何生成的：\n\n在图2的流程点生成SegmentMerge中生成MergeState对象\n\n图6：\n\n\n\n图6中MergeState的构造函数的参数之一是每个待合并的段对应的SegmentReader（CodecReader的实现类）的集合，而在文章SegmentReader（一）中我们知道，SegmentReader中有一个SegmentCoreReaders对象，它包含了段中所有索引文件对应的reader：\n\nStoredFieldsReader：从索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm中读取存储域的索引信息\nFieldsProducer：从索引文件tim&amp;&amp;tip、索引文件doc、索引文件pos&amp;&amp;pay中读取域的索引信息\nTermVectorsReader：从索引文件之tvd&amp;&amp;tvx&amp;&amp;tvm读取词向量的索引信息\nPointsReader：从索引文件之kdd&amp;kdi&amp;kdm中读取域值为数值类型的索引信息\nNormsProducer：从索引文件nvd&amp;&amp;nvm中读取域的打分信息\nFieldInfos：从索引文件fnm读取域的信息\n\n\n\n所以图5中MergeState对象中所有索引文件对应的’‘reader’'信息都是通过深拷贝从SegmentCoreReaders对象中获取的\n\n\n 统计每个段中域名的信息\n图7：\n\n  执行该流程点的目的是为了给后续流程中的合并方式给出依据条件，合并方式即图3中的naive merge、optimized merge、bulk merge。这三种合并方式描述了合并后生成的新的索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的生成方式以及读取待合并的索引文件的方式。\n  统计的逻辑为判断所有的段对应的域名与域的编号的映射关系是否都相同（Computes which segments have identical field name to number mappings）。至于在什么情况下会出现不一致的情况，我们将在介绍索引文件的合并之fnm中展开。\n 是否配置了IndexSort？\n图8：\n\n  该流程点通过判断IndexWriter的配置信息IndexWriterConfig来确定是否配置了IndexSort。如果配置了段内排序，那么合并后的段将会段内有序，即段内的文档将会根据IndexWriterConfig中的一个或者多个排序规则进行排序。\n 段内排序\n  待合并的段集合中，要么所有段有相同的段内排序规则，要么都没有段内排序，也就是不存在只有部分段有段内排序，也不存在不同的段有不同的排序规则。\n  在IndexWriter构造阶段，配置信息IndexSort属于不可变配置，使得通过该IndexWriter对象生成的段总是拥有相同的段内排序规则。另外IndexWriter的构造函数中包含一个Directory对象，如果构造IndexWriter时配置的OpenMode模式为CREATE_OR_APPEND或者APPEND，会先读取Directory对应的索引目录中&quot;旧的&quot;段，它们是其他IndexWriter对象生成的，可能这些&quot;旧的&quot;段中的段内排序规则跟当前IndexWriter中的不一致，那么Lucene不允许读取这些&quot;旧的&quot;的索引文件，会下图的validateIndexSort方法中抛出以下的异常：\n图9：\n\n  图9中segmentIndexSort为&quot;旧的&quot;索引文件中的段内排序规则，IndexSort为当前IndexWriter的段内排序规则。\n  为什么Lucene不允许配置了IndexSort的IndexWriter读取段内排序规则不相同的&quot;旧的&quot;段：\n\n\n如果允许读取，那么这个&quot;旧的&quot;段在跟新的段合并时，需要对这个&quot;旧的&quot;段按照IndexWriter中的排序规则进行重新排序，在这个issue中指出，在内存中进行重新排序是不安全或者影响合并性能的，详细的介绍见：https://issues.apache.org/jira/browse/LUCENE-8505 。\n\n\n在Lucene 8.0.0之前，如果&quot;旧的&quot;段没有段内排序规则，并且是Lucene 6.5.0或之前的版本生成的段，那么在段的合并期间会对这些&quot;旧的&quot;段先进行排序，然后再合并。原因是段内排序这个功能是Lucene 6.5.0新加入的，出于兼容的目的才允许这么做。而在Lucene 8.0.0之后则取消了，正如图9所示，&quot;旧的&quot;段中没有排序规则或者跟IndexWriter中的排序规则不一致就抛出异常，同样的上述的issue中说明了原因。如果一定要使用&quot;旧的&quot;索引文件，只能reIndex。注意的是IndexWriter.addIndexes(…)方法中同样进行跟图9中一样的非法验证。\n\n\n当然如果当前IndexWriter中没有配置段内排序，那么它就不会去检查&quot;旧的&quot;段中是否有段内排序， 正如图9中第1131行代码所示。\n\n\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","fdx","fdt","fdm"]},{"title":"索引文件的合并（三）之fdx&&fdt&&fdm（Lucene 8.7.0）","url":"/Lucene/Index/2020/1203/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  本文承接文章索引文件的合并（二）之fdx&amp;&amp;fdt&amp;&amp;fdm，继续介绍剩余的内容，下面先给出索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并流程图。\n 索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并流程图\n图1：\n\n 未配置IndexSort\n图2：\n\n  如果当前执行合并操作的IndexWriter中没有配置IndexSort，那么依次读取每一个待合并的段，依次读取段中的每篇文档的索引信息即可。回顾下在文章索引文件的合并（二）之fdx&amp;&amp;fdt&amp;&amp;fdm中，当IndexWriter设置了IndexSort，处理过程为从一个优先级队列中的待合并的段中选出一个优先级最高的段，并从这个段的选出一篇文档作为下一次处理的候选者。\n  根据当前正在处理的某个待合并的段的一些信息，下文中我们称这个段为**“旧段”**，去匹配图2中的两个条件来选择不同的合并方式。合并方式描述的是如何读取旧的索引文件以及如何生成新的索引文件。\n 满足bulk merge的条件\n  图2中的两个条件都是为了筛选出能使用bulk merge的旧段。条件1筛选出能使用bulk merge/optimized merge的旧段，条件2在条件一的基础上筛选出能使用bulk merge的旧段。\n 是否满足条件1？\n  条件1由三个子条件组成，他们之间是或&quot;||&quot;的关系，满足三个子条件中的任意一个即认为是满足条件1，那么随后使用 naive merge：\n\n子条件一：读取旧段对应使用的reader是否为CompressingStoredFieldsReader对象，只有这种类型的对象才可以使用bulk merge/optimized merge的合并方式，如果不是那么满足子条件一\n子条件二：在满足子条件一的前提下，如果读取旧段对应使用的reader与新段的版本不一致，那么满足子条件二\n\n版本不同，意味着新段跟旧段的索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的数据结构是不一致的，即无法使用bulk merge（下文中会介绍bulk merge为什么需要索引文件的数据结构是一致的）\n\n\n子条件三：在满足子条件一、二的前提下，如果设置了不允许使用bulk merge的参数，那么满足子条件三\n\n  这三个条件的判断对应源码 https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java 中的下面的代码：\n图3：\n\n  图3中，matchingFieldReader为null说明旧段不是CompressingStoredFieldsReader对象，即557行代码的注释中所谓的other format。\n还有哪些其他的format：\n图4：\n\n  图4中读取段的reader在Lucene core的模块中主要有两种读取方式，对应上述的两种StoredFieldsReader的实现。\n  当满足条件一后，旧段中所有文档对应的合并方式都是用naive merge，该合并方式在文章索引文件的合并（二）之fdx&amp;&amp;fdt&amp;&amp;fdm中已经介绍，不赘述。\n 是否满足条件2？\n  条件2由5个子条件组成，并且它们需要同时满足。如果满足条件2则使用bulk merge，否则使用optimized merge。另外随后会介绍为什么条件2会由这5个子条件组成：\n\n子条件一：旧段中的压缩模式（compressionMode）是否跟新段的一致，如果一致则满足子条件一\n\n压缩模式决定了索引文件.fdx中存储域的域值信息的压缩方式。例如在Lucene 8.7.0中引入了两种新的压缩模式，在Lucene 8.7.0版本中合并低版本的段时，条件一就不会满足\n\n\n子条件二：旧段中生成一个Chunk的触发条件之一的chunkSize是否跟新段的一致，如果一致则满足子条件二\n\n在文章索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm中介绍了chunkSize的概念，chunkSize会影响域值信息的压缩，不赘述\n\n\n子条件三：旧段中的PackedInts的版本是否跟新段的一致，如果一致则满足子条件三\n\n在文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm中我们说到，Chunk中的DocFieldCounts跟DocLengths字段最终用PackedInts压缩的，如下所示：\n\n\n\n    图5：\n    \n\n子条件四：旧段中不能有被标记为删除的文档，如果没有那么满足子条件四\n\n也就是说旧段中不能有索引文件.liv文件\n\n\n子条件五：这个子条件我们暂时不讨论，在介绍玩bulk merge的合并过程后再作介绍\n\n 执行bulk merge\n  当同时满足了图2的条件1、条件2之后，旧段中就可以通过bulk merge的方式写入到新的索引文件中，其执行过程分为下面的步骤：\n 第一步：从旧段中找到Chunk在索引文件.fdx中开始跟结束位置\n  从旧段中的第一个Chunk开始，第一个Chunk中的第一篇文档为旧段中的第一篇文档，我们称之为preChunkFirstDocId，这篇文档的文档号必定是 0，在文章索引文件的读取（十五）之fdx&amp;&amp;fdt&amp;&amp;fdm中我们介绍了如何通过文档号找到该文档所属的Chunk在索引文件.fdx的开始位置。随后通过读取这个Chunk中的ChunkDocs字段（见文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm的介绍），就可以获得当前Chunk中的文档数量bufferedDocs，那么下一个Chunk中的第一篇文档的文档号，我们称之为nextChunkFirstDocId，nextChunkFirstDocId=preChunkFirstDocId+bufferedDocsnextChunkFirstDocId = preChunkFirstDocId + bufferedDocsnextChunkFirstDocId=preChunkFirstDocId+bufferedDocs，通过nextChunkFirstDoc就可以知道下一个Chunk在索引文件.fdx中的开始位置，该位置也就是上一个Chunk的结束位置。\n 第二步：将Chunk的信息写入到新的索引文件中\n  获取了一个Chunk在旧段中的开始跟结束位置之后，就可以通过字节流拷贝实现所谓的bulk merge：\n图6：\n\n  bulk merge与naive merge/optimized merge相比，它不用读取Chunk中的内容，意味着免除了Chunk中各个字段的解压解码的过程，即不需要额外的内存来暂存解压解码后的Chunk的信息。关于Chunk中各个字段的编码压缩的介绍见文章索引文件的读取（十五）之fdx&amp;&amp;fdt&amp;&amp;fdm。\n  我们回顾下满足bulk merge的条件2，合并后的索引文件.fdx在读取阶段如果不满足子条件一、子条件二，那么会因为不同的压缩模式无法读取域值信息；如果不满足子条件三，那么会因为不同的压缩算法无法读取Chunk中的DocFieldCounts跟DocLengths字段；如果不满足子条件四，源码中给出的解释为：its not worth fine-graining this if there are deletions，个人感觉是为了符合索引文件合并的设计初衷之一：处理被删除的信息、LSM的设计原则？或者是需要额外生成一个新段对应的索引文件.liv来记录删除信息是not worth fine-graining的？这个我没有深刻的明白这个子条件的用意；\n 子条件五\n  在了解了bulk merge的合并过程后，我们可以讨论条件2中的子条件五了，它描述的是旧段中的dirty chunk满足下文的两个条件之一后，这个旧段就不满足子条件五，则不能使用bulk merge。\n dirty chunk\n  在文章索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm中我们说到，在生成索引文件.fdx的过程中，索引阶段跟flush阶段都会生成Chunk，其中flush阶段生成的Chunk即dirty chunk。dirty chunk中包含的文档数量或者域值信息的大小没有达到maxDocsPerChunk或者chunkSize所以被认为是&quot;脏&quot;的Chunk（maxDocsPerChunk跟chunkSize的概念见索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm）。\n  对于DWPT（见文章文档的增删改（二））生成的段，它包含的索引文件.fdx中的dirty chunk的数量最多是1个，而通过bulk merge合并了多个这样的旧段生成的新段就有可能包含多个dirty chunk。当dirty chunk满足下面任意一个条件后，这个新段在下一次作为待合并的段时就不能使用bulk merge：\n\n条件一：dirty chunk的数量大于1024个\n条件二：dirty docs的数量占段中文档总数的比例超过1%\n\n上述条件以注释方式写在源码中\n\n\n\n    图7：\n    \n   图7中，candidate描述的就是待合并的某个段。\n dirty docs\n  每当DWPT生成一个段或者合并后生成的新段后，会统计一个dirty docs的数量，它将被写入到索引文件中：\n图8：\n\n  如何计算dirty docs的数量\n  先给出在源码中的计算公式：\n    图9：\n    \n  图9的公式中：\n\nmaxDocsPerChunk描述的是一个Chunk中最多允许存储的文档数量\nchunkSize表示一个Chunk最多允许存储的域值信息的大小\nnumBufferedDocs描述的是dirty chunk中包含的文档数量\nbufferedDocs.size()描述的是dirty chunk中存储的域值信息的大小\n\n  图9中471行描述了通过dirty chunk中numBufferedDocs与bufferedDocs.size()的比例来推算 如果Chunk中存储chunkSize大小的域值信息对应的文档数量，即expectedChunkDocs，随后用expectedChunkDocs与numBufferedDocs的差值作为dirty docs，用dirty docs的数量来描述了dirty chunk的dirty程度。\n  dirty chunk的数量越大，说明dirty chunk的dirty程度越高，意味着这个dirty chunk的域值信息的大小越小，那么压缩率就会降低，因为域值信息越大，越可能从中找出相同的字节流区间（见文章LZ4算法（上）），从而提高压缩率。\n  如果dirty chunk中的dirty docs数量很小，比如说每次新增1023（比非dirty chunk包含的文档数量少一篇）篇文档生成一个段，使得条件二很难满足，如果没有条件一，那么待合并的段总是可以bulk merge，使得新段中存在多个dirty chunk，降低压缩率。故结合条件一，当dirty chunk的数量达到1024的阈值，使得这个新段在下一次作为待合并的段时使用optimized merge。optimized merge使得生成的新段最多包含一个dirty chunk（不明白？请阅读文章索引文件的合并（二）之fdx&amp;&amp;fdt&amp;&amp;fdm）。\n 结语\n  我们利用三篇文章介绍了索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并，在不同的合并模式下，索引文件.fdx的生成方式各有不同，然而对于索引文件fdm&amp;&amp;fdx，并不会因为不同合并模式有不同的生成方式，还是跟系列文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm中的一样。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","fdx","fdt","fdm"]},{"title":"索引文件的合并（二）之fdx&&fdt&&fdm（Lucene 8.7.0）","url":"/Lucene/Index/2020/1202/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  本文承接文章索引文件的合并（一）之fdx&amp;&amp;fdt&amp;&amp;fdm，继续介绍剩余的内容，下面先给出索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并流程图。\n 索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的合并流程图\n图1：\n\n DocMap[ ]数组\n  继续介绍图1的流程点之前，我们先介绍下DocMap[ ]数组，如果合并后的段是段内排序的，那么需要在构造MergeState（见文章索引文件的合并（一）之fdx&amp;&amp;fdt&amp;&amp;fdm的介绍）对象期间生成DocMap[ ]数组，数组中的数组元素描述了某个待合并的段的一个映射关系，它描述了段中文档号到新段文档号的映射关系。\n图2：\n\n  图2中描述了待合并的段中的文档号映射到新段后的文档号。其中待合并的第一个段中的文档号3以及待合并的第二个段的文档号2用红框额外标注，它们表示这两篇文档号是被标记为删除的，在合并后的新段中不会出现这两个文档号映射关系（代码中其实有删除文档号到新段文档号的映射，只是在合并的过程中会通过索引文件之liv过滤，所以相当于没有映射关系）。图2中的映射关系使用DocMap[ ]数组存储：\n图3：\n\n图2中的映射关系是如何生成的：\n  回答这个问题前必须再次强调在文章索引文件的合并（一）之fdx&amp;&amp;fdt&amp;&amp;fdm中的内容：待合并的段中的文档号是段内有序的，并且所有的段的排序规则都是相同的。\n  生成图2的映射关系的大概逻辑就是：\n\n把所有的段丢进一个优先级队列中，所以这个队列中的元素数量为待合并的段的数量\n优先级队列的排序规则为比较文档中的一个或者多个排序域（排序域即跟IndexWriter中的IndexSort相同域名的域），如果排序域仍然无法比较出结果，那么就根据文档所属段的编号（生成一个段的生成是同步的，先生成的段对应的编号小），编号越小则胜出。\n每个段按照文档号大小的顺序，每次取出一篇文档中的排序域参与比较，如果胜出，那么这篇文档的文档号建立与新段中的文档号的映射，接着这个段取出下一篇文档中的排序域继续参与比较，随后优先级队列重排\n\n  详细的过程可以看源码，实现方式易读易懂：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/index/MultiSorter.java  。虽然是Lucene 8.4.0的链接，但是跟Lucene 8.7.0中的实现是一样的。\n图4：\n\n  我们看下MultiSorter的注释，它提到所有的leaf reader必须是有序的并且使用相同的排序规则，即上文中提到的待合并的段中的文档号是段内有序的，并且所有的段的排序规则都是相同的。\n 初始化优先级队列\n图5：\n\n  当前流程点需要初始化了一个优先级队列，目的在于随后的剩余的流程中我们将通过这个优先级队列有序的取出获得一个文档号，它对应的索引文件信息将被写入到新的索引文件。\n  初始化优先级队列的大概逻辑为：\n\n把所有的段丢这个优先级队列中\n优先级队列的排序规则为比较段中的文档号映射到新段的文档号，我们称之为mappedDocID（源码中的变量名），mappedDocID越小优先级越高。获得mappedDocID的方式正是上文中提到的DocMap[ ]数组\n\n 是否每个段中的域名信息都相同？\n图6：\n\n  域名信息的概念在文章索引文件的合并（一）之fdx&amp;&amp;fdt&amp;&amp;fdm中以及介绍，不赘述。如果当前流程点为true，那么能使用比naive merger性能较高的optimized merge。\n 执行naive merge\n图7：\n\n  naive merge同图2中的optimized merge、bulk merge属于索引文件的合并方式。由于索引文件的合并实际是读取旧的索引文件以及生成新的索引文件的过程，所以我们结合索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的生成过程来介绍这几种合并方式的差异。\n  执行naive merge的过程可以划分下面几个步骤：\n 第一步：从图5的优先级队列中找到优先级最高的段对应的文档号\n  由于新的段也是段内排序的，所以我们从优先级队列中有序的取出后再写入到新段，使得新的段也能保持有序\n 第二步：根据文档号从索引文件.fdt中读取一个Chunk，然后这个Chunk中找到这个文档号对应的存储域信息\n  其读取过程正如文章索引文件的读取（十四）之fdx&amp;&amp;fdt&amp;&amp;fdm中的内容，按照下面的流程图读取：\n图8：\n\n  图8中根据文档号从一个段中取出了对应的存储域的信息，Document描述。\n 第三步：将存储域的信息写入到新的索引文件中\n  其写入过程正如文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm中的内容，按照下面的流程图写入：\n图9：\n\n  图9中用红框标注的流程点描述了当前步骤需要执行的写入操作。这里标注的目的是为了与其他的合并方式相比较，下文中会展开介绍。\n  重复上述3个步骤，直到所有待合并的段中的文档号都被处理结束。\n 执行optimized merge\n图10：\n\n  执行optimized merge的过程同naive merge一样可以划分为下面的步骤：\n 第一步：从图5的优先级队列中找到优先级最高的段对应的文档号\n  同naive merge。\n 第二步：根据文档号从索引文件.fdt中读取一个Chunk，然后这个Chunk中找到这个文档号对应的存储域信息\n  同naive merge。\n 第三步：将存储域的信息写入到新的索引文件中\n  其写入过程正如文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm中的内容，按照下面的流程图写入：\n图11：\n\n  相比较图9，发现optimized merge不需要执行处理存储域的域值信息这步骤。\n为什么不用执行处理存储域的域值信息这两个流程：\n  我们先看下存储域的域值信息包含了什么信息，如下所示：\n图12：\n\n  图12中，使用bufferedDocs[ ]数组存放存储域的域值信息（该数组的详细介绍见文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm），可见包含了红框标注的域的编号的信息。如果待合并的段集合中某一个或多个段的域的编号信息跟MergeState中的域的编号mergeFieldInfos（见文章索引文件的合并（一）之fdx&amp;&amp;fdt&amp;&amp;fdm的图4）不一致，就需要重新获取域的编号信息，意味着图12中的存储域的域值信息会发生变化。反之，如果某个待合并的段中的域的编号跟MergeState中的域的编号信息一致，则可以复用图12中的域值信息。那么只要拷贝文档对应在图12中的存储域的域值信息（字节流）到新段的bufferedDoc[ ]数组中即可，即不用通过执行处理存储域的域值信息给新段的bufferedDoc[ ]数组赋值。\n图13：\n\n  上文中我们提到合并过程是依次处理段中的每篇文档，那么如何根据某个待合并的段中的文档号，找到它对应的存储域的域值信息呢，即如果当前处理的文档号0，如何从找到图13中找到文档号0的存储域的域值信息在索引文件.fdx的数据区间呢？\n  在图8中，当我们执行了流程点读取一个Chunk之后，即获得了图13中的DocFieldCounts以及DocLengths字段信息，根据这两个信息就可以获得某篇文档的存储域的信息在索引文件.fdx的数据区间。感兴趣的朋友可以阅读文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm中的介绍，本文中不赘述。\n 结语\n  通过上文介绍，我们了解s了为什么要执行不同的合并方式以及naive merge以及optimized merge的区别，其实差别并不是太大。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["merge","fdx","fdt","fdm"]},{"title":"索引文件的合并（四）之kdd&kdi&kdm（Lucene 8.7.0）","url":"/Lucene/Index/2020/1222/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E5%90%88%E5%B9%B6%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8Bkdd&kdi&kdm/","content":"  本篇文章开始介绍索引文件kdd&amp;kdi&amp;kdm的合并，由于维度值为1和维度值大于等于2的点数据对应的索引文件的合并方式有较大的差异，故我们分开介绍。本篇文章先对维度值为1的情况展开介绍，建议先阅读文章索引文件的生成（二十五）之kdd&amp;kdi&amp;kdm，了解下维度值为1的点数据是如何生成索引文件的。\n 索引文件kdd&amp;kdi&amp;kdm的合并流程图（维度值为1）\n图1：\n\n  图1中，流程点构建BKD树的节点元数据（node metadata）跟生成索引文件.dim的元数据跟索引文件的生成（九）之dim&amp;&amp;dii是一模一样的，唯一的差异在构建BKD树的节点值（node value）中，我们看下该流程点是如何运作的。\n 构建BKD树的节点值（node value）流程图 维度值等于1\n图2：\n\n  图2中我们分别给出了维度为1的点数据在执行索引文件的合并以及索引文件的生成中对应的流程点构建BKD树的节点值（node value），可以看出，红框标注的流程点是一样，见文章索引文件的生成（二十五）之kdd&amp;kdi&amp;kdm的介绍，不赘述。在索引文件的生成中，流程点节点内的点数据排序执行结束后，就获得了有序的点数据集合，随后有序的读取每一个点数据来执行剩余的流程点；在索引文件的合并中，则是通过一个优先级队列，使得可以从所有的待合并的索引文件中获取有序的点数据，随后执行剩余的流程点。\n 待合并集合\n图3：\n\n  待合并集合即MergeState对象中用于描述所有段的点数据信息的PointsReader集合，该集合中每一个reader描述了一个段中索引文件kdd&amp;kdi&amp;kdm的信息。MergeState对象的详细介绍见文章索引文件的合并（一）之fdx&amp;&amp;fdt&amp;&amp;fdm。\n 初始化优先级队列\n图4：\n\n  使用优先级队列的目的是为了能从多个待合的段中按照字典序有序的获得点数据，因为bkd树中每个叶子节点中的点数据是有序的，并且对于某个叶子节点，它包含的最小的点数据肯定是大于等于左边的叶子节点中的最大的点数据，并且小于等于右边的叶子节点中的最小的点数据。从优先级队列中读取出的点数据可以看成生成一个有序的集合，使得可以实现按块划分（见文章索引文件的生成（二十五）之kdd&amp;kdi&amp;kdm的介绍）。优先级队列的排序规则为点数据的值的字典序。\n图5：\n\n 优先级队列中的元素\n  在源码中，这个优先级队列叫做BKDMergeQueue，该优先级队列中的元素是MergeReader对象。我们介绍下MergeReader对象中一些重要的成员：\n图6：\n\n BKDReader.IntersectState\n  在初始化优先级队列BKDMergeQueue期间，对于某个段来说，会将第一个叶子节点中的所有点数据的文档号写入到BKDReader.IntersectState对象中、点数据的值写入到字节数组packedValues[ ]中。随后在优先级队列出堆的过程中，每次取一个文档号，以及对应的点数据值。\n图7：\n\n  在文章索引文件的生成（八）之dim&amp;&amp;dii中我们说到，数值类型会被转为字节类型，故图6中，4个字节来表示一个数值（int类型）。由于每个数组占用的字节数量是固定，即4个字节，那么在读取packedValues[ ]时，可以随机读取第n个点数据的值。\n  另外文档号的集合使用int类型的数组存储。\n  注意的是，在当前阶段获得的文档号集合中，可能有些文档已经被标记为删除的，即图6中的packedValues[ ]数组中有些点数据值是不能被合并到新的段的。故在处理过程中，每次处理一篇文档时，会利用图5中的MergeState.DocMap来实现过滤。在初始化MergeState对象时，通过读取每个段中的索引文件.liv来初始化DocMap对象。DocMap对象还有一个作用就是用来实现待合并的段中的文档号与新段中的文档号的映射关系，该功能见文章索引文件的合并（二）之fdx&amp;&amp;fdt&amp;&amp;fdm的介绍。\n 剩余流程点\n  在初始化优先级队列BKDMergeQueue之后，通过出堆操作，就可以依次获得有序的点数据。对于某个待合并的段来说，当叶子节点所有点数据都处理结束后，会读取下一个叶子节点的信息，即重新初始化图6中MergeReader中的信息，直到该段中所有叶子节点的点数据被处理结束。在剩余的流程点中，它们的处理方式跟维度值为1的索引文件的生成过程是一模一样的，故不赘述。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["kdd","kdi","kdm"]},{"title":"索引文件的生成（一）之doc&&pay&&pos","url":"/Lucene/Index/2019/1226/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8Bdoc&&pay&&pos/","content":"  在执行flush()的过程中，Lucene会将内存中的索引信息生成索引文件，其生成的时机点如下图红色框标注：\n图1：\n\n  图一中的流程是flush()阶段的其中一个流程点，完整的flush()过程可以看系列文章文档提交之flush，索引文件的生成系列文章将会介绍图一中红框标注的每一个流程点，本篇文章先介绍生成索引文件 .tim、.tip、.doc、.pos、.pay流程点。\n 生成索引文件.tim、.tip、.doc、.pos、.pay\n  在添加文档阶段，一篇文档中的term所属文档号docId，在文档内的出现次数frequency，位置信息position、payload信息、偏移信息offset，会先被存放到倒排表中，随后在flush()阶段，读取倒排表的信息，将这些信息写入到索引文件.tim、.tip、.doc、.pos、.pay中。\n 生成索引文件.tim、.tip、.doc、.pos、.pay的流程图\n图2：\n\n 写入索引文件的顺序\n图3：\n\n  在文章倒排表中我们知道，倒排表中的内容按照域进行划分，域之间可能存在相同的term，但是一个域内term是唯一的，故其写入索引文件的顺序如图3所示， 域间（between filed）根据域名（field name）的字典序处理，域内（inner field）按照term的字典序进行处理。\n 生成索引文件.doc、.pos、.pay\n图4：\n\n  我们先介绍在一个域内，生成索引文件.doc、.pos、.pay的逻辑。\n 生成索引文件.doc、.pos、.pay的流程图\n图5：\n\n点击查看大图\n  图5描述的是同一个域内处理一个term，生成索引文件.doc、.pos、.pay的过程。\n 执行处理前的初始化的工作\n图6：\n\n  依次处理当前域中所有的term，并且是按照term的字典序处理。\n为什么要按照term的字典序处理：\n\n在后面介绍生成索引文件.tim、tip时，需要存储term的值，而相邻有序的term更有可能具有相同的前缀值，那么通过前缀存储（见索引文件之tim&amp;&amp;tip）就可以节省存储空间。\n\n  在处理一个term前，我们先要执行处理前的初始化的工作，工作内容为获取上一个term后处理结束后的信息，包括以下信息：\n\ndocStartFP：当前term在索引文件.doc中的起始位置，在后面的流程中，当前term的文档号docId、词频frequency信息将从这个位置写入，因为索引文件是以数据流的方式存储，所以docStartFP也是上一个term对应的信息在索引文件.doc中的最后位置+1\nposStartFP：当前term在索引文件.pos中的起始位置，在后面的流程中，当前term的位置position信息从这个位置写入，因为索引文件是以数据流的方式存储，所以posStartFP也是上一个term对应的信息在索引文件.pos中的最后位置+1\npayStartFP：当前term在索引文件.pay中的起始位置，在后面的流程中，当前term的偏移offset、payload信息从这个位置写入，因为索引文件是以数据流的方式存储，所以payStartFP也是上一个term对应的信息在索引文件.pay中的最后位置+1\n重置跳表信息：该信息在后面介绍跳表时再展开介绍\n\n图7：\n\n  图7中，如果当前开始处理第二个term，那么此时docStartFP（docStart File Pointer缩写）、posStartFP、payStartFP如上所示，这几个信息将会被写入到索引文件.tim、.tip中，本文中我们只需要知道生成的时机点，这些信息的作用将在后面的文章中介绍。\n 是否还有文档包含当前term？\n图8：\n\n  按照文档号从小到大，依次处理当前term在一篇文档中的信息，这些文档中都包含当前term。\n 记录当前文档号到docSeen\n图9：\n\n  使用FixedBitSet对象docSeen来记录当前的文档号，docSeen在生成索引文件.tim、tip时会用到，这里我们只要知道它生成的时间点就行。\n 记录term所有文档中的词频totalTermFreq\n图10：\n\n  这里说的所有文档指的是包含当前term的文档，一篇文档中可能包含多个当前term，那么每处理一篇包含当前term的文档，term在这篇文档中出现的次数增量到totalTermFreq，totalTermFreq中存储了term在所有文档中出现的次数，同样增量统计docFreq，它描述了包含当前term的文档数量。\n  totalTermFreq、docFreq将会被存储到索引文件.tim、tip中，在搜索阶段，totalTermFreq、docFreq该值用来参与打分计算（见系列文章查询原理）。\n 是否生成了PackedBlock?\n图11：\n\n  每当处理128篇包含当前term的文档，就需要将term在这些文档中的信息，即文档号docId跟词频frequency，使用PackeInts进行压缩存储，生成一个PackedBlock。\n图12：\n\n  图12中，红框标注的即PackedBlock，关于PackedBlock的介绍以及几个问题在后面的流程中会介绍，这里先抛出这几个问题：\n\n为什么要生成PackedBlock\n为什么选择128作为生成PackedBlock的阈值\n\n 写入到跳表skipList中\n图13：\n\n  如果生成了一个PackedBlock，那么需要生成跳表，使得能在读取阶段能快速跳转到指定的PackedBlock，跳表skipList的介绍将在后面的文章中详细介绍，这里只要知道生成的时机点即可。\n 记录文档号跟词频信息\n图14：\n\n  将文档号跟term在当前文档中的词频frequency分别记录到两个数组docDeltaBuffer、freqBuffer中，注意的是由于文档号是按照从小到大的顺序处理的，所以docDeltaBuffer数组存放的是与上一个文档号的差值，但是term在每个文档中的词频frequency是无序的，所以无法使用差值存储词频frequency，故在freqBuffer数组中，数组元素是原始的词频值。\n为什么使用差值存储：\n  能降低存储空间的使用量，如果我们有下面的待处理的文档号集合，数组中按照文档号从小到大有序：\nint[] docIds = &#123;1, 3, 7, 10, 12, 13, 17&#125;\n  如果我们使用固定字节存储（见PackedInts（一）），那么根据17（数组中的最大值）的二进制为00010001，最少可以使用5个bit位（有效数据位）才能描述17，那么数组的其他元素都是用5个bit位存储的话，一共7个数组元素，共需要5*7 = 35个bit，如果使用差值存储（当前数组元素与前一个数组元素的差值），在计算了差值后，数组docIds如下所示：\nint[] docIds = &#123;1, 2, 4, 3, 2, 1, 4&#125;\n  docIds数组中最大值为4，二进制位00000100，那么所有数组元素使用3个bit位存储，共需要3*7 = 21个bit，可见能有效的降低存储空间。\n 生成Block\n图15：\n\n  当数组docDeltaBuffer中的数组元素个数达到128个以后，意味着已经处理了128篇文档，此时需要生成Block，即将数组docDeltaBuffer、freqBuffer中的数据经过PackeInts处理后生成一个PackedBlock，如下所示：\n图16：\n\n 结语\n  基于篇幅，剩余的流程将在下一篇文档中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["doc","pos","pay"]},{"title":"索引文件的生成（七）之tim&&tip","url":"/Lucene/Index/2020/0117/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%B8%83%EF%BC%89%E4%B9%8Btim&&tip/","content":"  本文承接索引文件的生成（六）继续介绍剩余的内容，下面先给出生成索引文件.tim、.tip的流程图。\n 生成索引文件.tim、.tip的流程图\n图1：\n\n 统计每一个term的信息\n图2：\n\n  执行到该流程，我们需要将当前term的一些信息（图1中的IntBlockTermState，见文章索引文件的生成（五））的汇总到所属域的信息中（这里先提一下的是，这些信息在后面使用FieldMetaData封装），图2中出现的字段的含义如下：\n\nsumDocFreq：包含当前域的所有term的文档数量总和，注意的是当前域可能有多个term在同一文档中\nsumTotalTermFreq：当前域的所有term在所有文档中出现的次数总和\nnumTerms：当前域中的term数量\nminTerm：当前域中最小（字典序）的term\nmaxTerm：当前域中最大（字典序）的term\n\n  例如我们有如下几篇文档：\n图3：\n\n  其中用红色标注的term属于域名为&quot;content&quot;的域，那么在处理完&quot;content&quot;之后，图2中的字段的值如下所示：\n\nsumDocFreq：b（2）+ c（3）+ f（1）+ h（1）= 7\nsumTotalTermFreq：b（3）+ c（3）+ f（1）+ h（1）= 8\nnumTerms：b、c、f、h共4个term\nminTerm：b\nmaxTerm：h\n\n  再处理完所有域之后，上述的信息在索引文件.tim中的位置如下：\n图4：\n\n 生成NodeBlock\n图5：\n\n  当前域的所有term处理结束后，那么将term栈中剩余未处理的PendingEntry生成NodeBlock（见文章索引文件的生成（六））。\n 记录当前域的信息在.tip文件中的起始位置indexStartFP\n图6：\n\n  到此流程，Lucene将要在索引文件.tip中写入当前域的FSTIndex信息，在读取阶段，通过读取索引文件.tip中的FSTIndex信息来获取当前域在索引文件.tim的内容，而所有域的FSTIndex信息连续的存储在索引文件.tip中，那么需要indexStartFP来实现&quot;索引&quot;功能，如下图所示：\n图7：\n\n 生成当前域的FSTIndex信息\n图8：\n\n  在图5的流程中，当前域的所有term处理结束后，term栈中剩余未处理的PendingEntry会被处理为NodeBlock，最终只会生成一个PendingBlock（没明白？见文章索引文件的生成（六）），并且PendingBlock中的index信息，即FST信息将会被写入到FSTIndex中，数据结构如下：\n图9：\n\n  在文章FST算法（一）中我们说到，inputValues跟outputValues作为输入将被用于生成FST，在那篇文章的例子中，inputValues数组中每一个数组元素inputValue都不是&quot;[ ]“，如果inputValues数组中第一个inputvalue为”[ ]“，由于它不能生成node，那么这种情况将被特殊处理，”[ ]“对应的outputValue将使用图9中的emptyOutputBtes字段存储，同时使用emptyLen字段描述emptyOutputBtes占用的字节数，在源码中，这种特殊情况被称为&quot;empty string”、“empty input”。\n 标志位\n  该字段的可取值为 0、1，用于描述FSTIndex两种不用的数据结构，使得在读取阶段能正确的读取字段值。\n inputType\n  inputType可选值有三个，分别描述了label（lable的概念见文章FST算法（一））占用的字节数量：\n图10：\n\n\nBYTE1：一个字节\nBYTE2：两个字节\nBYTE4：四个字节\n\n startNode\n  该字段描述的是在读取FST阶段，current数组中第一个读取的位置，在文章FST算法（一）的例子中，startNode的值为40。\n numBytes\n  该字段描述的是current数组占用的字节数。\n bytes\n  该字段描述的是current数组中所有元素，也就是在文章索引文件的生成（六）之tim&amp;&amp;tip中所有PendingBlock中的prefix（除了prefix为&quot;[ ]&quot;的信息，它使用emptyOutputBytes存储）对应的FST的inputValue。\n 生成FieldMetaData\n图11：\n\n  由于Lucene的处理逻辑是先处理所有的域，最后把这些域的信息写入一次性到索引文件.tip中，故在处理完一个域后，要将该域的信息通过FieldMetaData来存储下，当所有的域处理结束后，遍历所有的FieldMetaData，将这些信息依次到索引文件.tip中，故有了图7中的数据结构，FieldMetaData中只有一个信息需要介绍下，其他信息可以自行看源码中https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java 的内部类FieldMetaData：\n\ndocCount：该值描述的是包含当前域的文档号数量，以图3为例，三篇文档都包含了域名为&quot;content&quot;的文档，所以docCount = 3，该值是在生成索引文件.doc、pos、pay（见文章索引文件的生成（一））的过程中统计的，统计的时机点如下图红框标注的流程点：\n\n图12：\n\n  至此，生成索引文件.tim、.tip的流程介绍完毕。\n 结语\n  相信看完这七篇的系列文章后，大家对于索引文件.doc、.pos、.pay、.tim、tip的生成以及他们之间的关系有了深刻的了解，自己品。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["tim","tip"]},{"title":"索引文件的生成（三）之跳表SkipList","url":"/Lucene/Index/2020/0103/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E8%B7%B3%E8%A1%A8SkipList/","content":"  在文章索引文件的生成（一）中我们说到，在生成索引文件.doc、.pos、.pay的过程中，当处理了128篇文档后会生成一个PackedBlock，并将这个PackedBlock的信息写入到跳表skipList中，使得在读取阶段能根据文档号快速跳转到目标PackedBlock，提高查询性能。\n  将PackedBlock的信息写入到跳表skipList的时机点如下图红色框所示：\n图1：\n\n  本篇文章介绍下skipList的写入/读取的过程，在介绍之前，我们先大致的描述下跳表是什么，如下图所示：\n图2：\n\n  图2中每一层中的数值描述的是信息的编号，例如在level=0中的数值&quot;3&quot;描述的是第3条信息，在写入的过程中，每处理skipMultiplier个信息就在上一层生成一个索引，例如在level=1中的第6条信息中有一个索引，他指向level=0的一个位置；level=2中的第38条信息，它包含一个索引，指向level=1中的一个位置。由此可见跳表是一个多层次的数据结构，除了第0层外，其他层中的每一条信息中拥有一个指向下一层的索引。\n  接着我们开始介绍在Lucene中如何生成以及读取SkipList。\n 写入到跳表skipList中的流程图\n图3：\n\n 初始化\n图4：\n\n  需要初始化的内容如下所示：\n\nskipInterval：该值描述了在level=0层，每处理skipInterval篇文档，就生成一个skipDatum，该值默认为128\nskipMultiplier：该值描述了在所有层，每处理skipMultiplier个skipDatum，就在上一层生成一个新的skipDatum，该值默认为8\n\n  skipInterval、skipMultiplier、skipDatum的关系在索引文件.doc中的关系如下所示：\n图5：\n\n\nnumberOfSkipLevels：该值描述的是我们即将生成的skipList中的层数，即图2中的level的最大值\n\n如何计算numberOfSkipLevels：\n  根据上文中，skipInterval跟skipMultiplier的介绍，可以看出我们根据待处理的文档数量就可以计算出numberOfSkipLevels，计算公式如下：\nint numberOfSkipLevels = 1 + MathUtil.log(df/skipInterval, skipMultiplier)\n  上述公式中，df（document frequency）即待处理的文档数量。\n待处理的文档数量是如何获得的：\n  在索引文件的生成（一）中我们介绍了生成索引文件.doc的时机点，即在flush阶段，所以就可以根据的段的信息获得待处理的文档数量。\n\nskipBuffer[ ]数组：该数组中存放的元素为每一层的数据，根据图2可以知道，该数据就是SkipDatum的集合，并且数组的元素数量为numberOfSkipLevels，skipBuffer[ ]中每一个元素在索引文件.doc中对应为一个SkipLevel字段，如下所示：\n\n图6：\n\n 计算写入的层数numLevels\n图7：\n\n  根据当前已经处理的文档数量，预先计算出将待写入SkipDatum信息的层数，计算方式如下：\nint numLevels = 1;// 计算出在level=0层有多少个SkipDatumdf /= skipInterval;while ((df % skipMultiplier) == 0 &amp;&amp; numLevels &lt; numberOfSkipLevels)&#123;    numLevels++;    // 每skipMultiplier个SkipDatum就在上一层生成一个SkipDatum    df /= skipMultiplier;&#125;\n 是否还有未处理的层？\n图8：\n\n  在上一个流程中，如果numLevels的值 &gt; 1，那么按照level从小到大依次处理。\n 层内处理\n图9：\n\n  在层内处理的流程中，首先将在图1中是否生成了PackedBlock流程中生成的block信息生成一个SkipDatum写入到skipData中，block包含的信息如下图红框所示：\n图10：\n\n点击查看大图\n  另外，在图10中，如果在level&gt;0的层写入一个SkipDatum后，相比较在level=0中的SKipDatum，它多了一个字段SkipChildLevelPointer，它是一个指针，指向下一层的一个SkipDatum。\n  SkipDatum中的字段含义在文章索引文件之.doc中已经介绍，不赘述，另外图10中的SkipDatum中的信息除了SkipChildLevelPointer，其他所有的字段都是用差值存储，所以在图9中，我们需要执行记录当前层的skipData信息的流程，使得下一个同一层内的新生成的SkipDatum可以用来进行差值计算。\nSkipDatum中的字段干什么用的\n  这些字段的作用正是用来展示跳表SkipList跳表在Lucene中的功能，它们作为指针来描述当前block在的其他索引文件中的位置信息。在文章索引文件的生成（二）中我们介绍图1中的处理完一篇文档后的工作流程点时，说到在该流程点生成了几个信息，他们跟SkipDatum中的字段的对应关系如下：\n\nlastBlockDocID：记录刚刚处理完的那篇文档的文档号，即DocSkip\nlastBlockPayFP：描述是处理完128篇文档后，在索引文件.pay中的位置信息，即PayFPSkip\nlastBlockPosFP：描述是处理完128篇文档后，在索引文件.pos中的位置信息，即PosFPSkip\nlastBlockPosBufferUpto：在posDeltaBuffer、payloadLengthBuffer、offsetStartDeltaBuffer、offsetLengthBuffer数组中的数组下标值，即PosBlockOffset\nlastBlockPayloadByteUpto：在payloadBytes数组中的数组下标值，即PayLength\n\n  另外还有DocFPSkip值并没有在处理完一篇文档后的工作流程中获取，该值描述的是在索引文件.doc当前可写入的位置，故直接可以获取。\n  SkipDatum中的字段与索引文件.doc、.pos、.pay的关系如下所示：\n图11：\n\n点击查看大图\n  图10中，由于是按照每处理128篇文档才执行写入到跳表skipList中的流程，那么有可能此时位置信息position、偏移信息offset，payload信息没有生成一个PackedBlock，那么SkipDatum需要两个指针的组合才能找到在索引文件.pos、.pay中的位置，比如说我们需要PosFPSkip+PosBlockOffset的组合值才能找到位置信息（没明白的话说明你没有看文章索引文件的生成（一）以及索引文件的生成（二））\n 结语\n  基于篇幅，读取的SkipList的逻辑将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["doc","skipList"]},{"title":"索引文件的生成（九）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Index/2020/0406/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%B9%9D%EF%BC%89%E4%B9%8Bdim&&dii/","content":" 索引文件的生成（九）（Lucene 8.4.0）\n  上一篇文章中，我们介绍了在索引（index）阶段，Lucene收集了跟点数据相关的信息，这些信息在flush阶段会被读取，用于生成索引文件.dim&amp;&amp;.dii，从本文开始介绍索引文件.dim&amp;&amp;.dii生成的详细过程，如图1所示，另外阅读本文中需要前置知识：索引文件之dim&amp;&amp;dii：\n图1：\n\n MutablePointValues\n  MutablePointValues为图1流程图的准备数据，该对象中包含了在索引阶段收集的点数据的信息，我们在文章索引文件的生成（八）之dim&amp;&amp;dii中已经详细的展开介绍了，故这里只简单的列出这些信息：\n\nnumPoints\ndocIDs\nnumDocs\nbytes\n\n  MutablePointValues对象中至少包含了上述的几个信息，但如果IndexWriter对象使用了IndexSort配置，那么MutablePointValues中还要额外包含一个信息：DocMap对象。\n  我们简单的回顾下IndexSort这个概念，文章构造IndexWriter对象（一）中我们说到，在构造一个IndexWriter对象的的过程中，其中一个流程是设置IndexWriter的配置信息IndexWriterConfig，当设置了IndexSort（IndexSort的一些介绍见文章文档提交之flush（三））配置后，段中的文档会按照IndexSort的排序规则进行段内的文档排序，由于每添加（例如IndexWriter.addDocument(…)方法）一篇文档，就排一次序，在实现（implementation）上不可能执行真正的数据排序（数据之间的交换），故通过一个映射关系，即DocMap对象来描述文档之间的排序关系，所以在一个段内，当设置了IndexSort配置后，每一篇文档有一个原始的段内文档号，该文档号按照文档被添加的先后顺序，是一个从0开始的递增值，而DocMap对象中提供了方法来描述文档之间基于IndexSort的排序关系，该方法在源码中的注释见 https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/index/Sorter.java 中内部类DocMap提供的两个方法，这里简单的给出：\n/* Given a doc ID from the original index, return its ordinal in the sorted index. */    abstract int oldToNew(int docID);/* Given the ordinal of a doc ID, return its doc ID in the original index. */    abstract int newToOld(int docID);\n 执行处理前的初始化的工作\n  在当前流程点，我们就可以基于MutablePointValues中的信息来执行处理前的初始化的工作，工作内容为初始化以下几个信息：\n\nnumLeaves\nsplitPackedValues\nleafBlockFPs\ndocsSeen\nparentSplits\nmaxPackedValue\nminPackedValue\n\n numLeaves\n  该值为long类型的变量，它用来描述我们随后即将构建的BKD树中的叶子节点的数量。\n  为什么能预先计算出BKD树中的叶子节点数量：\n  在图1的流程点构建BKD树的节点值（node value）中我们将会介绍Lucene将一个节点生成左右子树的划分规则（可以先看下文章Bkd-Tree简单的了解划分规则），该规则会使得总是生成一颗满二叉树，那么根据满二叉树的性质，我们只需要知道点数据的数量就可以计算出BKD树中的叶子节点的数量，而点数据的数量在收集阶段实现了统计，并且用numDocs（见上文中的MutablePointValues）来描述。\n  为什么要先计算出BKD树中的叶子节点数量：\n  在后面的流程中，我们可以根据当前处理的节点编号来判断当前节点是内部节点（inner node）还是叶子节点（leaf node），在介绍构建BKD树的节点值（node value）时会详细展开介绍numLeaves的作用，这里先介绍下节点编号是什么：\n图2：\n\n  图2中，按照广度遍历的顺序，依次为每一个节点赋予一个节点编号，节点编号的作用在后面的流程中会使用到。\n  另外numLeaves也用来初始化例如splitPackedValues、leafBlockFPs，见下文。\n splitPackedValues\n  splitPackedValues是一个字节数组，该值用来描述每一个节点使用哪个维度（维度编号）进行划分以及维度的值（在本篇文章中暂时不用理解这段话，在后面的文章中展开介绍），在当前流程点，我们只需要知道该数组的初始化的时机点，初始化的代码很简单，故直接给出：\nfinal byte[] splitPackedValues = new byte[numLeaves * (bytesPerDim + 1)];\n  上述代码中，numLeavs为即将构建的BKD树中的叶子节点的数量，bytesPerDim的值为每个维度的值占用的字节数量，例如int类型的维度值占用4个字节（见文章索引文件的生成（八）之dim&amp;&amp;dii中关于数值类型转为为字节类型的介绍）。\n leafBlockFPs\n  leafBlockFPs是一个long类型的数组，在当前流程点被初始化，如下所示：\nfinal long[] leafBlockFPs = new long[numLeaves];\n  leafBlockFPs在随后的流程中会记录每一个叶子节点的信息在索引文件.dim中的起始位置，如下所示：\n图3：\n\n  图2中的LeafNodeData描述的是每个叶子节点的信息，可见leafBlockFPs数组中的数组元素数量为叶子节点的数量，即上文中的numLeaves。\n docsSeen\n  docsSeen是一个FixedBitSet对象，用来去重记录包含当前点数据域的文档的数量，例如我们添加下面两篇文档：\n图4：\n\n  图3中，尽管有3条点数据内容，但是文档1中包含了2条，那么包含域名为&quot;content&quot;的点数据的文档的数量为2，docsSeen中统计的文档数量在后面的流程会被写入到索引文件.dim中，如下红框所示：\n图5：\n\n parentSplits\n  parentSplits是一个int类型的数组，首先看下初始化这个数组的源码：\nfinal int[] parentSplits = new int[numDims];\n  上述源码中，numDims指的是当前处理的点数据的维度数，例如图3中处理的是三维的点数据，那么numDims的值为3。\n  在文章Bkd-Tree中介绍关于选出切分维度的内容时候说到，选择的判断依据如下：\n条件1. 先计算出切分次数最多的那个维度，切分次数记为maxNumSplits，如果有一个维度的切分次数小于 (maxNumSplits / 2) ，并且该维度中的最大跟最小值不相同，那么令该维度为切分维度。条件2. 计算出每一个维度中最大值跟最小值的差值，差值最大的作为切分维度(篇幅原因，下面的例子中仅使用了这种判定方式)。\n  我们只看条件一，parentSplits数组就是用来存储某个维度被选为切分维度的次数，在条件一中通过读取parentSplits数组来获得对应信息。\n  对于上述的两个条件，在后面的文章会再次提交，到时候再作详细的介绍。\n maxPackedValue minPackedValue\n  maxPackedValue minPackedValue都是字节数组，在当前流程点执行处理前的初始化的工作中，通过遍历所有的点数据，找出每一个维度的最大值跟最小值，其中minPackedValue记录了每一个维度的最小值，maxPackedValue记录了每一个维度的最大值。\n  还是以图4为例，在遍历了3个点数据的信息后，maxPackedValue minPackedValue的数据如下所示：\nminPackedValue:&#123;1, 5, 12&#125;maxPackedValue:&#123;3, 6, 23&#125;\n  为什么要统计maxPackedValue minPackedValue\n  上文中说到了选出切问维度的两个条件，其中条件2中，需要知道每一个维度中最大值跟最小值，而当前的maxPackedValue minPackedValue就用来为第一个节点的划分提供了依据。\n  在后面的流程中，maxPackedValue minPackedValue的值将会被记录到索引文件.dim中，如下红框所示：\n图6：\n\n 结语\n  无。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dim","dii"]},{"title":"索引文件的生成（二十一）之dvm&&dvd（Lucene 8.4.0）","url":"/Lucene/Index/2020/0605/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89%E4%B9%8Bdvm&&dvd/","content":"  索引文件.dvm&amp;&amp;.dvd中根据文档中包含的不同类型的DocValuesFields，包含下面的DocValues信息：\n\nBinaryDocValues：\nNumericDocValues：见文章索引文件的生成（十五）之dvm&amp;&amp;dvd\nSortedDocValues：见文章索引文件的生成（十八）之dvm&amp;&amp;dvd\nSortedNumericDocValues：见文章索引文件的生成（十七）之dvm&amp;&amp;dvd\nSortedSetDocValues：索引文件的生成（十八）之dvm&amp;&amp;dvd\n\n  本篇文章开始介绍生成索引文件.dvd、.dvm之BinaryDocValues的内容，在此之前，我们先介绍下在索引（index）阶段以及flush阶段，Lucene是如何收集文档中的BinaryDocValues的信息。\n BinaryDocValues\n  BinaryDocValues信息对应的是在文档中BinaryDocValuesField域中的信息，它同NumericDocValues、SortedDocValues一样，在一篇文档中，相同域名的BinaryDocValuesField只能有一个，否则就会报错，如下所示：\n图1：\n\n图2：\n\n 收集文档的BinaryDocValues信息\n  收集BinaryDocValues信息的代码入口方法为：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java 的 addValue(int docID, BytesRef value)方法，收集的信息有：DocId、bytes、lengths。\n  我们根据一个例子来介绍上述收集的信息，注意的是，图3中的文档2并没有包含BInaryDocValuesField：\n图3：\n\n DocId\n  DocId即包含BinaryDocValuesField域的文档的文档号，并且使用DocsWithFieldSet存储，DocsWithFieldSet存储文档号的过程在文章索引文件的生成（十五）之dvm&amp;&amp;dvd已经介绍，不赘述。\n bytes\n  bytes在源码中是PagedBytes对象，我们不用关心PagedBytes类的具体内容，我们只需要知道，在这个类中，有一个二维字节数组blocks[ ][ ]以及一个字节数组currentBlock[ ]，他们用来存储BinaryDocValuesField的域值，如下所示：\n图4：\n\n  在实际写入过程中，二维数组blocks是动态成长的，当一个currentBlock[ ]中填满域值后，它会被写入到二维数组中，另外出于存储性能的考虑，currentBlock[ ]中每个字节都会被用于存储，这样使得可能某个域值会被分散在两个currentBlock[ ]中。\n lengths\n  lengths是一个PackedLongValues.Builder对象，同样的我们不需要关心PackedLongValues.Builder的具体内容， 我们只需要知道，在这个类中，有一个pending数组，它用来收集包含BinaryDocValuesField域的每篇文档中的对应的域值长度，对于图3的例子如下所示：\n图5：\n\n  域值的长度即域值占用的字节数量。\n  在索引阶段收集了lengths、bytes之后使得在生成索引文件.dvd、.dvm之BinaryDocValues时能获得域值的信息，下文将会作出介绍。\n 生成索引文件.dvd、.dvm之BinaryDocValues\n  在索引阶段收集完BinaryDocValues的信息后，随后在flush()阶段会将这些信息写入到索引文件.dvd、.dvd中，流程图如下：\n图6：\n\n 写入域值信息\n图7：\n\n  在当前流程点，即通过上文中的bytes以及lengths获取域值，然后写入到索引文件.dvd中，如下所示：\n图8：\n\n  根据pending[ ]数组中数组元素，读取出包含BinaryDocValues的文档的域值的长度，然后去blocks[ ][ ]中读取对应长度的字节数，这些字节组成一个term存储到索引文件TermsValue字段的Term中。另外需要在索引文件.dvm中生成TermsValueMata字段用来在读取阶段获取TermsValue字段在索引文件.dvd的数据区间。\n 写入文档号信息\n图9：\n\n  文档号信息的存储跟SortedDocValues、SortedSetDocValues、NumericDocValues一致的，都是生成图8中DocIdData、DocIdIndex字段的数据结构，不赘述，可以阅读文章索引文件的生成（十六）之dvm&amp;&amp;dvd。\n 写入TermsIndex信息\n图10：\n\n  从图8中可以看出，TermsValue中所有域值都是连续存储，那么还需要增加TermsIndex信息来描述一个term的长度，才能在读取阶段分离出每一个term，如下所示：\n图11：\n\n  图11中，TermsIndex中的address记录了每个term的相对于第一个term的距离，可以看出每个address的差值为一个term的占用的字节数量，另外还需要在索引文件.dvm中生成TermsIndexMeta用来在读取阶段能获取TermsIndex字段在索引文件.dvd中的数据区间。\n 结语\n  上文中未介绍的字段含义请看文章BinaryDocValues。\n  从下一篇文章开始，我们将介绍索引文件.dvm、dvd的读取过程，届时还会介绍这几种DocValues的用法差异等等。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dvd","dvm"]},{"title":"索引文件的生成（二十三）之fdx&&fdt&&fdm（Lucene 8.6.0）","url":"/Lucene/Index/2020/1015/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  从本篇文章开始介绍用于描述存储域（存储域的概念见文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm）的索引文件.fdx、.fdt、.fdm的生成过程，直接给出流程图：\n图1：\n\n  从图1中可以看出，生成完整的索引文件.fdx、.fdt、.fdm的过程分布在两个阶段：索引阶段、flush阶段。这也解释了为什么在文章文档提交之flush（三）的图5中，其他索引文件都是&quot;生成&quot;、而索引文件.fdx、.fdt、.fdm则是&quot;更新&quot;，注意的是那篇文章中是基于Lucene 7.5.0，故不存在索引文件.fdm。\n 索引阶段\n 处理存储域的域值信息\n图2：\n\n  图2中Document指定是索引阶段的一篇待处理的文档，我们集合一个例子来加以介绍：\n图3：\n\n  当开始处理一篇文档（Document）时，我们需要记录对存储域的域值，图1的例子中有两篇文档，在执行了流程点处理存储域的域值之后，域值信息将被写入到一个字节数组bufferedDocs[ ]数组（在源码中bufferedDocs其实是一个对象，它用来将数据写入到字节数组buffer[ ]中，为了便于介绍，所以我们直接认为bufferedDocs是一个字节数组）中，域值信息中包含两类信息：\n\nFieldNumAndType：该信息是域的编号跟域值的值类型的long类型的组合值，组合公式为：FieldNumAndType = (域的编号 &lt;&lt; 3) | 域值的值类型，也就是说long类型的FieldNumAndType，低3位用来存储域值的值类型，高61位用来存储域的编号\n\n域的编号是一个从开0开始的递增值，每种域名有唯一的一个编号，根据域名来获得一个域的编号，例如图3中域名”content“是第一个被处理的域名，所以该域的编号为0，同理域名”attachment“、&quot;author&quot;的编号分别为1、2。\n域值的值类型共有以下几种，例如图3中第54行的域值的值类型是STRING类型，第56行的域值的值类型是NUMERIC_INT类型\n\nSTRING：固定值：0x00，域值为String类型\nBYTE_ARR：固定值：0x01，域值为字节类型\nNUMERIC_INT：固定值：0x02，域值为int类型\nNUMERIC_FLOAT：固定值：0x03，域值为float类型\nNUMERIC_LONG：固定值：0x04，域值为longt类型\nNUMERIC_DOUBLE：固定值：0x05，域值为double类型\n\n\n以图3中第56行的域为例，域名&quot;author&quot;的域的编号为2，域值&quot;3&quot;的值类型为NUMERIC_INT，那么组合后FieldNumAndType=2&lt;&lt;3∣2=18FieldNumAndType = 2 &lt;&lt; 3 | 2 = 18FieldNumAndType=2&lt;&lt;3∣2=18\n\n\nValue：该信息包含了域值被编码成字节后的值以及占用的字节数量length\n\n  对于图3的例子，这两篇文档中存储域的域值写入到bufferedDoc[ ]数组后，如下所示：\n图4：\n\n  图3中文档0有四个域信息，但是域名&quot;attachment&quot;的域的属性为&quot;STORE.NO&quot;，那么域值就不会别写入到bufferedDoc[ ]数组中，这会导致在搜索阶段，当文档0满足某个查询条件后，我们无法获得文档0中域名&quot;attachment&quot;的域值。\n  bufferedDoc[ ]数组中的内容将被写入到索引文件.fdt中：\n图5：\n\n  图5中，每个Doc字段就描述了一篇文档的所有域存储域的域值信息。\n 增量统计存储域的信息\n图6：\n\n  在生成一个chunk之前，需要增量统计存储域的信息，满足生成一个chunk的条件后，存储域的信息将被写入到索引文件.fdt中。\n  在这个流程点，需要增量的记录下面的数据：\n\nnumBufferedDocs：该值是一个从0开始递增的值，每次处理一个文档（文档中可能不包含存储域），该值+1，它描述了一篇文档的的段内文档号，同时该值也描述了当前生成一个chunk前当前已经处理的文档数量，该信息在下文中中将会用于流程点是否生成一个chunk\nnumStoredFields[ ]数组：该数组的下标值是numBufferedDocs，数组元素描述的是每篇文档中存储域的数量，例如图3中的文档0，它就包含了3个存储域\nendOffsets[ ]数组：该数组的下标值是numBufferedDocs，数组元素是一个索引值，该值作为bufferedDocs[ ]数组的下标值，用来获取某篇文档的最后一个存储域的域值信息在bufferedDocs[ ]数组中的结束位置\n\n  为了能更好的描述这些信息，我们需要新给一个例子：\n图7：\n\n  图7中要注意的是，文档1中不包含存储域，处理完这三篇文档后，收集到的存储域信息如下所示：\n图8：\n\n  图8中，由于文档1不包含存储域，所以在numStoredFields[ ]数组中，下标值为1的数组元素为0，另外在endOffsets[ ]数组中，文档0的存储域信息存储为下标值1的数组元素，即11，描述了文档0中最后一个存储域的域值信息在bufferedDocs[ ]数组中的结束位置。\n  另外文档1中不包含存储域，为什么另它对应在endOffsets[ ]中的数组元素跟文档0是一致以及这些数组如何配合使用的介绍将在随后的介绍索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的读取的文章中再详细展开。\n  存储域的信息对应在索引文件.fdt中的字段如下所示：\n图9：\n\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["fdx","fdt","fdm"]},{"title":"索引文件的生成（二十二）之nvd&&nvm（Lucene 8.4.0）","url":"/Lucene/Index/2020/0828/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%89%E4%B9%8Bnvd&&nvm/","content":"  在执行flush()的过程中，Lucene会将内存中的索引信息生成索引文件，本篇文章继续介绍索引文件.nvd&amp;&amp;.nvm，其生成的时机点如下图红色框标注：\n图1：\n\n  图1的流程图属于Lucene 7.5.0，在Lucene 8.4.0中同样适用，该流程图为flush()过程中的一个流程点，详情见文章文档提交之flush（二）。\n  生成索引文件.nvd&amp;&amp;.nvm的目的在于存储normValue值以及文档号，我们先了解下在索引阶段（index phase），Lucene是如何计算、收集normValue值的。\n 计算文档的normValue值\n  在文章查询原理（四）中介绍打分公式时我们知道，计算一篇文档的分数时会考虑一个norm值，该值描述的是文档长度对打分的影响，并且norm值是通过cache[ ]（数组长度为256）数组获得的，而normValue则是作为该数组的下标值。\n  normValue的全名为normalization value，标准化的值，通过Lucene中的SmallFloat.intToByte4(int numTerms)方法生成一个标准化的值，该方法的参数numTerms描述的是文档的长度，文档的长度的计算方式取决于不同的打分公式，我们以默认的打分公式BM25为例展开介绍：\n图2：\n\n  图2中先计算出numTerms，然后通过SmallFloat.intToByte4方法生成一个标准化的值，即normValue。\n  由图2可知numTerms共有三种计算方式：\n\n119行代码：一篇文档中包含某个域的域值数量（去重，即重复的域值不纳入域值数量）\n123行代码：一篇文档中包含某个域的域值数量（非去重）\n121行代码：非去重的域值数量 与 state.getNumOverlap()的差值，该方法的含义在介绍分词会展开\n\n  注意的是在索引阶段，每处理一篇文档，会计算文档中每个域对应的文档长度，因为在查询阶段，无法知道会用哪个域作为查询条件。\n  为什么要执行标准化的操作\n  由numTerms的计算方式可以看出，如果直接采用numTerms，会造成突兀的域值数量对打分公式产生显著的影响，故需要通过SmallFloat.intToByte4方法平缓该影响，该方法的返回值为byte类型，类似归一化操作，该方法将文档长度标准化到[1, 255]的取值区间，即computerNorm的返回值的取值范围为[1, 255]，注意的是，normValue == 1时候为一个特殊值，它描述了当前域不考虑文档的长度，代码中，可以通过FieldType.setOmitNorms(true)方法设置。\n 收集文档的normValue值\n  同其他索引文件一样，每个域都会各自收集normValue值，对应在源码中，每个域都会对应生成一个NormValuesWriter来实现收集。\n  收集的过程中主要收集文档号跟normValue，其中文档号使用DocsWithFieldSet对象收集，该对象的处理逻辑见文章索引文件的生成（十五）之dvm&amp;&amp;dvd，另外使用PackedLongValues对象收集normValue值，该对象的介绍见文章PackedInts（一）。\n 生成索引文件.nvd&amp;&amp;.nvm\n  由于之前介绍过了其他索引文件的生成过程，相比较下来索引文件.nvd&amp;&amp;.nvm的生成过程过于简单并且雷同，所以就不写了。。。\n 结语\n  对于该索引文件的内容在随后介绍索引文件.doc、pos、pay的读取的文章中会顺便提及。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["nvd","nvm"]},{"title":"索引文件的生成（二十五）之kdd&kdi&kdm（Lucene 8.7.0）","url":"/Lucene/Index/2020/1217/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%89%E4%B9%8Bkdd&kdi&kdm/","content":"  在系列文章索引文件的生成（Lucene 7.5.0）中，我们介绍了存储维度（见文章Bkd-Tree）值大于等于2的数值类型对应的索引文件的生成过程。对于维度值等于1的情况，其生成过程有少许的不同。为了后续便于介绍该类型的索引文件的合并，我们需要再写一篇文章来介绍其生成过程。\n 索引文件的变更\n图1：\n\n  从Lucene 8.6.0开始，用于存储点数据（point value）的索引文件由原先的索引文件dim&amp;&amp;dii，改为三个索引文件kdd&amp;kdi&amp;kdm，其变更的目的可以看这个issue，本文中不展开讨论。\n  给出图1的目的是为了说明，点数据对应的索引文件，其生成的总体流程没有太大变动的，下文中介绍维度值为1的生成过程（基于Lucene 8.7.0）时，只会介绍与维度值大于等于2的不同的流程点，其相同的流程点可以参考系列文章索引文件的生成（基于Lucene 7.5.0）。\n 差异\n  维度值为1的生成索引文件的过程中，唯一不同点在于图1中的流程点构建BKD树的节点值（node value），如下所示：\n图2：\n\n点击查看大图\n  图2中，红框标注的流程点表示在这些流程点的处理方式是一致的。\n 生成索引文件kdd&amp;kdi&amp;kdm的流程图\n图3：\n\n 节点\n图4：\n\n  流程点节点中包含了在索引（Indexing）期间收集的点数据信息，收集相关的介绍见文章索引文件的生成（八）。\n 节点内的点数据排序\n图5：\n\n  无论维度的数量是多少，最终生成的bkd树的每个叶子节点中的点数据是有序的。对于维度大于等于2的情况，叶子节点中点数据的排序规则取决这个叶子节点的父节点在进行左右子树划分时选择的某个维度，对应图2中流程点内部节点的排序，该流程点的详细介绍见文章索引文件的生成（十）。对于维度等于1的情况，由于不用考虑选择哪个维度进行排序，所以对于某个点数据域来说，直接对flush/commit收集到的全量点数据进行排序即可，随后对这个有序的集合按块划分（下文会介绍）后，每一块中的点数据集合将被写入到一个叶子节点，如下所示。另外排序使用的算法为最大有效位的基数排序(MSB radix sort)。\n图6：\n\n点击查看大图\n  当然了，排序过程中并不会真正的去改变排序前的点数据集合，通过一个int类型的ord[ ]实现一个映射关系来描述排序关系。同样关于ord[ ]数组的介绍可以看文章 索引文件的生成（十）之dim&amp;&amp;dii。\n 统计leafCardinality以及其他一些信息\n图7：\n\n  在获得了有序的点数据集合后，接着有序的依次取出每一个点数据，每处理一个点数据，需要统计leafCardinality以及其他一些信息。\n leafCardinality\n  对于一个有序的集合，可以划分为一个或多个区间，要求每个区间内至少包含一个数据，如果包含多个数据，那么要求所有的数据是相同的，leafCardinality描述的正是区间数量。\n图8：\n\n  随后在图2的流程点写入不同后缀信息到索引文件.kdd中中，会根据根据leafCardinality的值来选择（用于来判断是否使用游标编码（Run Length Encoding）存储）存储开销较低的数据结构来存储点数据的后缀信息。同样的，详细的过程见文章索引文件的生成（十三）的介绍。\n 其他一些信息\n  处理统计leafCardinality的值，还需要计算下面一些信息：\n\ndocsSeen：docsSeen是一个FixedBitSet对象，用来去重记录包含某个点数据域的文档的数量：\n\n图9：\n\n  图9中，由于文档4中添加了两次IntPoint，那么在点数据的收集阶段，会认为一共收集了5个点数据。通过docsSeen可以正确的统计文档的数量，该值将被写入到索引文件.kdm的DocCount字段中：\n图10：\n\n\nleafDocs[ ]：用来统计即将写入到叶子节点的文档号，文档号为段内文档号。leafDocs[]中的数组元素不是有序的，数组元素对应的点数据的值是有序的。\n\n 是否生成一个叶子节点？\n图11：\n\n  在前面的流程中，当处理了maxPointsInLeafNode个点数据后（即上文中提到的按块划分），则满足条件生成一个叶子节点。\n maxPointsInLeafNode\n  maxPointsInLeafNode描述的是叶子节点中允许存储的点数据的数量最大值（max points allowed on a Leaf block）。maxPointsInLeafNode的默认值为512。\n 其他流程点\n  图3中未介绍的流程点可以查看文章索引文件的生成（十二）之dim&amp;&amp;dii、索引文件的生成（十三）之dim&amp;&amp;dii，不赘述。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["kdd","kdi","kdm"]},{"title":"索引文件的生成（二十四）之fdx&&fdt&&fdm（Lucene 8.6.0）","url":"/Lucene/Index/2020/1016/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  本文承接文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm，继续介绍剩余的内容，先给出生成索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的流程图：\n图1：\n\n 索引阶段\n 是否生成一个chunk\n图2：\n\n  在上一篇文章中我们知道，在索引阶段，每处理一篇文档会将存储域的域值写入到bufferedDoc[ ]数组中并且增量统计存储域的信息，其中一个信息就是numBufferedDocs，它描述了在生成一个chunk之前，当前已经处理的文档数量。当前流程点的判断条件就依赖上文的描述，由于该条件的判断方式比较简单，故直接给出源码：\n图3：\n\n  图3中，bufferedDocs.getPosition( )方法描述了存储在bufferedDoc[ ]数组中的域值长度（占用字节数量），chunkSize是一个阈值，达到阈值后即满足第一种生成一个chunk的条件，chunkSize的值有两个可选项：16384跟61440，他们分别对应两种生成索引文件.fdt的模式：FAST跟HIGH_COMPRESSION模式，这两种模式描述了对bufferedDoc[ ]数组中的域值使用不同的压缩算法，分别是LZ4跟JDK中的Deflater。生成索引文件.fdt的模式可以通过实现抽象类Codec来自定义。\n  第二种满足生成的条件为numBufferedDocs&gt;=maxDocsPerChunknumBufferedDocs &gt;= maxDocsPerChunknumBufferedDocs&gt;=maxDocsPerChunk，其中maxDocsPerChunk也是一个阈值，同样有两个可选项：128跟512，分别对应上文中的生成索引文件.fdt的模式。\n  故图3中满足流程点是否生成一个chunk的条件就是，目前处理的域值长度或者处理的文档数量是否超过阈值。\n 生成一个chunk\n图4：\n\n  在当前流程点生成一个chunk中，我们将把目前已经处理的信息（域值信息、存储域的信息）生成一个chunk，完成下面几个工作：\n\n记录chunk的两个信息\ndocBase、numBufferedDocs写入到索引文件.fdt中\n存储域的信息写入到索引文件.fdt中\n域值信息写入到索引文件.fdt中\n\n 记录chunk的两个信息\n  此流程中会使用两个临时文件记录这个即将生成的chunk的两个信息：\n\nchunk在索引文件.fdt中起始读取位置\nchunk中的文档数量\n\n图5：\n\n  图5中的两个临时文件会统计一个段中所有chunk的两个信息，在图1的flush阶段，将分别用于生成NumDocs的信息、生成startPoints的信息这两个流程点。\n  至于chunk的这两个信息的作用，我们随后将在索引文件的读取之fdx&amp;&amp;fdt&amp;&amp;fdm的文章中详细展开。\n docBase、numBufferedDocs写入到索引文件.fdt中\n  docBase是chunk中第一个文档的段内文档号、将被写入到索引文件.fdt的DocBase字段，numBufferedDocs将被写入到索引文件.fdt的ChunkDos字段，ChunkDos字段中还包含了sliceBit，该值是一个布尔类型，在下文中会具体介绍，故numBufferedDocs和sliceBit将作为一个组合值被写入到ChunkDos字段中，组合方式为(numBufferedDocs&lt;&lt;1)∣sliceBit(numBufferedDocs &lt;&lt; 1 )| sliceBit(numBufferedDocs&lt;&lt;1)∣sliceBit，即ChunkDos的最低位用来存储布尔类型的sliceBit，如下所示：\n图6：\n\n 存储域的信息写入到索引文件.fdt中\n  在文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm中我们知道，存储域的信息在内存中使用numStoredFields[ ]数组，endOffsets[ ]数组存储，他们分别将被写入到DocFieldCounts字段跟DocLengths字段，如下所示：\n图7：\n\n  numStoredFields[ ]数组，endOffsets[ ]数组中的信息被写入到索引文件.fdt之前，会先进行编码压缩处理，不同的处理方式使得DocFieldCounts、DocLengths有不同的数据结构，该内容在文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm已经介绍，不赘述。\n 域值信息写入到索引文件.fdt中\n  在文章索引文件的生成（二十三）之fdx&amp;&amp;fdt&amp;&amp;fdm中我们知道、域值信息在内存中使用bufferedDocs[ ]数组存储，它将被写入到CompressedDocs字段，如下所示：\n图8：\n\n  域值信息被写入到索引文件.fdt之前，会根据域值的总长度（占用字节数量）判断是否需要进行切片压缩，判断条件如下：\n图9：\n\n  图9中，当域值长度满足条件后，需要对域值进行切片压缩，将域值按照chunkSize划分为每一个切片，然后对每个切片使用LZ4算法（上）算法进行压缩处理。\n  至此，在索引阶段生成索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的流程点介绍完毕。从上文的介绍可以发现，有可能存在这么一种情况，添加完最后一篇文档后，还未满足图2中是否生成一个chunk的条件，也就是说，还有一些文档的域值信息、存储域信息未被写入到索引文件.fdt中，那么这些未处理的文档将会在flush阶段完成。\n flush阶段\n  在flush阶段，一方面将处理上文中说的未处理的文档的域值信息、存储域信息，另一方面还会根据图5中的两个临时文件中的信息生成索引文件.fdx、fdm。\n  图1中flush阶段的流程在执行flush（IndexWriter.flush()）的流程中的位置如下所示：\n图10：\n\n点击查看大图\n  注意的是图10中除了图1中的流程，其他的流程都是基于Lucene 7.5.0，与Lucene 8.6.0可能会有些差异，但是生成索引文件fdx&amp;&amp;fdt&amp;&amp;fdm在flush阶段的时机点是相同的。\n 生成一个chunk\n  这里生成一个chunk的逻辑跟图4中的流程点是一样，区别在于当前即将生成的chunk中的域值长度以及文档数量是不满足图2中的条件的，在执行完当前流程后，会统计增量统计一个变量numDirtyChunks，它随后将被写入到索引文件.fdt中。\n 生成NumDocs的信息\n图11：\n\n  在图1的索引阶段，使用了图5中的临时文件_0_Lucene85FieldsIndex-doc_ids_0.tmp存储了一个段中每个chunk中包含的文档数量，从上文的描述我们可以知道，一个chunk中的文档数量可能因为域值的长度而各不相同。\n  处理过程为读取每一个chunk中的文档数量，每处理1024（2 &lt;&lt; blockshift（blockshift的介绍见文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm））个chunk就生成一个NumDocsBlock，然后写入到索引文件.fdx中：\n图12：\n\n  图12中NumDoc字段为某个chunk中的文档数量，最后NumDocsBlock会先进行编码再使用PackedInts（一）进行压缩处理后再写入到索引文件.fdx中。\n  上文中说到每处理2 &lt;&lt; blockshift个chunk就生成一个NumDocsBlock，此时还会生成NumDocMeta信息来记录编码信息，使得在读取阶段能通过NumDocMeta中的编码信息恢复成原数据NumDoc，NumDocMeta信息会被写入到索引文件.fdm中：\n图13：\n\n  图13中Min、AvgInc、Offset、BitRequired为编码信息，其编码逻辑本文不会展开，编码的目的是为了能降低索引文件的文件大小，另外图13中的其他字段的介绍可以阅读文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm。\n 生成StartPoints的信息\n图14：\n\n  在图1的索引阶段，使用了图5中的临时文件_0_Lucene85FieldsIndexfile_pointers_1.tmp存储了一个chunk对应的数据块在索引文件.fdt中的起始读取位置。\n  处理过程为读取每一个chunk中的对应的数据块在索引文件.fdt中起始读取位置，每处理1024（2 &lt;&lt; blockshift（blockshift的介绍见文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm））个chunk就生成一个StartPointBlock，然后写入到索引文件.fdx中：\n图15：\n\n  图15中StartPoint为某个chunk对应的数据块在索引文件.fdt中的起始读取位置。同样的，StartPointBlock会先进行编码再使用PackedInts（一）进行压缩处理后再写入到索引文件.fdx中。\n  跟NumDocsBlock一样，每处理2 &lt;&lt; blockshift个chunk就生成一个StartPointBlock，此时还会生成StartPointMeta信息来描述编码信息，使得在读取阶段能通过StartPointMeta中的编码信息恢复成原数据StartPoint，StartPointMeta信息会被写入到索引文件.fdm中：\n图16：\n\n  图16中Min、AvgInc、Offset、BitRequired为编码信息，其编码逻辑本文不会展开。另外图13中的其他字段的介绍可以阅读文章索引文件之fdx&amp;&amp;fdt&amp;&amp;fdm。\n 结语\n  至此，生成索引文件fdx&amp;&amp;fdt&amp;&amp;fdm过程介绍完毕，在随后介绍读取索引文件fdx&amp;&amp;fdt&amp;&amp;fdm文章中，我们将对比Lucene 7.5.0版本的索引文件fdx&amp;&amp;fdt，介绍数据结构的差异以及优化。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["fdx","fdt","fdm"]},{"title":"索引文件的生成（二十）之dvm&&dvd（Lucene 8.4.0）","url":"/Lucene/Index/2020/0602/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%E4%B9%8Bdvm&&dvd/","content":"  本文承接文章索引文件的生成（十九）之dvm&amp;&amp;dvd继续介绍剩余的内容。\n 生成索引文件.dvd、.dvm之SortedDocValues、SortedSetDocValues\n  生成索引文件.dvd、.dvm之SortedDocValues、SortedSetDocValues的流程图：\n图1：\n\n 写入TermsDict信息\n图2：\n\n  在当前流程点，将存储SortedDocValues、SortedSetDocValues对应的所有域值按照字典序写入到索引文件中，在文章索引文件的生成（十八）之dvm&amp;&amp;dvd我们知道，在索引阶段，我们已经通过sortedValues[ ]数组收集了所有种类的域值。\n  我们通过例子来介绍TermDict的数据结构如下所示：\n图3：\n\n  图3的例子中，为了便于画图，我们只介绍前4篇文档的存储详情，在文章索引文件的生成（十八）之dvm&amp;&amp;dvd中我们已经介绍了termId的概念，故这里不赘述，直接给出前4篇文档中SortedDocValuesField中的域值对应的termId：\n\n\n\n域值\ntermId\n\n\n\n\nmop\n0\n\n\nstar\n1\n\n\nof\n2\n\n\nmonth\n3\n\n\n\n  sortedValues[ ]数组中的数组元素为termId，并且数组元素是有序的，但是排序规则不是按照数组元素的值，即termId，而是按照termId对应的域值的字典序，故sortedValues[ ]数组如下所示：\n图4：\n\n  随后依次读取sortedValues[ ]数组中每一个termId，找到termId对应的域值，将这些域值写入到索引文件.dvd中，可见是按照域值从小到大的顺序（字典序）写入的，如下所示：\n图5：\n\n查看大图\n  图5中，根据sortedValues[ ] 数组中的termId作为bytesStart[ ]数组的下标值，从bytesStart[ ]数组获取域值在buffers二维数组中的起始位置，最后在buffers二维数组中获取到在索引期间存储的域值，其中bytesStart[ ]数组、buffers二维数组的介绍见文章ByteRefHash，在那篇文章中，sortedValues[ ]数组即排序后的ids[]数组。\n  随后每处理16个域值，就生成一个block，并且通过BlockIndex字段来实现在索引阶段对block的随机访问，详细的读取过程将在后续的文章中介绍。\n  同时在索引文件.dvm中需要生成TermsDictMeta，它相当于作为索引，在读取阶段获取TermsDict字段在索引文件.dvd中的数据区间，如下所示：\n图6：\n\n  图6中，分别通过BlockMeta字段、BlockIndexMeta字段来获取所有的Block、BlockIndex字段在索引文件.dvd中的数据区间，另外其他字段的介绍见文章SortedDocValues。\n 写入TermsIndex信息\n图7：\n\n  在当前流程点，跟写入TermsDict信息一样，依次读取sortedValues[ ] 数组并获取到域值，不同的是，每处理1024个域值会生成一个PrefixValue，这里要说明的是PrefixValue这个名字起的不是很好，应该换成源码中的sortKey更为贴切，但由于在文章SortedDocValues已经用了PrefixValue，所以继续沿用，我们直接给出索引文件的数据结构来简单提下PrefixValue的作用，其详细的过程将会在后面的文章中展开：\n图8：\n\n  图8中，省去了通过sortedValues[ ] 数组的数组元素，即termId获取域值的过程，同图5，直接给出域值，故图中的sortedValues[ ]数组没有画出termId，以sortedValues[ ] 数组中下标值1023、1024为例，先找出两个域值的相同前缀，即&quot;3Q&quot;，然后读取下标值为1024的域值的后缀值的第一个字节，即&quot;R&quot;，然后将&quot;3QR&quot;作为PrefixValue写入到TermsIndex中，这么做目的在于，我们在搜索阶段，如果提供了一个域值，那么通过二分法可以快速的判断该域值在哪一个PrefixValue的区间，就可以获得这个PrefixValue对应的ord值，最后通过ord去TermsDict中继续查找，同样详细的过程将在后面的文章中展开。\n  同时在索引文件.dvm中需要生成TermsIndexMeta，它相当于作为索引，在读取阶段获取TermsIndex字段在索引文件.dvd中的数据区间，如下所示：\n图9：\n\n  SorteSetDocValues对应的TermsDict、TermsIndex信息跟SortedDocValues是一致的，不赘述。\n 结语\n  至此，生成索引文件.dvd、.dvm之SortedDocValues、SortedSetDocValues的流程介绍完毕，下一篇文章将会介绍在读取阶段如何通过上文中存储的DocValues信息来实现排序的原理。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dvd","dvm"]},{"title":"索引文件的生成（二）之doc&&pay&&pos","url":"/Lucene/Index/2019/1227/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8Bdoc&&pay&&pos/","content":"  本文承接索引文件的生成（一），继续介绍剩余的内容。\n 生成索引文件.tim、.tip、.doc、.pos、.pay的流程图\n图1：\n\n  我们继续介绍流程点生成索引文件.doc、.pos、.pay。\n 生成索引文件.doc、.pos、.pay的流程图\n图2：\n\n 记录位置信息position、payload、偏移信息offset\n图3：\n\n  当前term在一篇文档中的所有位置信息position以及偏移信息offset的起始位置是有序的，所以可以跟文档号一样（见索引文件的生成（一）关于数组docDeltaBuffer的介绍），分别使用差值存储到数组posDeltaBuffer、offsetStartDeltaBuffer中，而图3中其他数组，payloadLengthBuffer、payloadBytes、offsetLengthBuffer则只能存储原始数据。另外要说的是，在处理的过程中，有些位置是不带有payload信息，那么对应payloadLengthBuffer中的数组元素为0。\n  这几个数组对应在索引文件.pos、.pay中的位置如下所示：\n图4：\n\n  另外图4中的索引文件.pay中的字段SumPayLength描述的是当前block中PayData的的长度，在读取阶段用来确认PayData在索引文件.pay中的数据区间。\n 是否需要生成Block？\n图5：\n\n  当处理128个当前term的位置信息position后，即posDeltaBuffer数组中的元素个数达到128，那么就要生成三个block：PackedPosBlock、PackedPayBlock、PackedOffsetBlock，即图4中的灰色标注的字段。\n为什么要生成PackedBlock：\n  当然是为了降低存储空间的使用量，至于能压缩率是多少，可以看PackedInts文章的介绍。\n为什么选择128作为生成PackedBlock的阈值：\n  先给出源码中的注释：\nmust be multiple of 64 because of PackedInts long-aligned encoding/decoding\n  注释中要求阈值只要是64的倍数就行，目的是能字节对齐。因为在使用PackedInts实现压缩存储后的数据用long类型的数组存储，如果待处理的数据集（例如posDeltaBuffer数组）使用固定字节按位存储（见PackedInts（一）），那么只要数据集中的数量是64的倍数，就能按照64对齐，即long类型数组中的每一个long中每一个bit位都是有效数据。至于为什么是128，本人不做妄加猜测，目前没有弄明白。\n 处理完一篇文档后的工作\n图6：\n\n  每处理完一篇包含当前term的文档，我们需要判断下我们目前处理的文档总数是否达到128篇，如果没有达到，那么该流程什么也不做，否则需要记录下面的信息：\n\nlastBlockDocID：记录刚刚处理完的那篇文档的文档号\nlastBlockPayFP：描述是处理完128篇文档后，在索引文件.pay中的位置信息\nlastBlockPosFP：描述是处理完128篇文档后，，在索引文件.pos中的位置信息\nlastBlockPosBufferUpto：在posDeltaBuffer、payloadLengthBuffer、offsetStartDeltaBuffer、offsetLengthBuffer数组中的数组下标值\nlastBlockPayloadByteUpto：在payloadBytes数组中的数组下标值\n\n上述的信息有什么：\n  上述信息作为参数用来生成跳表SkipList，在介绍SkipList时再介绍这些参数，在这里我们只要知道这些信息的生成时机点。\n 执行处理后的收尾工作\n图7：\n\n  当处理完所有包含当前term的文档后，我们需要执行处理后的收尾工作。在前面的流程中，我们知道，每处理128篇文档或者128个位置信息position就会分别生成Block，如果包含当前term的文档的数量或者位置信息总数不是128的倍数，那么到此流程，docDeltaBuffer、freqBuffer（见索引文件的生成（一））、posDeltaBuffer、payloadLengthBuffer、payloadBytes、offsetStartDeltaBuffer、offsetLengthBuffer数组会有未处理的信息，而当前流程就是处理这些信息。\n  对于docDeltaBuffer、freqBuffer数组中的信息，将会被存储到索引文件.doc的VIntBlocks中，如下所示：\n图8：\n\n  在docDeltaBuffer、freqBuffer数组中，当前term在一篇文档的文档号以及词频信息用图8中的一个VIntBlock来存储，VIntBlock的个数跟docDeltaBuffer、freqBuffer数组的数组大小一致。\n  这里存储DocDelta、Freq的有一个优化设计：组合存储（见倒排表（上）中关于组合存储的介绍）。\n  对于posDeltaBuffer、payloadLengthBuffer、payloadBytes、offsetStartDeltaBuffer、offsetLengthBuffer数组中的信息，将会被存储到索引文件.pos的VIntBlocks中，如下所示：\n图9：\n\n 结语\n  至此，除了跳表SkipList（下一篇文章介绍），生成索引文件.doc、.pos、.pay的流程介绍完毕。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["doc","pos","pay"]},{"title":"索引文件的生成（五）之tim&&tip","url":"/Lucene/Index/2020/0110/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8Btim&&tip/","content":"  在前面的四篇文章中，我们介绍了生成索引文件.tim、.tip、.doc、.pos、.pay中.doc、.pos、.pay这三个索引文件的内容，接着我们继续图1中剩余的内容，即流程点生成索引文件.tim、.tip。\n 生成索引文件.tim、.tip、.doc、.pos、.pay的流程图\n图1：\n\n  对于图1的流程图的介绍，可以看文章索引文件的生成（一）之doc&amp;&amp;pay&amp;&amp;pos，我们同样以流程图的方法来介绍生成索引文件.tim、.tip的逻辑。\n 生成索引文件.tim、.tip的流程图\n图2：\n\n  图2的流程描述的是一个域生成索引文件.tim、.tip的流程图。\n 准备数据\n图3：\n\n  将包含当前term的文档号docId，以及在这些文档内的出现频率frequency，位置信息position、payload信息、偏移信息offset写入到索引文件.doc、.pay、.pos之后，会生成IntBlockTermState对象，该对象包含了以下的信息作为处理索引文件.tim、.tip的准备数据：\n\nsingletonDocID：该值如果不为-1，说明只有一篇文档包含当前term，那么singletonDocID的值为对应的文档号，singletonDocID的存在会影响索引文件的数据结构，在生成InnerNode流程点会介绍该值的影响\nlastPosBlockOffset：如果该值为-1，说明term在所有文档中的词频没有达到128，即没有生成一个block（见文章索引文件的生成（二）），如果至少存在一个block，那么该值描述的是VIntBlocks在索引文件.pos中的起始位置，见图4\ndocStartFP：当前term的文档号docId、词频信息frequency在索引文件.doc的起始位置\nposStartFP：当前term的位置信息position在索引文件.pos的起始位置\npayStartFP：当前term的偏移位置offset，payload在索引文件.pay的起始位置\nskipOffset：当前term的跳表信息在索引文件.doc的起始位置\n\n  上述值在索引文件中的位置如下所示：\n图4：\n\n  索引文件.tim又称为 Term Dictionary，所以在读取阶段，我们是先通过读取索引文件.tim来得到在索引文件.doc、.pos、pay的信息。\n\ndocFreq：包含当前term的文档数量\ntotalTermFreq：当前term在所有文档中出现的词频和值\n\n  上述的两个信息是在生成索引文件.doc、.pay、.pos的过程中记录的，其记录的时机点如下所示：\n图5：\n\n 生成NodeBlock\n图6：\n\n  每当处理一个新的term之前，我们先要统计term栈中的prefixTopSize。\nterm栈跟prefixTopSize是什么\n  term栈即存放term的栈，prefixTopSize是跟栈顶term有相同前缀的term数量，例如待处理的term集合如下所示：\nterm集合 = &#123;&quot;abc&quot;, &quot;acc&quot;, &quot;acd&quot;, &quot;acea&quot;, &quot;aceb&quot;, &quot;acee&quot;, &quot;acef&quot;&#125;\n图7：\n\n  从图7可以看出跟栈顶元素有最长相同前缀的term数量为4，前缀值为&quot;ace&quot;，那么此时prefixTopSize的值为4，如果prefixTopSize超过minItemsInBlock，那么就生成一个NodeBlock。\nminItemsInBlock是什么：\n  minItemsInBlock是一个Lucene中的默认建议值（Suggested default value），默认值为25，它描述了至少有minItemsInBlock个有相同前缀的term才能生成一个NodeBlock。\n  如果minItemsInBlock的值为5，在图7中，由于prefixTopSize的值为4，所以不能生成一个NodeBlock，那么就执行更新term栈中的prefixTopSize的流程点。\n如何更新term栈中的prefixTopSize：\n  根据新的term来更新term栈中的prefixTopSize。我们用两种情况来介绍其更新过程。\n\n如果新的term为&quot;aceg&quot;，那么将&quot;aceg&quot;添加到term栈以后如下图所示：\n\n图8：\n\n\n如果新的term为&quot;acfg&quot;，那么将&quot;acfg&quot;添加到term栈以后如下图所示：\n\n图9：\n\n为什么是生成一个或多个NodeBlock：\n  前面我们说到term栈就是存放term的栈，这种说法只是为了便于介绍上文中的内容，term栈里面实际存放了两种类型的信息，在源码中的对应的变量名如下所示，另外term栈在源码中对应的是pending链表：\n\nPendingTerm：该值代表了一个term包含的信息\nPendingBlock：该值代表的是具有相同前缀的term集合包含的信息\n\n  我们另minItemsInBlock的值为3（强调的是上文中minItemsInBlock的值为5），我们以图7为例，如果我们插入一个新的term，例如“ba”，由于此时prefixTopSize的值为4，那么就可以将&quot;acea&quot;, “aceb”, “acee”, &quot;acef&quot;这四个term的信息生成一个NodeBlock，生成NodeBlock的过程将在下一篇文章介绍，在这里我们只需要知道在生成NodeBlock之后，它就会生成一个以&quot;ace&quot;为前缀的PendingBlock，并且会添加到term栈中，如下所示：\n图10：\n\n  由图10可，在生成以&quot;ace&quot;为前缀的NodeBlock之后，还可以生成以&quot;ac&quot;为前缀的NodeBlock，如下所示：\n图11：\n\n  图10、图11生成NodeBlock的条件的判断逻辑是这样的：\n\n计算当前栈顶term的长度i\n计算出新term跟当前栈顶term的相同前缀个数pos\n如果栈顶term的相同前缀为n的prefixTopSize不小于minItemsInBlock，那么就生成一个NodeBlock，其中n的取值范围是(pos, i)\n\n  图10的例子中，栈顶term（”acef“）的长度 i = 4，新term为&quot;ba&quot;，跟栈顶term没有相同前缀，所以pos = 0，栈顶元素term的相同前缀n的取值范围为(0, 4)，当n=3（前缀为&quot;ace&quot;）时，此时的prefixTopSize为4，那么就可以生成一个block，即图10的内容；当n=2（前缀为&quot;ac&quot;）时，prefixTopSize为3，那么就可以生成一个block，即图11的内容；当n=1（前缀为&quot;a&quot;）时，prefixTopSize为2，由于小于minItemsInBlock，故不能生成一个PendingBlock。\n 结语\n  图6的流程图对应的是源码 https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java 中的pushTerm()方法，感兴趣的同学可以结合源码理解。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["tim","tip"]},{"title":"索引文件的生成（八）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Index/2020/0329/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%85%AB%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  在前面的文章中，我们介绍了在Lucene7.5.0中索引文件.dim&amp;&amp;.dii的数据结构，从本篇文章开始介绍其生成索引文件.dim&amp;&amp;.dii的内容，注意的是，由于是基于Lucene8.4.0来描述其生成过程，故如果出现跟Lucene7.5.0中不一致的地方会另外指出，最后建议先阅读下文章Bkd-Tree简单的了解下Lucene中点数据的使用。\n  在文章索引文件的生成（一）之doc&amp;&amp;pay&amp;&amp;pos中，简单的介绍了生成索引文件.dim&amp;&amp;.dii的时机点，为了能更好的理解其生成过程，会首先介绍下在生成索引文件之前，Lucene是如何收集每篇文档的点数据信息（Point Value），随后在flush阶段，会根据收集到的信息生成索引文件.dim&amp;&amp;.dii。\n 收集文档的点数据信息\n  在源码中，通过PointValuesWriter对象来实现文档的点数据的收集，并且具有相同域名的点数据使用同一个PointValuesWriter对象来收集，例如下图中添加三篇文档：\n图1：\n\n  在图1中，由于第49行、50行、55行。60行、61行，添加了具有相同的域名“content”的点数据，故他们三个的点数据信息会使用同一个PointValuesWriter对象来收集，同理第51行、56行。\n  PointValuesWriter对象收集的内容主要包括以下信息：\n numPoints：\n  int类型，numPoints是一个从0开始递增的值，可以理解为是每一个点数据的一个唯一编号，并且通过这个编号能映射出该点数据属于哪一个文档(document)（下面会介绍），由于是每一个点数据的唯一编号，所以该值还可以用来统计某个域的点数据的个数，在图1中，域名为&quot;content&quot;的点数据共有5个，那么这么5个点数据的numPoints的值分别为0、1、2、3、4。\n docIDs\n  int类型数组，每添加一条点数据，会将该点数据所属文档号作为数组元素添加到docIDs数组中 ，并且数组下标为该点数据对应的numPoints，对于域名为&quot;content&quot;的点数据，在完成三篇文档的添加后，docIds数组如下所示：\n图2：\n\n numDocs\n  该值描述的是对于某个点数据域，包含它的文档数量，例如在图1中，域名为&quot;content&quot;的点数据，包含该域的文档有文档0、文档1、文档2，故numDocs的值为3，同理对于域名为&quot;titile&quot;的点数据，numDocs的值为1。\n bytes\n  该值是ByteBlockPool类型，ByteBlockPool为Lucene中的类，在这里我们只需要知道，在这个类中使用了buff(byte[ ]数组)来存储点数据的域值，在Lucene中，数值类型的域值需要通过转化为字节类型来存储，故我们先介绍下在Lucene中数值类型到字节类型的转化实现。\n 数值类型到字节类型（无符号）的转化\n  Lucene中的提供了BigInteger、int、long、float、double到字节类型byte的转化，在NumericUtils类中有具体的实现，本文通过例子只介绍下int类型到byte类型的转化。\n  待转化的数值为3，过程分为两个步骤：\n 步骤一：将待转化的数值跟0x80000000执行异或操作\n  为什么要执行异或操作：\n\n等介绍完步骤二再做解释\n\n 步骤二：写入到数组大小为4的字节数组中\n  由于int类型的数值占用4个字节，所以只需要数组大小为4的字节数组存储即可，如下所示：\n图3：\n\n点击查看大图\n为什么要执行异或操作：\n\n当转化为字节数组后，可以通过比较两个字节数组的相同下标对应的数组元素大小来描述这两个字节数组的大小关系，使得转化后的字节数组依然能具有数值类型的比较功能（大小比较），比如说有两个数值类型3和4，假设不执行上文中的步骤一，即不跟0x80000000执行异或操作，转化后的字节数组如下所示：\n\n图4：\n\n点击查看大图\n  图4中，从第0个字节开始，我们依次比较两个字节数组相同下标对应的数组元素，显而易见，第0个、第1个、第2个字节都是相同的，当比较到第3个字节时，能区分出大小关系，如果我们比较的两个数值是-5、7，假设不执行上文中的步骤一，即不跟0x80000000执行异或操作，转化后的字节数组如下所示：\n图5：\n\n点击查看大图\n  从图5可知，由于在Java中使用补码来表示负数，所以当比较第0个字节时，会得出-5比7大的错误结果，如果我们先执行上文中的步骤一，即跟0x80000000执行异或操作，那么数值-5跟7转化后的字节数组如下所示：\n图6：\n\n点击查看大图\n  由图6可知，可以正确的比较-5与7的大小关系了。\n  上文中我们说到，ByteBlockPool类型的变量bytes使用字节数组buff来存储点数据的域值，存储的过程十分简单，就是将点数据的域值转化为字节数组后，拷贝到bytes的字节数组buff中，例如有下面的例子，同图1：\n图7：\n\n  我们将要介绍在添加了3篇文档后，域名为&quot;content&quot;的点数据的域值在字节数组buff中的数据分布，如下所示：\n图8：\n\n  由图8可以看出，在字节数组buff[ ]中，下标07对应的数组元素存储的是图7中代码第49行的点数据的域值，下标2431对应的数组元素存储的是图7中代码第60行的点数据的域值。\n  生成索引文件.dim&amp;&amp;.dii阶段，会读取buff中的域值，其读取的过程将会在后面的文章中介绍。\n 结语\n  本文介绍了在执行flush之前，Lucene是如何收集点数据的信息，即上文中的numPoints、docIDs、numDocs、bytes，那么在flush阶段，就可以通过这些信息来生成索引文件.dim&amp;&amp;.dii。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dim","dii"]},{"title":"索引文件的生成（六）之tim&&tip","url":"/Lucene/Index/2020/0115/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%85%AD%EF%BC%89%E4%B9%8Btim&&tip/","content":"  本文承接索引文件的生成（五）继续介绍剩余的内容，下面先给出生成索引文件.tim、.tip的流程图。\n 生成索引文件.tim、.tip的流程图\n图1：\n\n  上一篇文章中，我们介绍了执行生成一个或多个NodeBlock的触发条件，本文就其实现过程展开介绍，同样的，下文中出现的并且没有作出解释的名词，说明已经在文章索引文件的生成（五）中介绍，不在本文中赘述。\n 生成一个或多个NodeBlock的流程图\n图2：\n\n PendingEntry集合\n图3：\n\n  我们在上一篇文章中说到，term栈中存放了两种类型的信息：PendingTerm和PendingBlock，它们两个的类图关系如下所示：\n图4：\n\n  故图2流程图的准备数据是一个PendingEntry集合，它就是term栈（本人对该集合的称呼😂），在源码中即pending链表，定义如下：\nprivate final List&lt;PendingEntry&gt; pending = new ArrayList&lt;&gt;();\n 生成一个或者多个PendingBlock到newBlock中\n图5：\n\n  在介绍该流程点之前，我们需要了解下PendingBlock类中的一些信息，PendingBlock类的定义如下：\n图6：\n\n  第一次执行图2的流程时，PendingEntry集合中总是只存在PendingTerm信息，集合中的每一个元素代表了一个term包含的信息，在执行完图2的流程之后，PendingEntry集合中的信息就转化为一个PendingBlock（我们记为block1），它用来描述具有相同前缀的term集合的信息，并且重新添加到term栈中；如果下一次执行图2的流程时，PendingEntry集合中包含了PendingTerm和PendingBlock（block1）两种类型信息，那么在执行完图2的流程后，PendingEntry集合中的信息就会转化为一个新的PendingBlock（我们记为block2），由此可见执行图1的流程点生成一个或多个NodeBlock实际是通过嵌套的方式将所有term的信息转化为一个PendingBlock。\n  图6中红框标注的index描述的是一个PendingBlock自身的信息，而蓝框标注的subIndices描述的是该PendingBlock中嵌套的PendingBlock信息的集合（即每一个PendingBlock的index信息），对于block1跟block2来说，block2对象中的subIndices中包含了一条信息，该信息为block1中的index信息，至于index中包含了具体哪些信息，下文中会展开介绍。\n  我们回到生成一个或者多个PendingBlock到newBlock中的流程点的介绍。\n为什么可能会生成多个PendingBlock\n  对于待处理的PendingEntry集合，它包含的信息数量至少有minItemsInBlock个（为什么使用至少这个副词，见文章索引文件的生成（五）），因为这是图2的流程的触发条件，如果PendingEntry集合中的数量过多，那么需要处理为多个PendingBlock，这么处理的原因以及处理方式在源码中也给出了解释，见 https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-7.5.0/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java 中的 void writeBlocks(int prefixLength, int count)方法：\nThe count is too large for one block, so we must break it into &quot;floor&quot; blocks, where we record the leading label of the suffix of the first term in each floor block, so at search time we can jump to the right floor block.  We just use a naive greedy segmenter here: make a new floor block as soon as we have at least minItemsInBlock.  This is not always best: it often produces a too-small block as the final block:\n  上述的注释大意为：如果一个block太大，那么就划分为多个floor block，并且在每个floor block中记录后缀的第一个字符作为leading label，使得在搜索阶段能通过前缀以及leading label直接跳转到对应的floor block，另外每生成一个floor block，该block中至少包含了minItemsInBlock条PendingEntry信息，这种划分方式通常会使得最后一个block中包含的信息数量较少。\n  对于上述的注释我们会提出以下几个问题：\n\n问题一：划分出一个floor block的规则是什么\n问题二：在搜索阶段，如何通过前缀跟leading label快速跳转到对应floor block\n\n问题一：划分出一个floor block的规则是什么\n  使用下面两个阈值作为划分参数：\n\nminItemsInBlock：默认值为25\nmaxItemsInBlock：默认值为48，它的值可以设定的范围如下所示：\n\nminItemsInBlock &lt; maxItemsInBlock &lt; 2*(minItemsInBlock-1)\n  划分的规则：每处理图3中PendingEntry集合中N个信息，就生成一个floor block，其中N值不小于minItemsInBlock，并且PendingEntry集合中未处理的信息不小于maxItemsInBlock，如下所示：\n图7：\n\n  随后将floor block中包含的PendingEntry信息分别生成PendingBlock，如果PendingEntry集合中的元素数量没有达到划分条件，那么这些元素作为一个block，并且跟floor block一样生成PendingBlock，这两种情况生成的PendingBlock还是有些差异，差异性用图6中黑灰色标注的三个信息来区分：\n\nprefix：相同前缀 + leading label，所有block的相同前缀都是一样的，如果不是floor block生成的PendingBlock对应的prefix的值只有相同前缀，可以看下源码中prefix的定义来区分：\n\nfinal BytesRef prefix = new BytesRef(prefixLength + (hasFloorLeadLabel ? 1 : 0))\n  可见如果是floor block生成的PendingBlock，prefix会多出一个字节，节用来存储leading label。\n\nisFloor：布尔值，用来区分是否为floor block\nfloorLeadByte：该字段即leading label，如果不是floor block生成的PendingBlock，那么该值为 -1\n\n  在生成PendingBlock的过程中，同时也是将term信息写入到索引文件.tim文件的过程，即生成NodeBlock的过程，其中一个block（head block或者floor block）对应生成一个NodeBlock，上文中我们说到PendingEntry信息可分为两种类型：PendingTerm和PendingBlock，根据block中的不同类型的PendingEntry，在索引文件.tim中写入的数据结构也是不同的。\n block中只包含PendingTerm\n图8：\n\n  根据block中包含的PendingEntry的类型，可以细化的将block中只包含PendingTerm对应生成的NodeBlock称为OuterNode，block中至少包含PendingBlock（至少包含PendingBlock意思是只包含PendingBlock或者同时包含PendingBlock以及PendingTerm）对应生成的NodeBlock称为InnerNode。\n  图8中，所有的字段的含义已经在文章索引文件之tim&amp;&amp;tip介绍，不一一展开介绍，这里注意的是红框标注的8个字段描述的是一个term的信息，该信息在生成索引文件.doc、pos、.pay之后，使用IntBlockTermState封装这些信息，并且通过做为图1中的准备数据存储到索引文件.tim中，这8个字段的介绍见文章索引文件的生成（五）之tim&amp;&amp;tip。\n  当outerNode写入到索引文件.tim之后，它在索引文件中.tim的起始位置就用图6的fp信息描述，由于block中存在PendingTerm，故图6中的hasTerms信息被置为true，至此，图6中PendingBlock包含的所有信息都已经介绍完毕，并且我们也明白了PendingBlock的作用：用来描述一个NodeBlock的所有信息。\n block中至少包含PendingBlock\n  我们结合一个例子来介绍这种情况。\n图9：\n\n  待处理的PendingEntry集合如图9所示，其中除了&quot;ab&quot;是一个PendingBlock，其他都是PendingTerm，将该PendingEntry集合生成一个NodeBlock，它是InnerNode。\n图10：\n\n  由图10中红框标注的信息描述了PendingTerm以及PendingBlock对应的Suffix字段的差异性，对于PendingBlock，它对应的Suffix中多了一个index字段，该字段的值就是以&quot;ab&quot;为前缀的PendingBlock中的fp信息（即图6中的fp信息），它描述了&quot;ab&quot;为前缀的PendingBlock对应的NodeBlock信息在索引文件.tim中的起始位置。\n  最后根据图7中划分后的block在分别生成PendingBlock之后，将这些PendingBlock添加到链表newBlock中，newBlock的定义如下：\nprivate final List&lt;PendingBlock&gt; newBlocks = new ArrayList&lt;&gt;();\n  添加到链表newBlock中的目的就是为下一个流程合并PendingBlock做准备的。\n问题二：在搜索阶段，如何通过前缀跟leading label快速跳转到对应floor block\n  在下一篇文章介绍索引文件.tip时回答该问题。\n 所有PendingBlock合并到第一个PendingBlock\n图11：\n\n  在上文中，根据图2中的PendingEntry集合，我们生成了一个或多个PendingBlock，并且添加到了newBlock中，如下所示：\n图12：\n\n  在当前流程中，我们需要将newBlock中第二个PendingBlock开始后面所有的PendingBlock的信息合并到第一个PendingBlock中，合并的信息包含以下内容：\n\nfloorLeadByte\nfp\nhasTerms\n\n  随后将这些信息写入到FST中，其中第一个PendingBlock中的prefix将作为FST的inputValue，合并的信息作为FST的outValue。inputValue、outValue的概念见文章FST算法（上），不赘述，合并的信息的数据结构，即outValue，根据是否划分为floor block还有所区别：\n图13：\n\n  图13中，如果isFloor为false，说明没有对PendingEntry集合中进行划分，即newBlock中只有一个PendingBlock，那么就不需要信息合并，如果isFloor为true\n\nfp：该值描述了第一个block中包含的term信息在NodeBlock中的起始位置，例如图8、图10中的EntryCount字段的位置\nhasTerms标志位：该值描述了这个block中是否至少包含一个PendingTerm\nisFloor标志位：该值描述了是否划分了多个floor block\nfloorBLockNumber：除了第一个floor block，剩余floor block的数量\nFloorMetaData：一个floor block的信息\n\nfloorLeadByte：即上文中说到的leading label\nfpDelta：描述了当前floor block包含的term信息在NodeBlock中的起始位置，这里存储的是与第一个block的fp的差值\n\n\n\n  图12中，如果block中至少包含PendingBlock，那么对应PendingBlock中的subIndices中的合并信息也需要写入到FST中。\n 合并后的PendingBlock添加到term栈\n图14：\n\n  最后合并的PendingBlock添加到term栈，等待被嵌套到新的PendingBlock中，最终，经过层层的嵌套，一个域中产生的所有的PendingBlock都会被嵌套到一个PendingBlock中，该PendingBlock的index信息在后面的流程中将会被写入到索引文件.tip中。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["tim","tip"]},{"title":"索引文件的生成（十一）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Index/2020/0410/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本文承接索引文件的生成（十），继续介绍剩余的内容，为了便于下文的介绍，先给出生成索引文件.dim&amp;&amp;.dii的流程图以及流程点构建BKD树的节点值（node value）的流程图：\n图1：\n\n图2：\n\n 第一次更新parentSplits数组\n图3：\n\n  parentSplits数组的数组元素数量跟点数据的维度数量相同，下标值为0的数组元素描述的是维度编号（见文章索引文件的生成（十）之dim&amp;&amp;dii）为0的维度值在祖先节点中作为切分维度(见文章索引文件的生成（十）之dim&amp;&amp;dii)的次数（累加值）：\n图4：\n\n  如果某一时刻，parentSplits数组如上述所示，说明当前处理的点数据的维度为3（数组大小），并且维度编号为1（数组下标为1）的维度在祖先节点中已经3次作为切分维度，如果执行完图2的流程点选出切分维度后，维度编号1再次作为了切分维度，那么在当前流程点，我们就需要第一次更新parentSplits数组，更新后的parentSplits数组如下所示：\n图5：\n\n  为什么要更新parentSplits数组\n  如果当前节点划分后的左右子树还是内部节点，那么左右子树需要根据parentSplits数组提供执行流程点选出切分维度的条件判断依据。\n 设置左子树的准备数据、设置右子树的准备数据\n图6：\n\n  执行到当前流程点，内部节点的处理工作差不多已经全部完成（除了流程点第二次更新parentSplits数组），当前流程点开始为后续的左右子树的处理做一些准备数据，根据当前的节点编号（见文章索引文件的生成（十）之dim&amp;&amp;dii），即将生成的左右子树可能仍然是内部节点，或者是叶子节点。这两种情况对应的准备数据是有差异的，我们会一一介绍。另外根据满二叉树的性质，如果当前节点编号为n，那么左右子树的节点编号必然为2*n、2*n + 1，在文章索引文件的生成（十）之dim&amp;&amp;dii中我们说到，根据节点编号就能判断属于内部节点（非叶节点）还是叶子节点，不赘述。\n  处理节点（叶子节点或者内部节点）需要的准备数据有好几个，这些准备数据在源码中其实就是 https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter.java 类中的build(…)方法的参数：\n图7：\n\n  图6中的nodeID参数即节点编号，leafNodeOffset参数即最左叶子节点的节点编号值，其他的参数在下文中会部分介绍。\n 左右子树为内部节点\n  如果节点编号nodeId小于最左叶子节点的节点编号leafNodeOffset，那么当前节点是内部节点，对于内部节点的处理，主要关心的参数是minPackedValue、maxPackedValue。\n minPackedValue、maxPackedValue\n  在图2的流程点选出切分维度中，我们会使用到minPackedValue、maxPackedValue参与条件判断（见文章索引文件的生成（九）之dim&amp;&amp;dii），我们在处理根节点（内部节点）时，在图1的流程点MutablePointValues, ，设置了minPackedValue、maxPackedValue的值，为处理根节点做了准备，其初始化的内容见文章索引文件的生成（九）之dim&amp;&amp;dii，而当处理其他内部节点时，minPackedValue、maxPackedValue的值的设置时机点则是在当前流程点。\n  非根节点的内部节点的minPackedValue、maxPackedValue是如何设置的\n  我们先回顾下minPackedValue数组的作用是存放每个维度的最小值，maxPackedValue数组的作用是存放每个维度的最大值，比如当前节点中包含的点数据集合如图8所示，那么minPackedValue、maxPackedValue中的内容如下所示：\nminPackedValue:&#123;2, 1, 2&#125;maxPackedValue:&#123;9, 9, 9&#125;\n  在图2的执行完流程点选出切分维度、内部节点的排序之后，我们就获得了当前内部节点划分维度编号n，并且当前节点中点数据集合的每一个点数据按照维度编号n对应的维度值进行了排序（见文章索引文件的生成（十）之dim&amp;&amp;dii），如果我们另排序后的点数据都有一个从0开始递增的序号，假设有N个点数据，那么序号为 0~ (N/2 - 1)的点数据将作为当前节点的左子树的点数据集合，序号为 N/2 ~ N 的点数据将作为当前节点的右子树的点数据集合。\n  例如当前内部节点有如下的点数据集合，数量为7，并且假设按照维度编号2进行了排序：\n图8：\n\n  根据图8，当前的minPackedValue、maxPackedValue以及左右子树的minPackedValue、maxPackedValue如下所示：\n图9：\n\n  从图9可以看出左子树的maxPackedValue跟右子树的minPackedValue的维度编号为2的值被更新为同一个新值，而该值就是序号为(N/2 -1) 的点数据的维度编号为2（父节点的排序维度）对应的维度值，即图8中红色标注的维度值。\n  对于左右子树来说，如果他们还是内部节点，那么执行图1的选出切分维度的流程时就会使用到父节点提供给它们的参数minPackedValue、maxPackedValue。\n  回看上文中内部节点给左右子树的准备参数minPackedValue、maxPackedValue是有点问题的，比如左子树的点数据集合以及minPackedValue、maxPackedValue如下所示：\n图10：\n\n  图10中，左子树节点实际的minPackedValue、maxPackedValue应该是：\nminPackedValue:&#123;3, 5, 2&#125;maxPackedValue:&#123;8, 3, 4&#125;\n  而即将在流程点选出切分维度使用的minPackedValue、maxPackedValue是父节点提供的信息，即图10中的值，所以在执行选出切分维度的算法时，可能无法能得到最好的一个切分维度。\n  所以从Lucene 8.4.0版本后，在执行流程点选出切分维度会根据一个条件判断是否需要重新计算minPackedValue、maxPackedValue，而不是使用父节点提供的minPackedValue、maxPackedValue，该条件相对简单，直接给出：\nif (nodeID &gt; 1 &amp;&amp; numIndexDims &gt; 2 &amp;&amp; Arrays.stream(parentSplits).sum() % SPLITS_BEFORE_EXACT_BOUNDS == 0) &#123;    // 重新计算minPackedValue、maxPackedValue        &#125;\n  上述条件中，nodeID为1，即处理根节点的时候不用重新计算，原因是minPackedValue、maxPackedValue总是对的（不明白？见文章索引文件的生成（九）之dim&amp;&amp;dii）；numIndexDims描述的是点数据的维度数量例如图10中，numIndexDims的值为3；Arrays.stream(parentSplits).sum()描述的是每个维度当做切分维度的次数总和，SPLITS_BEFORE_EXACT_BOUNDS的值默认为4。\n  由于重新计算每个节点的minPackedValue、maxPackedValue的开销是较大的，所以在满足了上述的条件后才会重新计算，下面是源码中的注释：\nfor dimensions &gt; 2 we recompute the bounds for the current inner node to help the algorithm choose best split dimensions. Because it is an expensive operation, the frequency we recompute the bounds is given by SPLITS_BEFORE_EXACT_BOUNDS.\n  上述源码中的algorithm指的就是图2的流程点选出切分维度中的划分规则（见文章索引文件的生成（十）之dim&amp;&amp;dii）。\n 左右子树为叶子节点\n  在介绍处理叶子节点的时候再展开，留个坑。\n 第二次更新parentSplits数组\n  结合图7，第一次更新parentSplits数组时，是为了给当前节点的左右子树在执行流程点选出切分维度时提供信息，那么在处理完左右子树后，我们需要恢复parentSplits数组，将parentSplits数组中的信息恢复到执行第一次更新parentSplits数组流程前的状态，原因是如果当前节点是左子树，那么当前节点处理结束后，需要处理它的兄弟节点，而兄弟节点（右子树）的parentSplits数组必须是跟左子树一致的。\n 结语\n  至此我们介绍完了内部节点的处理流程，在下一篇文章中，我们将继续介绍叶子节点的处理流程，在该流程中，将会把点数据的信息写入到索引文件.dim中。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dim","dii"]},{"title":"索引文件的生成（十七）之dvm&&dvd（Lucene 8.4.0）","url":"/Lucene/Index/2020/0526/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E4%B9%8Bdvm&&dvd/","content":"  本文承接索引文件的生成（十六）之dvm&amp;&amp;dvd继续介绍剩余的内容，先给出流程图：\n 生成索引文件.dvd、.dvm之NumericDocValues的流程图\n图1：\n\n 使用域值映射存储\n图2：\n\n  介绍本流程点之前，先介绍下什么是域值映射存储：\n\n域值映射存储通过一个映射关系，将每一种域值映射为一个新的数值，并且存储该数值到索引文件.dvd中，在读取阶段通过该数值和映射关系，获得原始的域值。\n\n  生成映射关系encode需要几下几个步骤：\n\n步骤一：收集域值种类\n步骤二：排序域值\n步骤三：写入到映射关系encode\n\n  我们通过一个例子来介绍生成映射关系encode的过程，假设有以下的域值集合：\n&#123;1, 8, 127, 6, 259, 3, 8, 6&#125;\n 步骤一：收集域值种类\n  该步骤即在文章索引文件的生成（十六）之dvm&amp;&amp;dvd中提到的使用一个Set&lt;Long&gt;对象uniqueValues来收集。在本例子，收集后的uniqueValues中包含的域值种类如下所示：\n&#123;1, 8, 127, 6, 259, 3&#125;\n 步骤二：排序域值\n  排序后的域值如下所示：\n&#123;1, 3, 6, 8, 127, 259&#125;\n  排序的目的在于使得映射后的域值仍具有跟映射前的域值的相同的排序属性（域值之间的大小关系）。\n 步骤三：写入到映射关系encode中\n  该步骤直接给出源码更容易理解：\nMap&lt;Long, Integer&gt; encode = new HashMap&lt;&gt;();for (int i = 0; i &lt; sortedUniqueValues.length; ++i) &#123;    encode.put(sortedUniqueValues[i], i);&#125;\n  上述代码中，sortedUniqueValues即排序后的uniqueValues，从代码可以看出映射后的域值是从0开始的递增值，下面给出当前例子生成映射关系encode的步骤：\n图3：\n\n  图3中可以看出，映射后的域值之间的大小关系（数值大小）与映射前的一致，当然我们需要将映射前的域值，即sortedUniqueValues中的域值集合写入到索引文件.dvm中，在读取阶段用于解码映射后的域值，这些域值在索引文件.dvm中的位置如下所示：\n图4：\n\n  图4中，length描述的是域值种类数量，FieldValue即sortedUniqueValues中的域值集合。\n  我们回到当前流程点的介绍，使用域值映射需要满足下面的条件：\nif (uniqueValues.size() &lt;= 256          &amp;&amp; uniqueValues.size() &gt; 1          &amp;&amp; DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) &lt; DirectWriter.unsignedBitsRequired((max - min) / gcd)) &#123;    ... ... // code          &#125;\n  上述条件中，域值种类数量在区间(1, 256]时使用域值映射，因为存储一个域值时，只需要最多一个字节，即8个bit来存储一个域值，如果不使用映射存储，只能使用(max - min) / gcd的方式（域值集合无序）存储域值，那么可能需要多个字节来存储，即在读取阶段，可能需要读取多个字节才能解码出一个域值，故使用域值映射能优化使用一个字节存储域值的读写性能；\n  但是相比较读写性能，Lucene更优先考虑存储性能，如果使用域值映射后需要占用更多的存储空间，那么就不使用域值映射存储，上述条件中，DirectWriter.unsignedBitsRequired()条件描述的是存储一个域值占用的bit数量。\n  另外关于上述条件的设计思想见这个issue： https://issues.apache.org/jira/browse/LUCENE-4936?jql=text ~ &quot;uniqueValues&quot; 。\n 使用block存储域值\n图5：\n\n  是否使用以及为什么使用多个block存储域值的原因已经在文章索引文件的生成（十六）之dvm&amp;&amp;dvd中介绍，这里不赘述。\n  从图1可以看出在使用单个block存储域值时，可能会使用域值映射存储，那么实际写入到索引文件.dvd中的域值为映射后的域值，如下图所示：\n图6：\n\n  上述域值在索引文件.dvd中的位置如下所示：\n图7：\n\n  图7中其他字段的介绍见文章NumericDocValues。\n  以上就是生成索引文件.dvd、.dvm之NumericDocValues的全部内容，我们接着介绍生成索引文件.dvd、.dvm之SortedNumericDocValues的内容。\n 生成索引文件.dvd、.dvm之SortedNumericDocValues\n  SortedNumericDocValues的数据结构跟NumericDocValues相似，它新增了字段用于描述SortedNumericDocValues相对于NumericDocValues新的特性，即一篇文档中允许存在多个域名相同的SortedNumericDocValuesField域，在下图中用红框标注：\n图8：\n\n点击查看大图\n图9：\n\n点击查看大图\n  注意的是上图中描述的是只有一种域名的情形。\n  在前面的文章中，我们知道图9中DocIdData描述了所有的文档号信息，由于每一个SortedNumericDocValues在一篇文档中可能存在多个值，故使用DocValueCount来实现索引，该索引描述了一篇文档中有多少个相同域名对应的SortedNumericDocValues信息，如下所示：\n图10：\n\n点击查看大图\n  图10中，address描述的是每篇文档中包含的相同域名对应的域值数量，例如用红色虚线表示这一篇文档中有1个域值，同理绿色虚线表示这一篇文档中有2个域值，在读取过程中，所有的域值会被读取到数组中，而address正是描述的数组的下标。\n  另外在索引文件.dvm中，DocValueCountMeteData用来描述DocValueCount字段在索引文件.dvd中的位置，如下所示：\n图11：\n\n点击查看大图\n  图11中，Offset跟DataLength两个字段描述了DocValueCount的信息在索引文件.dvm中的数据区间。\n 结语\n  至此，生成索引文件.dvd、.dvm之NumericDocValues、SortedNumericDocValues的内容介绍完毕。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dvd","dvm"]},{"title":"索引文件的生成（十九）之dvm&&dvd（Lucene 8.4.0）","url":"/Lucene/Index/2020/0531/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89%E4%B9%8Bdvm&&dvd/","content":"  在文章索引文件的生成（十八）之dvm&amp;&amp;dvd中，我们介绍了Lucene在索引阶段跟flush阶段收集到的SortedDocValues、SortedSetDocValues信息，这些信息将作为生成索引文件.dvm、dvd的依据。\n 生成索引文件.dvd、.dvm之SortedDocValues、SortedSetDocValues的流程图\n图1：\n\n 是否所有文档中都是单值？\n图2：\n\n  如果每篇文档中的某个域名的SortedSetDocValueField只有一个，即所谓的单值，判断numDocsWithField跟numOrds的值是否相同，相同意味着所有文档中都是单值：\n\nnumDocsWithField：该值描述了包含当前SortedSetDocValueField的文档数量，在文章索引文件的生成（十八）之dvm&amp;&amp;dvd中我们说到，在索引阶段，通过DocsWithFieldSet收集文档号，在当前流程点就是通过DocsWithFieldSet获取文档号，DocsWithFieldSet存储文档号的内容在文章索引文件的生成（十五）之dvm&amp;&amp;dvd已经介绍，不赘述。\nnumOrds：在文章索引文件的生成（十八）之dvm&amp;&amp;dvd我们说到，每种域值对应一个ord值，如果一篇文档中出现了相同域名的SortedSetDocValueField的多个域值，这些域值如果不全都相等，显而易见，numDocsWithField跟numOrds的值是不同的，意味着多值。获得一篇文章中numOrds的值的方法是通过收集阶段生成的ordMap数组。\n\n那么此时对应生成的索引文件跟SortedDocValue是一致的，如下所示：\n图3：\n\n  在读取阶段，根据DocValuesType字段判断出是SortedSetDocValues后，接着读取索引文件的下一个字节，即SingleValue，根据SingleValue的值判断索引文件的数据结构：\n\nSingleValue == 0：单值，那么SingleValue字段后的所有字段跟SortedDocValues一致\nSingleValue == 1：至少有一篇文档中包含了多个（大于1）某个域名的SortedSetDocValueField\n\n  图3中，DocValuesType字段的值描述了当前的DocValues类型，字段的可选值如下所示：\n图4：\n\n 写入文档号信息（单值）\n图5：\n\n  根据numDocsWithField不同值，生成不同数据结构的DocIdIndex字段，该内容跟存储NumericDocValues的文档号是一模一样的，故不赘述，见文章索引文件的生成（十六）之dvm&amp;&amp;dvd。\n 写入文档号信息（多值）\n图6：\n\n  同样根据numDocsWithField的值生成不同数据结构的DocIdIndex字段，但跟写入文档号信息（单值）不同点在于，numDocsWithField的值不可能为0（numDocsWithField的值有三种可能，见文章索引文件的生成（十六）之dvm&amp;&amp;dvd），因为如果numDocsWithField的值为0，那么必定numOrds的值也是0，意味着numDocsWithField与numOrds相等，那么会与图1中是否所有文档中都是单值？的判断会为假发生矛盾，故numDocsWithField不可能为0。\n  文档号信息（值）会被写入到索引文件.dvd中，文档号信息（值）在索引文件.dvd中的位置信息（即DocIdIndex）与numDocsWithField都会被记录到索引文件.dvm中，关系如下所示：\n图7：\n\n图8：\n\n  注意的是在文章SortedSetDocValues、SortedDocValues中的索引文件数据结构是Lucene7.5.0，对于DocIdIndex，跟Lucene 8.4.0中的差异介绍见文章索引文件的生成（十六）之dvm&amp;&amp;dvd。\n 写入Ords信息（单值）\n图9：\n\n  我们以文章索引文件的生成（十八）之dvm&amp;&amp;dvd中的图3为例，该例子对应的currentValues[ ]数组跟ordMap[ ]如下所示，这两个数组的介绍已经介绍过了，这里不赘述，直接列出：\n图10：\n\n图11：\n\n  由于是单值，所以currentValues[ ]数组下标的大小关系正好描述了文档的处理顺序，比如下标为0，描述的是在索引阶段收集的第一篇包含SortedDocValues的域值对应的termId，随后termId作为ordMap[ ]的下标值，找到termId对应的ord值，Ords信息将会被存储到索引文件.dvd中的Ords字段，对应Ords信息在索引文件.dvd中的位置信息会被记录到索引文件.dvm中的OrdsIndex中。在本文中，我们还是仅仅描述索引文件.dvd、dvm中需要存储的信息，至于存储ords信息的作用是什么，会在后面文章中介绍：\n图12：\n\n 写入Ords信息（多值）\n  我们以文章索引文件的生成（十八）之dvm&amp;&amp;dvd中的图11为例，该例子对应的currentValues[ ]数组跟ordMap[ ]如下所示，同样不作出介绍：\n图13：\n\n图14：\n\n  下图中对应的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/lucene/DcoValues/SortedSetDocValues2Test.java ：\n图15：\n\n  图15中，pendingCounts中的currentValues[ ]数组的下标的大小关系描述的是处理的文档的顺序，例如下标值为1，它对应的数组元素为2，描述了这篇文档中有两个域值，随后从pengding中的currentValues[ ]数组中取出这两个域值对应的termId，接着termId作为ordMap[ ]数组的下标值，取出对应的ord值，然后存储到索引文件.dvd中。\n 写入OrdsAddress信息\n图16：\n\n  从图15的索引结构可以看出，在读取阶段，由于多值的存在，我们无法区分每篇文档对应的ords值，所以需要OrdsAffress信息来作为索引，同样结合图15中的例子，如下所示：\n图17：\n\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dvd","dvm"]},{"title":"索引文件的生成（十三）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Index/2020/0418/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本文承接索引文件的生成（十二）之dim&amp;&amp;dii，继续介绍剩余的内容，为了便于下文的介绍，先给出生成索引文件.dim&amp;&amp;.dii的流程图以及流程点构建BKD树的节点值（node value）的流程图：\n图1：\n\n图2：\n\n 写入不同后缀信息到索引文件.dim中\n图3：\n\n  在文章索引文件的生成（十二）之dim&amp;&amp;dii中我们将每个维度的最长公共前缀的信息写入到了索引文件.dim中，同样后缀信息也需要将写进去。\n  将维度值拆分为相同的前缀跟不相同的后缀两部分并且分别存储的目的在于降低存储开销（reduce the storage cost）,为了能进一步降低存储开销，在当前流程点，会遍历一次叶子节点中的所有点数据，找出一个或者多个区间，在某个区间里面的点数据的排序维度（见在文章索引文件的生成（十二）之dim&amp;&amp;dii）对应的维度值，该维度值的不同后缀的第一个字节是相同的。这种处理的思想称为游标编码（Run Length Encoding）。\n图4：\n\n  图4中，我们假设排序维度为维度编号2，可以看出排序维度对应的维度值的最长公共前缀的长度为2个字节，随后我们就比较不同后缀的第一个字节（即公共前缀的下一个字节），这个字节相同的点数据集合认为是同一个区间的，那么就可以划分出三个区间，他们包含的点数据如下所示：\n区间1（红框）：&#123;12, 5, 12&#125;、&#123;23, 1, 13&#125;区间2（蓝框）：&#123;3, 5, 268&#125;、&#123;20, 3, 270&#125;、&#123;4, 5, 271&#125;区间3（绿框）：&#123;8, 1, 780&#125;\n为什么划分区间的规则只考虑不同后缀的第一个字节并且能降低存储开销\n  以图4为例，维度值的排序方式为从高到低依次比较一个字节，由于图4中维度编号2的最长公共前缀的长度为2，说明是按照不同后缀的第一个字节排序的，这就意味着，存在连续的一个或多个维度值，这些维度值在这个字节的值一样的，那么这个相同的字节就不用重复存储，提取出来之后存储一次即可，从而降低存储开销。\n  不同后缀信息对应在索引文件.dim中的位置如下所示：\n图5：\n\n  我们以图4为例结合图5，图5中的PackedValuesRange的数量就是3，假设PackedValuesRange描述的是图4中蓝框对应的区间，那么PrefixByte的值就是 二进制0B00000001，RunLen的值为3，也就是PackedValue的数量为3。\n  同时还要记录当前叶子节点中每一个维度的最大值跟最小值，并且只保存后缀值，在读取阶段通过跟commonPrefixes就可以拼出原始值以及排序维度编号，这两个信息在索引文件.dim中的位置如下所示：\n图6：\n\n  以图4为例，SortedDim的值为2（维度编号）。\n  在上文中，我们说到，通过区间划分能降低存储开销，但是只考虑了不同后缀的一个字节，如果有一个或者多个（不是全部）点数据是完全一样，点数据的每个维度的值都是相同的，那么即使使用了区间划分，图5中的一个或者PackedValue之间中还是会存储相同的Suffix，在Lucene8.2.0之后，针对这个现象作了一些优化。\n Lucene 8.2.0的优化\n  在生成图5的PackedValues之前，会先计算几个参数，leafCardinality（源码中的变量）、lowCardinalityCost、highCardinalityCost。\n leafCardinality\n  在执行完图2中的流程点叶子节点的排序之后，通过遍历一次叶子节点中的所有点数据，将这些有序的点数据划分出n个区间，单个区间内至少包含一个点数据并且区间内的点数据的每个维度的值都是相同，leafCardinality的值即区间的数量。\n图7：\n\n  图7中，划分出4个区间，那么leafCardinality的值就为4。\n  在Lucene 8.2.0版本之后，图6中的PackedValues中的信息会根据lowCardinalityCost、highCardinalityCost两个参数的大小关系，使用不同的数据结构来存储点数据的不同后缀信息，并且对应的数据结构占用存储空间较小，也就是说lowCardinalityCost、highCardinalityCost两个参数描述了存储点数据的不同后缀信息的需要占用的存储空间大小，其计算方式如下所示：\nhighCardinalityCost = count * (packedBytesLength - prefixLenSum - 1) + 2 * numRunLens;lowCardinalityCost = leafCardinality * (packedBytesLength - prefixLenSum + 1);\n  这两个参数计算的是图6中PackedValues占用的存储空间，其中highCardinalityCost对应的数据结构即图6中的PackedValues，即Lucene 8.2.0之前的版本，而lowCardinalityCost对应的数据结构在下文中会给出。\n\nhighCardinalityCost\n\n  上述公式中，count的值就是叶子节点中点数据的数量，(packedBytesLength - prefixLenSum - 1)描述的是单个点数据后缀信息占用的字节数，(2* numRunLens) 描述的是图6中每一个PackedValuesRange中PrefixByte跟RunLen的累加和。\n图8：\n\n  上图中，红框标注的字段占用的存储空间大小即highCardinalityCost。另外蓝框标注的两个字段SortedDim、ActualBounds在Lucene8.2.0之后对调了位置，可以跟图5中的内容（Lucene 7.5.0）比较下。\n\nlowCardinalityCost\n\n  我们先介绍下lowCardinalityCost对应的数据结构：\n图9：\n\n  在上文的计算lowCardinalityCost公式中，leafCardinality就是划分出的区间数量，即图9中PackedValuesRange的数量，以图7中的数据为例，PackedValuesRange的数量就是4，cardinality描述的是当前区间内点数据的数量，由于单个区间的点数据完全相同，所以只需要存储一个PackedValue即可。\n 结语\n  至此，图2的所有流程点都介绍完毕。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dim","dii"]},{"title":"索引文件的生成（十二）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Index/2020/0415/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本文承接索引文件的生成（十一），继续介绍剩余的内容，为了便于下文的介绍，先给出生成索引文件.dim&amp;&amp;.dii的流程图以及流程点构建BKD树的节点值（node value）的流程图：\n图1：\n\n图2：\n\n  在前面的文章中，我们介绍了图2中处理内部节点的所有流程点，在介绍处理叶子节点前，我们先填个坑，即当某个内部节点划分后生成的左右子树为叶子节点时，内部节点为左右子树提供哪些准备数据，即流程点设置左子树的准备数据、设置右子树的准备数据。\n 设置左子树的准备数据、设置右子树的准备数据\n 左右子树为叶子节点\n  在文章索引文件的生成（十一）之dim&amp;&amp;dii中我们提到，处理节点（叶子节点或者内部节点）需要的准备数据有好几个，这些准备数据在源码中其实就是 https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter.java 类中的build(…)方法的参数：\n图3：\n\n  对于左右子树为叶子节点的情况，我们只需要关心图3中的leafBlockFPs数组。\n leafBlockFPs数组\n  每当处理完一个叶子节点，我们需要将叶子节点对应的信息写入到索引文件.dim中，又因为所有叶子节点对应的信息以字节流形式存储在索引文件.dim中，leafBlockFPs数组正是用来记录每一个叶子节点对应的信息在索引文件.dim中的起始位置，同样具体的内容将在下文中展开。\n 选出叶子节点的排序维度\n图4：\n\n  选出叶子节点的排序维度前需要先计算两个数组：commonPrefixLengths数组以及usedBytes数组。\n commonPrefixLengths数组\n  commonPrefixLengths为int类型的数组，该数组的下标用来描述维度编号（见文章索引文件的生成（十）之dim&amp;&amp;dii），数组元素描述的是维度编号对应的所有维度值的最长公共前缀的长度。\n usedBytes数组\n  usedBytes为FixedBitSet类型数组，如果你不熟悉FixedBitSet对象也没关系，我们只需要知道usedBytes数组的下标同样用来表示维度编号，数组元素描述的是维度编号的对应的所有维度值的公共前缀后的下一个字节的种类数量（按照字节的不同值作为种类的划分），只是种类数量用FixedBitSet来统计而已。\n图5：\n\n  假设叶子节点中有3个点数据，我们以维度编号2为例，图4中，点数据蓝色部分的字节是相同的，即点数据在这个维度的维度值的最长公共前缀的长度为2（2个字节），即在commonPrefixLengths数组中，下标值为2的数组元素的值为2，其他维度同理，故commonPrefixLengths数组如下所示：\n图6：\n\n  同样以维度编号2为例，除去相同的2个前缀字节的下一个字节的种类数量如图4中绿框与红框标注共有2个种类，即有两种，其他维度同理，故usedBytes数组如下所示：\n图7：\n\n  在源码实现上，需要先计算出commonPrefixLengths数组，利用该数组再计算出usedBytes数组。\n  在计算出usedBytes数组之后，遍历该数组，找到第一个数组元素最小的值，对应的数组下标，即维度编号，作为叶子节点的排序维度，在图6中，编号维度1将作为排序维度。\n 叶子节点的排序\n图8：\n\n  使用内省排序（IntroSorter）对叶子节点中的点数据进行排序，而不是在文章索引文件的生成（十）之dim&amp;&amp;dii中提到的内部节点排序使用的最大有效位的基数排序(MSB radix sort)，源码中也给出了注释：\nNo need for a fancy radix sort here, this is called on the leaves only so there are not many values to sort\n  简单来说就是叶子节点中的点数据的数量相对于内部节点较少而选择使用内省排序（IntroSorter）。\n  叶子节点中的点数据数量的取值范围是什么\n  先给出源码中的注释：\nThe tree is fully balanced, which means the leaf nodes will have between 50% and 100% of the requested maxPointsInLeafNode\n  上述注释中的maxPointsInLeafNode指的是叶子节点中最多包含的点数据数量，默认是1024，也就说当内部节点中的点数据数量大于1024个时就需要继续切分，那么叶子节点中包含的点数据的数量区间为 [512, 1024]。\n  同内部节点的排序一样，叶子节点中点数据之间的排序关系同样用ord数组（见文章索引文件的生成（十）之dim&amp;&amp;dii）来描述，不赘述。\n 更新leafBlockFPs数组\n图9：\n\n  到此流程点，我们即将开始把叶子节点的信息写入到索引文件.dim中，故需要将索引文件.dim当前可写入的位置记录到leafBlockFPs数组中，在读取阶段，就可以通过leafBlockFPs数组找到当前叶子节点的信息在索引文件.dim中的起始读取位置，如下所示：\n图10：\n\n  图10中，leafNodeData为叶子节点的信息，leafBlockFPs数组的下标值为叶子节点的差值节点编号。\n  差值节点编号是什么\n  在文章索引文件的生成（十）之dim&amp;&amp;dii我们介绍了节点编号的概念以及最左叶子节点的节点编号的获得方式，差值节点编号值的就是叶子节点编号跟最左叶子节点的节点编号的差值：\nleafBlockFPs[nodeID - leafNodeOffset]\n  上述代码中，nodeID即节点编号，leafNodeOffset为最左叶子节点的节点编号，如果当前处理的是最左叶子节点，那么差值节点编号为0 ，即下标值为0，那么对应的数组元素描述的就是最左叶子节点的信息在索引文件.dim中的起始读取位置。\n 写入文档号信息到索引文件.dim中\n图11：\n\n  该流程点描述是将叶子节点中的点数据对应的文档号写入到索引文件.dim中。\n  如果获得每个点数据对应的文档号\n  在文章索引文件的生成（八）之dim&amp;&amp;dii中我们说到，在点数据的收集阶段，使用了 docIDs数组存储了文档号信息，该数组的数组元素为文档号，下标值为numPoints，那么我们只需要知道点数据对应的numPoints就可以获取点数据对应的文档号，而在ord数组（见文章索引文件的生成（十）之dim&amp;&amp;dii）中，数组元素正是numPoints，所以我们只要遍历ord数组就能获得当前叶子节点中的文档号信息，并且这些文档号是有序的，注意的是：不是文档号的值有序，而是文档号对应的点数据是有序，排序规则即上文中的流程点选出叶子节点的排序维度、叶子节点的排序中的内容。\n图12：\n\n  叶子节点中包含的文档号信息在索引文件.dim中的位置如下所示：\n图13：\n\n  上图中，红框标注的两个字段为当前流程点写入的信息，其中Count描述的是文档号的数量，DocIds为文档号的集合。DocIds的详细数据结构见文章索引文件之dim&amp;&amp;dii 。\n 写入相同前缀信息到索引文件.dim中\n图14：\n\n  在当前流程点，我们根据在上文中计算出的commonPrefixLengths数组，将每个维度的最长相同前缀的值写入到索引文件.dim中，对应在索引文件.dim中的位置如下所示：\n图15：\n\n  上图中，Length描述的每一个维度的最长公共前缀的长度，Value为最长公共前缀的值，在读取阶段就可以根据Length的值确定Values在索引文件.dim中的位置区间。\n  以图5的维度编号2为例，Length跟Value的值如下所示：\nLength: 2Value: &#123;10000000, 00000000&#125;\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dim","dii"]},{"title":"索引文件的生成（十五）之dvm&&dvd（Lucene 8.4.0）","url":"/Lucene/Index/2020/0507/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E4%B9%8Bdvm&&dvd/","content":"  在前面的文章中，我们介绍了在Lucene7.5.0中索引文件.dvd&amp;&amp;.dvm的数据结构，从本篇文章开始介绍其生成索引文件.dvd&amp;&amp;.dvm的内容，注意的是，由于是基于Lucene8.4.0来描述其生成过程，故如果出现跟Lucene7.5.0中不一致的地方会另外指出，索引文件.dvd&amp;&amp;.dvm中的包含了下面几种类型：\n\nBinaryDocValues\nNumericDocValues\nSortedDocValues\nSortedNumericDocValues\nSortedSetDocValues\n\n  本篇文章从NumericDocValues开始介绍，建议先阅读下文章NumericDocValues，简单的了解NumericDocValues类型的DocValues的数据结构。\n  在文章索引文件的生成（一）之doc&amp;&amp;pay&amp;&amp;pos中，简单的介绍了生成索引文件.dvd&amp;&amp;.dvm的时机点，为了能更好的理解其生成过程，会首先介绍下在生成索引文件之前，Lucene是如何收集每篇文档的NumericDocValues信息。\n 收集文档的NumericDocValues信息\n  在源码中，通过NumericDocValuesWriter对象来实现文档的NumericDocValues信息的收集，并且具有相同域名的NumericDocValues信息使用同一个NumericDocValuesWriter对象来收集，例如下图中添加三篇文档：\n图1：\n\n  在使用NumericDocValuesField来生成NumericDocValues信息时要注意，在一篇文档中，仅能添加一条相同域名的NumericDocValuesField，但是能添加多条不同域名的NumericDocValuesField，如图1中，在文档0中添加了域名分别为&quot;age&quot;、&quot;level&quot;的NumericDocValuesField，如果我们在一篇文档中添加两个或以上相同域名的NumericDocValuesField，那么会抛出以下的异常：\n图2：\n\n图3：\n\n  图3中抛出的异常对应的代码实际就是上文提到的NumericDocValuesWriter类中的：\n图4：\n\n  上文中说到相同域名的NumericDocValuesField对应的NumericDocValues信息使用同一个NumericDocValuesWriter收集，在收集的过程中，如果一篇文档中包含了两个或以上相同域名的NumericDocValuesField，如图4所示，docID &lt;= lastDocId的条件就会成立，即抛出图3中的异常。\n  在收集NumericDocValues信息的过程中，我们仅仅关心下面两个信息：\n\ndocId：即包含NumericDocValues信息的文档号\n域值：即NumericDocValuesField的域值，例如图1的文档0中，域名为&quot;age&quot;的NumericDocValuesField的域值为88\n\n docId\n  在源码中，使用DocsWithFieldSet对象收集文档号docId，使得在某个特殊条件（见下文流程点是否使用了FixedBitSet？的介绍）下，在索引阶段能更少的占用内存，在读取阶段有更好的读写性能。\n  我们通过介绍DocsWithFieldSet存储文档号的的流程来解释上述的内容：\n图5：\n\n  在介绍图5的流程之前，我们先介绍下图中几个变量：\n\nlastDocId：该值描述的是DocsWithFieldSet上一次处理的文档号\ncost：该值的初始值为0，它其中一个作用是描述DocsWithFieldSet已经处理的文档数量，其他的作用在下文中会介绍\nFixedBitSet：用于存储文档号，见文章工具类之FixedBitSet的介绍\n\n 文档号\n图6：\n\n  图5的流程描述了DocsWithFieldSet存储一个文档号的过程，所以流程图的准备数据为一个文档号。\n 文档号是否不大于已收集的文档号？\n图7：\n\n  通过比较当前处理的文档号docId跟lastDocId的值来判断文档号是否不大于已收集的文档号，如果判断为否，那么直接抛出异常：\n图8：\n\n  当前流程点判断的目的是想说明DocsWithFieldSet只处理从小到大有序的文档号集合。\n FixedBitSet\n图9：\n\n  介绍图9的流程点之前，我们先说下DocsWithFieldSet存储文档号的两种方式：\n\nFixedBitSet：见文章工具类之FixedBitSet\ncost：这里的cost即上文中提到的cost，它一方面描述了DocsWithFieldSet已经处理的文档数量，同时在某个特殊条件下，它也能用来描述DocsWithFieldSet存储的文档号集合（见下文介绍）\n\n某个特殊条件是什么？\n  该特殊条件指的是DocsWithFieldSet处理的文档号集合中的文档号是从0开始有序递增的，这意味着段（段内的文档号是从0开始有序递增的）中每一篇文档中都包含某个域名的NumericDocValues信息。\n  例如图1中，如果添加了三篇文档后生成了一个段，那么对于域名为&quot;age&quot;的NumericDocValues信息就满足上述的特殊条件，而对于域名为&quot;level&quot;的NumericDocValues信息，由于文档1中没有添加这个域，那么就不满足。\n满足或者不满足特殊条件有什么不同\n  如果满足特殊条件，意味着我们不需要存储每个文档号，只需要知道cost的值就行了，cost的默认值为0，每处理一个文档号，cost的值就执行+1操作，那么在读取文档号阶段我们就可以根据cost获得文档号集合区间，即[0, cost]。\n  如果不满足特殊条件，那么只能通过FixedBitSet来存储每一个文档号。\n  可见如果满足了特殊条件，在索引阶段，我们就不需要额外使用FixedBitSet对象来存储文档号，即上文中提到的在索引阶段能更少的占用内存；同时在读取阶段，我们只要顺序遍历0~cost的值就可以获取文档号，而不需要通过FixedBitSet来读取文档号（见文章工具类之FixedBitSet），即上文中提到的在读取阶段有更好的读写性能。\n  回到当前流程点的介绍，如果流程点是否使用了FixedBitSet？为否，说明之前处理的文档号集合是从0开始有序递增的，如果为是，那么只能通过FixedBitSet存储文档号，即执行流程点使用FixedBitSet存储文档号，接着在流程点是否新建FixedBitSet？的判断中，通过比较当前处理的文档号docId跟cost值来进行判断：\n\ndocId 等于cost：说明目前仍然处于满足特殊条件的情况，即流程点是否新建FixedBitSet？的判断为假，那么直接更新lastDocId的值更新为dociId，用于处理下一个文档号时，判断图7的流程点文档号是否不大于已收集的文档号，接着更新cost的值，即cost++的操作，在这个操作之后，通过上文中的介绍可以知道，目前DocsWithFieldSet已经处理（存储）的文档号集合为[0, cost]\ndocId 不等于cost（docId与cost的差值大于1）：说明从当前处理的文档号开始就不满足特殊条件了，即流程点是否新建FixedBitSet？的判断为真（是），那么此时需要执行流程点使用新建的FixedBitSet存储文档号，同时将之前处理的文档号（通过cost的值获得）以及当前文档号存储到新建的FixedBitSet对象中。当然还得继续更新lastDocId，因为在处理下一个文档号时，lastDocId要用于判断图7的流程点文档号是否不大于已收集的文档号；cost的值依然执行cost++的操作，因为cost的功能不仅仅是用来判断在特殊条件下，用来描述存储的文档号集合，它还要用来描述DocsWithFieldSet处理的文档号数量（见上文cost的介绍）\n\n 域值\n  域值即NumericDocValuesField的域值，例如图1的文档0中，域名为&quot;age&quot;的域值为88，我们需要收集88这个域值。\n  源码中使用了PackedLongValues来实现压缩存储，关于PackedLongValues的内容请参看文章PackedInts（一），本文不赘述，在后面的文章中，会再次介绍PackedLongValues的内容。\n 结语\n   无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dvd","dvm"]},{"title":"索引文件的生成（十八）之dvm&&dvd（Lucene 8.4.0）","url":"/Lucene/Index/2020/0528/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%E4%B9%8Bdvm&&dvd/","content":"  索引文件.dvm&amp;&amp;.dvd中根据文档中包含的不同类型的DocValuesFields，包含下面的DocValues信息：\n\nBinaryDocValues\nNumericDocValues：见文章索引文件的生成（十五）之dvm&amp;&amp;dvd\nSortedDocValues\nSortedNumericDocValues：见文章索引文件的生成（十七）之dvm&amp;&amp;dvd\nSortedSetDocValues\n\n  本篇文章开始介绍生成索引文件.dvd、.dvm之SortedDocValues、SortedSetDocValues的内容，在此之前，我们先介绍下在索引（index）阶段以及flush阶段，Lucene是如何收集文档中的SortedDocValues、SortedSetDocValues信息。\n SortedDocValues\n  SortedDocValues信息对应的是在文档中SortedDocValuesField域中的信息，它同NumericDocValues一样，在一篇文档中，相同域名的SortedDocValuesField只能有一个，否则就会报错，如下所示：\n图1：\n\n图2：\n\n  图1中，在一篇文档中有两个域名为&quot;level&quot;的SortedDocValuesField，那么会抛出图2中的错误，如果你期望支持多值，那么可以使用SortedSetDocValues，下文中会详细介绍。\n 收集文档的SortedDocValues信息\n  收集SortedDocValues信息的代码入口方法为：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java 的 addValue(int docID, BytesRef value)方法，收集的信息有：DocId、TermId、pending、sortedValues[ ]数组、ord、ordMap[ ]数组。\n  我们根据一个例子来介绍上述收集的信息：\n图3：\n\n DocId\n  DocId即包含SortedDocValuesField域的文档的文档号，并且使用DocsWithFieldSet存储，DocsWithFieldSet存储文档号的过程在文章索引文件的生成（十五）之dvm&amp;&amp;dvd已经介绍，不赘述。\n TermId\n  根据处理文档的顺序，有序的处理每一篇文档中的SortedDocValuesField，并且对每一个SortedDocValuesField的域值用一个从0开始递增的termId来描述，相同的域值具有相同的termId，图3的例子对应的termId如下所示：\n表一：\n\n\n\n域值\ntermId\n\n\n\n\na\n0\n\n\nd\n1\n\n\nb\n2\n\n\nc\n3\n\n\n\n pending\n  pending是PackedLongValues.Builder对象，这个对象我们不需要具体了解，我们只需要知道，在这个对象有一个currentValues[ ]数组，它用来收集每篇文档中SortedDocValuesField域的域值对应的termId，图3的例子对应的currentValues[ ]如下所示：\n图4：\n\n  图4中，注意的是currentValues[ ]数组的数组下标描述的不是文档号。\n sortedValues[ ]数组 &amp;&amp; ord\n  sortedValues[ ]数组的数组元素是termId（去重），数组下标为ord。下面的一句话很重要：数组元素是有序的，但是排序规则不是根据termId的值，而是根据termId对应的域值的字典序，图3的例子对应的sortedValues[ ]数组如下所示：\n图5：\n\n ordMap[ ]数组\n  sortedValues[]数组中实现了 数组下标ord 到 数组元素termId的映射，而ordMap[]数组则是实现了 数组下标termId 到 数组元素 ord的映射，图3的例子对应的ordMap[ ]数组如下所示：\n图6：\n\n  以上就是索引（index）阶段以及flush阶段收集的内容，至于收集这些信息的作用，将在后续的文章中提及。\n SortedSetDocValues\n  SortedSetDocValues信息对应的是在文档中SortedSetDocValuesField域中的信息，它跟SortedDocValues不用的是，一篇文章中允许多个相同域名的SortedSetDocValuesField，使得在搜索阶段，能提供更灵活的排序，同样我们以一个例子来介绍所谓的更灵活的排序。\n图7：\n\n  在搜索阶段，对于某一个SortedSetDocValues，可以支持四种方式的排序：\n\nMIN：选取文档中域值最小的参与排序\nMIDDLE_MIN：如果域值的数量为偶数，选取文档中的中间值较小的域值参与排序\nMIDDLE_MAX：如果域值的数量为偶数，选取文档中的中间值较大的域值参与排序\nMAX：选取文档中域值最大的参与排序\n\n  以下是源码中对上述的解释：\n图8：\n\n  如果在搜索阶段，我们定义了MIDDLE_MAX的排序规则，如下所示：\n图9：\n\n  对于上述排序规则，由于图6中每篇文档中的SortedSetDocValuesField数量为偶数，那么选取中间值较大的域值参与排序，如下所示：\n\n\n\n文档号\n参与排序的域值(MIDDLE_MAX)\n\n\n\n\n0\nl\n\n\n1\ni\n\n\n2\nj\n\n\n3\nk\n\n\n\n  那么搜索出来的文档是有序的，如下所示：\n图10：\n\n  上述的demo见 https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/lucene/DcoValues/SortedSetDocValuesTest.java 。\n 收集文档的SortedSetDocValues信息\n  介绍完SortedDocValues用法之后，我们继续介绍在索引（index）阶段以及flush阶段，收集SortedDocValues信息的内容，收集信息的代码入口方法为：https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java 的 addValue(int docID, BytesRef value)方法，需要收集的信息，包括：DocId、TermId、sortedValues[ ]数组、ord、ordMap[ ]数组、pending、pendingCounts。\n  其中DocId、TermId、sortedValues[ ]数组、ord、ordMap[ ]数组的含义跟SortedDocValues是一致的，故只介绍pending、pendingCounts。\n pending\n  同样使用pending中的currentValues[ ]数组收集每篇文档中SortedSetDocValues域的域值对应的termId，注意的是一篇文档中可能包含多个域值，并且可能还是重复的域值，为了能更好的理解pending，我们需要重新写一个例子：\n图11：\n\n  图11的例子对应的termId如下所示：\n\n\n\n域值\ntermId\n\n\n\n\na\n0\n\n\nb\n1\n\n\nc\n2\n\n\nd\n3\n\n\n\n  图11的例子对应的currentValues[ ]数组如下所示：\n图12：\n\n  对于上述的currentValues[ ]数组，需要说明两个重点：\n\n重点一：如果一篇文档中包含重复的域值，那么只需要记录一个即可，例如文档0中，有两个域值为&quot;b&quot;的SortedSetDocValuesField\n重点二：一篇文档中域值对应的termId是有序存放到currentValues[ ]数组中的，例如文档4，按照域值处理顺序为先处理域值&quot;d&quot;，对应termId为3.再处理域值&quot;a&quot;，对应termId为0，但最终根据termId从小到大的顺序存储到了currentValues[ ]数组中，至于这么处理的原因，暂时不介绍，在后面的文章中会介绍。\n\n pendingCounts\n  pendingCounts跟pending一样都是PackedLongValues.Builder对象，所以同样我们只需要关系pendingCounts中的currentValues[ ]数组，它存储了每篇文档中包含的termId的数量，图11的例子对应的currentValues[ ]数组如下所示：\n图13：\n\n  同样的，收集SortedSetDocValues信息的作用，将在介绍生成索引文件.dvd、.dvm之SortedDocValues、SortedSetDocValues的文章中。\n 结语\n  无。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dvd","dvm"]},{"title":"索引文件的生成（十六）之dvm&&dvd（Lucene 8.4.0）","url":"/Lucene/Index/2020/0518/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%E4%B9%8Bdvm&&dvd/","content":"  在文章索引文件的生成（十五）之dvm&amp;&amp;dvd中，我们介绍了在索引（index）阶段收集文档的NumericDocValues信息的内容，随后在flush阶段，会根据收集到的信息生成索引文件.dvd&amp;&amp;.dvm。如果已经阅读了文章NumericDocValues，那么能快的理解本文的内容，注意的是那篇文章中，是基于Lucene 7.5.0，下文中的内容基于Lucene 8.4.0，不一致（优化）的地方会特别说明。\n 生成索引文件.dvd、.dvm之NumericDocValues的流程图\n图1：\n\n 准备工作\n图2：\n\n  根据在索引阶段收集的文档的NumericDocValuses信息（见文章索引文件的生成（十五）之dvm&amp;&amp;dvd），在当前流程点需要执行一些准备工作，即计算出下面的信息：\n\ngcd（greatest common divisor）\nminMax和blockMinMax\nuniqueValues\nnumDocsWithValue\n\n  在流程点准备工作中，通过遍历每一个包含NumericDocValues信息的文档来完成上述信息的计算。\n gcd（greatest common divisor）\n  gcd即最大公约数，获得所有NumericDocValues的域值的最大公约数，根据数学知识可知，最大公约数的最小为1。\n  为什么要计算gcd：\n  在文章NumericDocValues中已经作出了解释，本文中再详细的介绍一次，其实目的很简单：降低存储开销。\n  例如我们有以下的域值集合：\n&#123;150、140、135&#125;\n  上述三个域值的最大公约数为5，即gcd的值为5，按照下面的公式我们计算出编码后的域值：\n(v - min) / gcd\n  上述公式中，v为编码前的域值，min为域值集合中的最小值，在本例子中，min的值为135，编码后的域值集合如下所示：\n&#123;3, 1, 0&#125;\n  可见编码后的域值集合中，最大值为3，当使用固定位数按位存储（见文章PackedInts（一））时，只需要6个bit存储即可，在读取阶段，根据min、gcd的值就可以获得编码前的域值集合。\n minMax、blockMinMax\n  minMax跟blockMinMax在源码中都是类MinMaxTracker的对象，都是用来收集下面的信息：\n\nmin：域值集合中的最小值\nmax：域值集合中的最大值\nnumValues：域值集合中的元素数量\nspaceInBits：存储域值占用的bit数量\n\nspaceInBits的计算方式：(bitCount(max - min)) * num，其中num指的是域值的数量，bitCount( )方法描述的是域值占用的有效bit数量，例如bitCount(3) = 2，数值3的二进制为0b00000011，有效的bit数量为2个\n\n\n\n  两个信息的区别在于，minMax收集的是域值集合中全量数据下的min、max、spaceInBits，而blockMinMax则是将域值集合分为N个block，每处理4096个域值就作为一个block，每个block单独收集min、max、spaceInBits。\n  为什么要计算minMax、blockMinMax\n  为了判断出使用单个block和多个block存储域值时，哪一种方式有更低的存储开销。\n  例如我们有以下域值集合，为了便于描述，计算blockMinMax时，每处理3个域值就作为一个block。\n图3：\n\n  图2中，对于minMax，域值集合的全量数据下的min、max分别为1、18，并且有9个，那么spaceInBits = (bitCount(18 - 1)) * 9，数值17的二进制为0b00010001，有效的bit数量为5个，故spaceInBits的值为5*9 = 45。\n图4：\n\n  图3中，对于blockMinMax，域值集合被分为3个block，每个block的spaceInBits的和值作为blockMinMax的spaceInBits，故spaceInBits = (bitCount(2 -1)) * 3 + (bitCount(18 -16)) * 3 + (bitCount(8 -5)) * 3 = 3 + 6 + 9 = 18。\n  源码中，如果blockMinMax和minMax的spaceInBits满足下面的条件，那么就使用多个block存储域值：\n(double) blockMinMax.spaceInBits / minMax.spaceInBits &lt;= 0.9\n  显而易见，上述的例子将会使用多个block存储域值。当然了，当前的流程点是准备工作，仅仅判断出存储域值的方式，其具体的存储域值逻辑将在后面的流程中执行。\n uniqueValues\n  uniqueValues是一个Set&lt;Long&gt;对象，用来统计域值的种类，uniqueValues将在后续的流程点是否使用域值映射存储作为判断的依据，在本文中不展开介绍，我们只需要知道收集域值种类的时机点即可，例如图3中的域值集合，在执行完流程点准备工作后，uniqueValues中的内容如下所示：\n&#123;1, 2, 3, 5, 8, 16, 17, 18&#125;\n numDocsWithValue\n  numDocsWithValue用来描述包含当前域名的文档数量，在后面的流程中它将作为一个判断条件，用于选择使用哪种数据结构来存储包含当前域的文档号集合。\n 写入文档号信息\n图5：\n\n  文档号信息包含两个信息：\n\n文档号的值信息：文档号的值将被写入到索引文件dvd中\n文档号的索引信息：文档号索引信息将被写入到索引文件.dvm中，在读取阶段根据文档号的索引信息来读取在索引文件.dvd中的文档号的值区间，下文中会详细说明\n\n  另外根据在流程点准备工作中计算出的numDocsWithValue，对应有三种数据结构存储文档号索引信息。\n 0 &lt; numDocsWithValue &lt; maxDoc\n  maxDoc的值为段中的文档数量，在下文中会介绍该值，如果numDocsWithValue的值在区间(0, maxDoc)，文档索引信息将会被存储到索引文件.dvm中的DocIdIndex字段，文档号的值信息将被存储到索引文件.dvd中的DocIdData字段：\n图6：\n\n   图6中，文档号的索引信息包含offset跟length，它们描述了文档号的值信息在索引文件.dvd中的值区间，其他字段的解释见文章NumericDocValues，我们再看Lucene 8.4.0中的数据结构：\n图7：\n\n   在Lucene 8.4.0版本中，DocIdIndex字段中多了jumpTableEntryCount跟denseRankPower两个信息，在读取阶段，通过这两个信息能获得查找表（lookup table）的信息，jumpTableEntryCount跟denseRankPower以及查找表的概念在系列文章IndexedDISI已经介绍，这里不赘述。\n numDocsWithValue == 0\n  如果numDocsWithValue == 0，那么将固定的信息写入到索引文件.dvm中，固定信息在索引文件.dvm中的位置如下所示：\n图8：\n\n  图8中，offset跟length被置为固定信息，在读取阶段，当读取到offset的值为-2时，就知道numDocsWithValue == 0，下面给出Lucene 8.4.0的索引文件.dvm，由于numDocsWithValue == 0，文档号的值信息不用写入到索引文件.dvd中：\n图9：\n\n  同样的查找表的信息用固定信息填充。\n numDocsWithValue == maxDoc\n  如果numDocsWithValue == maxDoc，说明正在执行flush的段中的每篇文档都包含当前域的信息，意味着我们也不用存储文档号的值信息，因为在读取阶段，文档号的值就是 [0, maxDoc]区间内的所有值，故同样只要将固定的信息写入到索引文件.dvm中：\n图10：\n\n  在读取阶段，当读取到offset的值为-1时，就知道numDocsWithValue == maxDoc，同样地给出Lucene 8.4.0的数据结构：\n图11：\n\n  在写入了文档号信息之后，将域值数量写入到索引文件.dvm中，域值数量在上文中的minMax中numValues获得，如下所示：\n图12：\n\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dvd","dvm"]},{"title":"索引文件的生成（十四）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Index/2020/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  在前面的文章中，我们介绍了构建BKD树的节点值（node value）的流程，其中叶子节点的信息被写入到了索引文件.dim中，本文开始的内容将会介绍将内部节点（非叶节点）的信息写入到索引文件.dim，为了便于下文的介绍，先给出生成索引文件.dim&amp;&amp;.dii的流程图：\n图1：\n\n 构建BKD树的节点元数据（node metadata）\n  在图1的流程点构建BKD树的节点元数据（node metadata）中，即将内部节点的信息写入到索引文件.dim的过程，该流程点对应的是源码 https://github.com/LuXugang/Lucene-7.5.0/blob/master/solr-8.4.0/lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter.java 中的**writeIndex(IndexOutput out, int countPerLeaf, long[] leafBlockFPs, byte[] splitPackedValues)**方法。\n  在这个方法中写入的信息即下图中的BKD字段，它对应在索引文件.dim中的位置如下所示：\n图2：\n\n  图2中，所有字段的含义在文章索引文件之dim&amp;&amp;dii中已经介绍，我们仅挑选出某些字段来进一步的介绍。\n PackedIndexValue\n  该字段描述的是内部节点的信息，该字段中包含的四种类型的数据结构分别描述不同类型的内部节点（非叶节点）信息：\n\nLeftSubtreeHasLeafChild：非叶节点的子树是叶子节点，并且它是父节点的左子树\nRightSubtreeHasLeafChild：非叶节点的子树是叶子节点，并且它是父节点的右子树\nLeftSubtreeHasNotLeafChild：非叶节点的子树不是叶子节点，并且它是父节点的左子树\nRightSubtreeHasNotLeafChild：非叶节点的子树不是叶子节点，并且它是父节点的右子树\n\n  在PackedIndexValue中，非叶节点之间的关系如下所示：\n图3：\n\n  如果我们有以下的BKD树，树中的非叶节点信息对应在PackedIndexValue字段中的位置如下所示：\n图4：\n\n点击查看大图\n  顺便给出叶子节点的信息对应在PackedIndexValue字段中的位置如下所示：\n图5：\n\n点击查看大图\n  篇幅不够，画图来凑：\n图6：\n\n点击查看大图\n 生成索引文件.dim的元数据\n  生成索引文件.dim的元数据的过程即生成索引文件.dii。在读取索引阶段，通过读取索引文件.dii来获得每一个点数据域的数据块（block），该block在索引文件.dim中的偏移值，如果我们在索引阶段的点数据域信息如下所示：\n图7：\n\n  图7中，有两个点数据域，它们对应的域名分别是&quot;book&quot;、“tilte”，那么对应在索引文件.dii中的位置如下所示：\n图8：\n\n  同样的，图8中各个字段的含义在文章索引文件之dim&amp;&amp;dii已经介绍，不赘述。\n  在读取阶段，就可以通过读取索引文件.dii的IndexFP字段找到某个点数据域在索引文件.dim中的偏移值，即读取起始位置。\n  至于详细的读取过程将在后续文章中介绍。\n 结语\n  至此，生成索引文件.dim&amp;&amp;dii的过程已经全部介绍结束，在后续的文章中，将通过数值类型的范围查询来介绍索引文件.dim&amp;&amp;dii的读取过程，即读取BKD树。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dim","dii"]},{"title":"索引文件的生成（十）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Index/2020/0408/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本文承接索引文件的生成（九），继续介绍剩余的内容，下面先给出生成索引文件.dim&amp;&amp;.dii的流程图：\n图1：\n\n  在上一篇文章中，我们介绍了流程点执行处理前的初始化的工作，在这个流程中涉及到的一些信息贯穿整个流程，请务必先行阅读，例如一些变量名如果没有展开说明，说明已经在上一篇文章中介绍。\n 构建BKD树的节点值（node value）\n  先给出该流程点的流程图：\n图2：\n\n  图2的流程中，描述的是处理一个节点的流程，该节点如果是内部节点（非叶节点），那么就划分出左右子树，即左右两个节点，随后递归处理，是一个深度遍历的过程。\n 节点\n图3：\n\n  流程图的准备数据为一个节点，该节点可能是叶子节点或者内部节点，在生成BKD树的开始阶段，该节点为根节点。\n 是否为叶子节点？\n图4：\n\n  在文章索引文件的生成（九）之dim&amp;&amp;dii中我们说到，在构建BKD树之前，我们已经能提前计算出BKD树中内部节点以及叶子节点的数量 numleaves，并且为每个节点都赋予了一个节点编号，如下图所示：\n图5：\n\n  如何判断当前节点是不是叶子节点：\n  源码中通过判断当前节点编号是否小于最左叶子节点的编号，如果满足，说明当前节点是内部节点，否则就是叶子节点，而最左叶子节点的编号正是numleaves（满二叉树的性质）。\n 选出切分维度\n图6：\n\n  在当前流程点，我们需要选出切分维度，使得在后面的流程中根据该维度值进行左右子树的划分，规则如下：\n\n条件一：先计算出切分次数最多的那个维度（根据parentSplits数组），切分次数记为maxNumSplits，如果有一个维度的切分次数小于 (maxNumSplits / 2) ，并且该维度中的最大跟（maxPackedValue）最小值（minPackedValue）不相同，那么令该维度为切分维度\n条件二：计算出每一个维度中最大值跟最小值的差值，差值最大的作为切分维度\n\n  从条件一可以看出这条规则的目的就是保证所有的维度都能被用来切分，当条件一无法选出切分维度时，再考虑条件二。\n  Lucene8.4.0中的优化改进\n  Lucene 7.5.0与8.4.0两个版本中，选出切分维度的条件是一致的，而Lucene8.4.0对上述条件中的最大跟（maxPackedValue）最小值（minPackedValue）进行了优化处理，这部分内容将在介绍流程点设置左子树的准备数据、设置右子树的准备数据时候展开。\n 内部节点的排序\n图7：\n\n  在上一个流程点选出切分维度选出切分维度后，接着以这个维度的值作为排序规则对点数据进行排序，排序算法使用的是最大有效位的基数排序(MSB radix sort)，在源码中，先计算出maxPackedValue、minPackedValue中切分维度对应的维度值的相同前缀，目的在于使得在排序时只需要比较不相同的后缀值，提高排序性能。\n  我们在文章索引文件的生成（八）之dim&amp;&amp;dii中提到，在收集阶段，已经将所有的点数据的域值（维度值）都存放在了ByteBlockPool对象的字节数组buff中，当前流程点对点数据的排序并不会真正的在buff数组中移动（交换）维度值来实现排序，而是通过一个int类型的ord数组来描述点数据之间的排序关系。\n ord[ ]数组\n  ord数组是一个int类型的数组，数组元素是numPoints（点数据的唯一标示，见文章索引文件的生成（八）之dim&amp;&amp;dii），执行完流程点内部节点的排序后，ord[ ]数组中的数组元素是有序的。注意的是：不是数组元素的值有序，而是数组元素（numPoints）对应的维度值是有序的\n  如何通过numPoints获得维度值\n  通过下面的公式来找到numPoints对应的维度值在字节数组buff中的起始位置\nfinal long offset = (long) packedBytesLength * ords[i];\n  介绍上述公式之前我们先说下维度编号的概念：\n\n图10中，每个点数据有三个维度，对于代码的第49行，维度值3的维度编号是0，维度值5的维度编号是1，维度值12的维度编号是2，即维度编号是一个从开始递增的值。\n\n  我们继续介绍上述公式：packedBytesLength指的是一条点数据的域值占用的字节数，例如在图10中，一条点数据中有3个维度值，维度值是int类型，一个int类型的数值转化为字节数组后占用的字节数为4（见文章索引文件的生成（八）之dim&amp;&amp;dii），那么packedBytesLength的值为3*4 = 12，另外，假设根节点中包含了图8中的三个点数据，并且假设按照维度编号2排序，那么ord数组如下所示：\n图8：\n\n  如果我们需要获得numPoints为2的点数据域值，根据上述公式offset = 12 * 2 = 24，就可以将字节数组buff中数组下标为24的位置作为起始地址，读取packedBytesLength（12）个字节就可以获得点数据的域值，图9中的字节数组buff中存放的域值对应图10：\n图9：\n\n图10：\n\n 设置splitPackedValues数组\n图11：\n\n  在上一篇中我们简单的介绍了splitPackedValues，它是一个字节数组，该值用来描述每一个节点使用哪个维度（维度编号）进行划分以及维度的值。\n图12：\n\n  splitPackedValues数组中数组元素描述的信息如图所示：\n图13：\n\n  我们假设当前处理的是根节点，并且图12为在根节点中待处理的点数据集合，同时切分维度的维度编号为1，那么在执行了流程点内部节点的排序之后，排序后的点数据集合如下所示，为了便于描述，该集合的每个元素都有一个序号，序号是从0开始递增的值，例如下图中点数据{1, 2}的序号为0，点数据{2, 8}的序号为5：\n&#123;1,2&#125; -&gt; &#123;4,3&#125; -&gt; &#123;3,4&#125; -&gt; &#123;4,6&#125; -&gt; &#123;6,7&#125; -&gt; &#123;2,8&#125; -&gt; &#123;8,9&#125; -&gt; &#123;7,11&#125;\n  在上文有序的点数据集合中，选出序号的中位数，该例子中一共有8个点数据，那么根据源码中的中位数计算公式：\nfinal int mid = (number) &gt;&gt;&gt; 1;\n  上述公式中，number即点数据的数量，即8，故mid的值为4，即点数据{6, 7}，由于我们假设的是根节点，它的节点编号是1，那么根据下面的公式就可以找到该节点编号在splitPackedValues数组中写入的起始地址：\nfinal int address = nodeID * (1+bytesPerDim);\n  上述公式中，nodeId即节点编号，bytesPerDim描述的是每个维度值占用的字节数量，在文章索引文件的生成（九）之dim&amp;&amp;dii中说到int类型在转化为字节数组后占用4个字节，故bytesPerDim的值为4，那么address的值为：\naddress = 1 * (1 + 4) = 5\n  所以splitPackedValues数组下标值为5的位置作为写入的起始地址，分别写入维度编号1（上文中我们了假设切分维度的维度编号是1）以及切分维度的维度值7，如下图所示：\n图14：\n\n 结语\n  基于篇幅，剩余的流程点将在下一篇文章中展开介绍。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["dim","dii"]},{"title":"索引文件的生成（四）之跳表SkipList","url":"/Lucene/Index/2020/0106/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E8%B7%B3%E8%A1%A8SkipList/","content":"  在文章索引文件的生成（三）中我们介绍了在Lucene中生成跳表SkipList的流程，通过流程图的方法介绍了源码中的实现方式，而对于读取SkipList的内容，决定直接以例子的方式来介绍其读取过程，下文中出现的名词如果没有作出介绍，请先阅读文章索引文件的生成（三）。\n 例子\n  直接给出一个生成后的跳表：\n图1：\n\n  在图1中，为了便于介绍，我们处理的是文档号0~3455的3456篇文档，我们另skipInterval为128，即每处理128篇文档生成一个PackedBlock，对应一个datum；另skipMultiplier为3（源码中默认值为8），即每生成3个datum就在上一层生成一个新的索引，新的索引也是一个datum，它是3个datum中的最后一个，并且增加了一个索引值SkipChildLevelPointer来实现映射关系（见索引文件的生成（三）），每一层的数值为PackedBlock中的最后一篇文档的文档号，例如level=2的三个数值1151、2303、3455。\n 哨兵数组skipDoc\n  哨兵数组skipDoc的定义如下所示：\nint[] skipDoc;\n  该数组用来描述每一层中正在处理的datum，datum对应的PackedBlock中的最后一篇文档的文档号作为哨兵值添加到哨兵数组中，在初始化阶段，skipDoc数组中的数组元素如下所示（见图2红框标注的数值）：\nint[] skipDoc = &#123;127, 383, 1151, 3455&#125;\n  初始化阶段将每一层的第一个datum对应的PackedBlock中的最后一篇文档的文档号作为哨兵值。\n docDeltaBuffer\n  docDeltaBuffer是一个int类型数组，总是根据docDeltaBuffer中的文档集合来判断SkipList中是否存在待处理的文档号。\n  在初始化阶段，docDeltaBuffer数组中的数组元素是level=0的第一个datum对应的PackedBlock中文档集合。\n SkipList在Lucene中的应用\n  了解SkipList在Lucene中的应用对理解读取跳表SkipList的过程很重要，在Lucene中，使用SkipList实现文档号的递增遍历，每次判断的文档号是否在SkipList中时，待处理的文档号必须大于上一个处理的文档号，例如我们在文章文档号合并（SHOULD）中，找出满足查询要求的文档就是通过SkipList来实现。\n Lucene中使用读取跳表SkipList的过程\n  读取过程分为下面三步：\n\n步骤一：获得需要更新哨兵值的层数N\n\n从skipDoc数组的第一个哨兵值开始，依次与待处理的文档号比较，找出所有比待处理的文档号小的层\n\n\n步骤二：从N层开始依次更新每一层在skipDoc数组中的哨兵值\n\n如果待处理的文档号大于当前层的哨兵值，那么另当前层的下一个datum对应的PackedBlock中的最后一篇文档的文档号作为新的哨兵值，直到待处理的文档号小于当前层的哨兵值\n在处理level=0时，更新后的datum对应的PackedBlock中的文档集合更新到docDeltaBuffer中\n\n\n步骤三：遍历docDeltaBuffer数组\n\n取出PackedBlock中的所有文档号到docDeltaBuffer数组中，依次与待处理的文档号作比较，判断SkipList中是否存在该文档号\n\n\n\n 读取跳表SKipList\n  我们依次处理下面的文档号，判断是否在跳表SKipList中来了解读取过程：\n文档号：&#123;23, 700, 701, 3000&#125;\n 文档号：23\n  更新前的skipDoc数组如下所示：\nint[] skipDoc = &#123;127, 383, 1151, 3455&#125;\n  当前skipDoc数组对应的SkipList如下所示：\n图2：\n\n  由于文档号23小于skipDoc数组中的所有哨兵值，故不需要更新skipDoc数组中的哨兵值，那么直接遍历docDeltaBuffer，判断文档号23是否在该数组中即可。\n 文档号：700\n  更新前的skipDoc数组如下所示：\nint[] skipDoc = &#123;127, 383, 1151, 3455&#125;\n  文档号700小于1151、3455，执行了步骤一之后，判断出需要更新哨兵值的层数N为2（两层），即只要从level=1开始更新level=1以及level=0对应的skipDoc数组中的哨兵值。\n  对于level=1层，在执行了步骤二后，更新后的skipDoc数组如下所示：\nint[] skipDoc = &#123;127, 767, 1151, 3455&#125;\n图3：\n\n  随后更新level=0层，在执行了步骤二后，更新后的skipDoc数组如下所示：\nint[] skipDoc = &#123;767, 767, 1151, 3455&#125;\n图4：\n\n  这里要注意的是，level=0层的datum更新过程如下所示：\n图5：\n\n  从图5中可以看出，在更新的过程中，跳过了两个datum，其原因是在图3中，当更新完level=1的datum之后，该datum通过它包含的SkipChildLevelPointer字段（见索引文件的生成（三））重新设置在level=0层的哨兵值，随后在处理level=0时，根据待处理的文档号继续更新哨兵值。\n 文档号：701\n  更新前的skipDoc数组如下所示：\nint[] skipDoc = &#123;767, 767, 1151, 3455&#125;\n  文档号701小于所有的哨兵值，所以直接遍历docDeltaBuffer数组即可。\n 文档号：3000\n  不重复用文字描述其更新过程了，直接以下图给出每一层最终的哨兵值以及更新过程，自己品…，更新后的skipDoc数组如下所示：\nint[] skipDoc = &#123;3071, 3071, 3455, 3455&#125;\n图6：\n\n 结语\n  可以看出，SkipList在Lucene中的实现适用于存储有序递增的文档号，性能上取决于待处理的文档号在datum的分布，分布在越少的datum，性能越高。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["doc","skipList"]},{"title":"索引文件的读取（一）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Search/2020/0427/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本系列的文章会通过例子来介绍索引文件的读取，本篇文章先介绍索引文件.dim&amp;&amp;.dii的读取，为了便于理解，请先阅读索引文件的生成（八）之dim&amp;&amp;dii至索引文件的生成（十四）之dim&amp;&amp;dii的文章。\n  在生成SegmentReader期间，会生成PointsReader（PointsReader为抽象类，实现的子类就是Lucene60PointsReader对象），它用来描述某个段中的点数据信息，下面先列出该对象包含的部分信息：\n\nfinal Map&lt;Integer,BKDReader&gt; readers = new HashMap&lt;&gt;();\n\n  在上述的Map对象中，存放的是不同域名的点数据信息，key为域的编号（FieldNumber），value为域对应的点数据信息，通过读取索引文件.dii来初始化readers对象，BKDReader中包含的主要信息如下所示：\n\nnumDataDims：点数据的维度数量\nmaxPointsInLeafNode：每个叶子节点中的最多包含的点数据数量\nbytesPerDim：一个维度值占用的字节数量\nnumLeaves：叶子节点的数量\nminPackedValue：MinPackedValue中每个维度的值都是所在维度的最小值\nmaxPackedValue：MinPackedValue中每个维度的值都是所在维度的最大值\n\nminPackedValue跟maxPackedValue两者描述了BKD树中点数据的数值范围（见下文）\n\n\npointCount：当前域中的点数据的数量\ndocCount：包含当前域中的点数据域的文档数量。 一篇文档中可以包含多个相同域名的点数据域，但是docCount的计数为1\npackedIndex：PackedIndex存放了非叶节点的信息\n\n  上述的字段在索引文件.dim中的位置如下所示：\n图1：\n\n  上图中，根据索引文件.dii中的Count获得不同域名的点数据的数量，随后依次遍历每一个Field，通过下面的两个信息读取某个点数据域在索引文件.dim中的信息：\n\n\nFiledNumber：域的编号，它作为Map对象readers的key\n\n\nIndexFP：该字段指向了该点数据域对应的BKD信息在索引文件.dim中起始读取位置，随后BKD字段的信息被封装到BKDReader对象中，最后作为Map对象readers的value\n\n\n  接着在Search阶段，我们就可以通过PointsReader来读取每一个点数据域的点数据信息了。\n 读取索引文件.dim&amp;&amp;dii\n  下面的流程图基于数值类型的范围查询IntPoint.newRangeQuery(String field, int[ ] lowerValue, int[ ] upperValue)方法，流程图中每个流程点描述的内容依赖BKDReader对象中提供的信息：\n图2：\n\n点击查看大图\n  在介绍每一个流程点之前，我们先介绍下图2中CELL_OUTSIDE_QUERY、CELL_INSIDE_QUERY、CELL_CROSSES_QUERY的概念：\n  先给出源码中的注释：\npublic enum Relation &#123;    /** Return this if the cell is fully contained by the query */    CELL_INSIDE_QUERY,    /** Return this if the cell and query do not overlap */    CELL_OUTSIDE_QUERY,    /** Return this if the cell partially overlaps the query */    CELL_CROSSES_QUERY  &#125;\n  在索引文件的生成（十一）之dim&amp;&amp;dii文章中我们提到，在生成BKD树的过程中，每生成一个内部节点，该节点的父节点会分别提供给子节点两个值：minPackedValue、maxPackedValue。\n\nminPackedValue：内部节点中每个维度的最小值\nmaxPackedValue：内部节点中每个维度的最大值\n\n  minPackedValue跟maxPackedValue用来描述该节点中的点数据的数值范围，另外读取BKD树的方式为深度遍历，即从根节点开始，逐个节点查询，在读取节点信息前，先用查询条件跟节点的minPackedValue、maxPackedValue进行比较，计算出节点中的点数据范围跟查询条件的边界关系，即上文中的Relation，我们通过例子来介绍上述的三种边界关系，为了便于介绍以及图形化，点数据的维度数量为2，并且我们比较的节点为根节点。\n  根节点中包含的点数据如下所示：\n&#123;5, 7&#125;, &#123;5, 8&#125;, &#123;4, 6&#125; -&gt; &#123;4, 3&#125; -&gt; &#123;3, 4&#125; -&gt; &#123;7, 11&#125; -&gt; &#123;8, 9&#125; -&gt; &#123;6, 7&#125;\n  那么根节点中的minPackedValue、maxPackedValue（根节点中如何获得minPackedValue、maxPackedValue见文章索引文件的生成（九）之dim&amp;&amp;dii）如下所示：\nminPackedValue：&#123;3, 3&#125;maxPackedValue：&#123;8, 11&#125;\n  根据上述的minPackedValue、maxPackedValue，根节点中的点数据的数值范围如下所示红框，由于维度数量是2，所以数值范围可以用一个平面矩形表示，同理如果维度数量是3，那么数据范围就是一个立方体：\n图3：\n\n CELL_OUTSIDE_QUERY\n  CELL_OUTSIDE_QUERY描述的是查询条件的数值范围跟节点的数值范围没有交集，即没有重叠（overlap），在IntPoint.newRangeQuery(String field, int[ ] lowerValue, int[ ] upperValue)方法中，如果lowerValues跟upperValue的值如下所示：\nlowerValues：&#123;1, 1&#125;upperValue：&#123;2, 2&#125;\n  lowerValues跟upperValue描述的数值范围如下所示：\n图4：\n\n  图4可以明显看出，查询条件的数值范围跟根节点没有重合，即CELL_OUTSIDE_QUERY\n CELL_CROSSES_QUERY\n  CELL_CROSSES_QUERY描述的是查询条件的数值范围跟节点的数值范围部分重叠（partially overlaps），如果我们以下的lowerValues跟upperValue：\nlowerValues：&#123;1, 1&#125;upperValue：&#123;5, 6&#125;\n  lowerValues跟upperValue描述的数值范围如下所示：\n图5：\n\n CELL_INSIDE_QUERY\n  CELL_INSIDE_QUERY描述的是查询条件的数值范围包含节点的中的所有点数据，如果我们以下的lowerValues跟upperValue：\nlowerValues：&#123;1, 1&#125;upperValue：&#123;9, 12&#125;\n  lowerValues跟upperValue描述的数值范围如下所示：\n图6：\n\n  计算CELL_OUTSIDE_QUERY、CELL_INSIDE_QUERY、CELL_CROSSES_QUERY的目的是什么：\n  目的就是为了以读取性能最高的方式读取完BKD树的信息，例如如果根节点跟查询条件计算出的边界为CELL_OUTSIDE_QUERY，那么就可以不用递归的去读取左右子树的信息，特别是索引目录中有多个段时，那么就可以跳过该段；又例如如果叶子节点的内部节点与查询条件计算出的边界为CELL_INSIDE_QUERY，那么直接读取叶子节点中的所有点数据对应的文档号即可，而对于CELL_CROSSES_QUERY的情况，我们需要递归的去该节点的左右子树中继续处理，处理流程相对复杂，我们将在后面的流程中展开介绍。\n 结语\n  无。\n点击下载\n","categories":["Lucene","Search"],"tags":["index","dim","dii"]},{"title":"索引文件的读取（七）之tim&&tip（Lucene 8.4.0）","url":"/Lucene/Search/2020/0804/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%B8%83%EF%BC%89%E4%B9%8Btim&&tip/","content":"  本篇文章开始介绍索引文件tim&amp;&amp;tip的读取，通过TermRangeQuery的例子来介绍如何从索引文件.tim&amp;&amp;.tip中获取满足查询条件的所有term。\n  为了便于介绍，使用了文章Automaton（二）中提供的例子：\n图1：\n\n  结合图1的例子，获取满足查询条件（第79行代码）的所有term的过程可以简单的用一句话来描述：根据域名&quot;content&quot;，从索引文件.tim&amp;&amp;.tip中获取该域对应的term集合，随后遍历集合中的每一个term，使用DFA（见文章Automaton（二））筛选出满足条件的term，流程图如下所示：\n 获取满足TermRangeQuery查询条件的term集合的流程图\n图2：\n\n BlockTreeTermsReader\n图3：\n\n  在介绍BlockTreeTermsReader的概念或者说包含的信息之前，我们先简单的介绍下该对象的生成时机点，在生成StandardDirectoryReader对象期间，会生成SegmentReader对象，该对象中的FieldsProducer信息描述了索引文件tim&amp;&amp;tip、索引文件doc、索引文件pos&amp;&amp;pay中所有域的索引信息，而BlockTreeTermsReader作为FieldsProducer信息的成员之一，作为索引文件tim&amp;&amp;tip在内存中的描述方式。\n  BlockTreeTermsReader对象中，最重要也是我们唯一关心的信息就是一个名为fieldMap的Map容器，定义如下所示：\n图4：\n\n  fieldMap容器中，key为域名，value，即FieldReader对象，为该域名对应一些信息，在生成BlockTreeTermsReader对象的过程中，所有域的信息将用FieldReader对象来描述，接着我们介绍哪些信息会被用于生成FieldReader对象中。\n 获取索引文件.tip&amp;&amp;tim中数据的起始读取位置\n  通过读取索引文件.tip&amp;&amp;tim来获得生成FieldReader对象的信息， 两个索引文件都是通过各自的DirOffset字段来获取数据的起始读取位置，先给出这两个索引文件的数据结构：\n图5：\n\n  在当前版本Lucene 8.4.0中，由于DirOffset跟Footer字段分别占用固定的8、16个字节，共24个字节，故这两个索引文件可以从后往前定位到DirOffset字段的起始读取位置，然后根据DirOffset字段的值再获取数据的读取位置，最后相关信息写入到FieldReader中，如图4所示。\n FieldSummary\n  对于索引文件.tim，它包含的FieldSummary字段中包含了所有域的一些MetaData，如下所示：\n图6：\n\n  图6中，除了FieldNumber字段，其他字段都会被写入到FIeldReader对象中，简单了解这些字段的介绍可以看文章索引文件之tim&amp;&amp;tip，详细了解这些字段的生成过程可以阅读文章索引文件的生成（五）之tim&amp;&amp;tip、索引文件的生成（六）之tim&amp;&amp;tip以及索引文件的生成（七）之tim&amp;&amp;tip。\n IndexStartFP\n  对于索引文件.tip，IndexStartFP描述了每个域对应的FSTIndex字段在索引文件.tip中的起始读取位置，如下所示：\n图7：\n\n  FSTIndex字段描述了该域的第一个读取NodeBlock对应的FST信息，介绍见文章索引文件的生成（七）之tim&amp;&amp;tip。\n FieldReader\n  在生成FieldReader对象的过程中，我们着重关心FST的导入模式（load Mode），目前有两种模式可选择：\n\noff-heap：该模式可以简单理解为使用时载入，载入动作为磁盘I/O操作，只将需要使用的信息读入到内存中\non-heap：该模式为一次性载入，直接将所有信息读入内存\n\n  在文章索引文件的生成（六）之tim&amp;&amp;tip中我们说到，在合并了PendingBlock之后，会生成一个FST，其中第一个PendingBlock中的prefix将作为FST的inputValue，合并的信息作为FST的outValue。\n  通过图7中的FSTIndex中的信息来生成FST，该字段的结构如下所示，各个字段的含义在文章索引文件的生成（七）之tim&amp;&amp;tip已经介绍，不赘述：\n图8：\n\n  图8中bytes字段记录了FST的主要信息，意味着这个字段占用的字节是最大的，它包含的信息就是在文章FST（一）中，文章结尾最终生成的current[ ]数组。\n off-heap\n  当导入模式为off-heap时，只会保存图8中的offset的值，它描述了bytes的读取位置，由于当前流程点处在BlockTreeTermsReader，还没有到查询阶段，目前是不知道查询条件中会对哪个域名进行查询，故使用off-heap后就不用载入所有域的FST的主要信息，在图2的流程点获取迭代器IntersectTermsEnum根据查询条件获取某个域的迭代器（查询阶段）时才会真正的去磁盘读取该域的FST的主要信息，换个说法就是在当前流程点只是获取所有域的FST主要信息的一个&quot;句柄&quot;跟文件指针（offset），并没有获得完整的FST信息。\n on-heap\n  当导入模式为on-heap时，则会从磁盘读取所有域的FST的主要信息，即读取所有域的FSTIndex字段的bytes字段。\n  上述的导入模式在Lucene 8.0.0才加入，在之前的版本中，只使用on-heap，该issue地址：https://issues.apache.org/jira/browse/LUCENE-8635?jql=project %3D LUCENE AND text ~ &quot;FST BytesStore&quot; 。\n  在使用on-heap一次性载入所有域的FST信息时还会根据FST信息的大小使用不用数据结构存储，原因在于Java中的byte[ ]数组大小存在上限，Lucene通过图8中的numBytes字段判断出bytes字段的大小，分别使用下面两个数据结构存储：\n\nbyte[ ]数组：如果numBytes小于等于1GB，那么直接使用byte[ ]数组存储\n\n图9：\n\n\nBytesStore对象：如果numBytes大于1GB，那么使用该数据结构\n\n该对象中使用了多个byte[ ] 存储bytes字段的信息，定义如下：\n\n\n\n图10：\n\n  该对象中定义了一个链表：\n图11：\n\n 结语\n  至此，图1的流程点BlockTreeTermsReader介绍结束，基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","tim","tip"]},{"title":"索引文件的读取（三）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Search/2020/0430/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本文承接索引文件的读取（二）之dim&amp;&amp;dii继续介绍剩余的内容，下面先给出读取索引文件.dim&amp;&amp;dii的流程图：\n图1：\n\n点击查看大图\n 读取索引文件.dim&amp;&amp;dii\n 判断节点与查询条件的边界关系\n图2：\n\n  当前节点与查询条件的边界关系即CELL_INSIDE_QUERY、CELL_OUTSIDE_QUERY、CELL_CROSSES_QUERY共三种关系（见文章索引文件的读取（一）之dim&amp;&amp;dii）。\n CELL_OUTSIDE_QUERY\n图3：\n\n  如果当前节点为根节点，并且根节点中的点数据的数值范围跟查询条件的数值范围的边界关系为CELL_OUTSIDE_QUERY，意味着BKD树中没有任何点数据满足查询条件，此时就可以直接跳过（skip）当前点数据域的查询，故可以结束BKD树的读取。\n  如果当前节点不是根节点，不管它是内部节点（非叶节点）还是叶子节点，我们都不用继续读取这个节点的信息以及以这个节点为祖先节点的所有子节点，故返回到上一层递归继续处理。\n CELL_INSIDE_QUERY\n图4：\n\n  如果当前节点为根节点，并且根节点中的点数据的数值范围跟查询条件的数值范围的边界关系为CELL_INSIDE_QUERY，意味着BKD树中所有的点数据都满足查询条件，那么直接通过深度遍历统计BKD树中的所有叶子节点中的文档信息，注意的是在深度遍历的过程中当遍历到一个内部节点时，只需要获取该节点的左右子树在索引文件.dim中位置即可，直到遍历到叶子节点，最后统计所有叶子节点中点数据对应的文档号，最后退出。\n  另外要提下的是，在图1的流程点是否段中的文档都满足查询条件？中，满足的条件之一是 BKD树中的点数据数量跟段中的文档数量一致，当这个前提不满足时并且根节点中的点数据的数值范围跟查询条件的数值范围的边界关系为CELL_INSIDE_QUERY，就会执行图4的流程。\n  如果当前节点是内部节点，处理的方式跟根节点的是一样的，差别在于处理完当前节点后是返回到上一层递归。\n  如果当前节点是叶子节点，那么读取该节点中的文档信息后返回到上一层递归。\n  在上面的描述中，我们至少会有以下两个疑问：\n\n\n问题一：如何获得当前节点的左右子树节点在索引文件.dim中的起始读取位置\n\n\n问题二：如何读取叶子节点中的文档信息\n\n\n  如果你没有看过文章索引文件之dim&amp;&amp;dii，建议跳过这一段。\n  问题一：如何获得当前节点的左右子树节点在索引文件.dim中的起始读取位置\n  由于BKD树的遍历方式为深度遍历，故我们以根节点为例来作介绍：\n图5：\n\n点击查看大图\n  在文章索引文件的生成（十四）之dim&amp;&amp;dii中我们介绍了描述节点（内部节点/叶子节点）的信息在索引文件.dim中的位置，由于生成BKD树也是深度遍历的方式，那么读取完根节点的信息后，索引文件.dim的下一个字节就是左子树（左节点）的起始读取位置，对于右节点，描述根节点信息的RightSubtreeHasNotLeafChild字段中的leftNumBytes值，它描述的是根节点的所有子孙节点（不包括叶子节点）在索引文件.dim占用的字节数，那么通过leftNumBytes的值，就可以找到根节点的右子树在索引文件.dim中的起始读取位置。\n  在图5中，读取完根节点的信息，即RightSubtreeHasNotLeafChild之后，索引文件.dim的下一个字节就是根节点的左子树信息的起始读取位置，随后跳过leftNumBytes个字节，就是根节点的右子树信息的起始读取位置。\n  问题二：如何读取叶子节点中的文档信息\n  在文章索引文件的读取（一）之dim&amp;&amp;dii中我们了解到，通过读取索引文件.dii的IndexFP字段，获取的信息是**内部节点（非叶节点）**信息在索引文件.dim中的起始读取位置，在索引文件.dii中并没有其他的提供叶子节点信息在索引文件.dim中的位置，如下所示：\n图6：\n\n点击查看大图\n  实际上叶子节点信息在索引文件.dim中的起始读取位置是通过叶子节点的父节点获得的，在文章索引文件之dim&amp;&amp;dii中，我们介绍了存储内部节点（非叶节点）信息一共有四种数据结构：\n\n非叶节点的子树是叶子节点，并且它是父节点的左子树：LeftSubtreeHasLeafChild\n非叶节点的子树是叶子节点，并且它是父节点的右子树：RightSubtreeHasLeafChild\n非叶节点的子树不是叶子节点，并且它是父节点的左子树：LeftSubtreeHasNotLeafChild\n非叶节点的子树不是叶子节点，并且它是父节点的右子树：RightSubtreeHasNotLeafChild\n\n RightSubtreeHasLeafChild\n  RightSubtreeHasLeafChild描述的是当前内部节点的左右子树都是叶子节点，并且该节点是父节点的右子树，RightSubtreeHasLeafChild中通过LeftLeafBlockFP、RightLeafBlockFP分别存放了它的左右节点信息在索引文件.dim中的起始读取位置，如下图所示：\n图7：\n\n点击查看大图\n LeftSubtreeHasLeafChild\n  LeftSubtreeHasLeafChild描述的是当前内部节点的左右子树都是叶子节点，并且该节点是父节点的左子树，通过LeftSubtreeHasLeafChild中的RightLeafBlockFP字段获得右子树在索引文件.dim中的起始读取位置，并且通过该节点的某个祖先节点传递左子树在索引文件.dim中的起始读取位置。\n  哪个祖先节点传递左子树在索引文件.dim中的起始读取位置\n  该祖先节点满足的要求条件是：该节点是它的父节点的右子树（右节点），即节点的数据结构为RightSubtreeHasNotLeafChild，并且是最近的。例如下图中，传递左子树在索引文件.dim中的起始读取位置的祖先节点就是根节点， 存储根节点的数据结构正是RightSubtreeHasNotLeafChild，在深度遍历的过程中，根节点对应的RightSubtreeHasNotLeafChild中的LeftLeafBlockFP一层一层往下传递，如下图所示：\n图8：\n\n点击查看大图\n  为了便于理解，我们再给出另一个数据结构类型为LeftSubtreeHasLeafChild的节点：\n图9：\n\n点击查看大图\n  在介绍了如何获得叶子节点的信息在索引文件.dim中的起始读取位置之后，我们就可以开始读取叶子节点中的文档号信息了，基于篇幅，读取文档号的逻辑将在介绍图1的流程点收集叶子节点中满足查询条件的文档号时展开。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","dim","dii"]},{"title":"索引文件的读取（九）之tim&&tip（Lucene 8.4.0）","url":"/Lucene/Search/2020/0810/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%B9%9D%EF%BC%89%E4%B9%8Btim&&tip/","content":"  本文承接文章索引文件的读取（八）之tim&amp;&amp;tip，继续介绍剩余的流程点，先给出流程图：\n 获取满足TermRangeQuery查询条件的term集合的流程图\n图1：\n\n 获取迭代器IntersectTermsEnum\n图2：\n\n  IntersectTermsEnum类继承于抽象类TermsEnum，TermsEnum在Lucene中到处可见，它封装了一个段中term的相关操作，下面罗列了TermsEnum类中的几个常用/常见的方法：\n TermsEnum\n 判断段中是否存在某个term\n图3：\n\n  seekExact(BytesRef text)方法用来判断term是否在当前段中，其中参数text即term，下文中参数text统称为term。\n 获取迭代状态\n图4：\n\n  根据某个term判断当前迭代状态，一共有三种迭代状态，如下所示：\n图5：\n\n 根据ord判断段中是否存在某个term\n图6：\n\n  根据ord值判断段中是否存在某个term，ord描述了term之间的大小关系，在文章索引文件的读取（五）之dvd&amp;&amp;dvm中我们介绍了ord跟term的内容，这里不赘述。\n 获取当前term 的ord值\n图7：\n\n  根据迭代器目前迭代位置对应的term找到该term对应的ord值。\n 获取当前term的docFreq、totalTermFreq\n图8：\n\n  docFreq即包含当前term的文档数量，totalTermFreq为当前term在每一篇文档中出现的次数的总和。\n IntersectTermsEnum\n  在获取迭代器IntersectTermsEnum的过程中，即在IntersectTermsEnum的构造函数中，本篇文章中我们只关心其中的一个逻辑，那就是初始化IntersectTermsEnumFrame对象，它用来描述段中第一个term所在的NodeBlock（见文章索引文件tim&amp;&amp;tip）的信息，生成该对象的过程即读取索引文件tim&amp;&amp;tip的过程：\n图9：\n\n  在文章索引文件的读取（七）之tim&amp;&amp;tip中我们说到，索引文件.tim&amp;&amp;.tip中的RootCodeLength跟RootCodeValue字段会用于生成FiledReader对象，在构造该对象期间，会根据这两个字段计算出long类型的rootBlockFP，它就是前缀为&quot;[ ]&quot;的PendingBlock合并信息（见文章索引文件的生成（六）之tim&amp;&amp;tip），当rootBLockFP执行右移两个bit的操作后，就获得了fp，它指向了段中第一个term所属的NodeBlock在索引文件.tim中的起始读取位置，随后读取NodeBlock中的信息，这些信息用于生成IntersectTermsEnumFrame对象。\n 获取Term并判断是否满足查询条件\n图10：\n\n  在这个两个流程点中核心的两个步骤为获取term跟判断term是否满足查询条件。\n 获取term\n  在图9中，我们已经获得了第一个term所属的NodeBlock，如果想要获取下一个term，根据Suffix字段的数据结构，获取方式有所不同，我们先看下图9中Suffix字段的数据结构，它具有两种类型：\n图11：\n\n  当读取到类型一的数据结构时，意味着term的信息都在当前NodeBlock中，当读取到类型二的数据结构，意味着term信息在其他NodeBlock中，注意的是：Length字段是一个组合编码，该字段最后一个bit为0时说明Suffix为类型一，为1时候Suffix为类型二。\n  在文章索引文件tim&amp;&amp;tip中已经给出了例子，我们这里再次列出：\n图12：\n\n图13：\n\n  图13中可以看出，前缀为&quot;ab&quot;的信息在另一个NodeBlock中，至于为什么会分布在另一个NodeBlock中，见文章索引文件的生成（六）之tim&amp;&amp;tip的介绍。\n 判断term是否满足查询条件\n  判断的逻辑用一句话可以总结：在DFA（Deterministic Finite Automaton）中，如果term的每个字符（label）能根据转移函数从当前状态转移到下一个状态，并且要求字符的当前状态为前一个字符的下一个状态，并且最后一个字符对应的下一个状态为可接受状态，那么term满足查询条件。\n  上述逻辑的例子在文章Automaton中已经介绍，不赘述。\n  到这里，我们应该会提出这样的疑问，既然知道了TermRangeQuery的查询范围minValue、maxValue，为什么不直接通过简单的字符比较来判断term是否满足查询条件，简单的字符比较例如FutureArrays.compareUnsigned方法，从最高位的字符开始按照字典序进行比较，这块内容将在下一篇文章中展开。\n 收集Term\n图14：\n\n  收集满足查询条件的term后，如果term的数量超过某个阈值，处理方式也是不同的，在源码中，通过BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD定义阈值，默认值为16。\n 未达到阈值\n  term的数量不大于16，那么TermRangeQuery转变为BooleanQuery，其中每个term生成TermQuery，作为BooleanQuery的子查询，并且TermQuery之间的关系为SHOULD。\n 达到阈值\n  基于篇幅，这块内容将在下一篇文章中展开。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","tim","tip"]},{"title":"索引文件的读取（二）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Search/2020/0428/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本文承接索引文件的读取（一）之dim&amp;&amp;dii继续介绍剩余的内容，下面先给出读取索引文件.dim&amp;&amp;dii的流程图：\n图1：\n\n 读取索引文件.dim&amp;&amp;dii\n 收集段中所有的文档号\n图2：\n\n  在递归遍历BKD树之前，先判断下是否段中的文档都满足查询条件，如果满足，那么我们就不需要通过读取BKD树的信息来获取文档号，而是根据段中的reader.maxDoc()方法就可以获得满足查询条件的文档号集合。\n  需要先后同时满足两个条件才能通过reader.maxDoc()获得满足查询条件的文档号集合：\n\n条件一：包含当前点数据域的文档数量docCount必须等于段中的文档数量\n\n  包含当前点数据域的文档数量可以通过索引文件.dim的字段获得，如下红框所示所示：\n图3：\n\n  上图中各个字段的含义见文章索引文件的读取（一）之dim&amp;&amp;dii。\n  段中的文档数量通过reader.maxDoc()获取，即读取索引文件.si中的SegSize字段，如下红框所示所示：\n图4：\n\n  满足条件一意味着，段中的每篇文档中至少有一个点数据域，这些点数据域对应的域名就是当前流程处理的域。\n\n条件二：BKD树中点数据的数值范围与查询条件的数值范围的边界关系为CELL_INSIDE_QUERY（见文章索引文件的读取（一）之dim&amp;&amp;dii的介绍），即查询条件的数值范围包含节点中的所有点数据\n\n  如何通过reader.maxDoc()方法获得足查询条件的文档号集合：\n  由于满足了条件一，那么在后续的Search过程中，只需要从0开始遍历到reader.maxDoc()的值作为文档号传给Collector（一）即可。\n 反向收集文档号信息\n图5：\n\n  反向收集文档号意味着存在正向收集文档号，都属于收集文档的方式：\n\n正向收集文档号：当某个点数据满足查询条件，那么收集对应的文档号\n反向收集文档号：先假定段中所有的文档号满足查询条件，在随后的匹配中，当某个点数据不满足查询条件时，那么就移除对应的文档号\n\n  为什么要区分收集文档的方式\n  在回答这个问题前，我们先介绍下满足反向收集文档号的三个条件，这三个条件需要先后同时满足：\n\n条件一：包含当前点数据域的文档数量docCount必须等于段中的文档数量\n\n  上文已经介绍，不赘述。\n\n条件二：包含当前点数据域的文档数量docCount必须等于BKD树中的点数据数量pointCount\n\n  包含当前点数据域的文档数量docCount跟BKD树中的点数据数量pointCount可以通过索引文件.dim的字段获得，如下红框所示所示：\n图6：\n\n\n条件三：满足查询条件的点数据数量（注意的是这是一个估算值，源码中称之为cost）占段中的文档数量的一半以上（&gt; 50%）\n\n  在满足条件一、条件二的前提下，那么通过条件三我们就可以知道，在满足条件三的情况下，不满足查询条件的点数据数量小于满足查询条件的点数据数量，意味着使用反向收集文档号的方式能更快的读取BKD树。\n  如何估算满足查询条件的点数据数量cost\n  估算满足查询条件的点数据数量cost的过程就是从根节点开始通过深度遍历来计算，详细的过程不详细展开，只介绍计算过程中的一些关键内容。\n  每次处理一个节点（内部节点（非叶节点）/叶子节点）时，总是用查询条件的数值区间（lowValues、upperValues）跟节点的数值区间（minPackedValue、maxPackedValue）计算边界关系，我们按照边界关系的类型进一步介绍：\n\nCELL_OUTSIDE_QUERY：查询条件的数值范围跟节点的数值范围没有交集，那么以当前节点为祖先节点的所有叶子节点中的点数据都不满足查询条件，那么就不用再读取当前节点的子节点信息了\nCELL_INSIDE_QUERY：查询条件的数值范围包含节点中的所有点数据，这种情况根据节点的不同类型再做区分：\n\n叶子节点：将maxPointsInLeafNode的值作为当前叶子节点中包含的点数据数量，可见它是一个估算值，最后增量到cost中\n内部节点（非叶节点）：计算出以当前内部节点为祖先节点的叶子节点的数量numLeaves，然后通过公式(maxPointsInLeafNode∗numLeaves)(maxPointsInLeafNode * numLeaves)(maxPointsInLeafNode∗numLeaves)计算出的结果作为这些叶子节点中包含的点数据数量总和，可见这同样是一个估算值，最后增量到cost中\n\n\n\n  maxPointsInLeafNode同样通过读取索引文件.dim获得，如下红框所示：\n图7：\n\n\nCELL_CROSSES_QUERY：查询条件的数值范围跟节点的数值范围部分重叠（partially overlaps），这种情况同样需要根据节点的不同类型再做区分：\n\n内部节点（非叶节点）：继续递归处理子节点\n叶子节点：通过公式(maxPointsInLeafNode+1)/2(maxPointsInLeafNode + 1) / 2(maxPointsInLeafNode+1)/2 的计算结果作为叶子节点中满足查询条件的点数据数量，同样是估算值，最后增量到cost中\n\n\n\n  从上文的内容可以看出，计算cost可能是个开销昂贵（expensive）的过程，源码中也给出了对应的注释：\nComputing the cost may be expensive, so only do it if necessary\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","dim","dii"]},{"title":"索引文件的读取（五）之dvd&&dvm（Lucene 8.4.0）","url":"/Lucene/Search/2020/0714/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8Bdvd&&dvm/","content":"  本篇文章开始介绍索引文件.dvm&amp;&amp;dvd的读取，阅读本系列文章建议先看下文章索引文件的生成（十八）之dvm&amp;&amp;dvd、索引文件的生成（十九）之dvm&amp;&amp;dvd、IndexedDISI（一）、IndexedDISI（二），了解写入的过程能快的理解读取的逻辑。\n  DocValues的其中一个用途用于对查询结果的进行排序，在搜索阶段，当获取了满足查询条件的文档号之后，它会交给Collector实现收集功能，并且在收集过程中实现文档的排序。本文先介绍在使用SortedDocValues或者SortedSetDocValues的情况下，如何获取文档之间的排序关系，而通过读取索引文件.dvm&amp;&amp;dvd的过程即获取排序关系的过程：\n 通过索引文件.dvd、.dvm之SortedDocValues、SortedSetDocValues获取排序关系的流程图\n图1：\n\n 文档号\n图2：\n\n  流程图的准备数据为一个文档号，Collector收集器只接受满足查询条件的文档号。\n DocIdData中是否包含该文档号？\n图3：\n\n  DocIdData中存储了所有包含用于排序的DocValues的文档的文档号，如下红框所示所示：\n图4：\n\n  图4中，通过读取索引文件.dvm的DocIdIndex字段，根据字段中的offset跟length来确定DocIdData在索引文件.dvd中的数据区间，而对于索引文件.dvm中的DocIdIndex以及其他字段的信息，则是根据索引文件.dvm的固定的数据结构依次轮流读取，如下所示：\n图5：\n\n  图5中，meta.readLong()描述的是读取一个long类型大小的字节数量，另外readTermDict的方法的逻辑也是类似的，即读取TermsDictMeta、TermsIndexMeta的信息。不详细展示了。\n  在获取了DocIdData的信息之后就可以判断是否包含图2提供的文档号，包含意味着这篇文档中包含正在用于排序的DocValues信息。\n  如何判断DocIdData中是否包含该文档号\n  在文章IndexedDISI（一）、IndexedDISI（二）中我们介绍了详细的读取过程，这里不赘述。\n DocIdData中包含该文档号\n  如果包含，那么返回一个段内编号index，index的概念在文章IndexedDISI（一）中已经介绍过了，我们这里再次说明下：\n段内编号index：index是一个从0开始递增的值，index的值描述的是在生成索引文件.dvd阶段，依次处理的第index个文档号。\n  index实际上跟currentValue[ ]数组的下标值是同一个意思，见文章索引文件的生成（十八）之dvm&amp;&amp;dvd的介绍。\n DocIdData中不包含该文档号\n  如果不包含，那么返回的missingOrd值可以为 -1 或者 Integer.MAX_VALUE，至于missingOrd值的用途在下文中我们再介绍。\n 根据index从Ords中获取ord值\n图6：\n\n  Ords即图4中索引文件.dvd的Ords字段的信息，Ords的写入过程见文章索引文件的生成（十九）之dvm&amp;&amp;dvd，这里可以简单的将Ords理解为一个数组，其中数组下标为上文中的index、数组元素为ord值（具有相同用于排序的DocValues域值的文档对应的ord值是相同的），其中ord描述了文档之间的排序关系，如果是递增排序，那么ord越小，文档排序越靠前，如果某篇文档不包含当前用于排序的DocValues，那么上文中的missingOrd就作为这篇文档的ord值参与排序。至于为什么ord值描述了文档文档之间的排序关系，相信在读完文章索引文件的生成（十八）之dvm&amp;&amp;dvd之后能明白。\n  从上文的内容可以看出，如果用SortedValues/SortedSetValues来排序， 并不是比较SortedValues/SortedSetValues对应的域值的字典序，而是在生成索引文件.dvd阶段将域值映射为一个ord值，通过比较int类型的ord值就能得出排序关系，性能上明显是大于字典序的比较方式，特别是较长的域值。当然ord值的用途不仅仅如此，下文中我们会继续介绍其他的用途。\n  到此流程，我们已经获得了文档之间的关系，如果还要取出每篇文档中参与排序的域值，即DocValues的域值，那么可以根据ord获得。\n 根据ord值从TermsDict中获取域值\n图7：\n\n  在生成索引文件.dvm&amp;&amp;.dvd阶段，会将有序的并且去重的域值依次写入，并且每处理16个域值就生成一个block、同时生成一个address用来描述这个block在索引文件.dvd中的起始位置，随后在block中找到对应的域值：\n图8：\n\n  当获得了ord值后，需要三步才能找到对应的域值：\n\n步骤一：计算出在BlockIndex的哪一个块中，计算方式为 ord &gt;&gt; 16，获取块中的address的值\n步骤二：根据address的值，找到Block在索引文件.dvd中的起始位置\n步骤三：依次读取Block中的每一个域值，并且判断该域值对应的ord值是否为我们期望的ord，如果不是，那么读取下一个域值，直到找到我们期望的ord\n\n  在步骤三中，相邻term的前缀存储，即如果需要读取第n个域值的完整值，需要知道第n-1个域值的完整值，故Block中第一个域值存储的是完整的域值（见文章SortedDocValues的介绍）。\n  至此流程点介绍完毕，可以看出，根据文档号我们能获得用于排序的DocValues的域值以及文档的排序关系，注意到的是索引文件.dvd中的TermsIndex在上文中没有涉及，我们接下来介绍该字段的用途。\n 读取TermsIndex\n  通过TermsIndex字段，可以用来判断为DocValues中是否包含某个域值，其中一个应用场景为利用DocValues来实现范围查询，demo如下：\n图9：\n\n  图9的demo完整代码见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/lucene/DcoValues/SortedDocValuesTest4Test.java 。\n  图9中，使用了SortedDocValuesField.newSlowRangeQuery(…)方法执行范围查询，查询条件为包含域名为&quot;level&quot;、域值范围为[b, c]的SortedDocValues的文档，可见文档3、文档4满足查询条件，故代码第88行的查询结果数量为2。\n  我们先给出TermsIndex的数据结构，然后介绍如何通过实现图9的范围查询。\n  同图4一样，要读取索引文件.dvd中的TermsIndex字段数据，需要通过索引文件.dvm中的TermsIndexMeta，如下所示：\n图10：\n\n  接着我们看索引文件.dvd中TermsIndex字段的数据结构：\n图11：\n\n  在生成索引文件.dvd的TermsIndex期间，每处理1024个域值，就生成一个PrefixValue，它描述的是在区间[0, 1023]的ord值对应的域值都小于PrefixValue（字典序，详细的介绍见文章索引文件的生成（二十）之dvm&amp;&amp;dvd）。\n  如果判断SortedDocValues是否包含某个域值，分为以下的步骤：\n\n步骤一：获取域值的数量termsDictSize，然后执行 termsDictSize - 1 &gt;&gt;&gt; 10获得一个ordHigh，这个操作计算的是TermsIndex中PrefixValue的数量，可见ordHigh是一个1024的倍数，例如0、1024、2048等，另外termsDictSize的值通过索引文件.dvm中TermsDict字段的TermsDictSize，下图红框标注：\n\n图12：\n\n\n步骤二：根据ordLow == 0跟ordHigh做二分查询，实际就是对图11中的TermsIndex中的PrefixValue进行二分查询，最后获得PrefixValue对应的blockLo跟blockHi，即找到一个ord区间\n步骤三：随后根据上个步骤计算出的ord区间，对区间内的ord继续做二分查找，实际就是对图8中TermsDict的Block中的第一个term进行二分查找（Block中的第一个term是一个完整的域值），最后确定待查找的域值所属的block\n步骤四：遍历block中的每一个域值，跟待查找的域值进行字节比较，如果相同说明DocValues中包含待查找的域值，并且返回该域值对应的ord值\n\n  对于图9中的范围查找，实际的过程为尝试找到第78行域值&quot;b&quot;、“c&quot;分别对应的ord值，注意的是：如果没有找到域值&quot;b&quot;对应的ord值，那么ord值为下一个block的第一个term对应的ord值，如果没有找到域值&quot;c&quot;对应的ord值，那么ord值为当前block的最后一个term，最后遍历域名为&quot;level&quot;中包含的所有文档号（DocIdData字段），并且找出文档号对应的ord值（上文中已经介绍如何通过文档号找到对应的ord值），如果ord值在&quot;b”、&quot;c&quot;对应的ord值区间，那么就认为该文档是满足查询条件的。\n  上述的内容查找&quot;b&quot;、&quot;c&quot;对应的ord只是介绍了部分的情况，其他一些边界的情况就不展开，相信你已经能了解如何通过DocValues实现范围查找。\n 结语\n  尽管本文中没有对SortedSetDocValues进行额外的介绍，实际上原理跟SortDocValue是一致的，故不赘述。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","dvd","dvm"]},{"title":"索引文件的读取（八）之tim&&tip（Lucene 8.4.0）","url":"/Lucene/Search/2020/0805/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%85%AB%EF%BC%89%E4%B9%8Btim&&tip/","content":"  本文承接文章索引文件的读取（七）之tim&amp;&amp;tip，继续介绍剩余的流程点，先给出流程图：\n 获取满足TermRangeQuery查询条件的term集合的流程图\n图1：\n\n BlockTreeTermsReader\n  在上一篇文章中，我们已经介绍了当前流程点BlockTreeTermsReader，并且提到在生成FieldReader期间，会采用on-heap/off-heap两种导入模式（loadMode）来获取所有域的FST的主要信息，但是没有说明Lucene是如何选择这两种导入模式的，在Lucene 8.4.0的版本中，Lucene提供以下的导入模式参数，在生成FieldReader期间，根据导入模式参数进行对应的逻辑判断，最后选择on-heap/off-heap中的一种，另外注意的是，在Lucene8.1.0之后，允许用户自定义的对不同的域设置不同的导入模式参数，设置的方法在下文中会介绍：\n 导入模式参数\n图2：\n\n  导入模式参数对应的处理逻辑如下：\n图3：\n\n OFF_HEAP\n  无论索引文件.tip是以何种方式打开（见文章Directory（上）），总是使用时才从磁盘读取（lazy load），但是这个参数未考虑索引文件.tip的打开方式，使用off-heap如果不结合合适的打开方式在I/O方面的性能表现不一，见下文中关于AUTO中的介绍。\n ON_HEAP\n  总是将所有域的FST的主要信息写入到内存。\n OPTIMIZE_UPDATES_OFF_HEAP\n  我们先看下图2中关于这个参数的注释：总是使用时才从磁盘读取（lazy load），即off-heap为true，但是有一个例外，如果域的类型为ID Field, 那么这种域的FST的主要信息总是全部都写入到内存。\n  什么是ID Field？\n  可以用来描述文档的唯一性的域就是ID Field，可以理解为数据库中的 primary key，图3中可以通过下面的代码来判断域是否为ID Field：\nthis.docCount != this.sumDocFreq\n\ndocCount：包含某个域的文档数量\nsumDocFreq：某个域在每一篇文档中出现的次数的总和\n\n  如果上述两个字段的值是相同的，那么这个域就被认为是ID Field，例如在索引期间添加了这篇两篇文档：\n图4：\n\n  图4的例子中，域名为&quot;content&quot;的域的docCount为2，sumDocFreq的值为3。\n  为什么ID Field就不使用off-heap呢\n  其原因是新增或者更新一篇文档时，这篇文档中如果也有这个ID Field，那么需要先通过查询的方式确保ID Field的域值是否被占用，故使用了off-heap会降低新增或者更新文档的性能。\n  具体的介绍可以看这篇文章：https://www.easyice.cn/archives/346 。\n  我们继续看图3中对于参数OPTIMIZE_UPDATES_OFF_HEAP的处理逻辑，可见如果当前域为ID Field，但同时openedFromWriter为false的话，还是会使用off-heap，跟参数OFF_HEAP一样，这个参数也未考虑索引文件.tip的打开方式，使用off-heap如果不结合合适的打开方式未必能获得较好的I/O性能，见下文中的介绍。\n AUTO\n  该参数实际是对参数OPTIMIZE_UPDATES_OFF_HEAP对应的模式加强，即在按照参数OPTIMIZE_UPDATES_OFF_HEAP的逻辑选择使用off-heap后，还需要满足索引文件.tip是使用MMapDirectory的方式打开的的条件才能使用off-heap，原因是使用内存映射I/O比FileChannel.open有更好的读写性能：\n图5：\n\n  图5中，用户空间缓冲区即Channel，上述内容内容选自**&lt;&lt;Linux/UNIX系统编程手册(下册)&gt;&gt;**。\n 自定义域的导入模式参数\n  从Lucene 8.0.1版本开始，可以通过IndexWriter的配置对象IndexWriterConfig为每一种域设置导入模式参数，没有设置的话则默认值为AUTO。\n 初始化FST\n  我们在文章索引文件的读取（七）之tim&amp;&amp;tip中简单的说明了下在off-heap跟on-heap不同导入模式下存储FST的主要信息的数据结构，这里为了加深理解，我们贴出代码来进一步介绍，这两种模式在代码中使用了不同逻辑实现了init()方法。\n on-heap\n图6：\n\n  图6中第54行代码判断FST的主要信息的大小是否超过1G，其中maxBlockBits的值为默认值30，numBytes的值描述的是FST的主要信息的大小，如下所示：\n图7：\n\n  当大于1G时，使用BytesStore对象存储，否则使用byte[ ] bytesArray存储存储，BytesStore跟bytesArray在文章索引文件的读取（七）之tim&amp;&amp;tip已经介绍过了，不赘述。\n off-heap\n图8：\n\n  图8中可以看出，off-heap下，只存储了numBytes字段跟offset字段，offset指向了FST的主要信息（bytes子弹）在FSTIndex中的起始读取位置。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","tim","tip"]},{"title":"索引文件的读取（六）之dvd&&dvm（Lucene 8.4.0）","url":"/Lucene/Search/2020/0715/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%85%AD%EF%BC%89%E4%B9%8Bdvd&&dvm/","content":"  本文接着介绍索引文件.dvd、.dvm之BinaryDocValues的读取，它同SortedValues一样，其中一个用途用于对查询结果的进行排序，在搜索阶段，当获取了满足查询条件的文档号之后，它会交给Collector实现收集功能，并且在收集过程中实现文档的排序，我们通过一个例子来介绍如何实现排序。\n 通过索引文件.dvd、.dvm之BinaryDocValues获取排序关系的流程图\n  获取文档之间的排序关系的过程实际就是根据文档号，通过索引文件.dvm&amp;&amp;dvd读取DocValues的域值，域值之间排序关系来描述文档之间的排序关系，流程图如下：\n图1：\n\n 返回段内编号index\n图2：\n\n  图2的流程跟SortedDocValues是一致的，本文不赘述这几个流程点的内容，详细过程见文章索引文件的读取（五）之dvd&amp;&amp;dvm。\n 根据index从TermsIndex中获取startOffset\n图3：\n\n  先给出索引文件.dvm、dvd之BinaryDocValues的数据结构：\n图4：\n\n  为了便于描述，我们这里可以简单的将图4中的TermsIndex字段认为是一个数组，在图2的流程中获得的index（段内编号）就是这个数组的下标值，接着根据index在TermsIndex中获得address的值，address的值即startOffset，它描述了域值信息TermsValue中的起始读取位置，但是至此我们还无法获得域值，因为读取一个域值需要域值在TermsValue的起始读取位置以及域值的长度length。\n 从TermsIndex中获得term的长度length\n图5：\n\n  在文章索引文件的生成（二十一）之dvm&amp;&amp;dvd中我们知道，在TermsIndex中相邻两个address的差值就是term的长度，故只需要获得段内编号index + 1和段内编号index对应的两个address，它们的差值就是段内编号index对应的域值的长度。\n 根据length跟startOffset从TermsValues中获取域值\n图6：\n\n  在当前流程点，根据图3、图5获得的startOffset跟length，就可以在TermsValue确定一个数据区间，该区间中的内容就是域值。\n  在Collector中，获得了用于排序的DocValues的域值后，接着使用字典序就可以比较出排序关系，即文档的排序关系。\n  至此我们可以发现，BinaryDocValues通过域值的字典序进行排序，而SortedDocValues/SortedSetDocValues则是通过ord值的数值大小进行比较，根据源码中的注释，通过BinaryDocValues实现排序的性能通常较差：\n图7：\n\n BinaryDocValues VS SortedDocValues\n  虽然通过BinaryDocValues进行排序的性能相比较SortedValues/SortedSetDocValues通常情况下较差，但是通过比较两者的数据结构，可以看出BinaryDocValues的优点：\n图8：\n\n 根据文档号获得域值\n  图8可以看出，对于SortedDocValues，通过address只能确定域值属于某个block，在block中获得该域值则必须从第一个域值开始遍历，直到对应的ord，即最坏的情况下需要在block遍历16次才能找到对应域值，并且需要&quot;拼接&quot;出前15个域值（前缀存储），而对于BinaryDocValues，只需要获得两个address就能获得域值，可见通过文档号找DocValues对应的域值，BinaryDocValues性能通常较好。\n 判断DocValues中是否包含某个域值\n  BinaryDocValues无法判断DocValues中是否包含某个域值，而SortedDocValues通过TermsIndex字段，则可以实现，并且可以实现DocValues的范围查询（见文章索引文件的读取（五）之dvd&amp;&amp;dvm）。\n 存储性能\n  BinaryDocValues的TermsValue字段中存储了每个域值的完整值，而SortedDocValues通过前缀存储，存储性能通常优于BinaryDocValues。\n 写入性能\n  使用BinaryDocValues存储域值时，在收集阶段，我们不需要对Term进行排序（意味着无法判断DocValues中是否包含某个域值），在生成索引文件.dvd阶段，不需要前缀存储，写入更少的字段，相比较SortedDocValues写入性能通常较高。\n 结语\n  下一篇文章中，我们将介绍如何通过BinaryDocValues实现范围域（RangeField）的查询。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","dvd","dvm"]},{"title":"索引文件的读取（十一）之tim&&tip（Lucene 8.4.0）","url":"/Lucene/Search/2020/0819/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%B9%8Btim&&tip/","content":"  在上一篇文章索引文件的读取（十）之tim&amp;&amp;tip中我们遗留了一个问题：\n  为什么要根据是否达到阈值使用不同的处理方式：\n  这个问题可以分解为两个小问题：\n\n问题一：为什么达到阈值后不使用BooleanQuery的方式做文档号的收集\n问题二：为什么未达到阈值使用BooleanQuery的方式做文档号的收集\n\n 处理方式\n  这两种处理方式的不同之处就在于如何根据每个term对应的文档号集合，并从这些集合中获取满足查询条件的文档号集合。\n 未达到阈值\n  未达到阈值的情况下，会根据每个term从索引文件.doc中获取包含这个term的文档号集合，并且用一个long类型的数组docBuffer[ ]来存储文档号集合。注意的是，数组docBuffer[ ]实际上只会存储一个block（见文章索引文件之doc）中的文档号集合，当这个block中的文档号信息读取结束后，会载入下一个block的文档号信息，并写入到数组docBuffer[ ]中，这里为了便于描述，我们假设数组docBuffer[ ]中存储了所有的文档号。\n  接着使用一个优先级队列DisiPriorityQueue来存储每个term对应的正在被读取的文档号，该队列的排序规则为比较文档号的大小，在执行了排序后，堆顶元素的文档号是最小的，通过排序，更新term对应正在被读取的文档号，直到所有term对应的正在被读取的文档号为Integer.MAX_VALUE，最终文档号按照从小到大的顺序都被取出，我们这里通过一个例子简单介绍下该原理。\n图1：\n\n  根据图1中74行的查询条件肉眼可知，域值bcd、ga、gc、gch满足查询条件，这几个term对应的文档号集合，如下所示：\n图2：\n\n  获取过程：\n图3：\n\n  在最开始，四个docBuffer[ ]数组的第一个数组元素作为每个term正在被读取的文档号存储到优先级队列中，调整堆后如上所示，此时堆顶元素1为满足查询的文档号，它是域值&quot;gc&quot;对应docBuffer[ ]的第一个元素（这里由于域值&quot;bcd&quot;对应的文档号数量多，在源码中用cost来描述，当元素相同时，cost越小，排序位置就越靠前，故尽管它对应的docBuff[ ]的第一个元素也是1，但是堆顶元素选择了域值&quot;gc&quot;对应docBuffer[ ]的第一个元素），随后域值&quot;gc&quot;对应的正在被读取的文档号更新为dcoBuff[ ]数组的下一个数组元素，即文档号3，替换当前的堆顶元素，如下所示：\n图4：\n\n  由于栈顶元素被更新了，故需要执行调整堆的操作，调整后的堆如下所示：\n图5：\n\n  调整堆之后，堆顶元素为文档号1，由于在图3中，我们已经收集了该文档号，故不需要重复收集。接着域值&quot;bcd&quot;对应的正在被读取的文档号更新为dcoBuff[ ]数组的下一个数组元素，即文档号3，替换当前的堆顶元素1，如图6所示，由于栈顶元素被更新了，故需要执行调整堆的操作，如图7所示：\n图6：\n\n图7：\n\n  调整堆以后，堆顶元素为文档号2，并且该文档号还没有被处理过，故它为满足查询的文档号。\n  至此相信大家已经理解了其原理，下面直接给出下一次执行了调整堆操作以后的状态：\n图8：\n\n  图8中，堆顶元素文档号3为满足查询的文档号，接着更新、调整堆的操作，如下所示：\n图9：\n\n  按照上文描述的处理逻辑，下一步应该更新域值&quot;gc&quot;对应的正在被读取的文档号，由于它对应的文档号都已经取出，故在源码中通过另下一个正在被读取的文档号为Integer.MAX_VALUE，在排序之后如下所示：\n图10：\n\n  接着我们直接给出剩下的更新、堆排序后的状态：\n图11：\n\n图12：\n\n图13：\n\n图14：\n\n  在图14中，直到所有term对应的正在被读取的文档号的值为Integer.MAX_VALUE，那么获取满足查询条件的文档号集合的过程就完成了。\n  上文中的逻辑可以简单的用更新堆顶元素、调整堆两个步骤来归纳，源码中的核心实现位于DisjunctionDISIApproximation类中的nextDoc()方法，如下所示：\n图15：\n\n 达到阈值\n  达到阈值的情况下，在文章索引文件的读取（十）之tim&amp;&amp;tip中我们说到，所有term对应的文档号总是优先使用数组存储（数组中可能会有重复的文档号），当达到某个阈值后，会使用FixedBitSet存储，如果最终使用数组存储，那么在收集结束后对数组进行排序、去重操作后就能获取满足查询条件的文档号集合即可，我们详细的介绍下使用FixedBitSet存储时，如何收集以及读取文档号。\n  为了便于描述我们还是以图1的例子为例，每个term对应的文档号集合如下所示：\n图16：\n\n 构造FixedBitSet对象\n  我们首先了解下FixedBitSet是如何存储文档号的，这块内容在文章工具类之FixedBitSet已经介绍过了，我们这里再简单的说明下：该对象中包含一个long类型的数组bits[ ]，其中每一个数组元素，即一个long类型的值，使用该值的每个bit用来代表一篇文档号，那么一个long类型的值可以用来描述64篇文档号，故bits[ ]数组的第一个数组元素描述的是文档号063，第二个数组元素描述的是文档号64127，以此类推。\n  例如我们有以下的文档号，用bits[ ]数组存储后如下所示：\n图17：\n\n  图17中，bit为1描述了存储了对应的文档号，文档号跟bit的映射关系通过下面的公式来描述：\nint wordNum = docId &gt;&gt; 6;      // 步骤一long bitmask = 1L &lt;&lt; docId;    // 步骤二bits[wordNum] |= bitmask;      // 步骤三\n\n步骤一：docId右移6位，即除以64，计算出该文档号所属数组元素对应bits[ ]的下标值，图17中，以文档号125为例， 执行 125 &gt;&gt; 6 = 1, 说明文档号125将用bits[ ]数组中下标值为1的数组元素来存储\n步骤二：通过左移操作，找到在当前数组元素，即在long类型的数值的bit位置，以文档号125为例，执行 1L &lt;&lt; 125 的二进制结果为：0b00100000_00000000_00000000_00000000_00000000_00000000_00000000_00000000\n步骤三：执行或操作，更新bits[1]的值\n\n  由上述的操作可以知道，bits[ ]数组的大小取决于文档号集合中最大的文档号。所以由于不知道满足查询的最大文档号是多少，构造FixedBitSet对象时候只能根据段中的文档总数来确定bits[ ]数组的大小。\n FixedBitSet存储文档号\n  存储文档号的总流程可以用一句话概括：依次处理每一个term，将每一个term对应的文档号写入到FixedBitSet中，对于图16的例子，term的处理顺序为：“bcd” —&gt; “ga” —&gt; “gc” —&gt; “gch”。\n图18：\n\n  bits[ ]数组的最大长度为 ((maxDoc - 1) &gt;&gt; 6) + 1。\n FixedBitSet读取文档号\n  获取某个文档号的值，需要上一个文档号的值target才能获取，target的初始值为0，由于代码比较简单，我们直接给出：\npublic int nextSetBit(int target) &#123;    // Depends on the ghost bits being clear!    // 获取target所属的数组元素在bits[]数组中的下标值    int i = target &gt;&gt; 6;    // 判断下一个文档号是不是跟target在同一个long类型的数值word中    long word = bits[i] &gt;&gt; target;  // skip all the bits to the right of index    if (word!=0) &#123; // 下一个文档号跟target在同一个long类型的数值中      // Long.numberOfTrailingZeros(word)的结果为 target跟下一个文档号bit位置的差值      return target + Long.numberOfTrailingZeros(word);    &#125;    // numWords为bits[]数组的元素数量    while(++i &lt; numWords) &#123;// 下一个文档号跟target不在同一个long类型的数值中      // 尝试从下一个long类型的数值word中查找      word = bits[i];      if (word != 0) &#123;// word不为0，说明word中至少有一个bit位的值为1，最低位并且不为0的bit即对应下一个文档号        return (i&lt;&lt;6) + Long.numberOfTrailingZeros(word);      &#125;    &#125;    return DocIdSetIterator.NO_MORE_DOCS;&#125;\n  从上述代码可以看出，获取一个文档号的性能取决于相邻的文档号的数值大小，相邻的文档号差值越大，查找速度越慢。\n  为什么未达到阈值使用BooleanQuery的方式做文档号的收集\n\n最多只有16个（默认阈值BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD，见文章索引文件的读取（九）之tim&amp;&amp;tip）的term的文档信息作为DisiPriorityQueue的元素进行堆的排序，内存开销，排序开销很低\n避免匹配了少量term仍可能会占用较大内存存储文档号的问题，例如term对应的文档号的数值差值很大，使用FixedBitSet存储会有无用的内存开销，内存开销取决于段中的文档总数，而docBuffer[ ]的数组是固定，大小为一个block（索引文件.doc中的block）中包含的文档数量，默认值为128，\n\n  为什么达到阈值后不使用BooleanQuery的方式做文档号的收集\n\n达到阈值后，term的数量无法确定，DisiPriorityQueue的排序，内存开销取决于term的数量\n每个term都会生成一个TermQuery作为BooleanQuery的子查询，导致更容易抛出TooManyClauses的异常，BooleanQuery所能包含的子查询数量是有上限限制的，取决于BooleanQuery中的maxClauseCount参数，默认值为1024\n当满足查询的term的数量较大时，通过FixedBitSet对象只记录文档号相比通过BooleanQuery的方式（使用PostingsEnum对象存储，每个term都有自己的PostingsEnum对象）会占用更少的内存，在获取到满足查询的文档号集合之前（堆排序），PostingsEnum对象会常驻内存，它至少包含了文档号的信息以及其他信息（在以后介绍索引文件.doc的读取的文章中会展开介绍），而在TermRangeQuery中，我们只关心文档号，使用FixedBitSet对象存储文档号的期间，也会获取每一个term对应的PostingsEnum对象，但当获取了term对应的文档号集合之后，该对象能及时的释放。\n\n  在执行TermRangeQuery时，获取满足查询的文档号的时机点是不同的，取决于满足查询的term数量是否达到阈值，时机点如下图所示，该流程图的介绍见系列文章查询原理（一）：\n图19：\n\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","tim","tip"]},{"title":"索引文件的读取（十三）之doc&&pos&&pay（Lucene 8.4.0）","url":"/Lucene/Search/2020/0911/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%E4%B9%8Bdoc&&pos&&pay/","content":"  本文承接文章索引文件的读取（十二）之doc&amp;&amp;pos&amp;&amp;pay，继续介绍剩余的内容。索引文件.doc、.pos、.pay的读取过程相比索引文件.tim&amp;&amp;.tip较为简单，核心部分为如何通过读取这三个索引文件，生成一个PostingsEnum对象，该对象中描述了term在一篇文档中的词频frequency、位置position、在文档中的偏移offset、负载payload以及该文档的文档号docId，其中docId和frequency通过索引文件.doc获得、position通过索引文件.pos获得、offset和payload通过索引文件.pay获得。\n PostingsEnum\n  PostingsEnum是一个抽象类，其子类的实现有很多，本文中仅仅介绍在Lucene84PostingsReader类中的子类，如下所示：\n图1：\n\n  图1中共有5个子类实现，用红框标注，在搜索阶段，通过下面两个条件来选择其中一种实现：\n\n条件一：Flag\n条件二：打分模式ScoreMode\n\n Flag\n  Flag描述了在搜索阶段中，我们需要获取term在文档中的哪些信息。这里的信息即上文中提到的frequency、position、offset以及payload。由于不是所有的查询都需要所有的这些信息，选择性（optional）的获取这些信息能降低搜索阶段的内存开销，同时减少读取索引文件时产生的磁盘I/O，下文中会详细介绍。\n  Flag的可选值如下所示：\n图2：\n\n 打分模式ScoreMode\n  ScoreMode描述的是搜索模式，正如源码中的注释：\n图3：\n\n  本文中我们不展开ScoreMode的详细介绍，我们仅仅看下图3中红框标注的TOP_SCORES，该值影响了上文中PostingsEnum子类的选择，该注释源码大意为：在搜索阶段，不会匹配所有满足查询条件的文档，会跳过那些不具竞争力的文档。其原理就是利用了索引文件.doc中的Impacts字段实现的，这里简单提下，在以后介绍WAND（weak and）算法时候再详细介绍。\n 选择PostingsEnum的实现类\n图4：\n\n  图4的流程图描述了如何根据Flag跟打分模式选择PostingsEnum的实现类。是否有跳表信息的判断依据为：如果包含term的文档数量小于128，即一个block的大小，那么在生成索引文件.doc阶段就不会有跳表信息（不懂？见文章索引文件的生成（三）之跳表SkipList）;在读取阶段由于是先读取索引文件.tim，故通过该索引文件中的DocFreq字段来获取包含term的文档数量，如下图红框标注的字段：\n图5：\n\n  图5索引文件.tim的字段介绍以及生成过程可以分别阅读文章索引文件tim&amp;&amp;tip、索引文件的读取（七）之tim&amp;&amp;tip。\n PostingsEnum中的信息\n  我们先直接列出每个实现类中包含的数据，随后介绍获取这些信息的方式。\n\nBlockDocsEnum：\n\ndocId集合\nfrequency集合\n\n\nEverythingEnum：\n\ndocId集合\nfrequency集合\nposition集合\noffset集合\npayload集合\n\n\nBlockImpactsDocsEnum：\n\ndocId集合\nfrequency集合\nimpactData信息\n\n\nBlockImpactsPostingsEnum：\n\ndocId集合\nfrequency集合\nposition集合\nimpactData信息\n\n\nBlockImpactsEverythingEnum：\n\ndocId集合\nfrequency集合\nposition集合\noffset集合\npayload集合\nimpactData信息\n\n\n\n  无论哪一种PostingsEnum的实现类，在读取过程中，每次总是从索引文件.doc、pos、pay只读取一个block的信息，并把block中的信息写入到多个数组中（下文会介绍这些数组），这些数组用来描述上文中docId集合、frequency集合、position集合、offset集合、payload集合、impactData信息。\n如何读取一个block中的信息\n  为了便于描述，我们不考虑没有生成跳表以及**不满128条信息的block（见文章索引文件之doc中VIntBlocks的介绍）**的读取，只介绍生成跳表后的读取方式。\n  在读取了跳表SkipList之后（读取过程见文章索引文件的生成（四）之跳表SkipList），我们就获得了一个SkipDatum的信息。\n  这里需要简单说明下，在生成跳表SkipList的过程中，在第0层中每当处理skipInterval（默认值为128）篇文档就生成一个SkipDatum，另外每生成skipMultiplier（默认值为8）个SkipDatum就在上一层，即第1层，生成一个SkipDatum。注意的是第1层的该SkipDatum中包含的指针信息是指向第0层中最后一个SKipDatum的结束读取位置，同时意味着指针信息指向了第0层的最后一个SkipDatum的下一个待写入的SkipDatum的起始读取位置，如果不了解这段描述，请阅读文章索引文件的生成（三）之跳表SkipList。\n  那么此时问题来了，在读取阶段，最高层的第一个跳表是如何读取的；term的第一个docId信息、frequency信息、position信息、offset信息、payload信息是如何获得的呢？\n\n通过图5中索引文件.tim中的TermMetadata字段中的SkipOffset获得最高层的第一个跳表信息，通过DocStartFP、PosStartFP、PayStartFP获取term的第一个docId信息、frequency信息、position信息、offset信息、payload信息分别在索引文件.doc、.pos、.pay中的起始读取位置。\n\n图6：\n\n  在SkipDatum字段包含的信息中，DocFPSkip描述了存储docId跟frequency的block在索引文件.doc中的起始读取位置，PosFPSkip描述了存储position的block在索引文件.pos中的起始读取位置，PosBlockOffset描述了block中的块内偏移（不明白的话，请阅读文章索引文件的生成（一）之doc&amp;&amp;pay&amp;&amp;pos、索引文件的生成（二）之doc&amp;&amp;pay&amp;&amp;pos）剩余的字段同理，下文中会进一步介绍，最终读取后的信息写入到上文提到的各种集合中：\n图7：\n\n点击查看大图\n docId集合、frequency集合\n  在源码中，使用docBuffer[ ]、freqBuffer[ ]两个数组来描述在内存中一个block中的的docId、frequency集合信息，从下面的定义也可以看出这两个数组都只存储一个block大小的信息：\n图8：\n\n  通过读取索引文件.doc的TermFreqs字段中的PackedDocDeltaBlock跟PackedFreqBlock字段，就可以获得docId集合跟frequency集合，并将这两个字段的信息分别写入到docBuffer[ ]、freqBuffer[ ]中：\n图9：\n\n  这里要特别说明的是，在读取block时，总是会将PackedDocDeltaBlock的信息，即文档号信息，写入到docBuffer[ ]中，而PackeddFreqBlock的信息，即frequency词频信息，则是采用read lazily，它描述的是在索引阶段存储了文档的frequency信息（基于IndexOptions选项），但是在搜索阶段，某种查询并不需要frequency信息，那么只有在需要frequency信息时候才读取PackedFreqBlock，并且写入到freqBuffer[ ]中。\n position集合、offset集合、payload集合\n  在源码中，使用posDeltaBuffer[ ]描述position信息；使用offsetStartDeltaBuffer[ ]、offsetLengthBuffer[ ]描述offset信息；使用payloadLengthBuffer[ ]、payloadBytes[ ]描述payload信息：\n图10：\n\n  对于这三个信息的读取相比较读取文档号跟词频稍微复杂，原因在于term在一篇文档中的这三个信息可能分布在一个或多个block（图7中的PackedPosBlock、packedPayBlock）中、甚至有可能在同一个PackedPosBlock中，保存term在两篇文档或更多篇文档的position信息，并且这些文档的属于不同的SkipDatum管理。例如term在一篇文档中的词频为386次，由于每128个位置信息就生成一个PackedPosBlock，故需要3个block存储。\n  对于上述的情况，我们以position为例，通过SkipDatum中的PosFPSkip先从索引文件.pos中找到block，即PackedPosBlock，的起始读取位置，然后将PosBlockOffset作为块内偏移找到在block中的起始读取位置即可。\n  由于position信息、offset信息、payload信息的数量总是保持一致的，即term在文档中的某个位置，必定对应有一个offset以及payload（可以为空），所以存储这些信息的posDeltaBuffer[ ]、offsetStartDeltaBuffer[ ]、offsetLengthBuffer[ ]、payloadLengthBuffer[ ]这四个数组的数组下标是保持一致的，注意的是payloadLengthBuffer[ ]描述的是在某个位置的term的payload长度length，根据这个长度去payloadBytes[ ]读取payload数据：\n图11：\n\n impactData信息\n  最后剩余读取impactData信息就相对简单了，在源码中，用二维数组impactData[ ] [ ]、impactDataLength[ ]来描述所有层的Impacts信息（Impacts的概念见文章索引文件的读取（十二）之doc&amp;&amp;pos&amp;&amp;pay）、图6中，先读取ImpactLength字段，确定Impacts字段的读取区间，然后将Impacts字段的信息写入到这两个数组中，这两个数组跟存储payload信息的两个数组用法一致，不赘述。\n 结语\n  关于索引文件.doc、.pos、.pay的一些基本的读取逻辑就暂时介绍到这里，在后面的文章介绍WAND（weak and）算法时，我们再进一步展开几个方法，例如advance( )、nextDoc( )、slowAdvance( )、advanceShallow( )等等，这些方法更细节的实现了读取索引文件.doc、.pos、.pay的过程。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","doc","pos","pay"]},{"title":"索引文件的读取（十二）之doc&&pos&&pay（Lucene 8.4.0）","url":"/Lucene/Search/2020/0904/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%B9%8Bdoc&&pos&&pay/","content":"  在前几篇索引文件的读取的系列文章中，我们介绍索引文件tim&amp;&amp;tip的读取时机点时说到，在生成StandardDirectoryReader对象期间，会生成SegmentReader对象，该对象中的FieldsProducer信息描述了索引文件tim&amp;&amp;tip、索引文件doc、索引文件pos&amp;&amp;pay中所有域的索引信息，故我们从本篇文章开始介绍索引文件.doc、.pos、.pay的读取。\n 索引文件.doc的数据结构（Lucene 8.4.0）\n  在文章索引文件的生成（三）之跳表SkipList跟索引文件的生成（四）之跳表SkipList中，我们基于Lucene 7.5.0介绍了跳表的数据结构，然而从Lucene 8.0.0开始，对跳表的数据的结构进行了调整，即对索引文件.doc的数据结构进行了调整，故在介绍索引文件.doc、.pos、.pay的读取之前，我们先介绍下调整目的以及调整后的数据结构。\n 为什么要调整\n  本文仅仅给出两个链接，它们分别介绍了在elasticSearch跟Lucene两个层面的调整初衷，感兴趣的同学可以自行查阅。当然在随后的内容中也会提及这两篇文章中介绍的部分内容：\n\nelastic：https://www.elastic.co/cn/blog/faster-retrieval-of-top-hits-in-elasticsearch-with-block-max-wand\nLucene：https://issues.apache.org/jira/browse/LUCENE-4198\n\n 调整后的数据结构\n  我们先直接给出两个版本的索引文件.doc的数据结构：\n图1：\n\n点击查看大图\n图2：\n\n点击查看大图\n  比较图1跟图2的区别可以看出，Lucene 8.4.0中所有level的SkipDatum字段都增加了Impacts跟ImpactLength两个字段，其中ImpactLength字段用于描述Impacts字段的长度，使得在读取阶段，能通过ImpactLength确定Impacts字段的信息在索引文件.doc中的读取区间。\n Impact\n  Impact字段的数据结构有两种，由于使用差值存储，即图2中normValue实际存储的值为跟上一个normValue的差值，故当normValue的值可能为0，那么就不将该值写入到索引文件中，freq字段同样使用了组合存储，在读取阶段，根据freq对应的二进制值的最低bit来判断是否存储了normValue。freq跟normValue的介绍见下文。\n为什么存储freq跟normValue\n  在文章索引文件的生成（三）之跳表SkipList中我们知道，在索引阶段，处理某个term的文档号跟词频信息期间，即生成索引文件.doc期间，每处理128篇文档号就会生成一个block，同时生成一条跳表信息，即SkipDatum，在Lucene 8.0.0之后，这个SkipDatum中额外多出字段，即Impacts，它存储了当前block中一个或多个（不超过128个）**具有竞争力（Competitive）**的文档的freq跟norm信息（原因见下文介绍）。具有竞争力描述的是在文档打分阶段，某些freq跟norm这一对（pair）信息对应的文档能获得较高（注意这里的用词，是&quot;较高&quot;，不是最高）的打分值，至于为什么能根据freq跟norm能计算出文档的打分值，在下文中我们再介绍。\n 生成Impact的过程\n  在处理某个term对应的文档集合（包含term的文档集合）期间，每处理一篇文档， 就使用CompetitiveImpactAccumulator对象来收集term在这篇文档中的freq跟normValue的信息，其中freq描述的是term在这篇文档中的词频，即出现的次数；normValue描述的是标准化后的文档的长度（文档的长度、标准化的概念见文章索引文件的生成（二十二）之nvd&amp;&amp;nvm）。\n  CompetitiveImpactAccumulator对象中使用int类型的数组maxFreqs[ ]收集freq和normValue，由于收集的代码十分简单，我们直接给出介绍：\n/** Accumulate a (freq,norm) pair, updating this structure if there is no *  equivalent or more competitive entry already. */public void add(int freq, long norm) &#123;    if (norm &gt;= Byte.MIN_VALUE &amp;&amp; norm &lt;= Byte.MAX_VALUE) &#123;        int index = Byte.toUnsignedInt((byte) norm);    maxFreqs[index] = Math.max(maxFreqs[index], freq);     &#125; else &#123; // 这种情况我们暂时不考虑        add(new Impact(freq, norm), otherFreqNormPairs);    &#125;    assertConsistent();&#125;\n  上述代码中，参数norm即上文中的normValue。本文中，我们暂时只关心第4行到第6行的代码。可见maxFreq[ ]数组将normValue的值作为数组下标，freq作为数组元素，并且对于相同的normValue，只保存最大的freq。\n  通过上述的介绍，相信同学们会至少抛出下面的疑问：\n\n疑问一：为什么能根据freq跟normValue计算出文档的打分值\n疑问二：为什么在maxFreqs数组中，normValue可以用于数组的下标，并且相同的normValue，只保存最大的freq\n\n为什么能根据freq跟normValue计算出文档的打分值\n  因为从Lucene 8.0.0开始，对文档进行打分的score( )方法参数发生了变化，如下所示，注意的是下文中出现的norm，即上文中的normValue：\n图3：\n\n  Lucene 8.0.0之前，在score( )方法中，其实是根据参数doc间接的获取norm，即normValue，然后再结合freq执行文档的打分，Lucene 8.0.0之后的改动使得在搜索阶段能更快的完成打分逻辑。\n  详细的介绍可以阅读以下两个issue：\n\nLUCENE-4198：https://issues.apache.org/jira/browse/LUCENE-4198\nLUCENE-8116：https://issues.apache.org/jira/browse/LUCENE-8116\n\n为什么在maxFreqs数组中，normValue可以用于数组的下标，并且相同的normValue，只保存最大的freq\n  回答该问题，我们只需要通过了解打分公式的实现规范就可以得到答案，下图给出的是score方法的注释：\n图4：\n\n  图4中红框中的注释大意为：如果norm相等，那么freq较大对应的文档打分值会更高，如果freq相等，那么norm较小对应的文档打分值会更高，另外在文章索引文件的生成（二十二）之nvd&amp;&amp;nvm中，我们说到，Lucene默认的BM25打分公式在通过computeNorm( )方法计算出的normValue值的可选区间为[1, 255]，如果让normValue作为maxFreqs数组的下标，那么可以使得该数组的长度是固定的。\n  结合上文中CompetitiveImpactAccumulator对象收集freq跟normValue的代码，可以看出，对于相同的normValue，freq越小，文档打分值就越低，即所谓的不具有竞争力，故只需要保存最大的freq。\n  我们假设在处理完某个term对应的128篇文档后，CompetitiveImpactAccumulator对象收集了如下的freq跟normValue信息，不过不是所有的信息都是具有竞争力的，我们需要依次遍历maxFreqs数组，挑选出一个或多个具有竞争力的信息：\n图5：\n\n  由于normValue的值区间为[1, 255]，故图5中下标值为0的数组元素总是为0，并且从下标值为1的位置开始处理。\n  由于当前表1中没有信息，那么下标值1以及对应的数组元素freq = 4自动视为最具竞争力的：\n图6：\n\n  接着观察下标值2以及对应的数组元素freq = 3，由上文中图4关于打分公式的介绍可知，freq较小，同时normValue较大，对应的文档打分值肯定是较小的，故不具有竞争力。同理下标值3以及对应的数组元素freq = 2，也是不具有竞争力的。\n  我们接着看下标值4以及对应的数组元素freq = 8，尽管normValue = 4 大于表1中的normValue = 1，但是freq = 8 大于表1中的freq = 4，故它是有可能对应的文档打分值是较高的，所以它具有竞争力：\n图7：\n\n  接着观察下标值5以及对应的数组元素freq = 28，由于它比表中一freq的最大值8还要大，故它也是具有竞争力的：\n图8：\n\n  经过上面的介绍，可以知道，下标值255以及对应的数组元素freq = 23 ，它肯定是不具有竞争力的。\n  至此我们知道，在索引阶段，只是根据freq跟norm这一对信息粗略的选出一些具有竞争力的候选者，即并不会真正的调用score( )方法计算出文档的打分值，原因很明显，在索引阶段，需要处理包含term的每一篇文档号，此时对这些文档号执行打分操作在性能上是不现实的。\n  至此我们可以真正的回答上文中提出的问题，即为什么存储freq跟normValue，原因就是这种方式使得在搜索阶段，能根据这些最具竞争力的freq跟norm信息，计算出一个block中的128篇文档的最高的文档打分值maxScore。通过这个maxScore使得一些类似TopN的查询能快速的在block中跳转，最终找到满足查询条件的文档号。在后面的文章中我们会详细的介绍如何通过Impact实现性能更高的查询，这里就简单的提一下。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","doc","pos","pay"]},{"title":"索引文件的读取（十五）之fdx&&fdt&&fdm（Lucene 8.4.0）","url":"/Lucene/Search/2020/1113/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  本文承接文章索引文件的读取（十四）之fdx&amp;&amp;fdt&amp;&amp;fdm，继续介绍剩余的内容。为了更好的理解下文中的内容，建议先阅读文章DirectMonotonicWriter&amp;&amp;Reader。下面先给出读取索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的流程图。\n 读取索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的流程图\n图1：\n\n 读取一个Chunk\n图2：\n\n  当图1的流程点文档号是否在BlockState不满足条件后，需要根据文档号（段内文档号）重新找到文档号所属的chunk，大致分为四个步骤：\n图3：\n\n点击查看大图\n  在介绍每个步骤之前， 我们先回顾下文章索引文件的读取（十四）之fdx&amp;&amp;fdt&amp;&amp;fdm中很重要的内容，即索引文件.fdm中的信息在生成reader阶段就已经被全量读取到内存的，而索引文件.fdx的NumDocBlock字段以及StartPointBlock字段则是off-heap方式读取。\n 步骤一\n  执行步骤一的目的是找出一个index，该index描述的是第index个chunk，因为在文章索引文件的生成（二十四）之fdx&amp;&amp;fdt&amp;&amp;fdm我们说到，在flush阶段，每1024（2&lt;&lt; BlockShitf）个chunk就会生成一个NumDocsMeta，所以通过索引文件.fdm的TotalChunks得到chunk的数量以及二分法，随后通过下面的代码一判断出属于哪一个NumDocMeta，随后通过代码二判断出NumDocMeta这个block中的块内索引blockIndex\n代码一：\n(index &gt;&gt;&gt; blockShift)\n代码二：\nblockIndex = index &amp; ((1 &lt;&lt; blockShift) - 1)\n  获得了blockIndex之后，就可以根据Min、AngInc获得一个存放文档号最大跟最小的区间bounds，最后判断段内文档号是否在这个区间中，如果存在那么就进一步去索引文件.fdx中的NumDocsBlock中读取对应的NumDoc，最终获得一个index，即第index 个chunk中包含段内文档号，但是到这一步我们无法知道第index个chunk在索引文件.fdx中的起始读取位置，所以我们需要执行后面的步骤才能获得。\n  如果你看阅读过文章DirectMonotonicWriter&amp;&amp;Reader就会知道，根据NumDocMeta中的四个编码元数据只能获得一个粗略的文档号的区间，要获得准确的文档号信息，需要去NumDocsBlock中获取（I/O操作）。\n 步骤二、步骤三、步骤四\n  在步骤一中我们获得了段内文档号所在的chunk是第index个chunk，那么通过上文中的代码一跟代码二在步骤二中获得索引文件.fdm中的StartPointMeta以及在步骤三中获得索引文件.fdx中的StartPointBlock中的StartPoint 就可以获得一个pointer，根据该pointer执行步骤四之后就获得了索引文件.fdx中某个chunk的起始读取位置。\n 更新BlockState\n图4：\n\n  在上一个流程点获得一个chunk的信息后，那么读取该chunk中的内容用于更新BlockState。读取的过程就是将索引文件.fdx中chunk中的内容写入到内存中。\n 结语\n  从上文中的介绍可以看出，当随机读取某个文档中的存储域信息时，会导致频繁的读取一个Chunk的值，意味着更多的I/O操作，所以顺序读取才能保证性能最大化。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","fdx","fdt","fdm"]},{"title":"索引文件之vec&vem&vex（Lucene 9.8.0）","url":"/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&vem&vex/","content":"  本篇文章将介绍Lucene中向量搜索相关的索引文件。当前版本中由三个索引文件，即文件后缀名为.vec、.vex、.vem的文件，文件中包含的内容主要包括图的分层信息，每一层中节点的编号，向量值，相连的邻居节点等信息。\n  向量搜索的实现基于这篇论文:Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs，由于索引文件中有些信息的概念其实是先需要了解这篇论文或者HNSW算法才可以，并且写这篇文章的主要目的是为了在随后介绍NHSW在Lucene中的实现的文章做准备的，因此通过这篇文章只需要了解索引文件中存放了哪些信息，以及对应的数据结构就可以了。\n  先给出这三个索引文件的数据结构之间的关联图，然后我们一一介绍这些字段的含义：\n图1：\n\n点击查看大图\n 数据结构\n  这三个索引文件总体上由Header、一个或者多个Field、以及Footer组成。\n\nHeader：主要包含文件的唯一标示信息，版本号，索引名字等一些信息\nFooter：记录校验和（checksum）算法ID和校验和，在读取索引文件时可以校验索引文件的合法性\nField: 该字段中包含了某个域下的所有向量信息。注意到该字段可以是多个，取决于一个段中定义的向量域（KnnFloatVectorField）的数量，例如下图中定义了两个向量域，域名分别为别vector1以及vector2，那么在索引文件中就会有两个Field。\n\n图4：\n\n .vec\n  索引文件.vec中主要存放的数据为所有的向量值vectorData、文档号信息DocIdData以及节点编号与文档号的映射关系OrdToDocData。\n图5：\n\n AlignPaddingValue\n  对齐字节数。\n  Lucene按照字节单位写入到文件中，在后续的数据写入之前会先将当前的文件指针对齐到指定的字节倍数（写入填充值0），来优化内存映射文件（mmap）的读取性。AlignPaddingValue的值必须是2的幂。\n VectorData\n图6：\n\n  向量值。\n  图6中每一个Value对应一篇文档中某个向量域的向量值。每个向量值使用相同数量的字节存储。例如图4中第50行-0.18344f, 0.95567f, -0.46423f对应一个Value。\n DocIdData\n图7：\n\n  文档号集合。\n  该字段存放包含向量域的文档号。使用IndexedDISI存储文档号。\n OrdToDocData\n图8：\n\n  节点编号与文档号的映射关系。\n  每一个向量都有一个节点编号（node id），通过OrdToDocData，就可以根据节点编号找到对应的文档号，也就是包含这个向量的文档号。在添加文档过程中，每一个向量根据添加的先后顺序，都会被赋予一个从0开始递增的节点编号。例如图9中，添加了三篇文档，其中文档0中向量的节点编号为0，文档2中向量的节点编号为1。另外注意的是，同一篇文档中只允许定义一个相同域名的向量域。\n图9：\n\n  在图6中我们说到，所有的向量值都存放在VectorData，在读取阶段就可以根据节点编号以及向量值对应的字节数，实现随机访问向量值。\n  最终OrdToDocData经过DirectMonotonicWriter编码压缩后写入到索引文件中。\n .vex\n图10：\n\n LevelNodeNeighbor\n图11：\n\n  邻居节点集合。\n  图11中，先按照层级划分邻居节点集合，从左到右LevelNodeNeighbor分别代表第0层、1层。。。N层。随后在某一层中，按照该层中节点编号顺序，从左到右NodeNeighbor分别代表该层中第一个、二个、N个节点的邻居节点集合。\n\nNeighborNumber：邻居节点的数量\nNeighborNode：邻居节点的编号\n\n  另外由于节点的邻居节点集合已经按照节点编号排序，因此会先计算相邻之间的差值（差值存储），使得尽量使用少的bit来存储。例如有以下的邻居节点集合：\n[1, 12, 18, 27, 92, 94, 139, 167, 250]\n  差值计算后的集合如下：\n[1, 11, 6, 9, 65, 2, 45, 28, 83]\n  由于在存储这个集合时，会选择固定bit位数，即按照集合中最大值所需要的bit进行存储，优化前后所有的值分别使用250跟83对应的bit位数存储。\n LevelNodeOffsetsData\n图12：\n\n  邻居节点在索引文件.vex的位置信息。\n  在图11中，记录了所有层中所有节点的邻居节点的信息，LevelNodeOffsetsData则是用于记录每一层的每一个节点的所有邻居节点在索引文件.vex中长度。在源码中使用一个二维数组来描述，下图是示例中的实际数据：\n图13：\n\n  图13中，示例构建出的图结构有三层，并且第0层中有333个节点，第一层中有22个节点，第二层中有2个节点。比如下图中，在索引文件的读取阶段，第1层的第二个节点的所有邻居节点在索引文件vec中的区间如下所示：\n图14：\n\n  最终LevelNodeOffsetsData经过DirectMonotonicWriter编码压缩后写入到索引文件中。\n .vem\n图15：\n\n点击查看大图\n 简易字段\n图16：\n\n  图16中红框标注的字段都属于简易字段：\n\nFieldNumber：域的编号\nEncodingOrdinal：向量值的数据类型，可以用8bit或者32bit表示一个数值\nSimilarityFunctionOrdinal：向量相似度函数。用来计算两个节点之间的距离。目前支持EUCLIDEAN（欧几里得/L2距离）、DOT_PRODUCT（点积或数量积）、COSINE（余弦相似度）、MAXIMUM_INNER_PRODUCT（最大内积）\nVectorDimension：向量的维度。例如图9中的向量维度位3。\nM：节点可以连接的邻居数量上限。第0层的节点可以连接的邻居数量上限为2M。\nFieldEndMarker：固定值-1。在索引读取阶段，会所有域的信息逐个字节读入到内存中，该字段作为一个标记，当读取到该值时，说明已经读取完所有的域的信息。\n\n VectorDataMeta\n图17：\n\n  向量值在索引文件.vec中的起始读取位置以及读取的长度。\n LevelNodeNeighborMeta\n图18：\n\n  邻居节点在索引文件.vex中的起始读取位置以及读取的长度。\n DocIdMeta\n图19：\n\n  文档号信息在索引文件.vec中的起始读取位置以及读取长度，至于其他字段的描述见IndexedDISI。\n OrdToDocMeta\n图20：\n\n  节点编号与文档号的映射关系在索引文件.vec中的起始读取位置（Offset）以及读取的长度（OrdToDocDataLength）。其他字段见DirectMonotonicWriter。\n LevelNode\n图21：\n\n  除了第0层外，其他层的的节点信息。\n  由于第0层中的节点数量是全量的节点，所以只需要根据从DocIdMeta读取出count字段，即文档的数量，那么第0层的节点编号区间为[0, count - 1]。而其他层的节点编号不是连续的，所以需要一个一个记录。\n\nLevelNumber：图中层的数量。\nNode：每一层（除了第0层）的节点编号信息\nNodeNumer：当前层中节点的数量\nNodeId：节点的编号\n\n LevelNodeOffsetsMeta\n图22：\n\n邻居节点位置信息在索引文件.vex中的起始读取位置（Offset）以及读取的长度（LevelNodeOffsetsDataLength）。其他字段见·DirectMonotonicWriter。\n","categories":["Lucene","suoyinwenjian"],"tags":["index","vec","vem","vex","indexFile"]},{"title":"索引文件的读取（十）之tim&&tip（Lucene 8.4.0）","url":"/Lucene/Search/2020/0812/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%EF%BC%89%E4%B9%8Btim&&tip/","content":" 索引文件的读取（十）（Lucene 8.4.0）\n  本文承接文章索引文件的读取（九）之tim&amp;&amp;tip，继续介绍剩余的流程点，先给出流程图：\n 获取满足TermRangeQuery查询条件的term集合的流程图\n图1：\n\n 收集Term\n图2：\n\n  在文章索引文件的读取（九）之tim&amp;&amp;tip中我们说到，在查询期间，满足查询条件的term数量未达到阈值（默认值16）跟达到阈值后的处理方式是不同的。\n 未达到阈值\n  当满足查询条件的term数量未达到阈值（默认值16），会将TermRangeQuery转变为BooleanQuery，其中每个term生成一个TermQuery，作为BooleanQuery的子查询，并且TermQuery之间的关系为SHOULD，其后续的查询过程可以阅读查询原理系列文章，不过在文章查询原理（二）中，我们只是简单的介绍TermContext对象（Lucene 7.5.0）中包含的信息，这里我们详细的介绍下TermContext中的信息是在何时生成、如何获取、为什么要获取。\n 何时生成\n  在执行完图1的流程点是否满足查询条件后，如果term满足查询条件，那么接着马上去索引文件.tim中获取该term的信息，即TermContext中包含的信息：\n 如何获取\n图3：\n\n点击查看大图\n  由图3可见TermContext所需要的信息都在索引文件.tim的TermStat跟TermMetaData字段中，另外需要注意的是，TermMetadata字段的不总是包含上述六个字段，我们这里看下DocStartFP、PosStartFP、以及PayStartFP这三个字段，其中PosStartFP、以及PayStartFP是否存在取决与在索引期间某个域的IndexOptions属性，如下所示：\n图4：\n\n  假设我们有以下的例子：\n图5：\n\n  图5中， 对于&quot;content&quot;域来说，设置了IndexOptions.DOCS_AND_FREQS意味着不用存储term的位置position跟offset信息，即不会生成索引文件.pos、索引文件.pay，那么在TermMetadata字段中就不需要PosStartFP、以及PayStartFP字段，因为TermMetadata字段中的信息描述了该term的信息在索引文件.doc、索引文件.pay、索引文件.pos中起始读取位置，如下所示：\n图6：\n\n  图3中红框标注的字段longSize描述了在读取阶段应该读取几个FP，以图5为例，由于&quot;content&quot;域的TermMetadata只存在DocStartFP，故longSize的值为1，根据上文的描述也可以知道longSize的可选的值区间为[1, 3]。\n  为什么要获取\n  回顾图1的流程点，整个流程结束后，我们得到的是term以及term的相关信息（Lucene 8.4.0的源码中用TermStates对象描述），但查询的最终目的是找到满足查询条件的文档号，从上文中我们知道，我们获得了满足查询的term的TermMetadata，它包含了DocStarFP，使得我们能后续的流程中能通过DocStarFP在索引文件.doc中找到包含这个term的文档号。\n 达到阈值\n  当达到阈值后，会继续查找，直到所有满足查询条件的term都被找到，并同时获取每个term的信息，这些信息即上文中介绍的索引文件.tim中的TermStats、TermMetadata字段，不同的是，这些term不会生成TermQuery，而是每一个term根据TermMetadata字段中的DocStartFP从索引文件.doc中找到包含它的文档号，而这些文档号正是满足查询条件的。\n 收集文档号\n  基于满足查询条件的文档数量，会使用不同的数据结构存储，首先通过下面的公式计算出阈值，在收集的过程中，总是先用一个数组收集文档号，当文档数量超过阈值后，再改为使用FixedBitSet存储：\nthreshold = maxDoc &gt;&gt;&gt; 7\n  上述公式中，maxDoc指的是当前段中的文档总数（记住这个maxDoc，下一篇文章中将会介绍为什么要根据是否达到阈值使用不同的处理方式，这个变量是其中的原因之一），另外上述公式的设计原则在源码中也给出了注释：\nFor ridiculously small sets, we&#x27;ll just use a sorted int[], maxDoc &gt;&gt;&gt; 7 is a good value if you want to save memory, lower values such as maxDoc &gt;&gt;&gt; 11 should provide faster building but at the expense of using a full bitset even for quite sparse data\n  注释大意：如果阈值设置的太小，并且文档数量稀疏（比如说文档号的值跨度很大，但是文档数量很少）的情况下可能会造成使用一个全量的FixedBitSet存储，会造成内存的浪费，全量的概念、内存浪费的原因在阅读FixedBitSet就能明白，这里不赘述，阈值设置的太大，那么可能会用数组来存储，但是收集的性能没有FixedBitSet高，原因见下文。\n  使用数组在收集的文档号过程中，是可能收集到重复的文档号，处理办法为在收集结束后，对数组先进行排序，然后去重，最后输出的数组中的数组元素是有序并且唯一（去重）的，如果使用FixedBitSet就不会有这个困扰，收集到相同的文档号后只会对bit位重复赋值为1而已，并且不需要排序操作。\n 疑问\n  为什么要根据是否达到阈值使用不同的处理方式：\n  这个问题可以分解为两个小问题：\n\n问题一：为什么达到阈值后不使用BooleanQuery的方式做文档号的收集\n问题二：为什么未达到阈值使用BooleanQuery的方式做文档号的收集\n\n  基于篇幅原因，下一篇文章我们在作出解释。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","tim","tip"]},{"title":"索引文件的读取（十四）之fdx&&fdt&&fdm（Lucene 8.4.0）","url":"/Lucene/Search/2020/1102/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  在前几篇索引文件的读取的系列文章中，我们介绍索引文件tim&amp;&amp;tip的读取时机点时说到，在生成StandardDirectoryReader对象期间，会生成SegmentReader对象，该对象中的StoredFieldsReader信息描述了索引文件fdx&amp;&amp;fdt&amp;&amp;fdm中所有域的索引信息，故我们从本篇文章开始介绍索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的读取。\n StoredFieldsReader\n  在生成StandardDirectoryReader阶段，就已经开始读取索引文件fdx&amp;&amp;fdt&amp;&amp;fdm实现一些初始化，为随后的搜索阶段做准备，读取后的信息用StoredFieldsReader来描述。\n .fdm\n图1：\n\n  .fdm文件中存储了元数据，即描述存储域数据的信息，它包含的所有数据将被完整的读入到内存中。\n .fdx\n图2：\n\n  通过索引文件.fdm中的StartPointsIndex与NumDocsIndex的差值来确定在索引文件.fdx中的一段数据区间，该区间中存储了chunk中的文档数量，同理SPEndPointer与StartPointsIndex的差值确定在索引文件.fdx中的一段数据区间，该区间中存储了每个chunk在索引文件.fdt中的起始读取位置。\n  注意的是，在生成StandardDirectoryReader阶段，图2中NumDoc跟StartPoints的字段的值并没有读取到内存中，只是将这两块数据的起始读取位置读取到了内存，相比较在Lucene 8.5.0之前的版本，这种读取方式即所谓的&quot;off-heap&quot;。\n off-heap\n  在Lucene 8.5.0之前，描述存储域的索引文件为.fdx、.fdt，它们的数据结构完整介绍可以见文章索引文件之fdx&amp;&amp;fdt，本文中中我们暂时只给出索引文件.fdx来介绍&quot;off-heap&quot;：\n图3：\n\n  在Lucene 8.5.0之前，图3中所有的Block会在生成StandardDirectoryReader阶段就全部读取到内存中，注意的是图3中的DocBases对应图2中NumDocs，命名不同而已，描述的内容是一致。\n  此次优化的详细内容可以见这个issue的介绍：https://issues.apache.org/jira/browse/LUCENE-9147 。在文章索引文件的读取（七）之tim&amp;&amp;tip中我们提到，索引文件.tip的读取也是用了off-heap，并且在Lucene 8.0.0就早早实现了，为什么在Lucene 8.5.0之后才将存储域的索引文件的读取使用off-heap呢？原因有两点，直接贴出issue原文：\n图4：\n\n  图4的大意就是，在terms index（索引文件.tip）使用了off-heap之后，存储域（stored fields）的索引文件变成了占用内存的大头，但是它没有terms index那样对性能有很大的影响（见文章索引文件的读取（七）之tim&amp;&amp;tip）。\n图5：\n\n  图5中的更有意思，Erick Erickson说当在技术层面（technical aspects）无法创新时，那么从内存方面去考虑优化了。\n .fdt\n图6：\n\n  对于索引文件.fdt，在此阶段通过索引文件.fdm的maxPointer，读取出ChunkCount、DIrtyChunkCount以及ChunkSize、PackedIntsVersion字段而已，也就是说Chunk字段，占用内存最大的数据块，没有被读取到内存中，在搜索阶段才根据条件读取，下文中中会展开介绍。\n 读取索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的流程图\n图7：\n\n 准备数据\n图8：\n\n  准备数据是全局的文档号。该文档号就是满足搜索条件的文档对应的文档号，例如下图中,ScoreDoc[ ]对象中存放的就是满足查询条件的全局的文档号。\n图9：\n\n 计算出所属段并转化为段内文档号\n图10：\n\n  在生成StandardDirectoryReader对象期间，通过获取每个段中的文档数量，会初始化一个int类型的starts[ ]数组，随后根据这个数据就可以计算出某个全局文档号属于哪一个段。通过读取索引文件.si中的segSize字段来获取每个段中的文档数量，如下所示：\n图11：\n\n  我们直接以一个例子来介绍starts[ ]数组：\n图12：\n\n  图12的例子，每当count达到1000、3000、20000、100000、结束时就生成一个段，即索引目录中存在5个段。那么对应的starts[]数组如下所示：\n图13：\n\n  那么根据starts[ ]数组，通过二分法就能计算出某个全局文档号所属的段了。\n  另外starts[ ]数组中的元素还代表了某个段的段内的第一篇文档号，那么将全局文档号与之做减法就获得了段内文档号。注意的是，为了便于介绍，下文中出现的文档号都是段内文档号。\n 文档号是否在BlockState中？\n图14：\n\n  BlockState中存储的是当前正在读取的Chunk的一些元数据，至少包含以下的内容：\n\ndocBase\nchunkDocs\nsliced\noffsets[ ]数组\nnumStoredFields[ ]数组\n\n  这些元数据对应在索引文件中内容如下所示：\n图15：\n\n  通过docBase跟chunkDocs就可以判断当前chunk中是否包含某个文档号。\n  如果当前chunk中包含某个文档号，那么直接读取该Chunk即可，否则需要重新从.fdt中找到所属chunk（查找过程将在下一篇文中展开），同时更新BlockState，继而获得文档中的存储域信息。从这里我们可以看出，在实际使用过程中，获取存储域的信息时，最好按照全局的文档号的大小依次获取，随机的文档号会导致频繁的更新blockState，也就是需要从索引文件中不断的读取Chunk，即增加了I/O开销。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","fdx","fdt","fdm"]},{"title":"索引文件的读取（四）之dim&&dii（Lucene 8.4.0）","url":"/Lucene/Search/2020/0506/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8Bdim&&dii/","content":"  本文承接索引文件的读取（三）之dim&amp;&amp;dii继续介绍剩余的内容，下面先给出读取索引文件.dim&amp;&amp;dii的流程图：\n图1：\n\n点击查看大图\n 设置左子树、右子树的准备数据\n图2：\n\n  在图1的流程点判断节点与查询条件的边界关系中，如果当前节点中的点数据的数值范围跟查询条件的数值范围的边界关系为CELL_CROSSES_QUERY（见文章索引文件的读取（一）之dim&amp;&amp;dii），说明查询条件的数值范围跟节点中的点数据的数值范围部分重叠（partially overlaps），如果当前节点是内部节点（非叶节点），那么我们需要为左右子树设置准备数据，递归处理。\n  当前节点需要为子树设置多个准备数据，我们主要关心splitPackedValue，当前节点的minPackedValue和splitPackedValue将作为左子树的minPackedValue和maxPackedValue，同理，splitPackedValue和当前节点的maxPackedValue将作为右子树的minPackedValue和maxPackedValue。\n  当前节点由于是内部节点，那么根据内部节点的不同数据结构（见文章索引文件的读取（三）之dim&amp;&amp;dii），splitPackedValue的获取方式如下红框标注：\n图3：\n\n 收集叶子节点中满足查询条件的文档号\n图4：\n\n  由于当前叶子节点中的点数据的数值范围跟查询条件的数值范围边界关系为CELL_CROSSES_QUERY，那么我们只能通过依次判断每一个点数据，找出满足查询条件的点数据对应的文档号，这个处理过程主要分为两步：重新计算边界关系、取出满足查询条件的文档号。\n 重新计算边界关系\n  在图2中的流程中，我们知道，叶子节点的父节点会设置准备数据minPackedValue和maxPackedValue，其中minPackedValue描述的是叶子节点中每个维度的最小值，maxPackedValue描述的是叶子节点中每个维度的最大值。\n  在文章索引文件的生成（十一）之dim&amp;&amp;dii中我们知道，父节点提供的minPackedValue和maxPackedValue对应的数值范围实际是大于或等于叶子节点中真正的数值范围，使得在图1的流程点判断节点与查询条件的边界关系中计算出的结果（使用的是父节点提供的minPackedValue和maxPackedValue）并不能准确的反映叶子节点的点数据数值范围跟查询条件的数值范围的边界关系，所以我们需要重新边界关系，叶子节点的minPackedValue和maxPackedValue通过读取下面的字段获得：\n图5：\n\n  在生成索引文件.dim的过程中，节点的minPackedValue跟maxPackedValue基于一定条件可能需要重新计算，不直接使用父节点提供的minPackedValue跟maxPackedValue，这块内容请阅读文章索引文件的生成（十一）之dim&amp;&amp;dii中设置左子树的准备数据、设置右子树的准备数据的章节。\n  重新计算后的边界关系依然是存在三种情况：CELL_INSIDE_QUERY、CELL_OUTSIDE_QUERY、CELL_CROSSES_QUERY。\n CELL_OUTSIDE_QUERY\n  说明叶子节点中没有任何点数据满足查询条件，那么直接返回。\n CELL_INSIDE_QUERY\n  说明叶子节点中所有点数据满足查询条件，那么读取叶子节点中所有的文档号，从下图红框标注的字段获取所有的文档号：\n图6：\n\n CELL_CROSSES_QUERY\n  如果仍然是CELL_CROSSES_QUERY，那么就需要读取每一点数据的域值，通过字典序比较，找出满足查询条件的点数据对应的文档号。\n 取出满足查询条件的文档号\n  读取域值的过程就不详细介绍， 如果你阅读了索引文件.dim的生成过程，那么自然会了解，这里主要讲下，如果某个点数据满足查询条件，是如何找到对应的文档号的。\n  在重新计算边界关系前，Lucene就先读取了叶子节点中的文档号信息，并将文档号写入到数组docIDs中，该数组的大小即点数据的数量，又因为每个点数据同样连续的存储到一块连续的空间中，那么点数据域值跟文档号的对应关系如下所示，我们以highCardinalityCost（见文章索引文件的生成（十三）之dim&amp;&amp;dii）为例：\n图7：\n\n点击查看大图\n 结语\n  至此，索引文件.dim&amp;&amp;dii的读取过程介绍完毕。\n点击下载附件\n","categories":["Lucene","Search"],"tags":["index","dim","dii"]},{"title":"索引文件的载入（一）之fdx&&fdt&&fdm（Lucene 8.4.0、8.6.0、8.7.0）","url":"/Lucene/Index/2021/0218/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%BD%BD%E5%85%A5%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8Bfdx&&fdt&&fdm/","content":"  在文章SegmentReader（一）中，我们介绍了SegmentReader对象，它用于描述一个段中的索引信息，并且说到SegmentReader对象中包含了一个SegmentCoreReaders对象。\n图1：\n\n  图1中，蓝框标注的两个对象用于描述DocValues的索引信息，而红框标注的SegmentCoreReader则描述了下面的索引信息，注意的是在文章SegmentReader（一）中是基于Lucene 7.5.0的：\n表一：\n\n\n\n对象\n描述\n\n\n\n\nStoredFieldsReader\n从索引文件fdx&amp;&amp;fdt中读取存储域的索引信息\n\n\nFieldsProducer\n从索引文件tim&amp;&amp;tip、索引文件doc、索引文件pos&amp;&amp;pay中读取域的倒排信息\n\n\nTermVectorsReader\n从索引文件tvx&amp;&amp;tvd读取词向量的索引信息（用于高亮优化查询）\n\n\nPointsReader\n从索引文件dim&amp;&amp;dii中读取域值为数值类型的索引信息\n\n\nNormsProducer\n从索引文件nvd&amp;&amp;nvm中读取域的打分信息（作为对文档进行打分的参数）\n\n\nFieldInfos\n从索引文件fnm读取域的信息\n\n\n\n  在文章SegmentReader（一）中，并没有对每一种索引文件进行详细的读取过程的介绍，故索引文件的载入的系列文章对此将详细的展开。\n  该系列文章将要介绍的内容可以概述为这么一句话：在初始化一个读取索引信息的reader期间，索引文件如何被读取（载入）。由于只是初始化一个reader，而不是处于一个查询阶段，所以只有部分索引文件的信息会被载入到内存中。\n 索引文件的载入顺序\n  在SegmentCoreReader类的构造函数中可以看出表一中对应的索引文件的载入顺序：\n图2：\n\n  上文中说到，SegmentCoreReader对象是被包含在SegmentReader对象中，故在SegmentReader类的构造函数中，还可以看出DocValues对应的索引文件跟SegmentCoreReader对象中包含的索引文件的载入顺序：\n图3：\n\n  从图3可以看出，在构造SegmentReader对象时，先载入SegmentCoreReader对象，即表一中包含的索引文件， 随后再载入DocValues对应的索引文件。\n  为什么图3中第98行处又载入了索引文件.fnm，在图2中不是已经载入过了吗？\n  图3跟图2中载入的索引信息是不相同的，他们的区别在于索引文件.fnm的版本不同。这两个索引文件的区别在文章构造IndexWriter对象中介绍流程点更新SegmentInfos的metaData中详细介绍了，不赘述。\n  由于会依赖之前写过的跟索引文件的数据结构相关的文章，而那些文章又是依赖不同的Lucene版本，故注意版本区分。\n  为什么之前介绍索引文件的文章会依赖不同的版本\n  原因是Lucene版本迭代更新太快，索引文件的数据结构一直在优化，由于本人精力有限，无法对每一次的数据结构的优化重新写文章。\n 索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的载入（Lucene 8.6.0）\n  描述存储域的索引文件fdx&amp;&amp;fdt&amp;&amp;fdm的载入顺序依次如下所示：\n.fdt --&gt; .fdm --&gt; fdx --&gt; .fdt\n 索引文件.fdt的载入\n  索引文件.fdt中存放了存储域的信息，载入过程中只会将部分信息读入到内存，如下所示：\n图4：\n\n  图4中，红框标注的字段将会被读取到内存中。\n  对于Header、ChunkSize、PackedIntsVersion字段，可以从索引文件的第一个字节依次读取出这三个字段。下面是读取这三个字段的代码：\n图5：\n\n  图4中的Footer字段如何读取？\n  由于Footer占用固定的16个字节，随后根据索引文件.fdt的总长度，那么这两者的差值就是Footer字段在索引文件.fdt中的起始读取位置。\n 索引文件.fdm的载入\n  索引文件.fdm中的内容将被全量读取到内存中，如下所示：\n图6：\n\n 索引文件.fdx的载入\n  索引文件.fdx的Header、PackedIntsVersion、Footer先被读取到内存中：\n图7：\n\n  在图6中，从索引文件.fdm中读取了NumDocsIndex、StartPointsIndex字段，这两个字段用来描述图7中的NumDocs字段在索引文件.fdx中的位置区间。同理图6中读取的StartPointsIndex、SPEndPointer用来描述图7中StartPoints字段在索引文件.fdx中的位置区间。随后将NumDocs、StartPoints这两个字段的信息读取到内存中。\n  所以索引文件.fdx也是全量读取到内存的。\n图8：\n\n 索引文件.fdt的载入\n  图4中，索引文件.fdt的ChunkCount、DirtyChunkCount字段也要被读取到内存中，通过图6中从索引文件.fdm中读取的maxPointer来获取这两个字段在索引文件.fdt中的起始读取位置：\n图9：\n\n  至此可以看出，对于描述存储域的索引文件fdx&amp;&amp;fdt&amp;&amp;fdm，除了索引文件.fdt的Chunk字段，其他索引文件的所有字段都会被读取到内存中。\n  对于索引文件fdx&amp;&amp;fdt&amp;&amp;fdm详细的读取过程可以阅读系列文章索引文件的读取（十四）之fdx&amp;&amp;fdt&amp;&amp;fdm，该系列的文章介绍了索引文件.fdt的Chunk字段的读取方式。\n 结语\n  基于篇幅，其他索引文件的载入将在后面的文章中展开。\n","categories":["Lucene","Index"],"tags":["index","search","fdx","fdt","fdm"]},{"title":"索引文件锁LockFactory","url":"/Lucene/Store/2019/0604/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E9%94%81LockFactory/","content":"  LockFactory在Lucene中用来对索引文件所在的目录进行加锁，使得同一时间总是只有一个IndexWriter对象可以更改索引文件，即保证单进程内(single in-process)多个不同IndexWriter对象互斥更改（多线程持有相同引用的IndexWriter对象视为一个IndexWriter不会受制于LockFactory，而是受制于对象锁（synchronized(this)）、多进程内(multi-processes)多个对象互斥更改。\n LockFactory的具体实现类\n  LockFactory是一个抽象类，提供了以下几种子类，即NoLockFactory、SingleInstanceLockFactory、SimpleFSLockFactory、NativeFSLockFactory、VerifyingLockFactory，下面一一介绍。\n图1：\n\n NoLockFactory\n  该类的功能同类名一样，即不会对索引文件进行加锁，如果使用者有把握（certain）使得IndexWriter对象总是能互斥更改索引文件，那么可以不对索引文件所在的目录进行加锁。\n SingleInstanceLockFactory\n  该类是RAMDirectory默认使用的索引文件锁，RAMDirectory属于Directory类的子类，Directory类描述了索引文件所在目录的一些信息，以后会有文章介绍Directory类。\n  对于拥有相同RAMDirectory对象的多个IndexWriter对象，实现不同IndexWriter之间对索引文件的互斥更改。\n 获得索引文件锁\n  该过程十分的简单，故给出完整的代码：\nfinal HashSet&lt;String&gt; locks = new HashSet&lt;&gt;();// 尝试获得索引文件锁，如果已经占用则抛出异常public Lock obtainLock(Directory dir, String lockName) throws IOException &#123;  synchronized (locks) &#123;    if (locks.add(lockName)) &#123;      return new SingleInstanceLock(lockName);    &#125; else &#123;      throw new LockObtainFailedException(&quot;lock instance already obtained: (dir=&quot; + dir + &quot;, lockName=&quot; + lockName + &quot;)&quot;);    &#125;  &#125;&#125;\n  在IndexWriter类中，定义了一个不可更改的lockName，使得无论哪个线程通过IndexWriter来获得索引文件锁时，lockName的值都是相同的，这样就能通过判断该lockName是否在locks容器中来实现互斥，lockName在IndexWriter类中的定义如下：\n图2：\n\n 释放索引文件锁\n  释放锁的过程即从locks容器(HashSet对象)中移除键值为write.lock的元素。\n FSLockFactory\n  FSLockFactory是一个抽象类，它有两个子类分别是SimpleFSLockFactory，NativeFSLockFactory，用来专门指定给FSDirectory类提供索引文件锁(can only be used with FSDirectory subclasses)。\n SimpleFSLockFactory\n  该类只能用于FSDirectory，FSDirectory跟RAMDirectory一样是Directory的子类。\n  该类通过在索引文件所在目录创建一个名为write.lock文件的方式来实现索引文件锁，该方法的缺点在于如果JVM异常退出，那么索引文件锁可能无法被释放，即没有删除write.lock文件。\n  解决的方法只能是通过手动删除write.lock文件，注意是，手动删除前用户得自己保证(certain)目前没有IndexWriter正在写入，否则非常容易破坏（corrupt）索引文件，比如说由于删除了write.lock文件，使得多个IndexWriter对象同时更改了索引文件。\n 获得索引文件锁\n  在索引文件所在目录生成一个write.lock文件，并且记录该文件的创建时间，目的是在任意阶段可以检查该文件是否被外在力量（external force）篡改了，从而判定该锁的合法性(valid)，在该类中，如果发现被篡改，那么就抛出异常。\n  比如每次添加一篇文档（Document）后，将该Document的信息写入索引文件之前会做检查（调用该类的ensureValid( )方法），如果此时发现write.lock被篡改了（比如说被删除了），那么这次写入就会失败，后续的处理会在以后介绍IndexWriter时详细介绍。\n 释放索引文件锁\n  释放锁的过程即删除write.lock文件，如果发现write.lock文件的创建时间跟获得该锁的时间不同，那么就抛出异常来让用户决定如何处理这种情况，使用Files.delete(Path path)的方法来尝试删除write.lock文件，如果出错了，那么同样地抛出异常让用户决定如何处理这种情况。\n NativeFSLockFactory\n  NativeFSLockFactory同SimpleFSLockFactory一样，只能用于FSDirectory，它是Directory默认使用的LockFactory的，同样的通过在索引文件所在目录生成一个write.lock文件，但是该类还使用了FileChannel来管理write.lock文件。\n 获得索引文件锁\n  NativeFSLockFactory获得索引文件锁的过程分为两步：\n\n第一步：判断write.lock文件是否已经被**进程内(in-process)**的其他线程的不同IndexWriter对象占有，通过一个线程安全的同步Set容器(Collection.synchronizedSet())实现，最先到的(first come)的线程会将write.lock文件的绝对路径写入到同步Set容器中，后来的线程尝试添加路径时会抛出异常\n第二步：使用FileChannel来尝试获得**进程间（inter-process）**级别的文件锁FileLock，即判断write.lock文件是否被其他进程占用，如果占用则直接抛出异常。\n\n  另外也会记录write.lock文件的创建时间，用法跟SimpleFSLockFactory一样。\n  同SimpleFSLockFactory一样，在运行过程中，当更改索引文件时（添加文档、更新、commit、flush等变更索引文件的操作），依次判断下面的条件，任意一个条件不满足时说明当前索引文件锁是不合法的：\n\n条件1：进程内的某个线程调用了close()，如果该线程继续执行更改索引操作，会抛出异常\n条件2：如果同步Set容器不包含write.lock文件的绝对路径，会抛出异常\n条件3：FileChannel的锁FileLock是不合法的状态，这种情况是未知的外部力量（external force）导致的， 会抛出异常\n条件4：FileChannel中的channel的值不是0，Lucene不会对write.lock文件写入任何数据，所以如果发现该文件中被添加了数据则抛出异常\n条件5：当前write.lock文件的创建时间跟获得锁时的创建时间不一致，说明被未知的（external force）修改了，会抛出异常\n\n 释放索引文件锁\n  该类释放锁的过程分两步走：\n\n释放write.lock文件的FileLock\n清空同步Set容器中的内容\n\n SimpleFSLockFactory与NativeFSLockFactory各自的特点\n  尽管NativeFSLockFactory是默认的FSDirectory的索引文件锁，但基于实际场景，有时候使用SimpleFSLockFactory能更好的工作(work perfectly)。\n\nNativeFSLockFactory基于 java.nio.*来获得FileLock，但在某些文件系统下可能会受限，比如说在NFS下可能无法获得FileLock(the lock can incorrectly be double acquired)，此时使用SimpleFSLockFactory就不会有这个问题\n当JVM异常退出时，残留的（leftover）write.lock文件无法删除，如果使用SimpleFSLockFactory需要手动的去删除该文件，否则尝试获得索引文件锁时就直接抛出异常，而使用NativeFSLockFactory时，不用关心当前write.lock文件是否被正确删除，因为它只关心write.lock是否被其他进程占用，而JVM异常退出后，会自动释放FileLock(操作系统会释放FileLock)，所以不能通过判断write.lock文件在索引文件的目录中就认为索引文件被锁定了(locked)，Lucene从不会因为异常去删除write.lock文件\n\n VerifyingLockFactory\n  该类不同于上面提到的NoLockFactory、SingleInstanceLockFactory、SimpleFSLockFactory、NativeFSLockFactory，如果上述这些索引文件锁在实际业务还是无法正确的工作(not working properly)，那么可以使用VerifyingLockFactory封装上述的LockFactory，通过一个开启一个LockVerifyServer（单独的服务）来实现认证(verify)，保证不会发生同一时间两个进程同时获得锁的情况。\n 认证方法\n  Lucene7.5.0中通过Socket向LockVerifyServer发送特定的字节来尝试获得锁，同样的通过发送特定字节来释放锁，由于源码中的实现过于简单，一般不会直接使用，故不详细介绍。\n 结语\n  本文介绍了实现互斥访问索引文件的索引文件锁LockFactory。\n点击下载Markdown文件\n","categories":["Lucene","Store"],"tags":["directory","lock"]},{"title":"软删除softDeletes（三）（Lucene 8.4.0）","url":"/Lucene/Index/2020/0624/%E8%BD%AF%E5%88%A0%E9%99%A4softDeletes%EF%BC%88%E4%B8%89%EF%BC%89/","content":"  在文章软删除softDeletes（二）中我们说到，在Lucene 7.5.0版本中，使用了下面两个容器来存储软删除的删除信息、DocValues的更新信息：\n\nMap&lt;String,LinkedHashMap&lt;Term,NumericDocValuesUpdate&gt;&gt; numericUpdates：DocValuesUpdatesNode\nMap&lt;String,LinkedHashMap&lt;Term,BinaryDocValuesUpdate&gt;&gt; binaryUpdate：DocValuesUpdatesNode\n\n  而从Lucene 7.7.0版本之后，使用了下面的一个容器来优化存储：\n\nfinal Map&lt;String, FieldUpdatesBuffer&gt; fieldUpdates = new HashMap&lt;&gt;();\n\n  我们先给出源码的CHANGE.LOG中的介绍：\nLUCENE-8590: BufferedUpdates now uses an optimized storage for buffering docvalues updates that can safe up to 80% of the heap used compared to the previous implementation and uses non-object based datastructures.\n  该issue的地址见：https://issues.apache.org/jira/browse/LUCENE-8590 ，对于上述中优化存储的内容将在下一篇文章中展开。\n  在介绍软删除在flush/commit阶段的相关内容之前，我想先重新介绍在多线程下执行文档删除（更新）操作时，删除结点（即软删除softDeletes（二）中介绍的Node对象）如何被添加到全局的deleteQueue中，以及在flush阶段，DWPT（见文章文档的增删改（三））生成一个段时，如何最终确定这个段包含了哪些私有删除信息（见文章文档提交之flush（三））。\n 添加删除结点到全局deleteQueue\n  在触发主动flush（见文章文档提交之flush（一））之前，总是只存在一个全局的deleteQueue，它用来存储在上一次主动flush到下一次主动flush期间所有线程执行删除操作后对应的删除信息，即删除结点Node对象，为了便于描述，我们假设一个DWPT总是只由一个相同的线程来管理（一个DWPT如果执行了一次文档的增删改操作后，没有触发自动flush，那么下次再次执行文档的增删改操作时可能会由另一个线程管理，见文章文档的增删改（二））。\n  每个DWPT对象中有一个私有的DeleteSlice对象，DeleteSlice的结构很简单，如下所示：\n图1：\n\n  每当生成一个新的DWPT（意味着随后会生成一个新的段），DWPT中的DeleteSlice对象（为了区分全局的DeleteSlice，故我们称之为DWPT私有的DeleteSlice对象）的sliceHead以及sliceTail会同时被赋值为deleteQueue中最后一个删除结点，可以理解为sliceHead跟sliceTail作为指针指向deleteQueue中最后一个删除结点，如下所示：\n图2：\n\n  图2中，deleteQueue中已经有了一个删除结点TermNode，它是由其他线程添加的，当TheadA管理一个新生成的DWPT时，那么在初始化DWPT阶段，DWPT中的DeleteSlice对象的sliceHead以及sliceTail会指向deleteQueue中的最后一个删除结点，即上图中的TermNode。\n  为什么sliceHead以及sliceTail要指向deleteQueue中的最后一个结点：\n  这么做的目的是保证旧的删除操作不会错误的作用（apply）到新的段（DWPT最终会flush为一个段），注意的是在flush阶段总是从sliceHead指向的删除结点的下一个结点开始读取，故图2中的DWPT生成的段不会被上图中TermNode这个删除信息作用。\n  另外还存在一个全局deleteSlice，它的sliceHead以及sliceTail**总是分别指向（在完善删除结点之前）**deleteQueue中的第一个以及最后一个删除结点，全局deleteSlice的作用将在下文中中介绍。\n  在完善删除结点的前后全局deleteSlice的sliceHead以及sliceTail有什么区别：\n  在文章软删除softDeletes（二）中我们说到，删除结点在被**完善（添加哨兵值，见文章文档的增删改（四））**后使用新的容器来存储，在源码中的描述就是删除信息Node在被完善后用BufferedUpdate对象（新的容器的是对象成员，见在文章软删除softDeletes（二））来存储。\n  全局deleteSlice什么时候会发生完善删除结点：\n  在索引期间，任意线程往deleteQueue中添加删除结点后就会执行完善全局deleteSlice对应的删除结点的操作，在源码中通过调用tryApplyGlobalSlice()来执行完善删除结点的操作，以及在flush阶段也会发生完善删除结点的操作，如下所示：\n图3：\n\n  图3中可以看出，每次执行一次添加删除结点的操作后就会尝试执行完善删除结点，那么问题又来了，为什么是尝试执行，而不是强制执行，强制执行意味着任意线程添加一个删除结点都要完善全局deleteSlice对应的删除结点。\n  为什么是尝试完善删除结点，而不是强制完善删除结点：\n  在flush阶段，deleteQueue中所有的删除结点都会被完善删除结点，这些删除信息的作用（apply）对象为索引目录中已经生成的段，为了防止产生重复的删除信息，只会让第一个执行flush的DWPT在生成段的时候完善全局deleteSlice对应的删除结点，意味着这个DWPT对应的线程相对于其他线程会有额外的负担，那么如果在索引阶段，每个线程管理的DWPT在添加一个删除结点后就尝试完善全局deleteSlice对应的删除结点，就能减轻在flush阶段第一个生成段对应的线程的压力。索引期间的删除结点操作是个tryLock的同步操作，所以如果线程尝试完善全局DeleteSlice对应的删除结点时发现其他线程正在执行，那么就跳过此次操作。另外一个更重要的原因是，我们在文章文档的增删改（四）中知道，完善删除结点的主要目的就是添加哨兵值，哨兵值用来描述删除信息的作用范围，然而全局DeleteSlice中的的删除信息是用来作用索引文件已经生成的段的所有文档，所以这个哨兵值是个常量为0x7fffffff，代表了所有的文档号，但是对于DWPT的DeleteSlice中的删除信息，它们的作用（apply）对象是DWPT即将生成的段中的文档，所以此时的哨兵值为DWPT当前收集的文档数量，即numDocsInRAM（见文章文档的增删改（四）），下文会进一步介绍。\n  图2中，如果ThreadA在添加了删除结点后执行了tryApplyGlobalSlice()的操作，下图所示：\n图4：\n\n  为了便于描述，我们假设只有在flush阶段才会完善全局DeleteSlice对应的删除结点，故下文中，全局DeleteSlice中的sliceHead以及sliceTail总是分别指向deleteQueue中第一个和最后一个删除结点。\n  接着如果此时图2中的DWPT执行一个删除操作，那么会在deleteQueue中添加一个删除结点，我们假设执行了软删除的操作，故添加一个DocValuesUpdatesNode，并且让DWPT的私有DeleteSlice指向**它添加的那个删除结点（这里加黑的区域很重要，下文会介绍）**如下所示：\n图5：\n\n  同样的在添加完删除结点DocValuesUpdatesNode之后，ThreadA管理的DWPT同样会对它包含的DeleteSlice，即私有DeleteSlice对应的删除节点执行完善的操作，那么在删除结点DocValuesUpdatesNode被完善后用新的容器来存储，即被存储到BufferedUpdates对象中，并且BufferedUpdates对象是DWPT的私有的（看不懂？先看下文章文档的增删改（四）），最后更新DWPT的DeleteSlice的sliceHead，让它指向sliceTail指向的删除结点，如下所示：\n图6：\n\n  如果此时线程ThreadB管理一个新的DWPT，那么DWPT初始化结束后，如下所示：\n图7：\n\n  图4中可以看出ThreadA管理的DWPT添加的删除信息不会作用到ThreadA管理的DWPT对应的段，在多线程下往deleteQueue中添加删除结点是个同步操作，该同步通过synchronize实现，在源码中的代码如下所示：\n图8：\n\n  图7中通过synchronized关键字实现线程间的同步，上图中tail是一个全局的Node结点，它总是deleteQueue中最后一个删除结点的引用，它用来告知其他的线程以及全局的DeleteSlice的sliceTail如何指向deleteQueue中的最后一个删除结点。\n  从图7中我们了解到，多线程执行添加删除结点操作是同步，但是每个线程在添加结束后紧接着的完善删除结点的操作却是不同步的，例如ThreadA跟ThreadB管理的DWPT分别添加了一个TermNode跟DocValuesUpdatesNode，并且我们假设TermNode先被添加了，并且两个线程都没有开始执行完善删除结点的操作。如下所示：\n图9：\n\n  上文中我们说到，DWPT往deleteQueue中添加了一个删除结点后，会让DWPT的私有DeleteSlice的sliceTail指向它添加的那个删除结点，故如上所示。\n  在添加删除结点到deleteQueue中之后，每个线程管理的DWPT都需要执行完善结点的操作，从图9中可以看出，由于ThreadA在添加删除结点的竞争中获胜了，所以在完善删除结点后，只有TermNode作用于ThreadA管理的DWPT中的文档，而对于ThreadB，则是TermNode跟DocValuesUpdatesNode两个删除信息将作用于ThreadB管理的DWPT中的文档，同理如果在添加到deleteQueue的竞争中，ThreadB获胜了，那么如下所示：\n图10：\n\n  从图10中可以看出，当执行完善删除结点后，ThreadB管理的DWPT中的文档将被只被一个DocValuesUpdatesNode作用，而ThreadA管理的DWPT中的文档将只被2个删除信息作用。\n  我们此时肯定会产生一个疑问（其实这也是我第一次看这段源码时的疑惑😁），按照上文中的描述，DWPT生成的段中的删除信息是无法确定的。\n  其实不然，我们先抛出结论：在DWPT初始化之后，下次主动flush之前的所有新增的删除信息（可能由其他线程提供的删除信息）都会作用到这个DWPT中的文档。也就是说对于图9跟图10两种添加删除结点的情况，这两个线程管理的DWPT中的文档最终都将被TermNode、DocValuesUpdateNode作用。\n  我们以图10的ThreadB为例，目前它管理的DWPT中的文档只被DocValuesUpdatesNode作用，TermNode将在下面三种情况下作用（apply）\n\n情况一：ThreadB管理的DWPT又添加了一条删除信息，假设这条删除信息是TermNode，那么在完善结点时就能通过TheadB管理的DWPT对应的DeleteSlice中的sliceHead与sliceTail指向的删除结点区间来实现，如下所示：\n\n图11：\n\n\n\n情况二：ThreadB管理的DWPT添加了一篇文档，在添加完这篇文档后，会更新DWPT中的DeleteSlice的sliceTail，让它指向tail（tails的作用上文已经介绍），也就是指向deleteQueue中最后一个删除结点，随后执行完善删除结点的操作\n\n\n情况三：在flush阶段，DWPT生成一个段时，ThreadB管理的DWPT对应的DeleteSlice中的sliceTail将会被指向tail，这样所有其他线程新增的删除信息都能被作用到当前DWPT中的文档。\n\n\n  源码中是这样介绍删除结点作用DWPT中的文档的时机点的：\n图12：\n\n  图12中说到，当DWPT处理完一篇文档之后，会更新DWPT私有的DeleteSlice，如果这次的操作是更新操作，即带有删除信息，那么就通过add(Node, DeleteSlice)方法，即上文中的情况一；如果没有删除信息就调用updateSlice(DeleteSlice)的方法，即上文中的情况二。\n 结语\n  相信通过上文的介绍，能完全的理解Lucene在索引阶段处理删除信息的方式，在后面的文章中我们还会介绍在flush阶段跟删除信息相关的知识，为了更快的理解，建议先阅读系列文章文档提交之flush。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["delete","softDeletes"]},{"title":"软删除softDeletes（二）（Lucene 8.4.0）","url":"/Lucene/Index/2020/0621/%E8%BD%AF%E5%88%A0%E9%99%A4softDeletes%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"  在文章软删除softDeletes（一）中我们介绍了软删除的一些应用，从本篇文章开始，将根据索引（index）、flush/commit、段的合并、搜索这几个不同的阶段来介绍跟软删除相关的内容。\n 索引（index）\n  阅读本小结内容必须掌握文章文档的增删改（四）中的知识点，故下文中出现的一些名词不会作详细的介绍。\n  索引（index）这个阶段，即文档的增删改的阶段，由于软删除属于删除操作，故对应的删除信息会被添加到全局的deleteQueue中，deleteQueue中存放了四种类型的删除信息，这四种删除信息用下图中的Node对象来描述：\n图1：\n\n  上图中的四种删除信息通过IndexWriter类中提供的方法生成：\n\nQueryArrayNode：deleteDocuments(Querys)\nTermArrayNode：deleteDocuments(Terms)\nDocValuesUpdatesNode：updateBinaryDocValue( )、updateNumericDocValue( )、updateDocValues( )、softUpdateDocument( )、softUpdateDocuments( )\nTermNode：updateDocument(Term, Document)、updateDocuments( Term, Documents)\n\n  由上文可以看出，软删除对应的删除信息将会由DocValuesUpdatesNode来描述，\n我们通过一个例子来看下DocValuesUpdatesNode中描述了哪些删除信息：\n图2：\n\n  图2中，第56行执行了软删除的操作，它对应的删除信息DocValuesUpdatesNode包含如下信息：\n图3：\n\n  DocValuesUpdatesNode中包含了两个对象：next、item，其中next用来指向deleteQueue中下一个删除信息（见文章文档的增删改（四）），item（NumericDocValuesUpdate对象）中的type描述了使用NUMERIC类型的DocValues来描述被软删除的文档，item中的term描述了满足被软删除的文档的条件，即包含域名为&quot;abc&quot;、域值为&quot;document3&quot;的文档都会被软删除。\n  上述DocValuesUpdatesNode的信息我们可以看出，我们获得了满足软删除的条件，但是没有指出软删除的作用范围（作用域），例如在图2中，文档3也满足被软删除的条件，但是这篇文档是在第56行的软删除操作之后添加的，那么这次的软删除不能作用（apply）到这篇文档，即这次的软删除操作只能对文档1生效，故我们还需要根据DocValuesUpdatesNode的信息进一步完善删除信息，即添加软删除的作用范围，不同的删除信息在完善删除信息后用下列的容器来描述（Lucene 7.5.0）：\n\nMap&lt;Term,Integer&gt; deleteTerms ：TermArrayNode、TermNode\nMap&lt;Query,Integer&gt; deleteQueries：QueryArrayNode\nMap&lt;String,LinkedHashMap&lt;Term,NumericDocValuesUpdate&gt;&gt; numericUpdates：DocValuesUpdatesNode\nMap&lt;String,LinkedHashMap&lt;Term,BinaryDocValuesUpdate&gt;&gt; binaryUpdate：DocValuesUpdatesNode\n\n  BufferedUpdates类、deleteTerms、deleteQueries的内容在文章文档的增删改（四）中已经作出了介绍，不赘述。\n  如果DocValuesUpdatesNode中item中的type为NUMERIC，那么对应生成numericUpdates，如果item中的type为BINARY，那么对应生成binaryUpdate。\n numericUpdates\n  numericUpdates容器的key描述的是图3中，item中的field的值，而容器的value则是又一个LinkedHashMap的容器，该容器的key为一个term，例如图3中的item的term的值，而value则是图3中的item。\n  在文章构造IndexWriter对象（一）中我们说到，一个IndexWriter通过IndexWriterConfig配置只能设置一个软删除域，并且是个不可变配置，那么为什么numericUpdates使用Map存储，并且key为软删除域的值？\n\n上文中我们说到，IndexWriter类中提供的updateBinaryDocValue( )、updateNumericDocValue( )、updateDocValues( )方法（这三个方法为更新DocValues的操作）对应生成的删除信息跟软删除操作softUpdateDocument( )、softUpdateDocuments( )一样都是用DocValuesUpdatesNode来描述，故也使用同一个numericUpdates来完善删除信息，所以numericUpdates容器的key不一定就是软删除的域。\n\n  我们同样的以一个例子来介绍：\n图4：\n\n  图4的例子中，既有updateNumericDocValue()，又有softDocument()的操作，那么这两个操作对应在numericUpdates中的删除信息如下所示：\n图5：\n\n  我们接着继续介绍为什么numericUpdates容器的value是一个LinkedHashMap结构，先给出源码中对这种设计的注释：\n图6：\n\n  注释中讲述了使用LinkedHashMap结构的两个原因：\n\n原因一：在不同的term之间，即LinkedHashMap的不同的key，如果一篇文档中包含的term满足多个软删除操作中的term条件（见文章软删除softDeletes（一）），那么根据LinkedHashMap的插入有序的特点，这篇文档用最后一个软删除操作中的DocValues来描述该文档被软删除了\n原因二：在相同的term之间，如果多次调用的软删除中的term条件是一样的，那么可以进行去重，并且选取作用范围（作用域）最大的那个（下面的例子会介绍）\n\n  我们继续用一个例子来介绍上述的原因一：\n图7：\n\n  图7中，第56、61行执行了两次软删除操作，并且文档0都满足这两个软删除的条件，由于第61行的软删除操作晚于第56行，所以文档0将用61行中的域名为&quot;softDeleteField&quot;、域值为4的NumericDocValues来描述它被软删除了，至于使用这个DocValues来描述被软删除的用途，将在后面的文章中介绍。\n  我们接着通过一个例子介绍上述的原因二：\n图8：\n\n  图8中，第56、64行执行了两次软删除操作，并且他们的软删除条件是一样的，即被软删除的文档中需要包含域名为&quot;abc&quot;、域值为&quot;document1&quot;的信息，那么第56行的软删除对应的删除信息将被第66行的软删除对应的删除信息替换。\n  图8中两次软删除对应在numericUpdates容器中的信息有什么不同，为什么会发生替换？\n  我们直接给出这两次软删除的对应在numericUpdates容器中的信息来做介绍：\n图9：\n\n图10：\n\n  图9、图10中，docIDUpto描述的是删除信息的作用范围（作用域），图9中docIDUpto的值为2，它描述了从删除信息的作用范围为文档号区间为[0, 2)、图10中的docIDUpto的值为5，它描述了从删除信息的作用范围为文档号区间为[0, 4)，故图10中的作用范围包含了图9中的删除范围，这样就能过滤掉相同term条件的软删除操作。\n 注意点\n  上文中我们说到，updateBinaryDocValue( )、updateNumericDocValue( )、updateDocValues( )、softUpdateDocument( )、softUpdateDocuments( )的操作在索引期间使用相同的方式来存储删除信息，这使得我们用softUpdateDocument( )需要注意一个问题，如果该方法中用来描述文档被软删除的DocValues的域名不是IndexWriter的配置中定义的软删除域，那么这次软删除操作就会转变为DocValues的更新操作，我们通过下面的例子来说明：\n图11：\n\n  图11中，如果update为true，那么我们执行软删除操作，注意的是59行用来描述文档被软删除的DocValues的域名&quot;docValuesField&quot;不是38行的设置的软删除域”softDeleteField“，故这次的软删除操作转化为DocValues的更新操作，那么文档1中域名为&quot;docValuesFiled&quot;的NumericDocValuesField的域值&quot;5&quot;会被更新为&quot;3&quot;，故搜索结果在按照&quot;docValuesField&quot;排序之后， 如下所示：\n图12：\n\n  可见，文档1并没有被软删除，图11的demo看这里：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo/src/main/java/lucene/softDeletes/SoftDeletesTest3.java 。\n 结语\n  本文中，我们介绍了Lucene7.5.0版本中，在索引期间跟软删除相关的一些内容，即使用numericUpdates、binaryUpdate两个容器存储删除信息，然而在Lucene8.4.0中，取消了这两个容器，使用了其他的方法存储删除信息，这块内容将在下篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["delete","softDeletes"]},{"title":"软删除softDeletes（五）（Lucene 8.4.0）","url":"/Lucene/Index/2020/0708/%E8%BD%AF%E5%88%A0%E9%99%A4softDeletes%EF%BC%88%E4%BA%94%EF%BC%89/","content":"  在文章软删除softDeletes（二）中介绍了软删除在索引（index）阶段的相关内容，我们接着介绍在flush/commit阶段的内容\n flush/commit\n  在这个阶段，我们首先要介绍的是DWPT在转化为一个段的期间，即下图中用红框标注的流程点，跟软删除相关的内容：\n图1：\n\n点击查看大图\n  上图中，流程点将DWPT中收集的索引信息生成一个段newSegment的介绍见文章文档提交之flush（三），注意的是，图1中的流程点基于版本为Lucene 7.5.0，但是软删除相关的处理时机点跟Lucene 8.4.0是一致的。\n  在红框标注的流程点处理软删除文档中，统计了满足某个条件的文档的数量softDelCountOnFlush，这个条件是：文档中包含了DocValues（NumericDocValuesField或BinaryDocValuesField）的信息，并且DocValues的域名跟软删除的域是相同的（文档也有可能同时满足软删除的条件）。满足该条件的文档都被会认为是软删除的，在文章软删除softDeletes（一）中的第二个例子介绍了这种情况。\n  在红框标注的流程点处理软删除文档中是如何找到所有满足这个条件的文档集合的：\n  在文章索引文件的生成（十五）之dvm&amp;&amp;dvd中，我们介绍了在索引阶段Lucene收集DocValues信息的过程，并且说到使用了DocsWithFieldSet对象收集了文档号，同时介绍了DocsWithFieldSet收集的过程，即每一个DocValues域都有一个DocsWithFieldSet对象，那么DocValues域的域名跟软删除的域如果相同的话，DocsWithFieldSet对象中文档的数量会添加到softDelCountOnFlush中。\n  接着在下图红框标注的流程点更新DocValues域处理软删除的信息。\n图2：\n\n  上文中，DWPT转化为一个段的时候，计算出了softDelCountOnFlush，但是它并不包含满足软删除条件的文档并且文档中不包含跟软删除有相同域名的DocValues信息，那么到了图2中会将这种文档的数量进行统计，统计结果会跟softDelCountOnFlush一起写入到SoftDelCount，而SoftDelCount最终会被写入到用于描述段的索引信息SegmentCommitInfo中，即下图红框标注的字段，在文章索引文件之segments_N中我们知道，当执行了commit()操作后，一个段的索引信息SegmentCommitInfo将被写入到索引文件segments_N中：\n图3：\n\n  在执行图1流程点处理软删除文档之前，已经将TermNode的删除信息作用到了本段，故有些文档可能已经被硬删除了，如果这些文档同时满足上文中的条件，那么softDelCountOnFlush不会统计这篇文档，这里看出硬删除有更高的&quot;删除等级&quot;、这么做的目的也是为了能正确统计一个段中被删除（软删除和硬删除）的文档数量，因为在flush阶段，当一个段被作用了删除信息之后，会判断段中的文档是否都已经被删除了，如果满足会丢弃这个段，判断条件在文章文档提交之flush（六）中的已经作出了介绍，这里简单的给出判断条件：\ndelCount + softDelCount == maxDoc\n  delCount即被硬删除的文档数量、softDelCount即被软删除的文档数量、maxDoc为段中的文档总数。\n 段的合并\n  接着我们介绍在段的合并阶段，跟软删除相关的内容，在文章执行段的合并（三）中我们介绍了下图中红框标注的流程点获取SegmentReader的集合MergeReader，在这个流程点中，会对OneMerge（待合并的段的集合，见文章LogMergePolicy）中的被标记为软删除的文档的总数进行统计，通过读取每个段中的SegmentCommitInfo中的softDelCount获取，即图3中的字段，注意的是，如果被标记为软删除的文档满足其他删除条件，那么这些文档不会被认为是软删除的，最后统计出的数值将作为新段的softDelCount：\n图4：\n\n  其他一些在段的合并中跟软删除相关的内容，我们将结合下文的合并策略SoftDeletesRetentionMergePolicy一起介绍。\n 段的合并策略SoftDeletesRetentionMergePolicy\n  合并策略SoftDeletesRetentionMergePolicy是实现软删除机制最重要的一块，我们先看下源码中关于该策略的介绍，其他的合并策略见文章LogMergePolicy、TieredMergePolicy：\nThis MergePolicy allows to carry over soft deleted documents across merges. The policy wraps the merge reader and marks documents as &quot;live&quot; that have a value in the soft delete field and match the provided query. This allows for instance to keep documents alive based on time or any other constraint in the index. The main purpose for this merge policy is to implement retention policies for document modification to vanish in the index. Using this merge policy allows to control when soft deletes are claimed by merges.\n  我们依次介绍上文中的五段注释。\n 第一段\nThis MergePolicy allows to carry over soft deleted documents across merges. \n  该段注释大意为：使用这个合并策略能使得被标记为软删除的文档在段的合并之后仍然被保留。\n  在文章软删除softDeletes（一）中我们说到，在生成一个段后，段中被硬删除的文档用索引文件.liv描述、被软删除的文档用索引文件.dvd、dvm描述，如果不使用SoftDeletesRetentionMergePolicy，当段被合并后，新的段中不会包含这些被删除的文档，而通过这个合并策略，可以使得被软删除的文档仍然存在与新的段中，但是文档必须满足两个条件，见下文第二段中的描述。\n 第二段\nThe policy wraps the merge reader and marks documents as &quot;live&quot; that have a value in the soft delete field and match the provided query. \n  该段注释大意为：在封装merge reader（即上文图4中的MergeReader）时，使得那些被软删除的文档标记为live状态，即索引文件.liv中能找到这篇文档（如果找不到，说明被删除了，所以这个索引文件也能用来描述被硬删除的文档，见文章工具类之FixedBitSet），不过这些文档必须同时满足下面两个条件：\n\n被软删除的文档中必须包含软删除的域对应的DocValues信息\n被软删除的文档必须满足Query的查询条件\n\n该条件在构造SoftDeletesRetentionMergePolicy对象时提供，即参数retentionQuerySupplier中定义的Query：\n\n\n\npublic SoftDeletesRetentionMergePolicy(String field, Supplier&lt;Query&gt; retentionQuerySupplier, MergePolicy in)&#123;    ... ...&#125;\n 第三段\nThis allows for instance to keep documents alive based on time or any other constraint in the index. \n  该段注释大意为：使用这个合并策略能使得基于时间或者其他限制方法来搜索索引中的文档。\n  即通过参数Supplier&lt;Query&gt; retentionQuerySupplier来定义限制方式，使得搜到满足Query并且被标记为软删除的文档、比如我们可以一个限制方式：24小时内被软删除的文档。那么在执行搜索时，当前时间前24小时内被软删除的文档可以被搜索到。\n 第四段\nThe main purpose for this merge policy is to implement retention policies for document modification to vanish in the index. \n  该段注释大意为：设计这个合并策略的主要目的就是为了实现保留策略（retention policies）。\n  参考文章软删除softDeletes（一）中第三个例子，也可以直接查看这个 demo：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/softDeletes/HistoryRetention.java 。\n 第五段\nUsing this merge policy allows to control when soft deletes are claimed by merges.\n  该段注释大意为：使用这个合并策略使得在合并期间，能控制何时处理被标记为软删除的文档。\n  我们通过几个例子来介绍上述的SoftDeletesRetentionMergePolicy的使用方式以及实现原理。\n 第一个例子\n图5：\n\n  图5的完整demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/softDeletes/SoftDeletesTest8.java 。\n  图5中第39、40行红框标注定义了软删除对应的域名，在第42行蓝色标注定义了querySupplier，它将作为合并策略SoftDeletesRetentionMergePolicy的参数Supplier&lt;Query&gt; retentionQuerySupplier。\n  在71行执行了软删除的操作，如果文档中包含了域名为&quot;author&quot;、域值为&quot;D0&quot;的信息（绿色标注），那么这些文档都将被标记为软删除的，显而易见、灰色标注的两篇文档（第一个段中的文档0、文档1）满足软删除的条件，接着在72行、77行生成了两个段，最后在第78行执行了段的合并，强制合并为一个段。\n  由于在第43行，我们定义了一个合并策略SoftDeletesRetentionMergePolicy，满足条件的被软删除的文档在段的合并之后仍将存在与索引之中（第一段注释），这些文档必须满足两个条件（第二段注释），在图5的例子中，这两个条件如下所示：\n\n被软删除的文档中必须包含软删除的域对应的DocValues信息\n\n图5中灰色标注的两篇文档（第一个段中的文档0、文档1）由于满足软删除的条件，所以这两篇文档间接被认为是包含软删除的域对应的DocValues信息的，而白色标注的文档（第一个段中的文档2），它虽然不满足第71行软删除的条件，但是它直接包含了软删除对应的域名，所以满足当前条件（上文中，我们介绍了这种文档也是被认为软删除的）\n\n\n被软删除的文档必须满足Query的查询条件\n\n图5中第42行定义了Query的查询条件，即被软删除的文档中必须包含域名为&quot;sex&quot;、域值为&quot;male&quot;的信息\n\n\n\n  所以结合上述的两个条件，满足第一个条件的有三篇文档，而同时满足第二个条件后只有两篇文档，故图5的reader中，能找到这两篇文档（第一个段中的文档0、文档1）的信息，即第82、83行、85行分别为这两篇文档的信息。接着看第80、81行的代码，其中reader.numDocs()描述的是索引中没有标记为删除（软删除跟硬删除）的文档数量，而reader.maxDoc()描述的是索引中所有的文档（包含被标记为删除的文档），由于图5中没有硬删除的操作，所以reader.maxDoc()与reader.numDocs()的差值为2，正是说明了只有两篇被软删除的文档在段的合并后仍然在索引中，并且有一篇被软删除的文档在段的合并之后实现了真正的物理删除（对于当前的reader）。\n 流程图\n  找出那些在段的合并后依然保留在索引中的被软删除的文档的流程图以及时机点如下所示：\n图6：\n\n 是否还有未处理的段？\n图7：\n\n  执行段的合并时，依次处理待合并的段，直到所有的段处理结束。\n 段中是否有删除信息？\n图8：\n\n  只有段中有删除信息才有执行当前流程的必要性，删除信息包括软删除和硬删除，通过读取段中的liveDocs（FixedBitSet对象，见文章工具类之FixedBitSet的介绍）信息判断段中是否有删除信息，liveDocs描述了在内存中段中的删除信息，对于图5中的第一个段，由于段中一共5篇文档，其中3篇文档被软删除的，对应的liveDocs如下所示：\n图9：\n\n  图9的数组中，下标值用来描述段内文档号、数组元素使用0跟1来描述文档是否被删除的，对于第二个段，它没有删除信息，故不需要执行剩余的其他流程点。\n 生成查询条件BooleanQuery\n图10：\n\n  在当前流程点将两个查询条件作为一个组合查询BooleanQuery，这两个查询条件即上文中第二段提到的内容，对于第一个条件通过索引文件.dvm、dvd来查询，而第二个条件则是TermQuery的方法，这里不赘述，在前面的文章中已经介绍过了。\n 文档号添加到liveDocs\n图11：\n\n  当找出满足条件的文档号后，那么更新liveDocs即可，对于图5的例子，第一个段中的两篇被软删除的文档1、文档2的文档号将被条件到liveDocs中，如下所示：\n图12：\n\n  最后所有段的liveDocs信息都会传递给新段。\n 结语\n  基于篇幅，剩余的内容将在下一篇文章中展开。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["delete","softDeletes"]},{"title":"软删除softDeletes（六）（Lucene 8.4.0）","url":"/Lucene/Index/2020/0709/%E8%BD%AF%E5%88%A0%E9%99%A4softDeletes%EF%BC%88%E5%85%AD%EF%BC%89/","content":"  我们接着文章软删除softDeletes（五）继续介绍合并策略SoftDeletesRetentionMergePolicy，在文章近实时搜索NRT（一）中使用方法三&amp;&amp;方法四获取StandardDirectoryReader和文章文档提交之flush（八）中执行流程点更新ReaderPool的流程图时，会判断一个段中的文档是否都被删除（软删除跟硬删除），如果为真，那么这个段对应的索引文件，也就是索引信息将从索引目录中物理删除（如果没有其他reader占用的话），但是如果使用了合并策略SoftDeletesRetentionMergePolicy，那么上述的两个场景也不会删除这个段，我们通过下面的例子来展开介绍。\n图1：\n\n  图1完整的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/softDeletes/SoftDeletesTest9.java 。\n  图1中第62行执行了软删除的操作（红框），那么包含域名为&quot;author&quot;、域值为&quot;D0&quot;的文档将被软删除，故文档0将被标记为软删除的，同时添加了一篇新的文档newDoc，即文档1，随后第63行执行了硬删除（蓝色），那么newDoc也会被删除，最终在第64行执行commit()后，生成一个段，这个段中仅有的两篇文档都是被删除的。\n  如果不使用该合并策略，即图1中useSoftMergePolice为false，那么由于这个段中所有的文档都是被删除的，故reader的两个方法numDoc()、maxDoc()都是0，同时索引目录中的索引文件如下所示：\n图2：\n\n  可见索引目录中没有跟数据相关的索引文件，也就是说刚刚生成的段被物理删除了。如果我们使用了第67行的合并策略，那么索引文件就不会被删除：\n图3：\n\n  同时图1中第67、68行，reader.numDoc()的值为0，描述的是段中的文档都是标记为删除的（软删除或硬删除）；reader.maxDoc()的值为2，描述了段中的文档的数量（包含删除跟未删除的）。同时我们可以看出，这个合并策略的功能不是局限于执行段的合并时候才发挥作用。\n  上述的差异主要是合并策略SoftDeletesRetentionMergePolicy中实现了下面的这个方法：\n图4：\n\n  实现原理很简单，即通过定义一个Query，例如图5中第46行的querySupplier，然后根据Query去段中查找，如果至少找到一篇满足查询要求的文档，那么就不会删除这个段，注意的是满足查询要求的文档既可以是被硬删除的也可以是被软删除的。\n  看这里有些同学可能会有疑问，好像上述的机制跟软删除没有什么关系，也就说如果段中文档都是被硬删除的，甚至IndexWriterConfig不设置软删除的域名，是否也能实现上述的功能呢，看下面的例子：\n图5：\n\n  图5完整的demo见：https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/softDeletes/SoftDeletesTest10.java 。\n  图5中第50、51、52的代码明确了我们没有设置软删除域，并且在第64、65执行了硬删除，即文档0、文档4都将被硬删除，然后使用了第45行定义的合并策略后，这个段仍然是被保留的。不过在常用的使用场景下没有什么实际意义，因为被硬删除的文档号被记录在索引文件.liv中，即使这个段被保留了下来，使用IndexSearcher.search()时候查询的过程中，这些文档也会被过滤掉（硬删除的文档满足查询条件，但是这篇文档的文档号在传给Collector之前会使用Bits对象过滤，Bits对象为索引文件.liv在内存中的描述方式）。\n  在文章软删除softDeletes（五）中我们对SoftDeletesRetentionMergePolicy在源码中的注释划分了五段进行介绍，剩余未介绍还有第三段、第四段、第五段，其中第三、四段的例子在文章软删除softDeletes（一）的第三个例子中介绍故了，不赘述，我们仅仅看下第五段的注释。\n 第五段\nUsing this merge policy allows to control when soft deletes are claimed by merges.\n  该段注释大意为：使用这个合并策略使得在合并期间，能控制何时处理被标记为软删除的文档。\n  该段注释实际是对前四段注释的总结，从上文以及前面的文章中我们可以看出，控制软删除文档处理时机主要依靠这个合并策略的Supplier&lt;Query&gt; retentionQuerySupplier参数，它可以用来控制被软删除的文档在合并后是否能继续保留在新的段中，控制软删除的文档的有效期（History retention）等。\n numDeletesToMerge\n  numDeletesToMerge描述的是一个段在合并之后，段中被删除的文档将被处理（claim）的数量。\n  由于SoftDeletesRetentionMergePolicy这个对象并没有真正的合并逻辑（即生成OneMerger对象，见文章LogMergePolicy），它只是封装了其他的合并策略，实现扩展功能，numDeletesToMerge作为实现扩展的功能之一，它影响了其他的合并策略的合并逻辑，例如如果封装了合并策略LogMergePolicy，该策略是根据段的大小（Segment Size）挑选待合并的段，Segment Size的定义通过下面两种方式来描述：\n\n文档数量：一个段中的文档数量可以用来描述段的大小，例如LogDocMergePolicy（LogMergePolicy的子类）就使用了该方法来计算段的大小\n索引文件大小：一个段中包含的所有的索引文件大小总和，在Lucene7.5.0版本中除了LogDocMergePolicy，其他的合并策略都使用该方法\n\n  当使用文档数量作为Segment Size时，被删除的文档也会参数文档数量的计算，而SoftDeletesRetentionMergePolicy则可以根据需要更改numDeletesToMerge的值，从而影响Segment Size。实现方式跟之前文章介绍过的一样，即通过合并策略的Supplier&lt;Query&gt; retentionQuerySupplier参数来更改numDeletesToMerge。\n 结语\n  关于软删除的内容暂时就介绍这么多，欢迎各位同学指出文章中的错误。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["delete","softDeletes"]},{"title":"软删除softDeletes（一）（Lucene 8.4.0）","url":"/Lucene/Index/2020/0616/%E8%BD%AF%E5%88%A0%E9%99%A4softDeletes%EF%BC%88%E4%B8%80%EF%BC%89/","content":"  在文章文档的增删改（一）中我们介绍了Lucene中提供的几个接口用来实现文档的更新：\n\nupdateDocument(Term term, Iterable&lt;? extends IndexableField&gt; doc)\nupdateDocuments(Term delTerm, Iterable&lt;? extends Iterable&lt;? extends IndexableField&gt;&gt; docs)\n\n  对于上述的两个更新操作，实际的处理过程为：先删除、后添加，其中删除在源码中用hardDelete描述，即硬删除（或者可以翻译为实删除），该过程为先删除包含term的文档，然后添加新的文档doc或批量添加新的文档集合docs。\n\nsoftUpdateDocument(Term term, Iterable&lt;? extends IndexableField&gt; doc, Field… softDeletes)\nsoftUpdateDocuments(Term term, Iterable&lt;? extends Iterable&lt;? extends IndexableField&gt;&gt; docs, Field… softDeletes)\n\n  对于上述的两个更新操作，实际的逻辑过程为：先标记、后添加，其中标记在源码中用softDeletes描述，即软删除，该过程为先标记包含term的文档，这些文档使用域名为softDeletes的DocValues域（为什么softDeletes参数允许多个，下文中会介绍）来描述该文档被软删除了，然后添加新的文档doc或批量添加新的文档集合docs。\n 软删除跟硬删除的差异\n  我们接着从使用方式、被删除的文档的描述方式、被删除的的文档的生命周期这几个方面来描述两者的差异。\n 使用方式\n\n硬删除：不使用任何使用方式\n软删除：需要在构造IndexWriter对象时指定一个软删除的域名softDeletesField，即在设置IndexWriter的配置信息IndexWriterConfig时通过调用IndexWriterConfig类提供的setSoftDeletesField方法来指定一个软删除的域名softDeletesField，如下所示，另外设置IndexWriter的配置信息IndexWriterConfig的内容可以查看文章构造IndexWriter对象（一）：\n\n图1：\n\n 被删除的文档的描述方式\n\n硬删除：被删除的文档对应的文档号用索引文件.liv来描述。\n软删除：被标记为删除的文档不使用索引文件.liv来描述，而是通过索引文件.dvd、dvm来描述，其具体介绍将在后面的内容中展开\n\n 被删除的文档的生命周期\n\n硬删除：在执行段的合并之前，被删除的文档信息仍然在索引文件中，此时执行查询操作，如果被删除的文档满足查询条件，在查询的过程中仍然能获取这个文档号，只是在随后的逻辑中，通过索引文件.liv来实现过滤，使得不让这个文档号传递给Collector，并且最终根据段的合并策略，被删除的文档才会在合并的过程中实现物理删除，即索引文件中不存在该文档的信息。\n软删除：在执行段的合并之前，被标记为删除的文档信息仍然在索引文件中，此时执行查询操作，如果被软删除的文档满足查询条件，在查询的过程中仍然能获取到这个文档，只是在随后的逻辑中，通过索引文件.dvm&amp;&amp;.dvd来实现过滤，使得不让这个文档号传递给Collector，与硬删除不同的是，软删除可以通过使用合并策略SoftDeletesRetentionMergePolicy使得在执行段的合并之后，这些被删除的文档仍然被保留在索引文件中，并且可以被搜索到；同样的还是通过配置合并策略SoftDeletesRetentionMergePolicy使得这些被删除的文档无法被搜索到，或者使这些被删除的文档在下一次合并中同硬删除一样，实现物理删除，合并策略SoftDeletesRetentionMergePolicy的介绍将在后面的内容中展开\n\n 例子\n  接着我们通过几个例子，先从功能上来简单了解下软删除的功能。\n 第一个例子\n图2：\n\n图3：\n\n图4：\n\n  图2中的demo请点击 https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/softDeletes/SoftDeletesTest1.java 查看。\n  图2中的第63行，执行了软删除的操作，这个操作将软删除所有包含域名为&quot;abc&quot;、域值为&quot;document3&quot;的文档，并且添加一篇新的文档，即文档3，文档3中包含了一条域名为&quot;abc&quot;、域值为&quot;document3&quot;的信息。\n  图4中为执行完图2的代码后索引目录中的内容，由于图2中我们执行了两次commit操作，所以会生成2个段，其中以&quot;_0&quot;为前缀的索引文件描述是图2中文档0跟文档1的信息，可以看出这两篇文档中并没有添加DocValues的信息，但是索引目录中却有_0_1_Lucene80_0.dvd、_0_1_Lucene80_0.dvm这两个索引文件，它们正是用来描述软删除的信息，描述了文档1被标记为软删除的文档；另外图2中第64行，softUpdateDocument(…)的操作添加了一个newDoc，即文档3，它也满足软删除的要求，但由于softUpdateDocument(…)的执行逻辑为先先标记、后添加，故软删除的操作不会作用（apply）到文档3，从图4中的索引目录也可以看出，并没有.dvm&amp;&amp;.dvd的索引文件。\n  接着我们给出硬删除的例子作为对比：\n图5：\n\n图6：\n\n图7：\n\n  图5中，在第66行执行了硬删除的操作，这个操作将硬删除所有包含域名为&quot;abc&quot;、域值为&quot;document3&quot;的文档，并且添加一篇新的文档，即文档3，文档3中包含了一条域名为&quot;abc&quot;、域值为&quot;document3&quot;的信息，从图6的搜索结果可以看出，文档1满足删除要求，并且用图7索引目录中的索引文件_0_1.liv来描述被硬删除的文档1；同样地，updateDocument(…)的执行逻辑为先删除、后添加，故第66行的硬删除操作不会作用到文档3。\n 第二个例子\n图8：\n\n图9：\n\n图10：\n\n  图8中的demo请点击 https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/softDeletes/SoftDeletesTest2.java 查看。\n  在图8的例子中，代码第59、69行执行了两次commit()操作，故在图10的索引目录中会生成两个段的索引文件，其中以”_0“为前缀的索引文件描述的是第一次commit()对应的文档信息，以&quot;_1&quot;为前缀的索引文件描述的是第二次commit()对应的文档信息。\n  在图8的第48行，文档0中添加了一个域名为&quot;softDeleteField&quot;的NumericDocValuesField域，由于在第41行设置的软删除的域名也为&quot;softDeleteField&quot;，并且在第67行执行软删除时也使用了NumericDocValues来标记被删除的文档，使得在图9中的搜索结果中，文档0也被认为是被软删除的，尽管它没有满足第67行softUpdateDocument(…)的删除条件：包含域名为&quot;abc&quot;、域值为&quot;document3&quot;的文档。\n  这个例子是为了想说明，软删除的机制可以理解为：软删除使用DocValues来标记那些满足删除条件（即softUpdateDocument方法的第一个参数term）的文档，&quot;标记&quot;的过程就是为那些文档增加一个DocValues域。\n  所以图8中由于文档0使用了软删除对应的域名，使得文档0也被认为是删除。\n  我们继续观察图10中索引目录中的索引文件，_0_Lucene80_0.dvm、_0_Lucene80_0.dvd这两个文件是在图8中第59行第一次commit()的时候生成的，它们描述的是文档0中的DocValues域的信息，在文章索引文件的生成（十五）之dvm&amp;&amp;dvd的系列文章中介绍了生成这两个索引文件的过程，不赘述；而对于_0_1_Lucene80_0.dvm、_0_1_Lucene80_0.dvd这两个索引文件则是由于执行了第67行的软删除操作后用来描述第一个段中的被软删除的文档信息，原因是在生成第二个段的时候，需要将删除信息作用到已生成的段，即第一个段，如果这里看不明白，请先阅读系列文章文档提交之flush 😂。\n 第三个例子\n  在第二个例子中，我们了解到，一篇文档中只要包含了至少一个DocValues域，并且这个域的域名为软删除对应的域名，这篇文档就被认为是软删除的，基于这个知识点就可以开始介绍如何使用软删除机制，比如在elasticsearch中使用软删除实现的一个机制，即history retention。\n  elasticsearch中关于history retention的文档见： https://www.elastic.co/guide/en/elasticsearch/reference/7.7/index-modules-history-retention.html 。\n  同样的我们通过Lucene的例子来介绍如何实现history retention，demo见：\n图11：\n\n  图11中的demo请点击 https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.4.0/src/main/java/io/softDeletes/HistoryRetention.java 查看。\n  在系列文章执行段的合并我们说到，在段的合并过程中，被硬删除和软删除的文档的索引信息将实现物理删除，即对应的索引信息将从索引文件中删除，图11中的第47行，我们定义了一个段的合并策略SoftDeletesRetentionMergePolicy，该策略用来描述在执行段的合并期间的一个规则，该规则描述了满足条件的被软删除的文档在合并过程中不会被物理删除，使得在合并后仍然能被搜索到，该条件即第46行的docsOfLast24Hours，该条件描述的是保留过去24小时被软删除的文档。\n  接着我们看图11中的文档，如果使用硬删除，即useSoftDelete的值为false，那么在合并后，reader中只包含一篇文档，即包含域名为&quot;version&quot;、域值为&quot;5&quot;的那篇文档，即图11中的第121~125代码，可见，使用硬删除以后，无法保留被删除的文档信息；当使用了软删除，即useSoftDelete的值为true，除了包含域名为&quot;version&quot;、域值为&quot;5&quot;的那篇文档，其他所有的文档都被软删除了，并且由于合并策略SoftDeletesRetentionMergePolicy的条件为保留过去24小时被软删除的文档，所以那些包含域名为&quot;creation_date&quot;，域值在24小时的被软删除的文档将不会在合并期间物理删除，即能搜索到，可见，包含域名为&quot;version&quot;，域值为5、4、3的文档，他们包含的creation_date都在24小时内，故能被搜索到，而域名为&quot;version&quot;，域值为2、1的被软删除的文档，它们的&quot;creation_data&quot;分别为28小时、26小时，故在段的合并之后，这两篇文档被物理删除了。\n  故上述描述的就是保留24小时删除记录的history retention。\n 结语\n  本篇文章简单介绍了软删除的一些概念以及history retention的实现方式，在随后的系列文章中将会介绍，合并策略SoftDeletesRetentionMergePolicy保留满足条件的被软删除的文档的原理以及在索引期间、flush期间、合并期间跟软删除相关的内容。\n点击下载附件\n","categories":["Lucene","Index"],"tags":["delete","softDeletes"]},{"title":"软删除softDeletes（四）（Lucene 8.4.0）","url":"/Lucene/Index/2020/0629/%E8%BD%AF%E5%88%A0%E9%99%A4softDeletes%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"  在文章软删除softDeletes（二）中我们说到，在Lucene 7.5.0版本中，使用了下面两个容器来存储软删除的删除信息、DocValues的更新信息：\n\nMap&lt;String,LinkedHashMap&lt;Term,NumericDocValuesUpdate&gt;&gt; numericUpdates：DocValuesUpdatesNode\nMap&lt;String,LinkedHashMap&lt;Term,BinaryDocValuesUpdate&gt;&gt; binaryUpdate：DocValuesUpdatesNode\n\n  而从Lucene 7.7.0版本之后，使用了下面的一个容器来优化存储：\n\nfinal Map&lt;String, FieldUpdatesBuffer&gt; fieldUpdates = new HashMap&lt;&gt;();\n\n 为什么使用FieldUpdatesBuffer类存储\n  在介绍这两种存储的差异前，我们先通过源码中的注释来介绍下改用FieldUpdatesBuffer来存储完善后的删除结点的目的：\nThis class efficiently buffers numeric and binary field updates and stores terms, values and metadata in a memory efficient way without creating large amounts of objects. Update terms are stored without de-duplicating the update term.In general we try to optimize for several use-cases. For instance we try to use constant space for update terms field since the common case always updates on the same field. Also for docUpTo we try to optimize for the case when updates should be applied to all docs ie. docUpTo=Integer.MAX_VALUE. In other cases each update will likely have a different docUpTo.Along the same lines this impl optimizes the case when all updates have a value. Lastly, if all updates share the same value for a numeric field we only store the value once.\n 第一段\n  上文第一段的大意是，使用FieldUpdatesBuffer类代替numericUpdates/binaryUpdate存储能降低内存开销，同时Term中的域值不使用de-duplicating存储。\n\n降低内存开销：直接给出两种存储方式的内存占用情况，下图出自issue：https://issues.apache.org/jira/browse/LUCENE-8590\n\n图1：\n\n  图1中sameFiled、sameValue、sameDocUpTo、RandomDocUpTo的概念在下文中介绍。\n\n不使用de-duplicating存储：即不使用重复数据消除，同样源码中给出了不使用de-duplicating存储的原因：\n\nwe use a very simple approach and store the update term values without de-duplication which is also not a common case to keep updating the same value more than once...we might pay a higher price in terms of memory in certain cases but will gain on CPU for those. We also save on not needing to sort in order to apply the terms in order since by definition we store them in order.\n  上文注释可以看出一方面作者认为在实际使用过程中，每次更新的操作的条件（条件即updateDocument()、softUpdateDocument()方法的第一个参数Term）一般情况下都是不同的（注意的是Term由域名跟域值组成，两者都相同才认为是相同的Term），这里作者实际指的是Term中的域值的不同；另一方面作用（apply）删除信息顺序是依据添加顺序的，所以不用对Term中的域值进行排序，而de-duplicating存储在Lucene的应用中，很重要的一步就是排序，因为只有排序才能更好的提取出冗余的数据，例如前缀存储。\n 第二段\n  上文第二段的大意是：使用FieldUpdatesBuffer类优化存储的设计初衷是对这两个的优化：更新/删除的操作的条件通常是相同的域名，即Term的域名以及docUpTo为Integer.MAX_VALUE时的优化。\n 第三段\n  上文第二段的大意是：如果updateDocument()、softUpdateDocument()方法等更新方法中指定的DocValues域的域值为相同的，那么只需要存储一次。\n 如何使用FieldUpdatesBuffer类优化存储\n  我们先给出FieldUpdatesBuffer类的主要成员：\n图2：\n\n  接着我们通过例子来介绍如何实现优化存储。\n图3：\n\n  图3中执行了三次软删除后，FieldUpdatesBuffer对象中的内容如下所示：\n图4：\n\n  图4中，红框标注的部分描述的是在构造FieldUpdatesBuffer对象期间，就将第一个删除信息作为类中相关的信息的初始值，我们先看数组下标区间[3、10]的数组元素，这就是上文中第一段提到的存储图3的三个软删除的条件，即Term的域值不使用de-duplicating。\n图5：\n\n  图5中红框标注的部分为存储Term的域名，即上文中第二段提到的，当多个软删除、DocValues更新的条件，如果条件Term的域名一样时（上文中的sameFiled），域名只需要存储一次。\n图6：\n\n  图6中，由于图3的例子中，用相同的NumericDocValue来描述被删除的文档时，NumericDocValueFiled的域值都是&quot;3&quot;、那么正如上文中第三段讲的那样，只存储一次。\n  最后我们在介绍下上文中第二段介绍的关于docUpTo为Integer.MAX_VALUE时的优化，在文章软删除softDeletes（三）我们说到，存在一个全局的删除信息，当执行了主动flush后，这些删除信息将作用（apply）索引目录中已经存在的段中的文档，所以对应的FieldUpdatesBuffer中的docsUpTo永远是Integer.MAX_VALUE（上文中的sameDocUpTo），即作用对象是所有的文档，同样以图3为例，使用FieldUpdatesBuffer存储后如下所示：\n图7：\n\n  从图7可以看出，docsUpTo只需要存储一次，而如果使用Lucene 7.5.0版本中的numericUpdates存储，对于图3的例子，docsUpTo需要存储三次，因为三次软删除的条件Term是不相同的（Term的域名或域值至少一个不相同），故需要对于Map&lt;String,LinkedHashMap&lt;Term,NumericDocValuesUpdate&gt;&gt; numericUpdates，LinkedHashMap中需要存储三个NumericDocValuesUpdate，即存储了三次docsUpTo为Integer.MAX_VALUE的值。\n 结语\n  无\n点击下载附件\n","categories":["Lucene","Index"],"tags":["delete","softDeletes"]}]