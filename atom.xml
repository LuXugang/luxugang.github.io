<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-05-14T15:31:10.260Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Vector Quantization（一）（Lucene 9.10.0）</title>
    <link href="http://example.com/Lucene/gongjulei/2024/0513/VectorQuantization%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/gongjulei/2024/0513/VectorQuantization%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2024-05-12T16:00:00.000Z</published>
    <updated>2024-05-14T15:31:10.260Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章介绍下截止到9.10.0版本，Lucene中向量量化（Vector Quantization）技术相关的内容。</p><h2 id="lucene中为什么要引入vq"><a class="markdownIt-Anchor" href="#lucene中为什么要引入vq"></a> Lucene中为什么要引入VQ</h2><p>Lucene中使用<a class="link"   href="https://amazingkoala.com.cn/Lucene/Index/2024/0118/HNSW%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/" >HNSW<i class="fas fa-external-link-alt"></i></a> (Hierarchical Navigable Small World) 实现了高维数据的搜索，引入VQ基于以下几个方面的考虑：</p><ul><li><strong>实际数据的需求</strong>：对于给定的一个数据集，embeddings的每一个维度并不真正需要所有可能的亿级别的可选数值，这意味着每个维度的实际变化范围远小于float32类型所能提供的范围。</li><li><strong>数据的高保真（fidelity）和浪费问题</strong>：尽管浮点数类型可以提供最高的数据保真度（即数据的准确和细腻度），但在许多实际应用中，这种高保真度是过剩的。这是因为嵌入向量中真正重要的信息通常并不需要如此高的精度和如此多的数值选项。</li></ul><p>当然这通常是以“有损”的方式进行的，意味着在处理过程中会丢失一些原始数据信息，但与此同时，它能显著减少存储数据所需的空间。</p><h2 id="实现原理"><a class="markdownIt-Anchor" href="#实现原理"></a> 实现原理</h2><p>VQ的实现方式参考的是<strong>qdrant</strong>的子项目<a class="link"   href="https://github.com/qdrant/quantization" >quantization<i class="fas fa-external-link-alt"></i></a>，在Lucene源码中核心代码为一个名为<strong>ScalarQuantizer</strong>类，它提供了以下两个关键的功能：float32-&gt;int8、校正偏移（Corrective Offset）。</p><h3 id="float32-int8"><a class="markdownIt-Anchor" href="#float32-int8"></a> float32-&gt;int8</h3><p>float32-&gt;int8描述的是将32位的浮点型数值用一个8位的整数表示，即使用[0, 127]的整数来代替浮点数，最终用<code>byte</code>类型存储。其量化公式也不是很复杂，由于在flush阶段已知了向量数据集中的最大跟最小值，因此使用了<a class="link"   href="https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)" >min-max normalization <i class="fas fa-external-link-alt"></i></a>方法。</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/VectorQuantization/VectorQuantization（一）/1.png"><h4 id="向量距离"><a class="markdownIt-Anchor" href="#向量距离"></a> 向量距离</h4><p>我们进一步看下，当使用了上文的量化公式后，在计算向量距离时有什么新的发现。</p><p>下图为两个float32向量中某一个维度的两个浮点数的乘积：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/VectorQuantization/VectorQuantization（一）/2.png"><p>如果我们使用<code>α</code>表示<code>(max - min) / 127</code>，那么上述算式就变为：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/VectorQuantization/VectorQuantization（一）/3.png"><p>以点积（<a class="link"   href="https://en.wikipedia.org/wiki/Dot_product" >dot_product<i class="fas fa-external-link-alt"></i></a>）为例，如果向量的维度为<code>dim</code>，那么向量距离就是这<code>dim</code>个浮点数的乘积的和值，如下所示：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/VectorQuantization/VectorQuantization（一）/4.png"><p>从图4可以看出，两个量化后的float32向量除了<code>dotProduct(int8, int8')</code>部分，其他部分都可以提前计算，可以直接存放在索引中，或者在查询期间只计算一次。（见<a class="link"   href="https://github.com/qdrant/quantization" >quantization<i class="fas fa-external-link-alt"></i></a>中如何计算存储）。 然而在Lucene的实现中，我们只需要计算量化后的向量距离，而图4中其他部分对于打分的影响则是通过校正偏移来实现。</p><h3 id="校正偏移corrective-offset"><a class="markdownIt-Anchor" href="#校正偏移corrective-offset"></a> 校正偏移（Corrective Offset）</h3><p>由于图4中只计算了量化后的距离，因此相比较量化前的距离，是存在精度损失的。校正偏移用于调整最终的文档打分（注意的是，在遍历图中节点时，两个节点的距离计算只使用点积或其他相关度算法，不需要考虑校正偏移，另外简单提下，最终的文档打分也需要考虑查询向量在量化后的校正偏移值），在对向量进行量化期间实现，向量的每个维度在float32转化为int8时都会产生校正偏移值，并且这个向量总的校正偏移值是每个维度的校正偏移值总和。</p><p>校正偏移的定义不在本文中展开，可以见该部分源码<a class="link"   href="https://www.elastic.co/search-labs/author/benjamin-trent" >作者<i class="fas fa-external-link-alt"></i></a>的这几篇文章的介绍：<a class="link"   href="https://www.elastic.co/search-labs/blog/vector-db-optimized-scalar-quantization" >Scalar Quantization Optimized for Vector Databases<i class="fas fa-external-link-alt"></i></a>。</p><p>下图描述的是<strong>量化向量</strong>跟<strong>校正偏移</strong>在<a class="link"   href="https://amazingkoala.com.cn/Lucene/suoyinwenjian/2023/1225/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/" >索引文件<i class="fas fa-external-link-alt"></i></a>中的位置：</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/VectorQuantization/VectorQuantization（一）/5.png"><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>截止到2024年5月31日，Lucene的量化技术进一步实现了float32-&gt;int4（还未正式发布），即32位的浮点数量化为4位的整数，整数范围为[0~15]。其量化过程跟float32-&gt;int4是一致的，由于使用byte数组存储，因此对于int4（即一个字节可以存放两个int4）还可以进一步压缩存储，如下图所以：</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/VectorQuantization/VectorQuantization（一）/6.png"><p>实现代码很简单，因此直接贴出：</p><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/VectorQuantization/VectorQuantization（一）/7.png">]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本篇文章介绍下截止到9.10.0版本，Lucene中向量量化（Vector Quantization）技术相关的内容。&lt;/p&gt;
&lt;h2 id=&quot;lucene中为什么要引入vq&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#lucene中为什么要引</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="gongjulei" scheme="http://example.com/categories/Lucene/gongjulei/"/>
    
    
    <category term="hnsw" scheme="http://example.com/tags/hnsw/"/>
    
    <category term="sq" scheme="http://example.com/tags/sq/"/>
    
    <category term="scalar" scheme="http://example.com/tags/scalar/"/>
    
    <category term="quantization" scheme="http://example.com/tags/quantization/"/>
    
    <category term="vector" scheme="http://example.com/tags/vector/"/>
    
  </entry>
  
  <entry>
    <title>Constructing an HNSW Graph（Lucene 9.8.0）</title>
    <link href="http://example.com/Lucene/Index/2024/0126/ConstructinganHNSWGraph-html/"/>
    <id>http://example.com/Lucene/Index/2024/0126/ConstructinganHNSWGraph-html/</id>
    <published>2024-01-25T16:00:00.000Z</published>
    <updated>2024-01-26T10:07:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>Lucene has implemented the HNSW (Hierarchical Navigable Small World) logic based on the paper ‘<a class="link"   href="https://arxiv.org/abs/1603.09320" >Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs [2018]<i class="fas fa-external-link-alt"></i></a>.’ This article, in conjunction with the Lucene source code, introduces the implementation details during the construction process.</p><h2 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h2><p>Figure 1：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/1.png"  width="300"><p>Let’s first introduce some basic knowledge about the completed HNSW (Hierarchical Navigable Small World) graph, briefly explained through Figure 1 from the paper:</p><ol><li><strong>Hierarchical Structure</strong>: The HNSW graph has a multi-layered structure, where each layer is an independent graph. The number of layers is usually determined based on the size and complexity of the dataset. At the top layer (as shown in Figure 1 as layer=2), there are fewer nodes, but each node covers a broader range. Conversely, at the bottom layer (layer=0), there are more nodes, but each node covers a relatively smaller area.</li><li><strong>Node Connections</strong>: In each layer, nodes are connected to their neighbors through edges. These connections are based on distance or similarity metrics, meaning each node tends to connect with other nodes closest to it.</li><li><strong>Neighbor Selection</strong>: The choice of which nodes to consider as neighbors is based on certain heuristic rules aimed at balancing search efficiency and accuracy. Typically, this involves maintaining the <strong>diversity</strong>of neighbors and limiting the number of neighbors for each node.</li><li><strong>Search Path</strong>: Both the construction and querying of the HNSW graph involve a search process. The search path starts from a <strong>global entry node</strong> at the highest layer and then descends layer by layer until reaching the bottom layer. In each layer, the search follows the layer’s connection structure to find nodes closest to the target node.</li></ol><h2 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h2><p>To introduce the construction of the HNSW graph in Lucene, we will explain it through the process of adding/inserting a new node:</p><p>Figure 2：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/2.png"  width="800"><h3 id="new-node"><a class="markdownIt-Anchor" href="#new-node"></a> New Node</h3><p>Figure 3：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/3.png"  width="500"><p>In Lucene, each Document can only have one vector with the same field name, and each vector is assigned a node identifier (nodeId) in the order they are added. This nodeId is a value that starts from 0 and increases incrementally. In the source code, the new node is represented by this nodeId. When we need to calculate the distance between two nodes, we can find the corresponding vector values of the nodes through this mapping relationship for calculation.</p><h3 id="calculating-the-target-level"><a class="markdownIt-Anchor" href="#calculating-the-target-level"></a> Calculating the Target Level</h3><p>Figure 4：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/4.png"  width="500"><p>The target level is calculated as a random number that follows an exponential distribution.</p><p>The HNSW graph has a multi-layered structure, and the target level determines in which layers the new node will be added and establish connections with other nodes.</p><p>For example, if the target level is 3, then the node will be added to layers 3, 2, 1, and 0, respectively.</p><h4 id="calculation-formula"><a class="markdownIt-Anchor" href="#calculation-formula"></a> Calculation Formula</h4><p>In the source code, the target level is calculated using the formula: <code>-ln(unif(0,1)) * ml</code>, where <code>unif(0,1)</code> represents a uniformly distributed random value between 0 and 1, and <code>ml</code> is defined as <code>1/ln(M)</code>, where <code>M</code>(defined in source code as <code>maxConn</code>) is the maximum number of connections, i.e., a node can connect with up to M other nodes(<code>2*M</code> at the bottom). In the source code, the default value for M is 16.</p><p>The theoretical and experimental basis for using this formula is detailed in the paper; this article does not elaborate on it.</p><h3 id="are-there-any-unprocessed-levels-remaining"><a class="markdownIt-Anchor" href="#are-there-any-unprocessed-levels-remaining"></a> Are There Any Unprocessed Levels Remaining?</h3><p>Figure 5：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/5.png"  width="500"><p>After calculating the target level, the process starts from the highest level and works downwards, layer by layer, until the new node is added to all levels and connections with other nodes are established. This completes the insertion of the new node.</p><h3 id="retrieving-the-entry-node"><a class="markdownIt-Anchor" href="#retrieving-the-entry-node"></a> Retrieving the Entry Node</h3><p>Figure 6：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/6.png"  width="500"><p>An entry point is a starting point that guides the insertion of a new node into a certain layer (there may be multiple entry points, as described below).</p><p>The purpose of inserting a new node into a layer is to establish its connections with other nodes in that layer. Therefore, on one hand, it first connects with the entry node, and on the other hand, it tries to establish connections with the neighbors of the entry node, the neighbors of the neighbors of the entry node, and so on, based on a greedy algorithm. This process will be detailed in the flow point <code>Identifying Candidate Neighbors in the Current Layer</code>.</p><h4 id="types-of-entry-nodes"><a class="markdownIt-Anchor" href="#types-of-entry-nodes"></a> Types of Entry Nodes</h4><p>The types of entry nodes can be divided into: global entry nodes and layer entry nodes:</p><ul><li><strong>Layer Entry Nodes</strong>: In the insertion process, each layer may have one or more entry nodes (each new node may have different entry nodes in each layer). These nodes are determined in the process of descending through layers and are used to guide the search on each level. In other words, the entry nodes of the current layer are the TopK nodes connected to the new node in <strong>the previous layer</strong> (as will be introduced later). If the current layer is already the highest layer, then the entry node for that layer is the global entry node.</li><li><strong>Global Entry Node</strong>: Also known as <strong>initial entry node</strong>, it is a single node used as the entry node for the highest layer. When adding a new node, and when the target level of the new node is higher than the current number of layers in the HNSW graph, this node will serve as the new global entry node.</li></ul><h4 id="overview-of-obtaining-layer-entry-nodes"><a class="markdownIt-Anchor" href="#overview-of-obtaining-layer-entry-nodes"></a> Overview of Obtaining Layer Entry Nodes</h4><p>Obtaining layer entry nodes can be summarized in two scenarios, as shown below:</p><p>Figure 7：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/7.png"  width="800"><ul><li><p><strong>Target Layer &gt; Current Layer (highest layer in the graph)</strong>: In layers 4, 3, and 2, the addition of the new node introduces new layers, so the new node can be directly added to these layers. There is no need to consider the entry node in these cases; in layers 1 and 0, the approach is the same as the other scenario described below.</p></li><li><p><strong>Target Layer &lt;= Current Layer</strong>: Before reaching the target layer, we need to start from the highest layer and obtain the entry node for each layer, descending layer by layer.</p><ul><li><strong>Layer 3</strong>: As it is the highest layer, the global entry node serves as the entry node for this layer. Finding the candidate neighbors for the new node in this layer , and the <font color="red"><strong>Top1</strong></font> node (called <strong>ep3</strong>)  from the candidate neighbor collectionis selected as the entry node for layer 2.</li><li><strong>Layer 2</strong>: <strong>ep3</strong> serves as the entry node for this layer, and the process of finding candidate neighbors for the new node and selecting the <font color="red"><strong>Top1</strong></font> node (called <strong>ep2</strong>) as the entry node for layer 1.</li><li><strong>Layer 1</strong>: <strong>ep2</strong> serves as the entry node, and the process of finding candidate neighbors for the new node and the <font color="red"><strong>TopK</strong></font> <strong>node collection</strong> (called <strong>ep1</strong>) from the candidate neighbors is selected as the entry nodes for layer 0.</li><li>Layer 0: The <strong>node collection</strong> named <strong>ep1</strong> serves as the entry nodes for this layer.</li></ul><p>It is worth noting that in layers 1 and 2, only the <font color="red"><strong>Top1 (closest neighbor)</strong></font> from the previous layer’s neighbor collection is used as the entry node, while in layer 0, a collection of <font color="red"><strong>TopK</strong></font> <strong>nodes</strong> is selected. The rationale behind this is to balance the needs for search efficiency and accuracy:</p><ul><li><strong>Choosing the closest neighbor as entry node</strong>: This focuses on quickly narrowing down the search area and getting as close as possible to the target node.</li><li><strong>Using multiple neighbors  as entry node</strong>: This improves the comprehensiveness of the search, especially in complex or high-dimensional data spaces. Using multiple entry nodes allows exploring the space from different paths, increasing the chances of finding the best match.</li></ul></li></ul><p><strong>Why Descend Layer by Layer Instead of Directly Reaching the Target Level</strong></p><p>In Figure 7 notice that, when <code>Target Layer &lt;= Current Layer</code>, the approach is to descend layer by layer rather than starting directly from the target level. The considerations include:</p><ul><li><strong>Fast Navigation in Higher Layers</strong>: In higher layers, the connections between nodes cover larger distances. This means that searching in higher layers can quickly skip irrelevant areas and rapidly approach the target area of the new node. Jumping directly to the target layer might miss this opportunity for rapid approach.</li><li><strong>Gradually Refining the Search</strong>: Starting from higher layers and descending layer by layer allows the search process to become progressively more refined. In each layer, the search adjusts its direction according to the connection structure of that layer, approaching the position of the new node more precisely. This layered refinement process helps in finding a more accurate nearest neighbor.</li><li><strong>Avoiding Local Minima</strong>: Direct searching in the target layer could result in falling into local minima, where the nearest neighbor found is not the closest neighbor globally. Descending layer by layer helps avoid this as each layer’s search is based on the results of the previous layer, offering a more comprehensive perspective.</li><li><strong>Balancing Search Costs</strong>: While descending layer by layer might seem more time-consuming than jumping directly to the target layer, it is often more efficient. This is because fewer search steps are needed in higher layers, whereas direct searching in the denser lower layers might require more steps to find the nearest neighbor.</li></ul><h3 id="identifying-candidate-neighbors-in-the-current-layer"><a class="markdownIt-Anchor" href="#identifying-candidate-neighbors-in-the-current-layer"></a> Identifying Candidate Neighbors in the Current Layer</h3><p>Figure 8：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/8.png"  width="500"><p><code>Identifying candidate neighbors in the current layer</code> is a search process of a greedy algorithm. It starts from the entry node, initially considering it as the node that seems closest to the new node. If a node’s neighbor appears closer to the newly inserted node, the algorithm shifts to that neighbor and continues exploring its neighbors. Eventually, it finds the TopN closest nodes (note that these TopN nodes may not necessarily be the closest to the new node). The flowchart is as follows:</p><p>Figure 9：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/9.png"  width="800"><h4 id="candidate-neighbor-collection"><a class="markdownIt-Anchor" href="#candidate-neighbor-collection"></a> Candidate Neighbor Collection</h4><p>The data structure for the candidate node collection is a min-heap, sorted by node distance scores, with closer distances receiving higher scores. Node distance refers to the distance between the new node and the candidate neighbors.</p><h5 id="key-points-in-the-greedy-algorithms-search-process"><a class="markdownIt-Anchor" href="#key-points-in-the-greedy-algorithms-search-process"></a> Key Points in the Greedy Algorithm’s Search Process</h5><ul><li><strong>Distance Score Threshold</strong>: During the search process, a threshold called <code>minCompetitiveSimilarity</code> is continuously updated, which is the top element of the min-heap. If the distance between a neighbor and the new node is less than this threshold, nodes connected to this neighbor are no longer processed.</li><li><strong>Recording Visited Nodes</strong>: Due to the interconnections between nodes, the greedy algorithm can easily revisit the same nodes. Recording visited nodes improves search performance.</li><li><strong>Entry Node for the Next Layer</strong>: The TopN from the candidate neighbor collection of the current layer will serve as the entry nodes for the next layer, as mentioned above, the Top1 and TopK (here, K is defined in the source code by a variable named <code>beamWidth</code>, with a default value of 100).</li></ul><h4 id="close-enough-neighbors-vs-absolute-closest-neighbors"><a class="markdownIt-Anchor" href="#close-enough-neighbors-vs-absolute-closest-neighbors"></a> <strong>Close Enough Neighbors vs. Absolute Closest Neighbors</strong></h4><p>The greedy algorithm’s search process means that it might not find the absolute closest neighbors but usually finds neighbors close enough:</p><ul><li>The algorithm’s goal is to find neighbors close enough to the new node. Here, “close enough” means that while the neighbors found may not be the absolute closest (i.e., some nodes closer to the new node are not chosen as neighbors), they are sufficiently close to effectively represent the new node’s position in the graph.</li><li><strong>Efficiency and Accuracy Balance</strong>: In practical applications, finding the absolute closest neighbors can be very time-consuming, especially in large-scale or high-dimensional datasets. Therefore, the algorithm often seeks a balance point, finding close enough neighbors within an acceptable computational cost.</li><li><strong>Greedy Search Strategy</strong>: HNSW uses a greedy algorithm to progressively approximate the nearest neighbors of the new node. This means that at each step, the algorithm chooses the neighbor that currently appears closest to the new node. This method usually finds close enough neighbors quickly but does not always guarantee finding the absolute closest neighbors.</li><li><strong>Practical Considerations</strong>: In most cases, finding “close enough” neighbors meets the needs of most application scenarios, such as approximate nearest neighbor search. This approach ensures search efficiency while providing relatively high search accuracy.</li><li><strong>Limiting Neighbor Numbers</strong>: To control the complexity of the graph, the number of neighbors for each node is usually limited. This means that even if closer nodes exist, they might not be chosen as neighbors of the new node due to the limit on the number of neighbors.</li></ul><h3 id="selecting-candidate-neighbors-based-on-diversity"><a class="markdownIt-Anchor" href="#selecting-candidate-neighbors-based-on-diversity"></a> Selecting Candidate Neighbors Based on Diversity</h3><p>Figure 10：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/10.png"  width="500"><p>Although we have already found the TopN “close enough” neighbors in the previous step, <code>Identifying Candidate Neighbors in the Current Layer</code>, we still need to examine these neighbors for diversity based on the following factors. Neighbors that do not meet the diversity criteria will not be connected, and diversity checking is achieved by calculating the distances between neighbors:</p><ul><li><strong>Covering Different Areas</strong>: If the neighbors of a node are far apart from each other, it means they cover different areas around the node. This distribution helps quickly locate different areas during the search process, thereby improving search efficiency and accuracy.</li><li><strong>Avoiding Local Minima</strong>: In high-dimensional spaces, if all neighbors are very close, the search process may fall into local minima, where the nearest neighbor found is not the closest globally. Distance differences between neighbors provide more search paths to avoid this situation.</li><li><strong>Enhancing Graph Connectivity</strong>: Neighbors with distance differences can enhance the connectivity of the graph, making the paths from one node to another more diverse. This is important for quickly propagating information or finding optimal paths in the graph.</li><li><strong>Adapting to Different Data Distributions</strong>: In real applications, data is often not uniformly distributed. Ensuring distance differences between neighbors can better adapt to these non-uniform data distributions, ensuring the graph structure effectively covers the entire data space.</li><li><strong>Balancing Exploration and Exploitation</strong>: In search algorithms, there needs to be a balance between exploration (exploring unknown areas) and exploitation (using known information). Distance differences between neighbors help this balance, as it allows the algorithm to explore multiple directions from a node, rather than being limited to the closest neighbors.</li><li><strong>Improving Robustness</strong>: In dynamically changing datasets, the distribution of data points may change over time. If a node’s neighbors have a certain distance difference, this helps the graph structure adapt to these changes, maintaining its search efficiency.</li></ul><h4 id="diversity-check"><a class="markdownIt-Anchor" href="#diversity-check"></a> Diversity Check</h4><p>The checking process flowchart is as follows:</p><p>Figure 11：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/11.png"  width="700"><h5 id="neighbor-collection"><a class="markdownIt-Anchor" href="#neighbor-collection"></a> Neighbor Collection</h5><p>The elements in the neighbor collection are N nodes selected from the candidate neighbor collection after diversity checking, and they will become the official neighbors of the new node.</p><p>Note that in layer 0, a node can connect to up to <code>2*maxConn</code> neighbor nodes, while in other layers, it can connect to a maximum of <code>maxConn</code> nodes, which is the size of the neighbor collection. The default value of <code>maxConn</code> is 16.</p><p>In the source code, the <code>NeighborArray</code> object represents a node’s neighbor information:</p><p>Figure 12：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/12.png"  width="600"><ul><li><strong>size</strong>: The number of neighbors</li><li><strong>score</strong>: An array of distance scores between the node and its neighbors</li><li><strong>node</strong>: An array of neighbor node identifiers</li><li><strong>scoresDescOrder</strong>: Whether the <code>node</code> and <code>score</code> arrays are sorted in ascending or descending order by distance score</li><li><strong>sortedNodeSize</strong>: This is not relevant for this article</li></ul><h5 id="sorting-the-candidate-neighbor-collection"><a class="markdownIt-Anchor" href="#sorting-the-candidate-neighbor-collection"></a> Sorting the Candidate Neighbor Collection</h5><p>As mentioned earlier, the candidate neighbor collection is a min-heap, with the top element being the farthest distance. Since the subsequent flow <code>Does the Neighbor Node Meet the Diversity Criteria??</code> requires starting from the closest (highest score), the <code>Sorting the Candidate Neighbor Collection</code> in the source code involves writing the heap elements into a NeighborArray called <code>scratch</code>, sorted in ascending order by distance score.</p><h5 id="checking-each-candidate-neighbor-for-diversity"><a class="markdownIt-Anchor" href="#checking-each-candidate-neighbor-for-diversity"></a> Checking Each Candidate Neighbor for Diversity</h5><p>Starting with the highest <strong>scored</strong> element in <code>scratch</code>, compare its distance with <strong>each</strong> neighbor in the neighbor collection <code>d(neighbor, new node's other neighbors)</code>. If there is at least one other neighbor such that <code>d(neighbor, new node)</code> is less than <code>d(neighbor, new node's other neighbors)</code>, it does not meet diversity. If it meets diversity, add this candidate neighbor node to the neighbor collection; it will become an official neighbor of the new node in that layer.</p><h3 id="attempting-to-add-the-new-node-as-a-new-neighbor-of-its-neighbors"><a class="markdownIt-Anchor" href="#attempting-to-add-the-new-node-as-a-new-neighbor-of-its-neighbors"></a> Attempting to Add the New Node as a New Neighbor of Its Neighbors</h3><p>图13：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/13.png"  width="600"><p>This process point is closely related to the graph’s update strategy. When a new node is added to the graph, it is necessary to update the graph structure according to specific rules or strategies. This includes which nodes should be included in the new node’s neighbor list, as discussed above, and deciding which existing nodes should add the new node to their neighbor lists. These update strategies typically consider the following aspects:</p><ul><li><strong>Maintaining Graph Connectivity</strong>: The update strategy aims to maintain good connectivity of the graph, ensuring effective navigation from any node to others.</li><li><strong>Optimizing Search Efficiency</strong>: By appropriately updating neighbor lists, the structure of the graph can be optimized, thereby improving the efficiency of subsequent search operations.</li><li><strong>Maintaining Neighbor Diversity</strong>: Update strategies often aim to maintain diversity among neighbors, which helps to enhance the comprehensiveness and accuracy of searches.</li><li><strong>Controlling Graph Size and Complexity</strong>: To avoid making the graph overly complex, update strategies may include limiting the maximum number of neighbors for a node.</li><li><strong>Adapting to Data Changes</strong>: In dynamically changing datasets, update strategies help the graph adapt to the addition of new data, maintaining its ability to reflect the current state of the dataset.</li></ul><p>图14：</p><img src="http://www.amazingkoala.com.cn/uploads/luceneEnglish/index/ConstructinganHNSWGraph/ConstructinganHNSWGraph-image/14.png"  width="700"><p>The neighbor collection contains the new node’s official neighbors, and a <strong>unidirectional</strong> connection has already been established with them. If the neighbor’s neighbor list of the new node has not yet reached the connection limit, i.e., <code>2*maxConn</code> in layer 0 and <code>maxConn</code> in other layers, then the new node can be added as a neighbor of its neighbor. Otherwise, after being added to the neighbor’s neighbor list, a diversity check is performed (following the same logic as described earlier), and the node with the least diversity is removed from the list. Of course, this least diverse node might be the new node or some other node in the list.</p><h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2><p>  This article outlines the process of constructing an HNSW graph in Lucene 9.8.0, and the subsequent graph data will be stored using <a class="link"   href="https://amazingkoala.com.cn/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;vem&amp;vex/" >index files <i class="fas fa-external-link-alt"></i></a>.  However, currently, this article is only available in Chinese.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Lucene has implemented the HNSW (Hierarchical Navigable Small World) logic based on the paper ‘&lt;a class=&quot;link&quot;   href=&quot;https://arxiv.org/</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Index" scheme="http://example.com/categories/Lucene/Index/"/>
    
    
    <category term="vec" scheme="http://example.com/tags/vec/"/>
    
    <category term="vem" scheme="http://example.com/tags/vem/"/>
    
    <category term="vemf" scheme="http://example.com/tags/vemf/"/>
    
    <category term="vemq" scheme="http://example.com/tags/vemq/"/>
    
    <category term="veq" scheme="http://example.com/tags/veq/"/>
    
    <category term="vex" scheme="http://example.com/tags/vex/"/>
    
    <category term="hnsw" scheme="http://example.com/tags/hnsw/"/>
    
    <category term="eps" scheme="http://example.com/tags/eps/"/>
    
  </entry>
  
  <entry>
    <title>HNSW图的构建（Lucene 9.8.0）</title>
    <link href="http://example.com/Lucene/Index/2024/0118/HNSW%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/"/>
    <id>http://example.com/Lucene/Index/2024/0118/HNSW%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/</id>
    <published>2024-01-17T16:00:00.000Z</published>
    <updated>2024-01-19T05:56:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>  Lucene基于论文<a class="link"   href="https://arxiv.org/abs/1603.09320" >Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs [2018]<i class="fas fa-external-link-alt"></i></a>实现了HNSW的逻辑，本篇文章结合<a class="link"   href="https://chat.openai.com/share/04690eb3-4b9d-4382-a796-2efa381b443f" >ChatGPT-4<i class="fas fa-external-link-alt"></i></a>跟Lucene源码介绍构建过程中的实现细节，另外构建HNSW使用的索引数据结构可以参考文章<a class="link"   href="https://amazingkoala.com.cn/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;vem&amp;vex/" >索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）<i class="fas fa-external-link-alt"></i></a>。</p><h2 id="概述"><a class="markdownIt-Anchor" href="#概述"></a> 概述</h2><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/1.png"  width="300"><p>  我们先通过论文中的这张图，简要的介绍下构建完成的HNSW图的一些基本知识：</p><ol><li><strong>层次结构</strong>：HNSW图具有多层结构，每一层都是一个独立的图。层数通常是根据数据集的大小和复杂性确定的。在最高层（即图中的layer=2），节点数量最少，但每个节点覆盖的范围最广；而在最低层（即图中的layer=0），节点数量最多，但每个节点覆盖的范围相对较小。</li><li><strong>节点连接</strong>：在每一层中，节点通过边与其邻居相连接。这些连接是基于距离或相似性度量的，意味着每个节点倾向于与离它最近的其他节点相连。</li><li><strong>邻居的选择</strong>：选择哪些节点作为邻居是基于一定的启发式规则的，这些规则旨在平衡搜索效率和准确性。通常，这涉及到保持邻居的**多样性（Diversity）**和限制每个节点的邻居数量。</li><li><strong>搜索路径</strong>：构建/查询HNSW图都有一个搜索过程，搜索路径会从最高层的<strong>全局入口节点</strong>开始，然后逐层下降，直到达到最低层。在每一层中，搜索会根据当前层的连接结构来寻找离目标节点最近的节点。</li></ol><h2 id="实现原理"><a class="markdownIt-Anchor" href="#实现原理"></a> 实现原理</h2><p>  我们通过一个新节点的添加/插入过程来介绍下Lucene中HNSW图的构建：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/2.png"  width="800"><h3 id="新节点"><a class="markdownIt-Anchor" href="#新节点"></a> 新节点</h3><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/3.png"  width="500"><p>  Lucene中每一个Document只能有一个相同域名的向量，并且按照添加顺序为每一个向量映射一个节点编号（nodeId），它是一个从0开始递增的值。新节点在源码中即节点编号，当我们需要计算两个节点之间的距离时，就可以通过映射关系找到节点对应的向量值进行计算。</p><h3 id="计算目标层级"><a class="markdownIt-Anchor" href="#计算目标层级"></a> 计算目标层级</h3><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/4.png"  width="500"><p>  目标层级是通过一个遵循指数分布计算出的随机数。</p><p>  HNSW图具有多层结构，目标层级意味着新节点将被添加到哪些层中，并且在每一层与其他节点建立连接。</p><p>  例如目标层级为3，那么这个节点将被依次添加到第3、2、1、0层中。</p><h4 id="计算公式"><a class="markdownIt-Anchor" href="#计算公式"></a> 计算公式</h4><p>  源码中通过公式：<code>−ln⁡(unif(0,1)) * ml</code> 计算目标层级，其中<code>unif(0,1)</code>表示从0到1之间均匀分布的一个随机值，<code>ml</code>定义为<code> 1/ln(M)</code>，其中<code>M</code>是最大连接数，即最多可以跟M个节点进行连接。源码中M的默认值为16。</p><p>  使用该公式基于的理论以及实验基础在论文有详细的介绍，本文不详细展开。</p><h3 id="是否还有未处理的层级"><a class="markdownIt-Anchor" href="#是否还有未处理的层级"></a> 是否还有未处理的层级？</h3><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/5.png"  width="500"><p>  计算出目标层级后，将从最高层开始，从上往下逐层处理，直到新节点添加到所有层并建立与其他节点的连接，就完成了新节点的插入。</p><h3 id="获取入口节点"><a class="markdownIt-Anchor" href="#获取入口节点"></a> 获取入口节点</h3><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/6.png"  width="500"><p>  入口节点（entry point）是指导新节点插入到某一层中的起始点（可能存在多个入口节点，见下文介绍）。</p><p>  将新节点插入到某一层的目的是建立它在这层中跟其他节点的连接。因此一方面<strong>首先</strong>跟入口节点进行连接，另一方面基于贪心算法尝试与入口节点的邻居、入口节点的邻居的邻居等等建立连接。其过程将会在流程点<code>找出当前层的候选邻居</code>中详细展开。</p><h4 id="入口节点的类型"><a class="markdownIt-Anchor" href="#入口节点的类型"></a> 入口节点的类型</h4><p>  入口节点的类型可以分为：全局入口节点和层级入口节点。</p><ul><li>层级入口节点：在插入过程中，每一层都可能有一个或多个入口节点（每一个新节点在每一层的入口节点可能是不同的）。这些节点是在逐层下降过程中确定的，用于在每个层级上引导搜索。也就说当前层的入口节点是<strong>上一层</strong>中与新节点连接的节点集合中TopK个节点（下文会介绍）。如果当前层已经是最高层，那么该层的入口节点为全局入口节点</li><li>全局入口节点：又名<strong>初始入口节点</strong>，是单一的一个节点，用于最高层的入口节点。添加新节点时，并且当新节点的目标层级大于当前HNSW图中的层数时，该节点将作为新的全局入口节点</li></ul><h4 id="获取层级入口节点概述"><a class="markdownIt-Anchor" href="#获取层级入口节点概述"></a> 获取层级入口节点概述</h4><p>  获取层级入口节点可以概括为两种情况，如下所示：</p><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/7.png"  width="800"><ul><li><strong>目标层级 &gt; 当前层级（图中最高层）</strong>：在层级4、3、2中，添加新节点引入了新层，因此直接将新节点添加到这几层即可。添加过程中不需要考虑获取入口节点的问题；在层级1、0中，获取方式跟下面的另一种情况相同</li><li><strong>目标层级 &lt;= 当前层级</strong>：<ul><li>层级3：由于是最高层，因此全局入口节点将作为该层的入口节点，并找到新节点在这层中的候选邻居，并且将候选邻居集合中的<font color="red"><strong>Top1</strong></font>节点（称为<strong>ep3</strong>）作为层级2的入口节点</li><li>层级2：<strong>ep3</strong>作为当前层的入口节点，找到新节点在这层中的候选邻居，并且将候选邻居集合中的<font color="red"><strong>Top1</strong></font>节点（称为<strong>ep2</strong>）作为层级1的入口节点</li><li>层级1：<strong>ep2</strong>作为当前层的入口节点，找到新节点在这层中的邻居，并且将候选邻居集合中的<font color="red"><strong>TopK</strong></font><strong>节点集合</strong>（称为<strong>ep1</strong>）作为层级0的入口节点</li><li>层级0：<strong>ep1</strong>这个节点集合将作为当前层的入口节点</li></ul></li></ul><p>  注意到的是，上文中层级1、2只将上一层中新节点邻居中的<font color="red"><strong>Top1（最近的邻居）</strong></font>作为入口节点，而在层级0中选择了上一层的<font color="red"><strong>TopK</strong></font><strong>节点集合</strong>，其理由是搜索效率和准确性的需求平衡：</p><ul><li><p><strong>选择最近的邻居</strong>：侧重于快速缩小搜索范围，并尽可能快地接近目标节点。</p></li><li><p><strong>使用多个邻居</strong>：提高搜索的全面性，特别是在复杂或高维的数据空间中。使用多个入口节点可以从不同的路径探索空间，增加找到最佳匹配的机会。</p></li></ul><h4 id="为什么逐层下降而不是直接到达目标层级"><a class="markdownIt-Anchor" href="#为什么逐层下降而不是直接到达目标层级"></a> 为什么逐层下降，而不是直接到达目标层级</h4><p>  图7中，当<code>目标层级&lt;=当前层级</code>时，是从当前层级逐层下降，而不是直接从目标层级开始开始，考虑的因素有：</p><ul><li><strong>高层的快速导航</strong>：在高层，节点之间的连接覆盖了更大的距离。这意味着在高层进行搜索可以快速跳过不相关的区域，迅速接近新节点的目标区域。如果直接跳到目标层，可能会错过这种快速接近的机会。</li><li><strong>逐步精细化搜索</strong>：从高层开始并逐层下降允许搜索过程逐步变得更加精细。在每一层，搜索都会根据该层的连接结构调整方向，以更精确地接近新节点的位置。这种逐层精细化的过程有助于找到更准确的最近邻。</li><li><strong>避免局部最小值</strong>：如果直接在目标层进行搜索，可能会陷入局部最小值，即找到的最近邻并不是全局最近的邻居。逐层下降有助于避免这种情况，因为在每一层的搜索都是基于上一层的结果，从而提供了更全面的视角。</li><li><strong>平衡搜索成本</strong>：虽然逐层下降可能看起来比直接跳到目标层更耗时，但实际上它通常更高效。这是因为在高层进行的搜索步骤较少，而直接在密集的低层进行搜索可能需要更多的步骤来找到最近邻。</li></ul><h3 id="找出当前层的候选邻居"><a class="markdownIt-Anchor" href="#找出当前层的候选邻居"></a> 找出当前层的候选邻居</h3><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/8.png"  width="500"><p>  <code>找出当前层的候选邻居</code>是一个贪心算法的<strong>搜索过程</strong>，它从入口节点卡开始，首先将其作为看起来最接近新节点的节点，如果一个节点的邻居看起来更接近新插入的节点，算法会转向那个邻居节点，并继续探索其邻居。最终找到TopN个距离<strong>较近</strong>（TopN中不一定是离新节点<strong>最近</strong>的节点），流程图如下：</p><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/9.png"  width="500"><h4 id="候选邻居集合"><a class="markdownIt-Anchor" href="#候选邻居集合"></a> 候选邻居集合</h4><p>  候选节点集合的数据结构是一个最小堆，按照<code>节点距离</code>打分值排序，距离越近，打分值越高。<code>节点距离</code>描述的是新节点跟候选邻居的距离。</p><h5 id="贪心算法的搜索过程中的一些知识点"><a class="markdownIt-Anchor" href="#贪心算法的搜索过程中的一些知识点"></a> 贪心算法的搜索过程中的一些知识点</h5><ul><li>距离打分值阈值：搜索过程中会一直更新一个名为minCompetitiveSimilarity的阈值，即最小堆堆顶元素，如果某个邻居与新节点的距离小于该阈值，则不再处理跟这个邻居相连接的节点。</li><li>记录已经访问过的节点：由于节点之间的相互连接，贪心算法很容易重复访问相同的节点，通过记录已经访问过的节点，提高搜索性能</li><li>下一层的入口节点：当前层的候选邻居集合中的TopN将作为下一层的入口节点，即上文中提到的Top1跟TopK（这里的K在源码中是通过名为beamWidth的变量定义，默认值为100）。</li></ul><h4 id="足够近的邻居和绝对最近的邻居"><a class="markdownIt-Anchor" href="#足够近的邻居和绝对最近的邻居"></a> <strong>足够近的邻居和绝对最近的邻居</strong></h4><p>  贪心算法的搜索过程意味着它可能不会找到绝对最近的邻居，但通常会找到足够接近的邻居：</p><ul><li>算法的目标是找到与新节点足够接近的邻居节点。这里的“足够接近”意味着虽然找到的邻居可能不是绝对意义上最近的邻居（即有些离新节点更近的节点没有成为邻居），但它们与新节点的距离足够近，可以有效地代表新节点在图中的位置。</li><li>效率与准确性的平衡：在实际应用中，寻找绝对最近的邻居可能非常耗时，特别是在大规模或高维的数据集中。因此，算法通常会寻找一个平衡点，即在可接受的计算成本内找到足够近的邻居。</li><li>贪心搜索策略：HNSW使用贪心算法来逐步逼近新节点的最近邻居。这意味着在每一步，算法都会选择当前看起来最接近新节点的邻居。这种方法通常能快速找到足够近的邻居，但不保证总是找到绝对最近的邻居。</li><li>实用性考虑：在大多数情况下，找到“足够近”的邻居已经能够满足大部分应用场景的需求，如近似最近邻搜索。这种方法在保证搜索效率的同时，也能够提供相对高的搜索准确性。</li><li>限制邻居数量：为了控制图的复杂性，通常会限制每个节点的邻居数量。这意味着即使存在更近的节点，也可能因为邻居数量限制而不被选为新节点的邻居。</li></ul><h3 id="基于多样性筛选候选邻居"><a class="markdownIt-Anchor" href="#基于多样性筛选候选邻居"></a> 基于多样性筛选候选邻居</h3><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/10.png"  width="500"><p>  尽管在上一个流程点<code>找出当前层的候选邻居</code>中已经找到了TopN个&quot;足够近&quot;的邻居，但基于下面几个因素考虑，我们还要对这些邻居进行多样性的考察，不满足多样性的邻居则不会进行连接，并且通过计算邻居之间的距离实现多样性的检查：</p><ul><li>覆盖不同区域：如果一个节点的邻居彼此之间距离较远，这意味着它们覆盖了该节点周围的不同区域。这种分布有助于在搜索过程中快速定位到不同的区域，从而提高搜索的效率和准确性。</li><li>避免局部最小值：在高维空间中，如果所有邻居都非常接近，可能会导致搜索过程陷入局部最小值，即找到的最近邻并不是全局最近的邻居。邻居间的距离差异有助于提供更多的搜索路径，从而避免这种情况。</li><li>增强图的连通性：具有距离差异的邻居节点可以增强图的连通性，使得从一个节点到另一个节点的路径更加多样化。这对于在图中快速传播信息或找到最优路径非常重要。</li><li>适应不同的数据分布：在实际应用中，数据往往不是均匀分布的。通过确保邻居间的距离差异，可以更好地适应这些不均匀的数据分布，确保图结构能够有效地覆盖整个数据空间。</li><li>平衡探索和利用：在搜索算法中，需要平衡探索（探索未知区域）和利用（利用已知信息）之间的关系。邻居间的距离差异有助于这种平衡，因为它允许算法从一个节点出发探索多个方向，而不是仅限于最近的邻居。</li><li>提高鲁棒性：在动态变化的数据集中，数据点的分布可能会随时间变化。如果节点的邻居具有一定的距离差异，这有助于图结构适应这些变化，保持其搜索效率。</li></ul><h4 id="多样性检查"><a class="markdownIt-Anchor" href="#多样性检查"></a> 多样性检查</h4><p>  检查流程图如下所示：</p><p>图11：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/11.png"  width="600"><h5 id="邻居集合"><a class="markdownIt-Anchor" href="#邻居集合"></a> 邻居集合</h5><p>  邻居集合中的元素是从候选邻居集合中进行多样性检查后，筛选出的N个节点，它们将做新节点的正式邻居。</p><p>  注意的是，第0层中，节点最多可以连接<code>2*maxConn</code>个邻居节点，其他层最多可以连接<code>maxConn</code>个，也就是邻居集合的大小。其中<code>maxConn</code>的默认值为16。</p><p>  源码中通过NeighborArray对象来表示节点的邻居信息：</p><p>图12：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/12.png"  width="600"><ul><li><strong>size</strong>：邻居的数量</li><li><strong>score</strong>：节点跟邻居的距离打分值数组</li><li><strong>node</strong>：邻居的节点编号数组</li><li><strong>scoresDescOrder</strong>：<code>node</code>跟<code>score</code>数组按照距离打分值的升序还是降序排序</li><li><strong>sortedNodeSize</strong>：本篇文章不需要关注这个</li></ul><h5 id="候选邻居集合排序"><a class="markdownIt-Anchor" href="#候选邻居集合排序"></a> 候选邻居集合排序</h5><p>  上文中我们提到候选邻居集合是一个最小堆，即堆顶为距离最远的元素。由于后续<code>节点是否满足多样性</code>要求从距离最近（打分越高）的开始，因此这里的<code>候选邻居集合排序</code>在源码中是将堆中元素写入到一个NeighborArray中，我们称之为<code>scratch</code>，并且按照距离打分值升序排序。</p><h5 id="逐个候选邻居进行多样性检查"><a class="markdownIt-Anchor" href="#逐个候选邻居进行多样性检查"></a> 逐个候选邻居进行多样性检查</h5><p>  从<code>scratch</code>中距离<strong>打分值最高</strong>的元素开始，比较它与邻居集合中<strong>每一个</strong>邻居的距离<code>d(邻居，新节点的其他邻居)</code>，如果至少存在一个其他邻居使得<code>d(邻居，新节点)</code>小于<code>d(邻居，新节点的其他邻居)</code>，则不满足多样性，如果满足则把这个候选邻居节点添加到邻居集合中，它将作为新节点在该层中的正式邻居。</p><h3 id="尝试将新节点作为它邻居的新邻居"><a class="markdownIt-Anchor" href="#尝试将新节点作为它邻居的新邻居"></a> 尝试将新节点作为它邻居的新邻居</h3><p>图13：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/13.png"  width="600"><p>  这个流程点与图的更新策略密切相关。当新节点被添加到图中时，需要根据特定的规则或策略来更新图的结构。这包括上文中已经介绍的新节点的邻居列表应该包括哪些节点以及决定哪些现有节点应该将新节点加入其邻居列表。这些更新策略通常考虑以下几个方面：</p><ul><li><strong>保持图的连通性</strong>：更新策略旨在保持图的良好连通性，确保可以从任一节点有效地导航到其他节点。</li><li><strong>优化搜索效率</strong>：通过适当地更新邻居列表，可以优化图的结构，从而提高后续搜索操作的效率。</li><li><strong>维护邻居多样性</strong>：更新策略通常旨在保持邻居的多样性，这有助于提高搜索的全面性和准确性。</li><li><strong>控制图的大小和复杂性</strong>：为了避免图变得过于复杂，更新策略可能包括限制节点的最大邻居数量。</li><li><strong>适应数据变化</strong>：在动态变化的数据集中，更新策略有助于图适应新数据的加入，保持其反映数据集当前状态的能力。</li></ul><p>图14：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/index/HNSW图的构建/14.png"  width="700"><p>  邻居集合中包含了新节点的正式邻居，并且已经跟他们建立了<strong>单向的</strong>连接。如果新节点的邻居的邻居列表还未达到连接数上限，即第0层的<code>2*maxConn</code>和其它层的<code>maxConn</code>，那么新节点就可以作为它邻居的邻居。否则先添加到邻居的邻居列表后，再进行多样性检查（检查逻辑同上文中的描述），移除多样性最差的节点，当然这个最差的节点可能是新节点，也有可能是其他节点。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  无</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  Lucene基于论文&lt;a class=&quot;link&quot;   href=&quot;https://arxiv.org/abs/1603.09320&quot; &gt;Efficient and robust approximate nearest neighbor search using Hie</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Index" scheme="http://example.com/categories/Lucene/Index/"/>
    
    
    <category term="vec" scheme="http://example.com/tags/vec/"/>
    
    <category term="vem" scheme="http://example.com/tags/vem/"/>
    
    <category term="vemf" scheme="http://example.com/tags/vemf/"/>
    
    <category term="vemq" scheme="http://example.com/tags/vemq/"/>
    
    <category term="veq" scheme="http://example.com/tags/veq/"/>
    
    <category term="vex" scheme="http://example.com/tags/vex/"/>
    
    <category term="hnsw" scheme="http://example.com/tags/hnsw/"/>
    
    <category term="eps" scheme="http://example.com/tags/eps/"/>
    
  </entry>
  
  <entry>
    <title>索引文件之vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex（Lucene 9.9.0）</title>
    <link href="http://example.com/Lucene/suoyinwenjian/2023/1225/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/"/>
    <id>http://example.com/Lucene/suoyinwenjian/2023/1225/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/</id>
    <published>2023-12-24T16:00:00.000Z</published>
    <updated>2023-12-25T08:27:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>  在文章<a class="link"   href="https://amazingkoala.com.cn/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;vem&amp;vex/" >索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）<i class="fas fa-external-link-alt"></i></a>中介绍了Lucene 9.8.0版本向量数据相关的索引文件（必须先阅读下，很多重复的内容不会再提起），由于在Lucene 9.9.0中引入了<a class="link"   href="https://www.elastic.co/search-labs/blog/articles/scalar-quantization-101" >Scalar Quantization<i class="fas fa-external-link-alt"></i></a>（简称SQ）技术，因此再次对索引结构进行了改造。另外加上该<a class="link"   href="https://github.com/apache/lucene/pull/12729" >issue<i class="fas fa-external-link-alt"></i></a>，使得在Lucene 9.9.0中，对于向量数据的索引文件最多由以下6个文件组成，我们先给出简要的说明：</p><ul><li>.vex、.vem：HNSW信息</li><li>.vec、.vemf：原始的向量数据，即基于SQ量化前的数据，以及文档号、文档号跟节点编号映射关系的数据</li><li>.veq、.vemq（启用SQ才会有这两个索引文件，默认不启动）：量化后的向量数据，以及文档号、文档号跟节点编号映射关系的数据</li></ul><p>  先给出这几个索引文件的数据结构之间的关联图，然后我们一一介绍这些字段的含义：</p><h3 id="vexvem"><a class="markdownIt-Anchor" href="#vexvem"></a> .vex&amp;.vem</h3><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/1.png"><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/vem_vex.html" >点击<i class="fas fa-external-link-alt"></i></a>查看大图</p><h3 id="vecvemf"><a class="markdownIt-Anchor" href="#vecvemf"></a> .vec&amp;.vemf</h3><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/2.png"><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/vec_vemf.html" >点击<i class="fas fa-external-link-alt"></i></a>查看大图</p><h3 id="veqvemq"><a class="markdownIt-Anchor" href="#veqvemq"></a> .veq&amp;.vemq</h3><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/3.png"><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/veq__vemq.html" >点击<i class="fas fa-external-link-alt"></i></a>查看大图</p><h2 id="数据结构"><a class="markdownIt-Anchor" href="#数据结构"></a> 数据结构</h2><h3 id="vexvem-vecvemf"><a class="markdownIt-Anchor" href="#vexvem-vecvemf"></a> .vex&amp;.vem、.vec&amp;.vemf</h3><p>  相较于Lucene9.8.0中的索引数据结构，只是调整了某些字段的所属索引文件，比如说：</p><ul><li>对Lucene9.8.0中的元数据分别移到Lucene9.9.0中.vemf以及.vem中（调整的原因：<a class="link"   href="https://github.com/apache/lucene/pull/12729" >GITHUB#12729<i class="fas fa-external-link-alt"></i></a>）。</li><li>Lucene9.8.0中的.vec以及.vex的数据结构保持不变</li></ul><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/4.png"><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/_diff-vem.html" >点击<i class="fas fa-external-link-alt"></i></a>查看大图</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/5.png"><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/diff-vex.html" >点击<i class="fas fa-external-link-alt"></i></a>查看大图</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/6.png"><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6/vec&amp;vem&amp;vemf&amp;vemq&amp;veq&amp;vex/diff-vec.html" >点击<i class="fas fa-external-link-alt"></i></a>查看大图</p><p>  因此，对于Lucene9.9.0中的索引文件.vex&amp;.vem、.vec&amp;.vemf中的字段，相比较Lucene9.8.0，并没有新增或者移除字段，因此这些字段的含义在本文中就不重新介绍了，可以阅读文章<a class="link"   href="https://amazingkoala.com.cn/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;vem&amp;vex/" >索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）<i class="fas fa-external-link-alt"></i></a>。</p><h3 id="veqvemq-2"><a class="markdownIt-Anchor" href="#veqvemq-2"></a> .veq&amp;.vemq</h3><p>  如果启用SQ，那么段中会额外多出两个索引文件，即.veq&amp;.vemq。下图中除了<font color="red">红框标注</font>的字段，其他的在文章<a class="link"   href="https://amazingkoala.com.cn/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;vem&amp;vex/" >索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）<i class="fas fa-external-link-alt"></i></a>或者其他索引文件中同名字段有相同的含义。</p><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/7.png"><h4 id="quantizedvectordatameta"><a class="markdownIt-Anchor" href="#quantizedvectordatameta"></a> QuantizedVectorDataMeta</h4><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/8.png"><p>  该字段作为元数据，<code>Offset</code>跟<code>Length</code>对应的区间，用来描述量化后的数据信息在索引文件.veq中的位置信息，见图3</p><h4 id="confidenceinterval-lowerquantile-upperquantile"><a class="markdownIt-Anchor" href="#confidenceinterval-lowerquantile-upperquantile"></a> ConfidenceInterval、lowerQuantile、upperQuantile</h4><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/9.png"><p>  这三个字段用于量化操作：</p><ul><li>ConfidenceInterval：（0.9~1.0之间的值）置信区间。它本来应该是用来计算分位数的，但在目前版本中，还未使用该参数。</li><li>lowerQuantile、upperQuantile：（32位的浮点数）最大和最小分位数。引入这两个字段至少有以下的目的：<ul><li>离群值的影响：数据中的离群值（outliers）可能会对量化区间产生极端的影响。如果简单地选取数据的最小值和最大值作为量化的边界，一个异常的高或低值会导致整个量化区间的扩展，这会使得绝大多数的数据在量化后的动态范围内分布得非常紧凑。这样就会减少量化级别之间的区分度，增加了量化误差。</li><li>动态范围的优化：使用分位数可以有效地切除极端的离群值，使量化区间专注于数据的核心分布区域。这样做可以优化量化级别的使用，使得数据分布更均匀，从而减少量化误差。</li></ul></li></ul><p>  在以后Lucene中量化技术的文章中会介绍这几个字段的作用。</p><h4 id="quantizedvectordata"><a class="markdownIt-Anchor" href="#quantizedvectordata"></a> QuantizedVectorData</h4><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vemf&vemq&veq&vex/10.png"><p>  量化后的每一个向量信息由<code>QuantizedData</code>以及<code>OffsetCorrection</code>组成。</p><ul><li>QuantizedData：8bit大小，即使用一个字节来描述一个向量值，量化后的向量值的范围为[0,127]</li><li>OffsetCorrection： 用来调整量化误差。这个偏移量是为了补偿量化值因为舍入到最近的整数而失去的一些精度。在计算向量距离的打分公式中会使用。同样的，该字段具体的使用场景将在以后的文章中展开</li></ul><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  本文主要介绍了因引入<a class="link"   href="https://www.elastic.co/search-labs/blog/articles/scalar-quantization-101" >Scalar Quantization<i class="fas fa-external-link-alt"></i></a>，向量搜索对应的索引文件的从Lucene9.8.0到Lucene9.9.0的差异。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  在文章&lt;a class=&quot;link&quot;   href=&quot;https://amazingkoala.com.cn/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="suoyinwenjian" scheme="http://example.com/categories/Lucene/suoyinwenjian/"/>
    
    
    <category term="index" scheme="http://example.com/tags/index/"/>
    
    <category term="vec" scheme="http://example.com/tags/vec/"/>
    
    <category term="vem" scheme="http://example.com/tags/vem/"/>
    
    <category term="vemf" scheme="http://example.com/tags/vemf/"/>
    
    <category term="vemq" scheme="http://example.com/tags/vemq/"/>
    
    <category term="veq" scheme="http://example.com/tags/veq/"/>
    
    <category term="vex" scheme="http://example.com/tags/vex/"/>
    
    <category term="scalar" scheme="http://example.com/tags/scalar/"/>
    
    <category term="quantization" scheme="http://example.com/tags/quantization/"/>
    
    <category term="indexFile" scheme="http://example.com/tags/indexFile/"/>
    
  </entry>
  
  <entry>
    <title>Vector Similarity Computations FMA- style</title>
    <link href="http://example.com/blog/vector/2023/1222/vector-similarity-computations-fma-style/"/>
    <id>http://example.com/blog/vector/2023/1222/vector-similarity-computations-fma-style/</id>
    <published>2023-12-21T16:00:00.000Z</published>
    <updated>2023-12-22T03:59:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>  介绍下一篇基于FMA（Fused Multiply-Add）利用SIMD的文章。</p><p>  原文地址：<a class="link"   href="https://www.elastic.co/search-labs/blog/articles/vector-similarity-computations-fma-style" >https://www.elastic.co/search-labs/blog/articles/vector-similarity-computations-fma-style<i class="fas fa-external-link-alt"></i></a></p><h2 id="原文"><a class="markdownIt-Anchor" href="#原文"></a> 原文</h2><p>In Lucene 9.7.0 we added support that <a class="link"   href="https://www.elastic.co/blog/accelerating-vector-search-simd-instructions" >leverages SIMD instructions<i class="fas fa-external-link-alt"></i></a> to perform data-parallelization of vector similarity computations. Now we’re pushing this even further with the use of Fused Multiply-Add (FMA).</p><h3 id="what-is-fma"><a class="markdownIt-Anchor" href="#what-is-fma"></a> <strong>What is FMA</strong></h3><p>Multiply and add is a common operation that computes the product of two numbers and adds that product with a third number. These types of operations are performed over and over during vector similarity computations.</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/blog/vector-similarity-computations-fma-style/1.png"><p>Fused multiply-add (FMA) is a single operation that performs both the multiply and add operations in one - the multiplication and addition are said to be “fused” together. FMA is typically faster than a separate multiplication and addition because most CPUs model it as a single instruction.</p><p>FMA also produces more accurate results. Separate multiply and add operations on floating-point numbers have two rounds; one for the multiplication, and one for the addition, since they are separate instructions that need to produce separate results. That is effectively,</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/blog/vector-similarity-computations-fma-style/2.png"><p>Whereas FMA has a single rounding, which applies only to the combined result of the multiplication and addition. That is effectively,</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/blog/vector-similarity-computations-fma-style/3.png"><p>Within the FMA instruction the <code>a * b</code> produces an infinite precision intermediate result that is added with <code>c</code>, before the final result is rounded. This eliminates a single round, when compared to separate multiply and add operations, which results in more accuracy.</p><h3 id="under-the-hood"><a class="markdownIt-Anchor" href="#under-the-hood"></a> Under the hood</h3><p>So what has actually changed? In Lucene we have replaced the separate multiply and add operations with a single FMA operation. The scalar variants now use <a class="link"   href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/Math.html#fma(float,float,float)" >Math::fma<i class="fas fa-external-link-alt"></i></a>, while the Panama vectorized variants use <a class="link"   href="https://docs.oracle.com/en/java/javase/21/docs/api/jdk.incubator.vector/jdk/incubator/vector/FloatVector.html#fma(jdk.incubator.vector.Vector,jdk.incubator.vector.Vector)" >FloatVector::fma<i class="fas fa-external-link-alt"></i></a>.</p><p>If we look at the disassembly we can see the effect that this change has had. Previously we saw this kind of code pattern for the Panama vectorized implementation of dot product.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vmovdqu32 zmm0,ZMMWORD PTR [rcx+r10</span><br><span class="line">4+0x10]</span><br><span class="line">*vmulps zmm0,zmm0,ZMMWORD PTR [rdx+r10*</span><br><span class="line">4+0x10]</span><br><span class="line">vaddps zmm4,zmm4,zmm0</span><br></pre></td></tr></table></figure><p>The <code>vmovdqu32</code> instruction loads 512 bits of packed doubleword values from a memory location into the <code>zmm0</code> register. The <code>vmulps</code> instruction then multiplies the values in <code>zmm0</code> with the corresponding packed values from a memory location, and stores the result in <code>zmm0</code>. Finally, the <code>vaddps</code> instruction adds the 16 packed single precision floating-point values in <code>zmm0</code> with the corresponding values in <code>zmm4</code>, and stores the result in <code>zmm4</code>.</p><p>With the change to use <code>FloatVector::fma</code>, we see the following pattern:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmovdqu32 zmm0,ZMMWORD PTR [rdx+r11</span><br><span class="line">4+0xd0]</span><br><span class="line">*vfmadd231ps zmm4,zmm0,ZMMWORD PTR [rcx+r11*4+0xd0]</span><br></pre></td></tr></table></figure><p>Again, the first instruction is similar to the previous example, where it loads 512 bits of packed doubleword values from a memory location into the <code>zmm0</code> register. The <code>vfmadd231ps</code> (this is the FMA instruction), multiplies the values in <code>zmm0</code> with the corresponding packed values from a memory location, adds that intermediate result to the values in <code>zmm4</code>, performs rounding and stores the resulting 16 packed single precision floating-point values in <code>zmm4</code>.</p><p>The <code>vfmadd231ps</code> instruction is doing quite a lot! It’s a clear signal of intent to the CPU about the nature of the computations that the code is running. Given this, the CPU can make smarter decisions about how this is done, which typically results in improved performance (and accuracy as previously described).</p><h3 id="is-it-fast"><a class="markdownIt-Anchor" href="#is-it-fast"></a> Is it fast</h3><p>In general, the use of FMA typically results in improved performance. But as always you need to benchmark! Thankfully, Lucene deals with quite a bit of complexity when determining whether to use FMA or not, so you don’t have to. Things like, whether the CPU even has support for FMA, if FMA is enabled in the Java Virtual Machine, and only enabling FMA on architectures that have proven to be faster than separate multiply and add operations. As you can probably tell, this heuristic is not perfect, but goes a long way to making the out-of-the-box experience good. While accuracy is improved with FMA, we see no negative effect on pre-existing similarity computations when FMA is not enabled.</p><p>Along with the use of FMA, the suite of vector similarity functions got some (more) love. All of dot product, square, and cosine distance, both the scalar and Panama vectorized variants have been updated. Optimizations have been applied based on the inspection of disassembly and empirical experiments, which have brought improvements that help fill the pipeline keeping the CPU busy; mostly through more consistent and targeted loop unrolling, as well as removal of data dependencies within loops.</p><p>It’s not straightforward to put concrete performance improvement numbers on this change, since the effect spans multiple similarity functions and variants, but we see positive throughput improvements, from single digit percentages in floating-point dot product, to higher double digit percentage improvements in cosine. The byte based similarity functions also show similar throughput improvements.</p><h3 id="wrapping-up"><a class="markdownIt-Anchor" href="#wrapping-up"></a> Wrapping Up</h3><p>In Lucene 9.7.0, we added the ability to enable an alternative faster implementation of the low-level primitive operations used by Vector Search through SIMD instructions. In the upcoming Lucene 9.9.0 we built upon this to leverage faster FMA instructions, as well as to apply optimization techniques more consistently across all the similarity functions. Previous versions of Elasticsearch are already benefiting from SIMD, and the upcoming Elasticsearch 8.12.0 will have the FMA improvements.</p><p>Finally, I’d like to call out Lucene PMC member <a class="link"   href="https://github.com/rmuir" >Robert Muir<i class="fas fa-external-link-alt"></i></a> for continuing to make improvements in this area, and for the enjoyable and productive collaboration.</p><h2 id="译文"><a class="markdownIt-Anchor" href="#译文"></a> 译文</h2><p>  在Lucene 9.7.0中，我们新增了支持，利用SIMD指令来执行数据并行化的向量相似度计算（vector Similarity）。现在，我们更进一步地使用了融合乘法加法（Fused Multiply-Add，FMA）。</p><h3 id="what-is-fma-2"><a class="markdownIt-Anchor" href="#what-is-fma-2"></a> What is FMA</h3><p>  乘法跟加法是一种常见的操作，它计算两个数字的乘积，并将该乘积与第三个数字相加。在向量相似度计算过程中，这些类型的操作会一次又一次地执行。</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/blog/vector-similarity-computations-fma-style/1.png"><p>  FMA是种单一操作，它在一次操作中执行了乘法和加法 - 乘法和加法被“融合”在一起。FMA通常比分开的乘法和加法更快，因为大多数CPU将其建模为单一指令。</p><p>  FMA还能产生更准确的结果。分开的浮点数乘法和加法操作需要两轮，一轮用于乘法，一轮用于加法，因为它们是分开的指令，需要产生不同的结果。而FMA只有一轮舍入（round），这仅适用于乘法和加法的合并结果。这消除了与分开的乘法和加法操作相比的单一舍入，从而提高了准确性。</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/blog/vector-similarity-computations-fma-style/2.png"><p>  然而，FMA只进行一次舍入，而这个舍入只适用于乘法和加法的合并结果。相当于：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/blog/vector-similarity-computations-fma-style/3.png"><p>  在FMA指令中，<code>a * b</code>会产生一个无限精度的中间结果，然后与c相加，最后对最终结果进行舍入。与分开的乘法和加法操作相比，这消除了一次舍入，从而提高了准确性。</p><h3 id="under-the-hood-2"><a class="markdownIt-Anchor" href="#under-the-hood-2"></a> Under the hood</h3><p>  那么发生了什么真正的改变呢？我们用单个FMA操作替换了分开的乘法和加法操作。scalar variants现在使用Math::fma，而Panama vectorized variants则使用FloatVector::fma。</p><p>  如果观察下反汇编的结果，我们可以看到其发生的变化。之前对于这类代码模式，Panama对于点积的实现如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vmovdqu32 zmm0,ZMMWORD PTR [rcx+r10</span><br><span class="line">4+0x10]</span><br><span class="line">*vmulps zmm0,zmm0,ZMMWORD PTR [rdx+r10*</span><br><span class="line">4+0x10]</span><br><span class="line">vaddps zmm4,zmm4,zmm0</span><br></pre></td></tr></table></figure><p>  <code>vmovdqu32</code> 指令从内存位置加载512位 packed doubleword到<code>zmm0</code>寄存器中。接着，<code>vmulps</code> 指令将<code>zmm0</code>的值与来自内存位置的相应packed value相乘，并将结果存储在<code>zmm0</code>中。最后，<code>vaddps</code> 指令将<code>zmm0</code>中的16个packed 单精度的浮点值与<code>zmm4</code>中的相应值相加，并将结果存储在<code>zmm4</code>中。</p><p>  使用<code>FloatVector::fma</code>后的变化，我们可以看下下面的模式：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmovdqu32 zmm0,ZMMWORD PTR [rdx+r11</span><br><span class="line">4+0xd0]</span><br><span class="line">*vfmadd231ps zmm4,zmm0,ZMMWORD PTR [rcx+r11*4+0xd0]</span><br></pre></td></tr></table></figure><p>  再次的，第一条指令与之前的示例相似，它从内存位置加载512位packed doubleword values到<code>zmm0</code>寄存器中。然后，<code>vfmadd231ps</code>（这是FMA指令）将<code>zmm0</code>中的值与来自内存位置的相应packed values相乘，将中间结果添加到<code>zmm4</code>中，执行舍入，并将结果存储在<code>zmm4</code>中的16位packed single precision floating-point values。</p><p>  <code>vfmadd231ps</code>指令执行了多个操作，明确告诉CPU代码正在运行的计算性质。基于这一信息，CPU可以更明智地决定如何执行操作，通常会导致性能改进（如前面所述）和更高的准确性。</p><h3 id="is-it-fast-2"><a class="markdownIt-Anchor" href="#is-it-fast-2"></a> Is it fast</h3><p>  总的来说，使用FMA通常会导致性能提升，但仍需要进行基准测试。不过Lucene在确定是否使用FMA时处理了许多复杂性（有些情况下不允许使用FMA，后续介绍向量搜索时会提到），因此用户不需要担心这些细节。这包括CPU是否支持FMA，Java虚拟机是否启用FMA，以及仅在已证明比分开的乘法和加法操作更快的架构上启用FMA。尽管可以看出这种启发式方法并不完美，但它在提供良好的开箱即用体验方面已经做了很多。使用FMA可以提高准确性，而未启用FMA时，现有的相似性计算没有负面影响。</p><p>  除了使用FMA，向量相似性函数相关的内容（the suite of similarity functions）也进行了改进。 dot product、square 和 cosine distance 和Panama向量化都已更新。通过查看反汇编和实验经验应用了（apply）一些优化措施，这些措施带来了改进，有助于保持CPU忙碌的状态，主要通过更加的一致性和有针对性的循环展开（loop unrolling），以及消除循环内的数据依赖性。</p><ul><li><p>mostly through more consistent and targeted loop unrolling：这是一种优化技术，通过增加循环体中代码的实例数量来减少循环的迭代次数。这种方法可以提高程序的执行效率，因为它减少了循环控制逻辑的开销，并可能使得更多的数据在单个循环迭代中被处理。</p></li><li><p>在循环内移除数据依赖（removal of data dependencies within loops）：这是指修改循环中的代码，以减少或消除数据依赖，从而提高性能。数据依赖可能会导致循环迭代之间的延迟，因为后续的迭代可能需要等待前一个迭代完成数据处理。通过重构代码来减少这种依赖，可以使循环的不同迭代更加独立，从而提高运行效率。综合来看，这些技术有助于提高程序处理循环时的性能，特别是在涉及大量数据和复杂计算时。</p></li></ul><p>  具体的性能提升没那么直接通过数据来确定，因为影响涵盖了multiple similarity functions and variants，但我们看到了吞吐量方面的提升，从浮点型数值的点积操作的个位数百分比的提升到cosine操作的两位数百分比的提高。基于字节的相似性函数也显示出类似的吞吐量的提升。</p><h3 id="wrapping-up-2"><a class="markdownIt-Anchor" href="#wrapping-up-2"></a> Wrapping Up</h3><p>  在Lucene 9.7.0中，我们添加了通过SIMD指令来启用low-level primitive操作用于向量搜索。在即将发布的Lucene 9.9.0中，我们进一步利用更快的FMA指令，并更一致地应用于所有相似性函数的优化技术。早期版本的Elasticsearch已经受益于SIMD，即将发布的Elasticsearch 8.12.0将获得FMA改进。最后，我要感谢Lucene PMC成员Robert Muir在这个领域持续改进，并为愉快而富有成效的合作表示赞扬。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  介绍下一篇基于FMA（Fused Multiply-Add）利用SIMD的文章。&lt;/p&gt;
&lt;p&gt;  原文地址：&lt;a class=&quot;link&quot;   href=&quot;https://www.elastic.co/search-labs/blog/articles/vector-</summary>
      
    
    
    
    <category term="blog" scheme="http://example.com/categories/blog/"/>
    
    <category term="vector" scheme="http://example.com/categories/blog/vector/"/>
    
    
    <category term="simd" scheme="http://example.com/tags/simd/"/>
    
    <category term="vector" scheme="http://example.com/tags/vector/"/>
    
    <category term="fma" scheme="http://example.com/tags/fma/"/>
    
  </entry>
  
  <entry>
    <title>NumericUtils（Lucene 9.8.0）</title>
    <link href="http://example.com/Lucene/gongjulei/2023/1128/NumericUtils/"/>
    <id>http://example.com/Lucene/gongjulei/2023/1128/NumericUtils/</id>
    <published>2023-11-27T16:00:00.000Z</published>
    <updated>2023-11-28T11:00:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>  Lucene中用BKD存储的数值无论是哪种类型（long，int，float，double，BigInteger），为了便于使用相同的代码逻辑中实现BKD的遍历以及<a class="link"   href="https://amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdim&amp;&amp;dii/" >优化存储<i class="fas fa-external-link-alt"></i></a>，都统一使用字节数组byte[]描述原值（original value）并且其转化成字节数组的逻辑都在NumericUtils类中实现。</p><h2 id="long类型"><a class="markdownIt-Anchor" href="#long类型"></a> long类型</h2><p>  我们看下long类型的值如何转化为字节数组。</p><p>  因为long类型占用64bit，即8个字节（一个字节占8位），所以需要大小为8的字节数组。需要两步完成字节数组的转化：</p><ul><li>步骤一：将long中的最高位，即符号位，跟<code>1</code>执行异或操作。<ul><li>为了在转化为字节数组后，能让字节数组依旧有拥有大小比较的能力</li><li>对于用字节数组表示的数值，是通过对字节数组中的每个元素进行<strong>二进制</strong>比较来判断大小。<ul><li>二进制的比较方式为：从最高位开始，逐位比较，当遇到第一个不相同的bit，那么bit为1的值大于bit为0的值</li><li>不过在<a class="link"   href="https://issues.apache.org/jira/browse/LUCENE-10145" >LUCENE-10145<i class="fas fa-external-link-alt"></i></a>之后，不再是简单的两个字节数组的比较，这里简单提一下，使用更高效但依然是无符号的比较方式。</li></ul></li><li>因此在long类型中，对于一个正数，它的最高位总是0，而负数的最高位是1，如果直接将符号位写入到字节数组，会出现所有的负数都大于正数的情况</li></ul></li><li>步骤二：从long的最高位开始，每8个bit作为字节数组的一个数组元素。</li></ul><h3 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子</h3><p>  有两个long类型数值<code>2</code>跟<code>-2</code>其转化过程如下：</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/NumericUtils/1.png"><p>  </p><h2 id="int类型"><a class="markdownIt-Anchor" href="#int类型"></a> int类型</h2><p>  跟longToSortableBytes的处理逻辑是一致，差别就是生成的字节数组大小是4，而long类型转化成的字节数组大小为8。</p><h2 id="doublefloat类型"><a class="markdownIt-Anchor" href="#doublefloat类型"></a> double/float类型</h2><p>  Lucene中将double和float类型分别通过JDK提供的<code>Double.doubleToLongBits(double value)</code>以及<code>Float.floatToIntBits(float value)</code>将浮点型的数值转换成使用IEEE 754标准的值，即long跟int值，然后通过上文中的方法再进一步转化为字节数组。</p><h3 id="ieee-754"><a class="markdownIt-Anchor" href="#ieee-754"></a> IEEE 754</h3><p>  我们首先重新回顾下IEEE 754这个关于浮点数算术的国际标准。</p><h4 id="double类型"><a class="markdownIt-Anchor" href="#double类型"></a> double类型</h4><p>  对于双精度（64bit）的浮点数可以分为三个部分：</p><ul><li>符号位：（1位），决定数值的正负。0 表示正数，1 表示负数。</li><li>指数位：（11位），用于表示数值的范围。这个部分决定了浮点数的大小。<ul><li>指数位是结合偏移值（指数偏置，偏置值：1023）后的值，其中一个好处是可以将负指数跟正指数统一成正指数，那么在比较两个浮点的指数位时，比较两个无符号整数一样简单，不需要考虑正负。</li></ul></li><li>尾数位：（52位）：也称为有效数位或小数位，用于表示数值的精度。</li></ul><h4 id="float类型"><a class="markdownIt-Anchor" href="#float类型"></a> float类型</h4><p>  对于单精度（32bit）的浮点数可以分为三个部分：</p><ul><li>符号位：（1位）：决定数值的正负。0 表示正数，1 表示负数。</li><li>指数位：（8位）：用于表示数值的范围。这个部分决定了浮点数的大小。偏置值：127</li><li>尾数位：（23位）：也称为有效数位或小数位，用于表示数值的精度。</li></ul><h4 id="转化"><a class="markdownIt-Anchor" href="#转化"></a> 转化</h4><p>  接着我们给出一个双精度转化的例子。比如我们有一个双精度的浮点数，也就是double类型的值：1024.0256。</p><h5 id="步骤1十进制转二进制"><a class="markdownIt-Anchor" href="#步骤1十进制转二进制"></a> 步骤1：十进制转二进制</h5><p>  对浮点数的整数跟小数的十进制分别使用对应的规则转化为二进制：</p><h6 id="11-整数"><a class="markdownIt-Anchor" href="#11-整数"></a> 1.1 整数</h6><p>  1024的二进制就是2^10。</p><h6 id="12-小数"><a class="markdownIt-Anchor" href="#12-小数"></a> 1.2  小数</h6><p>  小数部分的计算逻辑如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">0.0256 × 2 = 0.0512 → 取 0</span><br><span class="line">0.0512 × 2 = 0.1024 → 取 0</span><br><span class="line">0.1024 × 2 = 0.2048 → 取 0</span><br><span class="line">0.2048 × 2 = 0.4096 → 取 0</span><br><span class="line">0.4096 × 2 = 0.8192 → 取 0</span><br><span class="line">0.8192 × 2 = 1.6384 → 取 1（取小数部分作为下一步骤的输入）</span><br><span class="line">0.6384 × 2 = 1.2768 → 取 1</span><br><span class="line">0.2768 × 2 = 0.5536 → 取 0</span><br><span class="line">0.5536 × 2 = 1.1072 → 取 1</span><br><span class="line">... </span><br></pre></td></tr></table></figure><p>  直到某一步的计算结果为0，或者超过尾数位的长度（双精度为52位）。最终小数部分的二进制是：<code>000001101000110110111000101110101100011100</code> 。</p><h6 id="13-整合整数跟小数两部分"><a class="markdownIt-Anchor" href="#13-整合整数跟小数两部分"></a> 1.3 整合整数跟小数两部分</h6><p>  那么十进制1024.0256对应的二进制就是：<code>10000000000.000001101000110110111000101110101100011100</code>。</p><h5 id="步骤2规格化"><a class="markdownIt-Anchor" href="#步骤2规格化"></a> 步骤2：规格化</h5><p>  将步骤1中的二进制格式化为<code>1.xxxxx</code>的形式并且计算出指数。</p><h6 id="21-格式化"><a class="markdownIt-Anchor" href="#21-格式化"></a> 2.1 格式化</h6><p>  即<code>1.0000000000000001101000110110111000101110101100011100 × 2^10</code>，也就是对步骤1中的二进制的小数点往左移动10位。</p><h6 id="22-指数偏移"><a class="markdownIt-Anchor" href="#22-指数偏移"></a> 2.2 指数偏移</h6><p>  执行2.1后计算出的指数为10，那么结合双精度的偏置值（1023），最终的指数值为：<code>10 + 1023 = 1033</code>，即<code>10000001001</code>。</p><h5 id="步骤3获取尾数部分"><a class="markdownIt-Anchor" href="#步骤3获取尾数部分"></a> 步骤3：获取尾数部分</h5><p>  双精度格式中，尾数部分需要 52 位。从规格化的数中取出尾数部分（不包括开头的 1），如果不够 52 位，则用 0 补足。即<code>0000000000000001101000110110111000101110101100011100</code>。</p><h5 id="步骤4合成ieee-754格式"><a class="markdownIt-Anchor" href="#步骤4合成ieee-754格式"></a> 步骤4：合成IEEE 754格式</h5><p>  对于这个双精度的浮点数：1024.0256，转化后的IEEE 754为：</p><ul><li>符号位: 0 （1位）</li><li>指数位: 10000001001（11位）</li><li>尾数位: 0000000000000001101000110110111000101110101100011100（52位）</li></ul><p>  单精度的转化方式类似于双精度，这里不赘述。</p><h3 id="负数的问题"><a class="markdownIt-Anchor" href="#负数的问题"></a> 负数的问题</h3><p>  尽管通过IEEE 754标准，我们可以将双精度/单精度的浮点数用long/int类型表示，但如果浮点数是负值，会存在以下的问题，我们以双精度为例：两个负的双精度浮点数在转化为long值后，这两个long值的大小关系会发生倒置，即不是转化前浮点数对应的大小关系。</p><p>  例如我们有以下两个浮点数：<code>-1.234</code>和 <code>-2.345</code>，转成long类型后的值如下所示：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/NumericUtils/2.png"><p>  图2可知，long类型的a于b之间的大小关系跟转化前的浮点数之间的大小关系是相反的。而两个正数之间则不会发生这样的倒置：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/NumericUtils/3.png"><p>  当然正/负的浮点数之间依然可以靠符号位保持原来的大小关系。所以要想一个办法，不能变更负数之间的大小关系。</p><h3 id="异或操作"><a class="markdownIt-Anchor" href="#异或操作"></a> 异或操作</h3><p>  看下Lucene中是如何处理负数问题的：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/utils/NumericUtils/4.png"><p>  图4中，bits为IEEE754标准的值。可以看到上述过程分为三步：</p><ul><li><p>第一步：带符号的右移63位的目的是取出符号位</p><ul><li>如果是正的浮点数，则结果为0，即所有bit位都是0</li><li>如果是负的浮点数，则结果为-1，即所有bit位都是1</li></ul></li><li><p>第二步：与<code>0x7fffffffffffffffL</code>，这里需要注意的是 <code>&amp;</code>的优先级比<code>^</code>高。</p><ul><li>如果是正的浮点数，则结果为<code>0b00000000_00000000_00000000_00000000_00000000_00000000_00000000_00000000</code></li><li>如果是负的浮点数，则结果为<code>0b01111111_11111111_11111111_11111111_11111111_11111111_11111111_11111111</code></li></ul></li><li><p>第三步：与IEEE754标准的值做异或操作</p><ul><li>如果是正的浮点数，那么bits会与<code>0b00000000_00000000_00000000_00000000_00000000_00000000_00000000_00000000</code>异或，由于跟0执行异或操作时不会改变任何bit位，即不对bits做调整，所以正如图4的注释说到：<code>or back to the original</code>，也就是说这个方法不会变更正的浮点数</li><li>如果是负的浮点数，那么bits会与<code>0b01111111_11111111_11111111_11111111_11111111_11111111_11111111_11111111</code>异或，由于它的最高位是0，其他位都是1，也就是bits的符号位不变，并且其他位（指数位跟尾数位）都会取反。这样就能逆转两个负数之间的大小关系，解决了上述的负数问题<ul><li>为什么取反后就能逆转两个负数之间的大小关系：因为指数位跟尾数位是不考虑符号位的二进制比较</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  Lucene中用BKD存储的数值无论是哪种类型（long，int，float，double，BigInteger），为了便于使用相同的代码逻辑中实现BKD的遍历以及&lt;a class=&quot;link&quot;   href=&quot;https://amazingkoala.com.cn/L</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="gongjulei" scheme="http://example.com/categories/Lucene/gongjulei/"/>
    
    
    <category term="util" scheme="http://example.com/tags/util/"/>
    
    <category term="byte" scheme="http://example.com/tags/byte/"/>
    
    <category term="sortable" scheme="http://example.com/tags/sortable/"/>
    
  </entry>
  
  <entry>
    <title>索引文件之vec&amp;vem&amp;vex（Lucene 9.8.0）</title>
    <link href="http://example.com/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;vem&amp;vex/"/>
    <id>http://example.com/Lucene/suoyinwenjian/2023/1023/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bvec&amp;vem&amp;vex/</id>
    <published>2023-10-23T14:39:00.000Z</published>
    <updated>2023-10-23T14:52:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>  本篇文章将介绍Lucene中向量搜索相关的索引文件。当前版本中由三个索引文件，即文件后缀名为.vec、.vex、.vem的文件，文件中包含的内容主要包括图的分层信息，每一层中节点的编号，向量值，相连的邻居节点等信息。</p><p>  向量搜索的实现基于这篇论文:<a class="link"   href="https://arxiv.org/abs/1603.09320" >Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs<i class="fas fa-external-link-alt"></i></a>，由于索引文件中有些信息的概念其实是先需要了解这篇论文或者HNSW算法才可以，并且写这篇文章的主要目的是为了在随后介绍NHSW在Lucene中的实现的文章做准备的，因此通过这篇文章只需要了解索引文件中存放了哪些信息，以及对应的数据结构就可以了。</p><p>  先给出这三个索引文件的数据结构之间的关联图，然后我们一一介绍这些字段的含义：</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/1.png"><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6/vec&amp;vem&amp;vex/combine.html" >点击<i class="fas fa-external-link-alt"></i></a>查看大图</p><h2 id="数据结构"><a class="markdownIt-Anchor" href="#数据结构"></a> 数据结构</h2><p>  这三个索引文件总体上由<code>Header</code>、一个或者多个<code>Field</code>、以及<code>Footer</code>组成。</p><ul><li>Header：主要包含文件的唯一标示信息，版本号，索引名字等一些信息</li><li>Footer：记录校验和（checksum）算法ID和校验和，在读取索引文件时可以校验索引文件的合法性</li><li>Field: 该字段中包含了某个域下的所有向量信息。注意到该字段可以是多个，取决于一个段中定义的向量域（KnnFloatVectorField）的数量，例如下图中定义了两个向量域，域名分别为别<code>vector1</code>以及<code>vector2</code>，那么在索引文件中就会有两个<code>Field</code>。</li></ul><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/4.png"><h3 id="vec"><a class="markdownIt-Anchor" href="#vec"></a> .vec</h3><p>  索引文件.vec中主要存放的数据为所有的向量值vectorData、文档号信息DocIdData以及节点编号与文档号的映射关系OrdToDocData。</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/5.png"><h4 id="alignpaddingvalue"><a class="markdownIt-Anchor" href="#alignpaddingvalue"></a> AlignPaddingValue</h4><p>  对齐字节数。</p><p>  Lucene按照字节单位写入到文件中，在后续的数据写入之前会先将当前的文件指针对齐到指定的字节倍数（写入填充值0），来优化内存映射文件（mmap）的读取性。AlignPaddingValue的值必须是2的幂。</p><h4 id="vectordata"><a class="markdownIt-Anchor" href="#vectordata"></a> VectorData</h4><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/6.png"><p>  向量值。</p><p>  图6中每一个<code>Value</code>对应一篇文档中某个向量域的向量值。每个向量值使用相同数量的字节存储。例如图4中第50行<code>-0.18344f, 0.95567f, -0.46423f</code>对应一个<code>Value</code>。</p><h4 id="dociddata"><a class="markdownIt-Anchor" href="#dociddata"></a> DocIdData</h4><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/7.png"><p>  文档号集合。</p><p>  该字段存放包含向量域的文档号。使用<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/gongjulei/2020/0511/IndexedDISI%EF%BC%88%E4%B8%80%EF%BC%89/" >IndexedDISI<i class="fas fa-external-link-alt"></i></a>存储文档号。</p><h4 id="ordtodocdata"><a class="markdownIt-Anchor" href="#ordtodocdata"></a> OrdToDocData</h4><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/8.png"><p>  节点编号与文档号的映射关系。</p><p>  每一个向量都有一个节点编号（node id），通过OrdToDocData，就可以根据节点编号找到对应的文档号，也就是包含这个向量的文档号。在添加文档过程中，每一个向量根据添加的先后顺序，都会被赋予一个从0开始递增的节点编号。例如图9中，添加了三篇文档，其中文档0中向量的节点编号为0，文档2中向量的节点编号为1。另外注意的是，同一篇文档中只允许定义一个相同域名的向量域。</p><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/9.png"><p>  在图6中我们说到，所有的向量值都存放在VectorData，在读取阶段就可以根据节点编号以及向量值对应的字节数，实现随机访问向量值。</p><p>  最终OrdToDocData经过<a class="link"   href="https://amazingkoala.com.cn/Lucene/yasuocunchu/2020/1030/DirectMonotonicWriter&amp;&amp;Reader/" >DirectMonotonicWriter<i class="fas fa-external-link-alt"></i></a>编码压缩后写入到索引文件中。</p><h3 id="vex"><a class="markdownIt-Anchor" href="#vex"></a> .vex</h3><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/10.png"><h4 id="levelnodeneighbor"><a class="markdownIt-Anchor" href="#levelnodeneighbor"></a> LevelNodeNeighbor</h4><p>图11：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/11.png"><p>  邻居节点集合。</p><p>  图11中，先按照层级划分邻居节点集合，从左到右LevelNodeNeighbor分别代表第0层、1层。。。N层。随后在某一层中，按照该层中节点编号顺序，从左到右NodeNeighbor分别代表该层中第一个、二个、N个节点的邻居节点集合。</p><ul><li>NeighborNumber：邻居节点的数量</li><li>NeighborNode：邻居节点的编号</li></ul><p>  另外由于节点的邻居节点集合已经按照节点编号排序，因此会先计算相邻之间的差值（差值存储），使得尽量使用少的bit来存储。例如有以下的邻居节点集合：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 12, 18, 27, 92, 94, 139, 167, 250]</span><br></pre></td></tr></table></figure><p>  差值计算后的集合如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 11, 6, 9, 65, 2, 45, 28, 83]</span><br></pre></td></tr></table></figure><p>  由于在存储这个集合时，会选择固定bit位数，即按照集合中最大值所需要的bit进行存储，优化前后所有的值分别使用250跟83对应的bit位数存储。</p><h4 id="levelnodeoffsetsdata"><a class="markdownIt-Anchor" href="#levelnodeoffsetsdata"></a> LevelNodeOffsetsData</h4><p>图12：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/12.png"><p>  邻居节点在索引文件.vex的位置信息。</p><p>  在图11中，记录了所有层中所有节点的邻居节点的信息，LevelNodeOffsetsData则是用于记录每一层的每一个节点的所有邻居节点在索引文件.vex中长度。在源码中使用一个二维数组来描述，下图是<a class="link"   href="https://github.com/LuXugang/Lucene-7.x-9.x/blob/master/LuceneDemo9.8.0/src/main/java/TestSparseKNN1.java" >示例<i class="fas fa-external-link-alt"></i></a>中的实际数据：</p><p>图13：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/13.png"><p>  图13中，<a class="link"   href="https://github.com/LuXugang/Lucene-7.x-9.x/blob/master/LuceneDemo9.8.0/src/main/java/TestSparseKNN1.java" >示例<i class="fas fa-external-link-alt"></i></a>构建出的图结构有三层，并且第0层中有333个节点，第一层中有22个节点，第二层中有2个节点。比如下图中，在索引文件的读取阶段，第1层的第二个节点的所有邻居节点在索引文件vec中的区间如下所示：</p><p>图14：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/14.png"><p>  最终LevelNodeOffsetsData经过<a class="link"   href="https://amazingkoala.com.cn/Lucene/yasuocunchu/2020/1030/DirectMonotonicWriter&amp;&amp;Reader/" >DirectMonotonicWriter<i class="fas fa-external-link-alt"></i></a>编码压缩后写入到索引文件中。</p><h3 id="vem"><a class="markdownIt-Anchor" href="#vem"></a> .vem</h3><p>图15：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/15.png"><p><a href="">点击</a>查看大图</p><h4 id="简易字段"><a class="markdownIt-Anchor" href="#简易字段"></a> 简易字段</h4><p>图16：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/16.png"><p>  图16中<font color="red">红框</font>标注的字段都属于简易字段：</p><ul><li>FieldNumber：域的编号</li><li>EncodingOrdinal：向量值的数据类型，可以用8bit或者32bit表示一个数值</li><li>SimilarityFunctionOrdinal：向量相似度函数。用来计算两个节点之间的距离。目前支持EUCLIDEAN（欧几里得/L2距离）、DOT_PRODUCT（点积或数量积）、COSINE（余弦相似度）、MAXIMUM_INNER_PRODUCT（最大内积）</li><li>VectorDimension：向量的维度。例如图9中的向量维度位3。</li><li>M：节点可以连接的邻居数量上限。第0层的节点可以连接的邻居数量上限为2M。</li><li>FieldEndMarker：固定值-1。在索引读取阶段，会所有域的信息逐个字节读入到内存中，该字段作为一个标记，当读取到该值时，说明已经读取完所有的域的信息。</li></ul><h4 id="vectordatameta"><a class="markdownIt-Anchor" href="#vectordatameta"></a> VectorDataMeta</h4><p>图17：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/17.png"><p>  向量值在索引文件.vec中的起始读取位置以及读取的长度。</p><h4 id="levelnodeneighbormeta"><a class="markdownIt-Anchor" href="#levelnodeneighbormeta"></a> LevelNodeNeighborMeta</h4><p>图18：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/18.png"><p>  邻居节点在索引文件.vex中的起始读取位置以及读取的长度。</p><h4 id="docidmeta"><a class="markdownIt-Anchor" href="#docidmeta"></a> DocIdMeta</h4><p>图19：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/19.png"><p>  文档号信息在索引文件.vec中的起始读取位置以及读取长度，至于其他字段的描述见<a class="link"   href="https://amazingkoala.com.cn/Lucene/gongjulei/2020/0511/IndexedDISI%EF%BC%88%E4%B8%80%EF%BC%89/" >IndexedDISI<i class="fas fa-external-link-alt"></i></a>。</p><h4 id="ordtodocmeta"><a class="markdownIt-Anchor" href="#ordtodocmeta"></a> OrdToDocMeta</h4><p>图20：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/20.png"><p>  节点编号与文档号的映射关系在索引文件.vec中的起始读取位置（Offset）以及读取的长度（OrdToDocDataLength）。其他字段见<a class="link"   href="https://amazingkoala.com.cn/Lucene/yasuocunchu/2020/1030/DirectMonotonicWriter&amp;&amp;Reader" >DirectMonotonicWriter<i class="fas fa-external-link-alt"></i></a>。</p><h4 id="levelnode"><a class="markdownIt-Anchor" href="#levelnode"></a> LevelNode</h4><p>图21：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/21.png"><p>  除了第0层外，其他层的的节点信息。</p><p>  由于第0层中的节点数量是全量的节点，所以只需要根据从<code>DocIdMeta</code>读取出<code>count</code>字段，即文档的数量，那么第0层的节点编号区间为[0, count - 1]。而其他层的节点编号不是连续的，所以需要一个一个记录。</p><ul><li>LevelNumber：图中层的数量。</li><li>Node：每一层（除了第0层）的节点编号信息</li><li>NodeNumer：当前层中节点的数量</li><li>NodeId：节点的编号</li></ul><h4 id="levelnodeoffsetsmeta"><a class="markdownIt-Anchor" href="#levelnodeoffsetsmeta"></a> LevelNodeOffsetsMeta</h4><p>图22：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/索引文件/vec&vem&vex/22.png"><p>邻居节点位置信息在索引文件.vex中的起始读取位置（Offset）以及读取的长度（LevelNodeOffsetsDataLength）。其他字段见·<a class="link"   href="https://amazingkoala.com.cn/Lucene/yasuocunchu/2020/1030/DirectMonotonicWriter&amp;&amp;Reader" >DirectMonotonicWriter<i class="fas fa-external-link-alt"></i></a>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  本篇文章将介绍Lucene中向量搜索相关的索引文件。当前版本中由三个索引文件，即文件后缀名为.vec、.vex、.vem的文件，文件中包含的内容主要包括图的分层信息，每一层中节点的编号，向量值，相连的邻居节点等信息。&lt;/p&gt;
&lt;p&gt;  向量搜索的实现基于这篇论文:&lt;a </summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="suoyinwenjian" scheme="http://example.com/categories/Lucene/suoyinwenjian/"/>
    
    
    <category term="index" scheme="http://example.com/tags/index/"/>
    
    <category term="vec" scheme="http://example.com/tags/vec/"/>
    
    <category term="vem" scheme="http://example.com/tags/vem/"/>
    
    <category term="vex" scheme="http://example.com/tags/vex/"/>
    
    <category term="indexFile" scheme="http://example.com/tags/indexFile/"/>
    
  </entry>
  
  <entry>
    <title>Scorer（Lucene 9.6.0）</title>
    <link href="http://example.com/Lucene/Search/2023/0814/Scorer/"/>
    <id>http://example.com/Lucene/Search/2023/0814/Scorer/</id>
    <published>2023-08-13T16:00:00.000Z</published>
    <updated>2023-10-10T03:07:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>  阅读文本之前，建议先看下文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0804/ImpactsDISI" >ImpactsDISI<i class="fas fa-external-link-alt"></i></a>，有助于理解。先直接给出Scorer在Lucene中的注释：</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/1.png"  width=""><p>  图1中的注释已经几乎不能完全用于理解Scorer类，因为这个类经过十几年的迭代，注释却没有保持更新，甚至部分描述还不准确。例如<font color="red">红框</font>标注的注释说到：使用给定的<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0827/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E5%9B%9B%EF%BC%89" >Similarity<i class="fas fa-external-link-alt"></i></a>对文档进行打分。由于在早期Scorer的构造函数的参数中需要提供Similarity对象，但在十年前提交的<a class="link"   href="https://issues.apache.org/jira/browse/LUCENE-2876" >LUCENE-2876<i class="fas fa-external-link-alt"></i></a>中移除了该参数。该<a class="link"   href="https://github.com/apache/lucene/pull/12494" >PR merge<i class="fas fa-external-link-alt"></i></a>后会更新这些注释。见旧版的Scorer类：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/2.png"  width=""><p>  因此我想通过直接介绍Scorer中的方法，让大家直观的了解Scorer在目前版本（Lucene 9.6.0）中定位。</p><h2 id="scorer的抽象方法"><a class="markdownIt-Anchor" href="#scorer的抽象方法"></a> Scorer的抽象方法</h2><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/3.png"  width="500"><p>  Scorer类继承Scorable类，因此同样会介绍父类中的方法。</p><h3 id="方法一"><a class="markdownIt-Anchor" href="#方法一"></a> 方法一</h3><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/4.png"  width=""><p>  Scorer的Iterator()方法说的是，它能提供一个文档号迭代器，即<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89" >DocIdSetIterator<i class="fas fa-external-link-alt"></i></a>对象。</p><p>  DocIdSetIterator中包含了满足查询条件（部分实现可能不满足查询条件，比如DocValuesIterator，下文会介绍）的文档号集合，以及遍历这些文档号的方式。在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89" >BulkScorer（一）<i class="fas fa-external-link-alt"></i></a>中介绍了DocIdSetIterator的概念，并且在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0804/ImpactsDISI" >ImpactsDISI<i class="fas fa-external-link-alt"></i></a>中介绍了DocIdSetIterator的一种实现方式，并且说到ImpactsDISI可以利用<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0904/Impact" >Impact<i class="fas fa-external-link-alt"></i></a>信息来实现特殊的文档号遍历方式。</p><h3 id="方法二"><a class="markdownIt-Anchor" href="#方法二"></a> 方法二</h3><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/5.png"  width=""><p>  通过这个方法可以看出，Scorer对象中含有状态值，即当前正在进行打分的文档号。</p><p>  我们在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89" >BulkScorer（一）<i class="fas fa-external-link-alt"></i></a>也提到了DocIdSetIterator对象也含有状态值。没错，Scorer对象中的状态值在很多子类实现中就是DocIdSetIterator对象中的状态值。下图是Scorer的TermScorer的部分实现：</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/6.png"  width=""><p>  图6中，从第39行的代码可以看出，Iterator跟postingEnum是同一个对象，第68、59行代码分别是上文中方法一、方法二的实现。</p><h3 id="方法三"><a class="markdownIt-Anchor" href="#方法三"></a> 方法三</h3><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/7.png"  width="800"><p>  该方法用于通知Scorer对象，目前已收集到的文档中最小的打分值（历史最低打分值）。</p><p>  通常在执行TopN查询并且使用文档打分值作为排序规则时会调用该方法。比如我们在收集器<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0812/Collector%EF%BC%88%E4%B8%80%EF%BC%89" >Collector<i class="fas fa-external-link-alt"></i></a>中收集到N篇文档后，可以通过一个排序规则为打分值的优先级队列获取堆中最小的打分值minCompetitiveScore，意思是后续的文档的打分值只有大于minCompetitiveScore才是具有竞争力的。此时收集器就会通过调用该方法通知Scorer对象：目前具有竞争力的文档的打分值必须大于minCompetitiveScore。使得一些Scorer的子类可以基于这个minCompetitiveScore实现优化。优化的方向其实就是对满足查询条件的待遍历的文档号集合进行&quot;瘦身&quot;，跳过掉那些打分值小于等于minCompetitiveScore的文档号，或者说筛选出高于minCompetitiveScore的文档号集合。</p><p>  我们还是以TermScorer为例，对于该方法的实现逻辑如下所示：</p><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/8.png"  width="800"><p>  图8中，TermScorer对象将minCompetitiveScore信息告知了ImpactsDISI对象。至于ImpactsDISI如何基于minCompetitiveScore实现文档号集合的&quot;瘦身&quot;，见文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0804/ImpactsDISI" >ImpactsDISI<i class="fas fa-external-link-alt"></i></a>。</p><h3 id="方法四"><a class="markdownIt-Anchor" href="#方法四"></a> 方法四</h3><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/9.png"  width="800"><p>  上文的方法二中描述的是当前正在进行打分的文档号，那么调用方法四就是对这篇文档进行打分。</p><p>  下图是TermScorer中的实现方式：</p><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/10.png"  width="800"><p>  图10中，<code>docScorer</code>是封装了Similarity的LeafSimScorer对象，只需要提供文档号（可以根据文档号获取到<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0305/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bnvd&amp;&amp;nvm" >标准化值<i class="fas fa-external-link-alt"></i></a>）以及在这篇文档中的词频就可以进行打分（见文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0804/ImpactsDISI" >ImpactsDISI<i class="fas fa-external-link-alt"></i></a>中关于打分公式的介绍）。</p><h3 id="方法五"><a class="markdownIt-Anchor" href="#方法五"></a> 方法五</h3><p>图11：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/11.png"  width="800"><p>  该方法是为了避免数据稀疏和零概率问题而使用的平滑技术，使得查询文档中不存在的term也可以进行打分。</p><p>  下图是TermScorer中的实现方式：</p><p>图12：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/12.png"  width="800"><p>  图12中，Lucene的打分公式支持计算平滑值，只需要将词频值设置为0即可。</p><h3 id="方法六"><a class="markdownIt-Anchor" href="#方法六"></a> 方法六</h3><p>图13：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/13.png"  width=""><p>  Scorer不仅仅提供<a href="###%E6%96%B9%E6%B3%95%E4%B8%80">方法一</a>中的DocIdSetIterator对象，还会根据具体的子类需要同时实现TwoPhaseIterator，当DocIdSetIterator中的文档号集合<strong>不全部</strong>满足查询条件时，即所谓的接近（approximation）满足查询结果，这时候就可以用TwoPhaseIterator进行准确的判断某个文档号对应的文档是否满足查询条件。</p><h4 id="twophaseiterator"><a class="markdownIt-Anchor" href="#twophaseiterator"></a> TwoPhaseIterator</h4><p>  我们通过SortedNumericDocValuesSetQuery来介绍下使用TwoPhaseIterator对接近（approximation）满足查询结果的文档号集合如何进行准确匹配。</p><p>图14：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/14.png"  width=""><p>  代码第44行，使用NumericDocValuesField.newSlowSetQuery进行查询，查询条件是代码43行中包含数值至少包含2或者3的文档。</p><p>  由于文档中只使用了<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0218/DocValues/" >DocValues<i class="fas fa-external-link-alt"></i></a>，因此不支持通过term（即例子中的2跟3两个数值）查询文档号，因此对于NumericDocValuesField.newSlowSetQuery，<strong>只能遍历所有的文档</strong>，随后每处理一篇文档就使用TwoPhaseIterator中的matches方法判断文档中是否包含2、3这两个正排值中的一个。下面给出matches的实现方式：</p><p>图15：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/15.png"  width=""><p>  由于图14中我们使用了SortedNumericDocValuesField，它属于多值的情况，即一篇文档中，某个域的正排值可以是多个。图15中代码第133行的count计算的是当前处理的文档中的正排值的数量。对于图14中的三篇文档，他们的count值分别是：3、1、2。</p><p>  图15中的numbers是一个包含了查询条件，即图14中代码第43行的对象。代码第134行的for循环描述的是依次读取当前文档中的正排值，判断numbers是否包含，如果包含，说明当前文档满足查询条件。</p><h3 id="方法七"><a class="markdownIt-Anchor" href="#方法七"></a> 方法七</h3><p>图16：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/16.png"  width=""><p>  注释中说到调用该方法能到达文档号target所在的block，目的是获取到打分信息。</p><p>  该方法跟方法八一样都是为了实现[block-max WAND](<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0916/block-max" >https://www.amazingkoala.com.cn/Lucene/Search/2020/0916/block-max<i class="fas fa-external-link-alt"></i></a> WAND（一）)算法在<a class="link"   href="https://issues.apache.org/jira/browse/LUCENE-8135" >LUCENE-8135<i class="fas fa-external-link-alt"></i></a>中出现的。尽管在本篇文章发布之前还未全部完成block-max WAND算法的介绍，但我们可以通过文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0804/ImpactsDISI" >ImpactsDISI<i class="fas fa-external-link-alt"></i></a>了解该方法以及方法八，在本篇文章中，只概述下这两个方法：</p><ul><li>文档号在<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0904/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%B9%8Bdoc&amp;&amp;pos&amp;&amp;pay" >索引文件.doc<i class="fas fa-external-link-alt"></i></a>中是按block进行划分存储的，默认每128篇文档号作为一个block，<code>advanceShallow</code>方法中会通过<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0106/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E8%B7%B3%E8%A1%A8SkipList" >跳表<i class="fas fa-external-link-alt"></i></a>找到target所在block</li><li>当跳到所在block后，就可以通过调用方法八 getMaxScore计算该block中所有文档中最大的打分值。至于为什么要获取maxScore，以及计算方式，请查看文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0804/ImpactsDISI" >ImpactsDISI<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="方法八"><a class="markdownIt-Anchor" href="#方法八"></a> 方法八</h3><p>图17：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/Scorer/17.png"  width=""><p>  见方法七中的介绍。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  至此，图3中的所有方法除了 <code>getChildren</code>和<code>getWeight</code>方法都已经介绍完毕。可以看出Scorer类型提供下面的功能：</p><ul><li>对文档进行打分<ul><li>打分逻辑取决于不同子类中定义的<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0827/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E5%9B%9B%EF%BC%89" >Similarity<i class="fas fa-external-link-alt"></i></a>对象</li></ul></li><li>提供用于遍历的文档号集合<ul><li>该集合在大部分实现中是一个满足查询条件的文档号集合，如果不是，那么会额外提供一个TwoPhaseIterator实现准确匹配</li></ul></li><li>记录历史最低打分值，用于doc skip<ul><li>用于在TopN的查询中，结合跳表、文档分块存储特点，不对那些低于历史最低分值的文档进行处理</li></ul></li></ul><p><a class="link"   href="https://www.amazingkoala.com.cn/attachment/Lucene/Search/Scorer/Scorer.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  阅读文本之前，建议先看下文章&lt;a class=&quot;link&quot;   href=&quot;https://www.amazingkoala.com.cn/Lucene/Search/2023/0804/ImpactsDISI&quot; &gt;ImpactsDISI&lt;i class=&quot;fas fa</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="impact" scheme="http://example.com/tags/impact/"/>
    
    <category term="scorer" scheme="http://example.com/tags/scorer/"/>
    
  </entry>
  
  <entry>
    <title>ImpactsDISI（Lucene 9.6.0）</title>
    <link href="http://example.com/Lucene/Search/2023/0804/ImpactsDISI/"/>
    <id>http://example.com/Lucene/Search/2023/0804/ImpactsDISI/</id>
    <published>2023-08-03T16:00:00.000Z</published>
    <updated>2023-10-09T07:55:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>  在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89" >BulkScorer（一）<i class="fas fa-external-link-alt"></i></a>中，我们介绍了抽象类DocIdSetIterator类，而ImpactsDISI是DocIdSetIterator的其中一种实现，当排序规则为文档打分值时，使得在查询TopN遍历文档时，可以跳过那些不具备竞争力的文档。</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/1.png"  width="500"><h2 id="前置知识"><a class="markdownIt-Anchor" href="#前置知识"></a> 前置知识</h2><p>  在展开介绍ImpactsDISI原理之前，我们需要先了解一些Lucene中的其他知识点，包括Impact、打分公式、跳表。</p><h3 id="impact"><a class="markdownIt-Anchor" href="#impact"></a> Impact</h3><p>  文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0904/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%B9%8Bdoc&amp;&amp;pos&amp;&amp;pay" >索引文件的读取（十二）<i class="fas fa-external-link-alt"></i></a>中详细的介绍了其概念，本文中我们只简单介绍提下Impact中几个重要的内容。</p><p>  Impact是Lucene中的一个类，在<strong>生成索引阶段</strong>，某个term在一篇文档中的词频freq和<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0305/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bnvd&amp;&amp;nvm" >标准化值<i class="fas fa-external-link-alt"></i></a>(normalization values)norm会使用Impact对象记录，并且写入到<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0904/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdoc" >索引文件.doc<i class="fas fa-external-link-alt"></i></a>中。</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/2.png"  width=""><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/3.png"  width=""><p>  图3<font color="red">红框</font>标注的内容即某个term在一篇文档中的freq和norm。注意的是，由于使用差值存储freq和norm，如果当前norm跟上一个norm的差值为0，则只存储freq。</p><h3 id="打分公式"><a class="markdownIt-Anchor" href="#打分公式"></a> 打分公式</h3><p>  当前版本Lucene9.6.0的打分公式如下所示：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/4.png"><p>  当需要计算包含某个term的文档的打分值时就会调用图3中的score方法。<font color="red">红框</font>中的注释大意是：</p><ul><li>如果norm相等，那么freq较大对应的文档打分值会相等或者更高</li><li>如果freq相等，那么norm较小对应的文档打分值会相等或者更高</li></ul><p>  这意味我们不需要真正的去调用图3中的打分方法获取文档的<strong>精确的</strong>打分值，而是可以仅通过freq和norm这两个值就可以<strong>粗略</strong>判断文档之间的打分值高低。</p><h3 id="跳表"><a class="markdownIt-Anchor" href="#跳表"></a> 跳表</h3><p>  文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0103/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E8%B7%B3%E8%A1%A8SkipList" >索引文件的生成（三）之跳表SkipList<i class="fas fa-external-link-alt"></i></a>、<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0106/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E8%B7%B3%E8%A1%A8SkipList" >索引文件的生成（四）之跳表SkipList<i class="fas fa-external-link-alt"></i></a>详细介绍了Lucene中跳表的构建过程以及读取过程，本文中不再赘述。Lucene中通过分块（block）、分层（level）的方式对文档号构建跳表。</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/5.png"  width=""><p>  <strong>图5是只基于Lucene中跳表的的实现思想，并不是真正的实现方式</strong>。该图描述的是对文档号集合[0, 3455]一共3456个文档号进行跳表的构建。Lucene默认每处理128（即图5中的skipInterval，源码中的变量）个文档号就构建一个block（分块），该block在源码中对应为图3中level=0层中第一个SkipDatum，并且每生成3（即图5中的skipMultiplier，源码中的变量，默认值为8）个SKipDatum就在上一层，即level=1层构建一个新的SkipDatum（分层）。</p><p>  图5中每一个SkipDatum跟图3的索引文件的对应关系如下所示：</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/6.png"  width=""><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/skiplistdoc.html" >高清大图<i class="fas fa-external-link-alt"></i></a></p><h3 id="impactsdisi的文档遍历"><a class="markdownIt-Anchor" href="#impactsdisi的文档遍历"></a> ImpactsDISI的文档遍历</h3><p>  ImpactsDISI作为<a href="(https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89)">DocIdSetIterator</a>的子类，其核心为如何实现图7中的抽象方法advance(int target)方法，该方法描述都是从满足查询条件的文档号集合中找到<strong>下一个大于等于</strong>target的文档号。</p><p>  如果target的值为3，并且有下面两个文档号集合：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">集合一：[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">集合二：[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>] </span><br></pre></td></tr></table></figure><p>  对于集合一，advance方法的返回值为3；对于集合二，advance方法的返回值为4.</p><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/7.png"  width=""><p>  该方法在ImpactsDISI中的实现流程图如下：</p><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/8.png"  width="800"><h4 id="名词解释"><a class="markdownIt-Anchor" href="#名词解释"></a> 名词解释</h4><p>  先介绍下流程图中的一些名词：</p><ul><li>block：在图6的<strong>level=0层</strong>，每处理128个文档号就构建一个block。在level=0层对应SkipDatum<br />- maxScore：某个block的所有文档打分的最大值<br />- minCompetitiveScorer：在执行TopN的查询时，当已经收集了N篇文档，通过规则为文档打分值的优先级队列进行排序后，堆中最小的文档打分值即minCompetitiveScorer。如果是升序，意味着后续收集到的文档的打分值必须大于该值才被认为是具有竞争力的<br />- NO_MORE_DOCS：Lucene中定义的边界值，常在遍历文档时使用，表示遍历结束</li></ul><h4 id="不需要进行skip"><a class="markdownIt-Anchor" href="#不需要进行skip"></a> 不需要进行skip</h4><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/9.png"  width=""><p>  我们先介绍下<code>根据target更新block</code>。它描述的是下一个进行遍历的block。在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0106/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E8%B7%B3%E8%A1%A8SkipList" >索引文件的生成（四）<i class="fas fa-external-link-alt"></i></a>中介绍的哨兵数组skipDoc记录了每一层当前遍历的block，使得可以快速定位target属于某个block。</p><p>  在level=0层，每128篇文档号就生成一个block，即图6中的SkipDatum。注意的是每个SkipDatum中<font color="Black">impacts</font>的<font color="orange">Impact</font>数量是<strong>小于等于</strong>128个，意味着如果这个block的maxScore大于minCompetitiveScorer，说明<strong>至少包含</strong>一篇文档是具有竞争力的，那么这个block中的所有文档我们都需要处理才能明确知道哪些文档是具有竞争力的，所以这种情况下就不能进行skip。</p><p><strong>为什么每个SkipDatum的<font color="Black">impacts</font>的<font color="orange">Impact</font>数量是小于等于128个?</strong></p><p>  首先一个block中的文档数量为128个，另外在索引阶段我们不需要记录某个term在这128篇文档中的Impact信息，即freq和norm。因为<strong>我们记录Impact的目的是为了在查询阶段能计算出block的maxScore值</strong>，根据上文中介绍的打分公式，如果某个term在两篇文档中的norm相同，那么只需要记录freq较大的Impact（详细的例子见<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0904/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%B9%8Bdoc&amp;&amp;pos&amp;&amp;pay" >索引文件的读取（十二）<i class="fas fa-external-link-alt"></i></a>）。</p><p>  如果当前target跟上一个target不在同一个block中，那么就需要<code>更新block</code>。否则就直接返回当前target，因为它对应的文档打分值有可能是具有竞争力的。</p><h4 id="进行skip"><a class="markdownIt-Anchor" href="#进行skip"></a> 进行skip</h4><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/10.png"  width="900"><p>  当前target跟上一个target不在同一个block，那么在执行<code>根据target更新block</code>后，先要计算这个block中的macScore，如果maxScore大于等于minCompetitiveScore，那么返回当前target，因为它对应的文档打分值有可能是具有竞争力的。</p><p>  在执行<code>能否skip到其他block中?</code>时，就可以通过跳表中其他层的SkipDatum的Impact信息来进行skip，例如下图中level=1的<font color="red">红框</font>标注的SkipDatum中它包含了下一层，即level=0中三个（源码中默认是8）SkipDatum中的所有Impact。如果根据这些Impact计算出的分数还是小于minCompetitiveScore，那么就可以跳过level=0这三个block。</p><p>图11：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/11.png"  width=""><p><a class="link"   href="http://www.amazingkoala.com.cn/uploads/lucene/Search/ImpactsDISI/skiplistdoc2.html" >高清大图<i class="fas fa-external-link-alt"></i></a></p><p>  上文中如果能实现skip，那么在流程点<code>更新target</code>中会将target的值更新为384。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  Lucene中，设计跳表的目的是为了能在遍历文档号时实现skip。而ImpactDISI利用之前已有的机制，通过额外新增的Impact索引信息实现排序规则为文档打分值的TopN查询的skip。</p><p><a class="link"   href="https://www.amazingkoala.com.cn/attachment/Lucene/Search/ImpactsDISI/ImpactsDISI.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  在文章&lt;a class=&quot;link&quot;   href=&quot;https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89&quot; &gt;BulkScorer（一</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="disi" scheme="http://example.com/tags/disi/"/>
    
    <category term="impact" scheme="http://example.com/tags/impact/"/>
    
  </entry>
  
  <entry>
    <title>BulkScorer（二）（Lucene 9.6.0）</title>
    <link href="http://example.com/Lucene/Search/2023/0724/BulkScorer%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2023/0724/BulkScorer%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2023-07-23T16:00:00.000Z</published>
    <updated>2023-10-09T07:32:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>  本篇文章我们继续介绍BulkScorer的其他子类，下图为BulkScorer主要的几个子类，其中<code>DefaultBulkScorer</code>的介绍可以见文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89" >BulkScorer（一）<i class="fas fa-external-link-alt"></i></a>：</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（二）/1.png"  width="700"><h2 id="reqexclbulkscorer"><a class="markdownIt-Anchor" href="#reqexclbulkscorer"></a> ReqExclBulkScorer</h2><h3 id="实现逻辑"><a class="markdownIt-Anchor" href="#实现逻辑"></a> 实现逻辑</h3><p>  ReqExclBulkScorer中包含了两个成员，一个是名为<code>req</code>的BulkScorer对象，另一个是名为<code>excl</code>的<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89" >DocIdSetIterator<i class="fas fa-external-link-alt"></i></a>对象，它包含了在查询条件中指定的不需要返回的文档号集合（MUST_NOT）。</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（二）/2.png"><p>  在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89" >BulkScorer（一）<i class="fas fa-external-link-alt"></i></a>中我们说到，BulkScorer的score()方法描述的是对某个文档号区间进行遍历，期间过滤掉被删除的文档号，最后使用<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0812/Collector%EF%BC%88%E4%B8%80%EF%BC%89" >Collector<i class="fas fa-external-link-alt"></i></a>收集文档号。在子类ReqExclBulkScorer的实现中，则是根据<code>excl</code>中的文档号将<code>req</code>的文档号集合划分为一个或多个更小的集合。</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（二）/3.png"><p>  图2中，excl中的文档号将req中待遍历的结合划分为3个区间。<strong>在源码实现中，每一个区间对应一次BulkScorer的score()方法的调用</strong>，差别在于不同的文档号区间范围。</p><h3 id="使用场景"><a class="markdownIt-Anchor" href="#使用场景"></a> 使用场景</h3><p>  例如在使用BooleanQuery时，特定查询条件下会使用到ReqExclBulkScorer，其具体逻辑会在介绍BooleanQuery中获取BulkScorer的文章中展开介绍。</p><h2 id="timelimitingbulkscorer"><a class="markdownIt-Anchor" href="#timelimitingbulkscorer"></a> TimeLimitingBulkScorer</h2><p>  TimeLimitingBulkScorer用于为BulkScorer设定一个超时时间，查询超时后通过抛出异常的方式结束BulkScorer的scorer方法的调用。</p><p>  TimeLimitingBulkScorer封装了一个名为<code>in</code>的BulkScorer对象，允许用户设定一个名为<code>queryTimeout</code>的QueryTimeout对象作为查询超时条件。</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（二）/4.png" width="1000"><p>  QueryTimeout类很简单，类中就包含一个<code>shouldExit</code>的方法，下文中会介绍该方法在何时会被调用。</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（二）/5.png" width="1000"><p>  超时条件通过IndexSearcher类中的setTimeout()方法设定。</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（二）/6.png" width="1000"><h3 id="实现逻辑-2"><a class="markdownIt-Anchor" href="#实现逻辑-2"></a> 实现逻辑</h3><p>  TimeLimitingBulkScorer实现BulkScorer的score()的逻辑中，跟ReqExclBulkScorer相似的地方是将封装的BulkScorer对象（图3中的<code>in</code>）的遍历区间拆封成多个一个或多个区间，每遍历完一个区间就调用图4中的<code>shouldExit()</code>方法判断是否已经超时。区间的划分规则则是基于根据文档号数量，每个区间中的文档号数量为interval，其公式如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">interval</span> <span class="operator">=</span> LastInterval + (LastInterval &gt;&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>  LastInterval的值为上一个区间的interval，并且LastInterval的初始值为100。</p><ul><li>第一个区间中的文档号数量：即初始值100</li><li>第二个区间中的文档号数量：(100 + 100 &gt;&gt;2) =  150</li><li>第三个区间中的文档号数量：(150 + 150 &gt;&gt; 2) = 225</li></ul><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（二）/7.png"><h4 id="使用场景-2"><a class="markdownIt-Anchor" href="#使用场景-2"></a> 使用场景</h4><p>  当在IndexSearcher对象中指定了setTimeout()方法后，所有的BulkScorer对象都会进行超时判断。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  基于篇幅，剩余的内容将在下一篇文章中展开介绍。</p><p><a class="link"   href="https://www.amazingkoala.com.cn/attachment/Lucene/Search/BulkScorer/BulkScorer%EF%BC%88%E4%BA%8C%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  本篇文章我们继续介绍BulkScorer的其他子类，下图为BulkScorer主要的几个子类，其中&lt;code&gt;DefaultBulkScorer&lt;/code&gt;的介绍可以见文章&lt;a class=&quot;link&quot;   href=&quot;https://www.amazingkoala</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="bulkScorer" scheme="http://example.com/tags/bulkScorer/"/>
    
  </entry>
  
  <entry>
    <title>BulkScorer（一）（Lucene 9.6.0）</title>
    <link href="http://example.com/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2023/0707/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2023-07-06T16:00:00.000Z</published>
    <updated>2023-10-09T07:29:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>  本篇文章介绍在查询阶段，<a class="link"   href="https://github.com/apache/lucene/blob/f527eb3b12df6d052d1b6dd107b56ab705a95ab1/lucene/core/src/java/org/apache/lucene/search/BulkScorer.java" >BulkScorer<i class="fas fa-external-link-alt"></i></a>相关的知识点，在查询流程中的流程点如下<font color="red">红框</font>所示：</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/1.png"  width="700"><h2 id="父类bulkscorer"><a class="markdownIt-Anchor" href="#父类bulkscorer"></a> 父类BulkScorer</h2><p>  BulkScorer类的定位是一个对满足查询条件的所有文档号进行收集的最初入口。在执行完该类的方法一后，Lucene就完成了一个段中文档号的收集工作。我们仅关注下BulkScorer类中下面的两个方法，如下所示：</p><h3 id="方法一"><a class="markdownIt-Anchor" href="#方法一"></a> 方法一</h3><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/2.png"   width="800"><p>  该方法说的是在某个文档号区间内，即参数<code>min</code>和<code>max</code>组成的区间，进行文档号的收集，同时使用参数<code>acceptDocs</code>过滤掉被删除的文档号，最终满足查询条件的文档号使用参数<code>collector</code>收集存储。</p><h4 id="acceptdocs"><a class="markdownIt-Anchor" href="#acceptdocs"></a> acceptDocs</h4><p>  <code>acceptDocs</code>是一个基于<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0404/FixedBitSet" >位图<i class="fas fa-external-link-alt"></i></a>实现，用来描述被删除的文档号信息的对象。由于在Lucene中执行删除文档操作时，并不会去修改现有的索引文件，而是额外的在名为<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0425/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bliv" >.liv<i class="fas fa-external-link-alt"></i></a>的索引文件中记录被删除的文档号。</p><p>  也就说，在方法一中依次处理满足查询条件的文档号时，有些文档号对应的文档尽管是满足查询条件的，但是它已经是被标记为删除的，故需要通过<code>acceptDocs</code>进行过滤。</p><h4 id="collector"><a class="markdownIt-Anchor" href="#collector"></a> collector</h4><p>  满足查询条件的文档号会使用<code>collector</code>进行收集，在<code>collector</code>中会对文档号进行排序等操作，详细内容见<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0812/Collector%EF%BC%88%E4%B8%80%EF%BC%89" >Collector（一）<i class="fas fa-external-link-alt"></i></a>。另外<code>collector</code>是<code>LeafCollector</code>对象，说的是对某个段进行收集。</p><h4 id="min-max"><a class="markdownIt-Anchor" href="#min-max"></a> min、max</h4><p>  min和max用于指定一个待遍历的文档号区间，在有些子类实现中如果根据某些条件可以尽可能的降低区间的范围，那么可以降低整个查询时间（在介绍ReqExclBulkScorer时会介绍）。默认的实现中，min的值为0，max的值为Integer.MAX_VALUE。</p><h3 id="方法二"><a class="markdownIt-Anchor" href="#方法二"></a> 方法二</h3><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/3.png" width="800"><p>  该方法描述的是遍历所有满足查询条件的文档号的开销，这个取决于不同的BulkScorer子类实现，在下文中再展开介绍。</p><h2 id="bulkscorer的子类"><a class="markdownIt-Anchor" href="#bulkscorer的子类"></a> BulkScorer的子类</h2><p>  主要介绍下这几个最常见的子类：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/4.png"  width="800"><h3 id="defaultbulkscorer"><a class="markdownIt-Anchor" href="#defaultbulkscorer"></a> DefaultBulkScorer</h3><p>  DefaultBulkScorer子类中实现图2中方法一的流程图如下所示：</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/5.png"  align = "left"  ><h4 id="从scorer中获取scoreriterator"><a class="markdownIt-Anchor" href="#从scorer中获取scoreriterator"></a> 从Scorer中获取scorerIterator</h4><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/6.png"   width="500"><p>  Scorer的概念会在以后的文章中详细介绍，在本篇文章中我们只需要暂时知道，该对象会提供一个DocIdSetIterator抽象类（<strong>下文中简称为DISI</strong>）的对象scorerIterator，通过该对象我们就可以获取到满足查询条件的所有文档号，并且在DocIdSetIterator类的不同实现中，获取每一个文档号的逻辑也是不同的。</p><h5 id="docidsetiterator"><a class="markdownIt-Anchor" href="#docidsetiterator"></a> DocIdSetIterator</h5><p>  DISI是一个有状态的，对non-decreasing类型的文档号集合进行遍历的迭代器，并且集合中的文档号都是满足查询条件的（被删除的文档号也在集合中）。non-decreasing指的是集合中的每个文档号都大于等于排在它前面的文档号。例如下面的集合就是non-decreasing：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;1, 2, 2, 3, 4&#125;</span><br></pre></td></tr></table></figure><p>  这个类中有四个核心的方法：</p><h6 id="方法一disi"><a class="markdownIt-Anchor" href="#方法一disi"></a> 方法一（DISI）</h6><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/7.png"   width="800"><p>  该方法体现出DISI是具有状态的，它返回DISI当前的状态值，即目前遍历到的文档号。</p><p>  注释中说到，如果在调用<code>nextDoc()</code>或者<code>advance(int)</code>方法前就调用当前方法，会返回-1，意思是DISI的最初的状态值是-1，也就是文档号为-1，当然了，该值不是一个合法的文档号。另外如果遍历完所有的文档号，那么当前方法会返回一个<code>NO_MORE_DOCS</code>作为结束标志，该值即Integer.MAX_VALUE。</p><p>图11：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/11.png" width="800"><h6 id="方法二disi"><a class="markdownIt-Anchor" href="#方法二disi"></a> 方法二（DISI）</h6><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/8.png"  width="800"><p>  该方法会基于<code>docID()</code>中的文档号（当前状态值），返回在集合中排在它后面，下一个位置的文档号，随后将状态值更新为该位置的文档号。</p><p>  如果DISI当前的状态值已经是集合中的最后一个文档号，那么调用该方法会返回<code>NO_MORE_DOCS</code>，并且将状态值更新为<code>NO_MORE_DOCS</code>。注意的是，不应该在这种情况下继续调用该方法，否则会导致不可预测行为。</p><h6 id="方法三disi"><a class="markdownIt-Anchor" href="#方法三disi"></a> 方法三（DISI）</h6><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/9.png"  align = "middle"  width="800"><p>  该方法会返回集合中第一个大于等于target的文档号，并且将状态值更新为这个文档号。</p><p>  如果未在集合中找到，那么会返回<code>NO_MORE_DOCS</code>。并且将状态值更新为<code>NO_MORE_DOCS</code>。</p><h6 id="方法四disi"><a class="markdownIt-Anchor" href="#方法四disi"></a> 方法四（DISI）</h6><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/10.png"  width="800"><p>  该方法描述的是遍历DISI中的文档号集合的开销，在有些子类实现中该值就是集合中文档号数量的一个准确值，然而在有些实现则是一个估计值。</p><h5 id="docidsetiterator的子类"><a class="markdownIt-Anchor" href="#docidsetiterator的子类"></a> DocIdSetIterator的子类</h5><p>  下图给出的是实现比较简单的一个子类：</p><p>图12：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/12.png" align = "middle"  width="500"><h4 id="是否可以获取实现skip-docs的disi"><a class="markdownIt-Anchor" href="#是否可以获取实现skip-docs的disi"></a> 是否可以获取实现Skip Docs的DISI？</h4><p>图13：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/13.png"  width="500"><p>  如果从数据集的角度去思考如何提高查询性能，通常可以通过以下两种方式实现：</p><ol><li>减少数据集的规模</li><li>收集到足够的结果后，提前退出</li></ol><p>  这里的第二点在图5的<code>collector收集docId</code>中实现，见<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0815/Collector%EF%BC%88%E5%9B%9B%EF%BC%89" >Collector（四）<i class="fas fa-external-link-alt"></i></a>。第一点则是通过可以实现Skip Docs的<strong>DISI对象competitiveIterator</strong>结合图6 Scorer中获取的scorerIterator，将这两个DISI对象组合成一个<strong>新的DISI对象filteredIterator</strong>来实现<code>减少数据集的规模</code>。</p><p>  <strong>注意点：只有在获取TopN的查询中，才有可能获取到实现Skip Docs的DISI对象competitiveIterator</strong></p><h5 id="filterediterator的实现逻辑"><a class="markdownIt-Anchor" href="#filterediterator的实现逻辑"></a> filteredIterator的实现逻辑</h5><p>  <strong>在收集了TopN篇文档号</strong>，随后继续遍历scorerIterator时，如果根据排序规则，可以添加到TopN中时，就会尝试获取/更新competitiveIterator，通过competitiveIterator实现skip docs。</p><p>  目前Lucene中有基于BKD树（<a class="link"   href="https://issues.apache.org/jira/browse/LUCENE-9280" >LUCENE-9280<i class="fas fa-external-link-alt"></i></a>）以及基于倒排（<a class="link"   href="https://issues.apache.org/jira/browse/LUCENE-10633" >LUCENE-10633<i class="fas fa-external-link-alt"></i></a>）来获得competitiveIterator。见文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0621/%E6%9F%A5%E8%AF%A2TopN%E7%9A%84%E4%BC%98%E5%8C%96%E4%B9%8BNumericDocValues%EF%BC%88%E4%B8%80%EF%BC%89" >查询TopN的优化之NumericDocValues（一）<i class="fas fa-external-link-alt"></i></a>、<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0629/%E6%9F%A5%E8%AF%A2TopN%E7%9A%84%E4%BC%98%E5%8C%96%E4%B9%8BNumericDocValues%EF%BC%88%E4%BA%8C%EF%BC%89" >查询TopN的优化之NumericDocValues（二）<i class="fas fa-external-link-alt"></i></a>了解基于BKD树获取competitiveIterator。</p><p>  下面的例子中，<strong>主要是表达优化思想，并不是真实的实现逻辑</strong>：</p><p>图14：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/14.png"><p>  图14中假设是Top3的查询，如果没能获取competitiveIterator，那么需要遍历集合中所有的文档号。优化后，在收集完Top3后，根据Top3中的竞争力最小的信息（基于排序规则对应的域值）获取一个集合为[7, 99]的competitiveIterator，意思是根据排序规则，在这个集合区间范围外的文档是没有竞争力（non-competitive），也就是没有必要去处理这些文档号。随后在处理完第56篇文档号，再次更新了competitiveIterator。</p><h4 id="遍历filterediterator"><a class="markdownIt-Anchor" href="#遍历filterediterator"></a> 遍历filteredIterator</h4><p>图15：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/BulkScorer/BulkScorer（一）/15.png"><h5 id="docid是否在区间内"><a class="markdownIt-Anchor" href="#docid是否在区间内"></a> docId是否在区间内</h5><p>  这里的区间指的是<a href="###%E6%96%B9%E6%B3%95%E4%B8%80">方法一</a>中参数<code>max</code>跟<code>min</code>组成的区间，由于从filteredIterator中依次获得的文档号是递增的，所以当出现文档号不在区间内时，就可以直接退出遍历。</p><h5 id="docid不是被删除的文档号并且满足二阶段遍历"><a class="markdownIt-Anchor" href="#docid不是被删除的文档号并且满足二阶段遍历"></a> docId不是被删除的文档号并且满足二阶段遍历？</h5><p>  在这个流程点，我们需要过滤被删除的文档号，尽管这些文档是满足查询条件的。</p><h6 id="二阶段遍历"><a class="markdownIt-Anchor" href="#二阶段遍历"></a> 二阶段遍历</h6><p>  二阶段遍历在大部分查询场景中不会出现，在此流程点如果二阶段遍历为空，则认为是满足条件的。在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0723/RangeField%EF%BC%88%E4%B8%80%EF%BC%89" >二阶段遍历（TwoPhaseIterator）<i class="fas fa-external-link-alt"></i></a>中详细的介绍了为什么要使用二阶段遍历，以及给出了某个场景作为例子。这里就不展开介绍了。</p><h5 id="collector收集docid"><a class="markdownIt-Anchor" href="#collector收集docid"></a> collector收集docId</h5><p>  在系列文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0812/Collector%EF%BC%88%E4%B8%80%EF%BC%89" >Collector（一）<i class="fas fa-external-link-alt"></i></a>中介绍常见的几个Collector，这里就不赘述了。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  基于篇幅，剩余的内容将在下一篇文章中展开介绍。</p><p><a class="link"   href="https://www.amazingkoala.com.cn/attachment/Lucene/Search/BulkScorer/BulkScorer%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  本篇文章介绍在查询阶段，&lt;a class=&quot;link&quot;   href=&quot;https://github.com/apache/lucene/blob/f527eb3b12df6d052d1b6dd107b56ab705a95ab1/lucene/core/src/java</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="bulkScorer" scheme="http://example.com/tags/bulkScorer/"/>
    
  </entry>
  
  <entry>
    <title>段的多线程查询（一）（Lucene 9.6.0）</title>
    <link href="http://example.com/Lucene/Search/2023/0626/%E6%AE%B5%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%9F%A5%E8%AF%A2%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2023/0626/%E6%AE%B5%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%9F%A5%E8%AF%A2%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2023-06-25T16:00:00.000Z</published>
    <updated>2023-10-08T14:37:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>  前段时间有个朋友问到我：对多个段进行查询时，为什么在定义IndexSearcher时使用了<a class="link"   href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Executors.html" >Executor<i class="fas fa-external-link-alt"></i></a>后，相比较单个线程轮询方式查询相同的多个段，查询速度并没有提升，有时候还下降了？本篇文章会介绍多线程查询中的一些知识点，给与大家在解决性能问题时提供一些思路。</p><h2 id="查询流程图"><a class="markdownIt-Anchor" href="#查询流程图"></a> 查询流程图</h2><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/段的多线程查询/段的多线程查询（一）/1.png" align = "left"  width="600"><p>  图1中的<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0821/%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89" >查询流程图<i class="fas fa-external-link-alt"></i></a>中，如果未使用多线程，那么Lucene将依次处理每一个段，否则每个线程会负责查询某个段，并在所有线程执行结束后对结果进行合并。</p><h2 id="slice"><a class="markdownIt-Anchor" href="#slice"></a> Slice</h2><p>  图1中的描述说到每个线程会负责一个段的查询工作，这其实只是一种特例（下文会介绍）。实际上，在初始化IndexSearcher对象阶段会所有的段进行划分，一个或者多个段根据<strong>划分规则</strong>被分配不同的Slice中，随后每一个Slice会被分配一个线程进行查询。</p><h3 id="划分规则"><a class="markdownIt-Anchor" href="#划分规则"></a> 划分规则</h3><ul><li>排序：首先将所有段根据段中包含的文档数量降序排序。</li><li>分配：从包含文档数量最大的段开始，依次处理每一个段，将段分配到Slice中。每一个Slice需要同时满足下面的条件：<ul><li>如果段中的文档数量大于250000（该值为源码中的<code>MAX_DOCS_PER_SLICE</code>），则分配到一个新的Slice中，并且不再将更多的段分配到这个Slice中</li><li>如果某个段分配到一个Slice后，Slice中的<strong>所有段的的文档总数</strong>大于等于250000（该值为源码中的<code>MAX_DOCS_PER_SLICE</code>），那么不再将更多的段分配到这个Slice中</li><li>Slice中<strong>段的数量</strong>必须小于等于5（该值为源码中的<code>MAX_SEGMENTS_PER_SLICE</code>）</li></ul></li></ul><h3 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子</h3><p>  如果索引文件中有以下9个段，即图3中的demo：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/段的多线程查询/段的多线程查询（一）/2.png" align = "left"><p>  图2中，段0中的文档数量超过了250000（<code>MAX_DOCS_PER_SLICE</code>），所以Slice 0中只有这一个段；段2中的文档数量小于250000，所以它被分配到Slice 1后，Slice 1还可以被分配其他的段，由于段1被分配到Slice 1后，Slice 1中的文档数量超过了250000，所以不再分配更多的段到Slice 1中；Slice 2中被分配段4~段8这5个段后，尽管此时Slice 2中的文档数量未超过250000，但是Slice 2中段的数量已经达到了5（<code>MAX_DOCS_PER_SLICE</code>），所以不再分配更多的段到Slice 2中。</p><p>  上文中说到，每个线程负责一个Slice的查询工作。因此每个线程负责的Slice实际上处理的段的数量是不一样的。</p><h2 id="查询性能差异"><a class="markdownIt-Anchor" href="#查询性能差异"></a> 查询性能差异</h2><p>  接下来介绍下一个多线程查询不及单线程的<a class="link"   href="https://github.com/LuXugang/Lucene-7.x-9.x/blob/master/LuceneDemo9.6.0/src/main/java/TestEarlyTerminal.java" >例子<i class="fas fa-external-link-alt"></i></a></p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/段的多线程查询/段的多线程查询（一）/3.png" align = "left"><p>  这个例子中，索引文件内有9个段，一共658000篇文档。查询条件是获取Top 1000的文档号，分别使用多线程跟单线程，查看<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0812/Collector%EF%BC%88%E4%B8%80%EF%BC%89" >Collector<i class="fas fa-external-link-alt"></i></a>中分别处理的文档数量，如下所示：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/段的多线程查询/段的多线程查询（一）/4.png" align = "left"><p>  可见使用多线程查询需要处理的文档数量（totalHits）反而比单线程多，那性能当然是不及单线程的。</p><h3 id="early-termination机制"><a class="markdownIt-Anchor" href="#early-termination机制"></a> early termination机制</h3><p>  Lucene中，会使用totalHits（图3中第65、69行代码）来统计Collector处理的文档数量，注意的是，由于在查询条件中定义了TopN，所以在Collector的处理逻辑中，收集完TopN篇文档后，如果能确定剩余满足查询条件的文档相比较已收集的TopN中的文档都不具备竞争力（competitive），那么就可以提前退出Collector，即<strong>early termination机制</strong>，直接返回TopN中的文档即可。</p><p>  图3中使用单线程的查询条件是<code>MatchAllDocsQuery</code>，那么在Collector中，文档号越小的文档越具备竞争力（基于<code>MatchAllDocsQuery</code>对应的<code>ConstantScoreWeight</code>，这里不展开介绍），所以Collector中收集完文档号区间为0~999的文档后，就可以提前结束查询，而不需要全量处理索引中的658000篇文档。</p><p>  对于使用多线程查询，根据图2的介绍，会对4个Slice进行并发查询。early termination机制只能作用在每一个Slice中，这个例子中每个Slice都分别实现了early termination，也就是每个Slice的Collector在收集完TopN后就提前退出，因此对于4个Slice，总的totalHits为4000（4 * topN）。</p><h4 id="early-termination的实现原理"><a class="markdownIt-Anchor" href="#early-termination的实现原理"></a> early termination的实现原理</h4><p>  因为篇幅原因就不在本文中展开了，简单的提一下。尽管每个Collector的实现原理各不相同，但early termination的核心内容都是通过调整<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/gongjulei/2021/0623/DocIdSet" >DocIdSetIterator<i class="fas fa-external-link-alt"></i></a>来减少后续待处理的文档集合的大小，比如在<code>TopScoreDocCollector</code>中，当收集了TopN篇文档后并且确定剩余的文档不具备竞争力后，就会将DISI调整为空的DISI，即<code>DocIdSetIterator.empty()</code>。</p><h2 id="合并slice的查询结果"><a class="markdownIt-Anchor" href="#合并slice的查询结果"></a> 合并Slice的查询结果</h2><p>  这个过程可以简单的描述为将多个Slice中的TopN根据排序规则，以及比较规则来获得最终的TopN。</p><ul><li>排序规则：例如我们在查询阶段定义了Sort对象，那么在合并查询结果时会使用该Sort对象，如果没有排序规则，或者该排序规则无法用于比较出优先级，则接着使用比较规则。</li><li>比较规则：Lucene提供了两个内置的比较规则，对应源码中的<code>SHARD_INDEX_TIE_BREAKER</code>和<code>DOC_ID_TIE_BREAKER</code>，以及这两个比较规则的组合使用，即默认的比较规则<code>DEFAULT_TIE_BREAKER</code>，直接给出源码：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Internal comparator with shardIndex */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Comparator&lt;ScoreDoc&gt; SHARD_INDEX_TIE_BREAKER =</span><br><span class="line">  Comparator.comparingInt(d -&gt; d.shardIndex);</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Internal comparator with docID */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Comparator&lt;ScoreDoc&gt; DOC_ID_TIE_BREAKER =</span><br><span class="line">  Comparator.comparingInt(d -&gt; d.doc);</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Default comparator */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Comparator&lt;ScoreDoc&gt; DEFAULT_TIE_BREAKER =</span><br><span class="line">  SHARD_INDEX_TIE_BREAKER.thenComparing(DOC_ID_TIE_BREAKER);</span><br></pre></td></tr></table></figure><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  基于篇幅，剩余的内容将在下一篇文章中展开介绍。</p><p><a class="link"   href="https://www.amazingkoala.com.cn/attachment/Lucene/Search/%E6%AE%B5%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%9F%A5%E8%AF%A2/%E6%AE%B5%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%9F%A5%E8%AF%A2%EF%BC%88%E4%B8%80%EF%BC%89/%E6%AE%B5%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%9F%A5%E8%AF%A2%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  前段时间有个朋友问到我：对多个段进行查询时，为什么在定义IndexSearcher时使用了&lt;a class=&quot;link&quot;   href=&quot;https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Exe</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="search" scheme="http://example.com/tags/search/"/>
    
    <category term="multiThread" scheme="http://example.com/tags/multiThread/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch-8.2</title>
    <link href="http://example.com/Elasticsearch/2022/0905/Elasticsearch-8-2/"/>
    <id>http://example.com/Elasticsearch/2022/0905/Elasticsearch-8-2/</id>
    <published>2022-09-04T16:00:00.000Z</published>
    <updated>2024-03-17T10:33:26.898Z</updated>
    
    <content type="html"><![CDATA[<p><strong><a class="link"   href="https://www.amazingkoala.com.cn/Elasticsearch/Elasticsearch-8.2.html" >点击这里<i class="fas fa-external-link-alt"></i></a></strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;a class=&quot;link&quot;   href=&quot;https://www.amazingkoala.com.cn/Elasticsearch/Elasticsearch-8.2.html&quot; &gt;点击这里&lt;i class=&quot;fas fa-external-link</summary>
      
    
    
    
    <category term="Elasticsearch" scheme="http://example.com/categories/Elasticsearch/"/>
    
    
    <category term="Elasticsearch" scheme="http://example.com/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>IndexSortSortedNumericDocValuesRangeQuery （一）（Lucene 9.0.0）</title>
    <link href="http://example.com/Lucene/Search/2022/0314/IndexSortSortedNumericDocValuesRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2022/0314/IndexSortSortedNumericDocValuesRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2022-03-13T16:00:00.000Z</published>
    <updated>2023-10-09T08:38:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>  我们先通过IndexSortSortedNumericDocValuesRangeQuery类的注释了解下这个Query。</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/1.png"><p>  图1中<font color=red>红框</font>标注的注释说到，范围查询可以通过利用<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2021/0915/IndexSort" >Index Sort<i class="fas fa-external-link-alt"></i></a>来提高查询效率。如果查询条件的域field正好是用于段内排序的域，那么就可以通过二分法找到满足查询条件的两个文档号。这两个文档号分别作为一个<strong>区间</strong>的上下界，满足查询条件的文档号肯定都在这个区间内。</p><p>  对于<font color=red>红框</font>注释有两个细节需要注意：</p><ul><li>用于段内排序的域可以设置多个排序规则，这些排序规则有先后顺序，但是只有第一个排序规则（primary sort）对应的域跟查询条件的域相同才能提高查询效率</li><li>上文中说到的<strong>区间</strong>，这个区间内的文档号不是都满足查询条件的，只能保证满足查询条件的文档号肯定都在这个区间内</li></ul><p>  下文中，我们将会这两个注意的细节作出详细的介绍。</p><p>  图1中<font color=blue>蓝框</font>标注的注释说到，只有同时满足下面的条件才能实现这种优化执行策略（optimized execution strategy）：</p><ul><li>条件一：索引是有序的，第一个排序规则对应的field必须跟查询条件的域相同</li><li>条件二：范围查询条件的域必须是<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0410/SortedNumericDocValues" >SortedNumericDocValues<i class="fas fa-external-link-alt"></i></a>或者<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0409/NumericDocValues" >NumericDocValues<i class="fas fa-external-link-alt"></i></a></li><li>条件三：段中每一篇文档中<strong>最多</strong>只能包含一个<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0410/SortedNumericDocValues" >SortedNumericDocValues<i class="fas fa-external-link-alt"></i></a>或者<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0409/NumericDocValues" >NumericDocValues<i class="fas fa-external-link-alt"></i></a></li></ul><p>  条件三的限制换个说话就是：一篇文档中要么不包含SortedNumericDocValues或者NumericDocValues，要么<strong>只能</strong>包含一个。具体原因在下文中说明。</p><p>  如果上述三个条件不同时满足，那么就无法使用这个优化执行策略，这次查询就会交给fallbackQuery（下文会介绍）去执行。</p><p>  图1中<font color=gray>灰框</font>标注的注释说道，fallbackQuery的查询条件必须跟IndexSortSortedNumericDocValuesRangeQuery一致，返回的结果必须是相同的文档集合并且每篇文档的分数是固定的。</p><p>  因为IndexSortSortedNumericDocValuesRangeQuery的查询逻辑是先尝试用IndexSort机制进行查询，如果无法同时满足上文中说道的<strong>三个条件</strong>，那么就将这次查询委托（delegate）给fallbackQuery。</p><p>  <font color=gray>灰框</font>标注的例子中可以看出，代码66行的LongPoint.newRangeQuery即fallbackQuery，它的查询条件跟代码67行的IndexSortSortedNumericDocValuesRangeQuery是一样的。</p><p>  从这个例子也可以看出，fallbackQuery跟IndexSortSortedNumericDocValuesRangeQuery的查询条件保持一致需要使用者自己来保证，意味着用户想要自己编写一个效率较高的数值范围查询有较高的学习成本，他至少要了解并且在索引阶段写入<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0410/SortedNumericDocValues" >SortedNumericDocValues<i class="fas fa-external-link-alt"></i></a>或者<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0409/NumericDocValues" >NumericDocValues<i class="fas fa-external-link-alt"></i></a>，还要了解各种数值类型范围查询，比如<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0701/IndexOrDocValuesQuery" >IndexOrDocValuesQuery<i class="fas fa-external-link-alt"></i></a>，<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89" >PointRangeQuery（一）<i class="fas fa-external-link-alt"></i></a>等等。所以社区已经开始着手开发一些&quot;sugar&quot;域跟Query（见<a class="link"   href="https://issues.apache.org/jira/browse/LUCENE-10162" >LUCENE-10162<i class="fas fa-external-link-alt"></i></a>），使得用户能简单透明的使用数值类型的范围查询，让Lucene来帮助用户生成一个高效的Query。</p><p>  我们看下Elasticsearch 8.0的NumberFieldMapper类中是如何生成一个高效的数值类型范围查询Query的，以Mapping类型为int的字段为例：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/2.png"><p>  图2描述了生成一个用于int类型的字段的范围查询的过程：首先通过IntPoint.newRangeQuery生成一个<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89" >PointRangeQuery<i class="fas fa-external-link-alt"></i></a>，我们称之为indexQuery；如果该字段开启了DocValues，那么通过SortedNumericDocValuesField.newSlowRangeQuery生成一个DocValues的范围查询，我们称之为dvQuery, 然后将indexQuery跟dvQuery封装为一个<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0701/IndexOrDocValuesQuery" >IndexOrDocValuesQuery<i class="fas fa-external-link-alt"></i></a>；最后如果分片的索引根据查询域排序了，那么进一步将<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0701/IndexOrDocValuesQuery" >IndexOrDocValuesQuery<i class="fas fa-external-link-alt"></i></a>封装为IndexSortSortedNumericDocValuesRangeQuery，此时<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0701/IndexOrDocValuesQuery" >IndexOrDocValuesQuery<i class="fas fa-external-link-alt"></i></a>即上文中的fallbackQuery。</p><p>  在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0701/IndexOrDocValuesQuery" >IndexOrDocValuesQuery<i class="fas fa-external-link-alt"></i></a>中详细的介绍了这个Query，本文中我们简单的提一下：IndexOrDocValuesQuery既利用了倒排中根据term快速获取满足查询条件的文档号集合能力，又利用了正排中根据文档号能快速check是否存在某个term的能力。使得IndexOrDocValuesQuery不管作为leader iterator还是follow iterator都能获得不错的读取性能。</p><p>  回到图2中构造过程，的确需要相当大的学习成本才能构造成一个高效的Query。<a class="link"   href="https://issues.apache.org/jira/browse/LUCENE-10162" >LUCENE-10162<i class="fas fa-external-link-alt"></i></a>的目标在于期望用户通过编写类似IntField.NumericRangeQuery(String field, long lowerValue, long upperValue)这种方式就可以获得图2中的Query。</p><h2 id="利用indexsort实现高效查询"><a class="markdownIt-Anchor" href="#利用indexsort实现高效查询"></a> 利用IndexSort实现高效查询</h2><p>  不管是哪种Query的实现，其需要解决的最重要的核心问题是这个Query如何提供一个<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/gongjulei/2021/0623/DocIdSet" >迭代器DocIdSetIterator<i class="fas fa-external-link-alt"></i></a>，迭代器中包含了满足查询条件的文档号集合，以及定义了读取这些文档号的方式。查询性能取决于迭代器的实现方式。</p><h3 id="boundeddocsetiditerator"><a class="markdownIt-Anchor" href="#boundeddocsetiditerator"></a> BoundedDocSetIdIterator</h3><p>  BoundedDocSetIdIterator即在利用了IndexSort后，IndexSortSortedNumericDocValuesRangeQuery实现的迭代器，注意的是这个迭代器的名字有书写错误typo，应该是BoundedDocIdSetIterator，也许会在这个<a class="link"   href="https://github.com/apache/lucene/pull/736" >LUCENE-10458<i class="fas fa-external-link-alt"></i></a>合并后修正。</p><p>  我们看下这个迭代器中包含的信息，即该类的成员变量：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/3.png"><p>  图3中的firstDoc、lastDoc中用来描述一个左闭右开的文档号集合的区间，这个区间内<strong>至少</strong>包含了满足查询条件的所有文档号。对于firstDoc跟lastDoc是如何计算的，我们将在下一篇文章中展开。</p><p><strong>为什么说BoundedDocSetIdIterator中至少包含了满足查询条件的所有文档号</strong></p><p>  因为某些文档中虽然不包含查询条件对应的域的信息，但是Lucene会给这篇文档添加一个默认值来参与段内的文档排序。该默认值就是MissingValue。这些被设置了MissingValue的文档号就有可能被统计到迭代器中。</p><h4 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子</h4><p>我们通过一个例子介绍下上述的问题，demo地址：<a class="link"   href="https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo9.0.0/src/main/java/facet/MissingValueTest.java" >https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo9.0.0/src/main/java/facet/MissingValueTest.java<i class="fas fa-external-link-alt"></i></a> 。</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/4.png"><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/5.png"><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/6.png"><p>  图4中，我们定义了一个IndexSortSortedNumericDocValuesRangeQuery的范围查询，其中代码66、67定义了查询条件的上下界分别为 1, 100，代码72行定义了查询条件的域名为&quot;number&quot;，显然，只有文档1跟文档2中包含&quot;number&quot;域，并且其域值满足查询条件，而文档0中不包含&quot;number&quot;域的信息，所以在图6中，满足查询条件的文档号分别是文档1跟文档2。</p><p>  图4的例子满足了利用IndexSort的三个条件，故IndexSortSortedNumericDocValuesRangeQuery会生成图5中的BoundedDocSetIdIterator迭代器。但是通过断点可以看到firstDoc跟lastDoc组成的左闭右开的区间中的文档号集合包含了文档0、文档1、文档2。这是因为在图4中我们定义了MissingValue的值为3，所以文档0也被BoundedDocSetIdIterator收集了（其被收集的原因也会在下一篇文章中展开介绍）。</p><p>  如果图4中不设置MissingValue，那么BoundedDocSetIdIterator的信息是这样的：</p><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/7.png"><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery（一）/8.png"><p>  由于在设置了MissingValue后，BoundedDocSetIdIterator中可能会包含不满足查询条件的文档号，所以在BoundedDocSetIdIterator中，如图8所示，它还包含一个成员变量delegate，它是上文中fallbackQuery对应的迭代器。因为fallbackQuery的迭代器中包含的文档号肯定是满足查询条件的，所以在读取BoundedDocSetIdIterator的文档号时，每次都会去delegate中检查是否存在这个文档号，来保证返回数据的准确性。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  剩余内容将在下一篇文章中展开。</p><p><a class="link"   href="http://www.amazingkoala.com.cn/attachment/Lucene/Search/IndexSortSortedNumericDocValuesRangeQuery/IndexSortSortedNumericDocValuesRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  我们先通过IndexSortSortedNumericDocValuesRangeQuery类的注释了解下这个Query。&lt;/p&gt;
&lt;p&gt;图1：&lt;/p&gt;
&lt;img src=&quot;http://www.amazingkoala.com.cn/uploads/lucene/Se</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="query" scheme="http://example.com/tags/query/"/>
    
    <category term="IndexSortSortedNumericDocValuesRangeQuery" scheme="http://example.com/tags/IndexSortSortedNumericDocValuesRangeQuery/"/>
    
  </entry>
  
  <entry>
    <title>PointRangeQuery（二）（Lucene 8.11.0）</title>
    <link href="http://example.com/Lucene/Search/2021/1128/PointRangeQuery%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2021/1128/PointRangeQuery%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2021-11-27T16:00:00.000Z</published>
    <updated>2023-10-10T02:52:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>  本文承接<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89" >PointRangeQuery（一）<i class="fas fa-external-link-alt"></i></a>，继续介绍数值类型的范围查询PointRangeQuery。</p><h2 id="节点访问规则intersectvisitor"><a class="markdownIt-Anchor" href="#节点访问规则intersectvisitor"></a> 节点访问规则IntersectVisitor</h2><p>  上一篇文章中我们说到，在收集文档号的策略中，除了策略一，不管哪一种策略，他们的<strong>相同点都是使用深度遍历读取BKD树，不同点则是访问内部节点跟叶子节点的处理规则，这个规则即IntersectVisitor</strong>。</p><p>  IntersectVisitor在源码中是一个接口类，我们通过介绍它提供的方法来了解：</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（二）/1.png"><h3 id="访问内部节点"><a class="markdownIt-Anchor" href="#访问内部节点"></a> 访问内部节点</h3><p>  正如图1中compare(…)方法中的注释说到，这个方法用来判断查询条件跟当前访问的内部节点之间的Relation（见<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89" >PointRangeQuery（一）<i class="fas fa-external-link-alt"></i></a>的介绍），决策出如何进一步处理该内部节点的子节点。我们先介绍下PointRangeQuery中如何实现<strong>compare(…)</strong>，随后在<strong>访问叶子节点</strong>时小结中介绍如何根据Relation作出访问子节点的策略。</p><h4 id="pointrangequery中计算relation的实现"><a class="markdownIt-Anchor" href="#pointrangequery中计算relation的实现"></a> PointRangeQuery中计算Relation的实现</h4><p>  其实现过程用一句话描述为：先判断是否为CELL_OUTSIDE_QUERY，如果不是再判断是CELL_CROSSES_QUERY还是CELL_INSIDE_QUERY。</p><h5 id="是否为cell_outside_query"><a class="markdownIt-Anchor" href="#是否为cell_outside_query"></a> 是否为CELL_OUTSIDE_QUERY</h5><p>  实现逻辑：依次处理每个维度，只要存在一个维度，查询条件在这个维度下的最小值比索引中的点数据在这个维度下的最大值还要大，或者查询条件在这个维度下的最大值比索引中的点数据在这个维度下的最小值还要小，那么它们的关系为CELL_OUTSIDE_QUERY。</p><p>  我们以二维的点数据为例，并且我们称第一个维度为X维度，第二个维度为Y维度：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（二）/2.png"><p>  图2中，在Y维度下，查询条件在Y维度下的最大值（1）比索引中的点数据在Y维度下的最小值（2）还要小，所以他们的关系是CELL_OUTSIDE_QUERY。</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（二）/3.png"><p>  图3中，在Y维度下，查询条件在Y维度下的最小值（8）比索引中的点数据在Y维度下的最大值（6）还要大，所以他们的关系是CELL_OUTSIDE_QUERY。</p><h5 id="cell_crosses_query还是cell_inside_query"><a class="markdownIt-Anchor" href="#cell_crosses_query还是cell_inside_query"></a> CELL_CROSSES_QUERY还是CELL_INSIDE_QUERY</h5><p>  实现逻辑：<strong>在不是CELL_OUTSIDE_QUERY的前提下</strong>，只要存在一个维度，索引中的点数据在这个维度下的最小值比查询条件在这个维度下的最小值还要小，或者索引中的点数据在这个维度下的最大值比查询条件在这个维度下的最大值还要大，那么它们的关系为CELL_CROSSES_QUERY。如果所有维度<strong>都不</strong>满足上述条件，那么他们的关系为CELL_INSIDE_QUERY。</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（二）/4.png"><p>  图4中，在不是CELL_OUTSIDE_QUERY的前提下，查询条件在Y维度下的最小值不会大于6, 那么因为索引中的点数据在Y维度下的最小值（2）比查询条件在这个维度下的最小值（4）还要小，那么他们的关系为CELL_CROSSES_QUERY。</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（二）/5.png"><p>  图5中，在不是CELL_OUTSIDE_QUERY的前提下，不管是哪一个维度，都不满足索引中的点数据在这个维度下的最小值比查询条件在这个维度下的最小值还要小，也都不满足索引中的点数据在这个维度下的最大值比查询条件在这个维度下的最大值还要大，所以他们的关系为CELL_INSIDE_QUERY。</p><h4 id="compare方法的实现"><a class="markdownIt-Anchor" href="#compare方法的实现"></a> compare(…)方法的实现</h4><p>  在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89" >PointRangeQuery（一）<i class="fas fa-external-link-alt"></i></a>中说到，收集文档号集合有不同的策略，对于策略一由于不用遍历BKD树，所以不需要实现这个方法。而策略二跟策略三对compare(…)方法的实现有着少些的区别。</p><h5 id="策略三"><a class="markdownIt-Anchor" href="#策略三"></a> 策略三</h5><p>  该策略的对应的实现逻辑即上文中<strong>PointRangeQuery中计算Relation的实现</strong>介绍的内容。源码中的详细实现见类PointRangeQuery#getIntersectVisitor中的compare(…)方法。</p><h5 id="策略二"><a class="markdownIt-Anchor" href="#策略二"></a> 策略二</h5><p>  该策略首先采用同策略三一样的方式判断出查询条件跟内部节点的Relation，由于它采用反向收集文档号，所以它对应的实现也是&quot;反向Relation&quot;，由于该实现代码量较小，我们直接贴出源码：</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（二）/6.png"><p>  从图6可以看出，当代码225行计算出Relation的值为CELL_INSIDE_QUERY时，其&quot;反向Relation&quot;的值为CELL_OUTSIDE_QUERY，就如代码228行注释说的那样，如果内部节点下的所有子节点中的点数据都满足查询条件的话，那么就不用处理该内部节点的子节点，即不用再继续深度遍历该内部节点。同样的，当代码230行计算出Relation的值为CELL_OUTSIDE_QUERY，其&quot;反向Relation&quot;的值为CELL_INSIDE_QUERY。其代码231行注释中的&quot;clear all documents&quot;说的是在访问叶子节点的处理方式，我们在随后下一篇中会介绍。源码中的详细实现见类PointRangeQuery#getInverseIntersectVisitor中的compare(…)方法。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  基于篇幅，剩余内容将在下一篇文章中展开。</p><p><a class="link"   href="http://www.amazingkoala.com.cn/attachment/Lucene/Search/PointRangeQuery/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  本文承接&lt;a class=&quot;link&quot;   href=&quot;https://www.amazingkoala.com.cn/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89&quot; &gt;PointR</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="point" scheme="http://example.com/tags/point/"/>
    
    <category term="query" scheme="http://example.com/tags/query/"/>
    
    <category term="dim" scheme="http://example.com/tags/dim/"/>
    
    <category term="dii" scheme="http://example.com/tags/dii/"/>
    
    <category term="rangeQuery" scheme="http://example.com/tags/rangeQuery/"/>
    
  </entry>
  
  <entry>
    <title>PointRangeQuery（一）（Lucene 8.11.0）</title>
    <link href="http://example.com/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2021/1122/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2021-11-21T16:00:00.000Z</published>
    <updated>2023-10-10T02:49:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>  该系列文章开始介绍数值类型的范围查询PointRangeQuery，该类数据在Lucene中被称为点数据Point Value。</p><p>  点数据按照基本类型（primitive type）可以划分IntPoint、LongPoint、FloatPoint、DoublePoint。点数据可以是多维度的，即一个点数据可以用多个维度值来描述。下图中我们分别定义了一维、二维、三维的点数据。可以理解为oneDim是直线上的一个点，twoDim是平面上的一个点，而threeDim是一个三维空间中的一个点。</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/1.png"><h2 id="相关文章"><a class="markdownIt-Anchor" href="#相关文章"></a> 相关文章</h2><p>  在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdim&amp;&amp;dii" >索引文件之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>、<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2020/1027/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bkdd&amp;kdi&amp;kdm" >索引文件之kdd&amp;kdi&amp;kdm<i class="fas fa-external-link-alt"></i></a>中介绍了存储点数据对应的索引文件；在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0329/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%85%AB%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件的生成（八）之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>到<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件的生成（十四）之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>以及<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0427/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件的读取（一）之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>到<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0506/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件的读取（四）之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>中分别介绍了索引文件的生成、读取过程。另外在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0422/Bkd-Tree" >Bkd-Tree<i class="fas fa-external-link-alt"></i></a>中介绍存储点数据使用的数据结构以及通过一个例子概述了生成一颗BKD树的过程。</p><h2 id="relation"><a class="markdownIt-Anchor" href="#relation"></a> Relation</h2><h3 id="minpackedvalue-maxpackedvalue"><a class="markdownIt-Anchor" href="#minpackedvalue-maxpackedvalue"></a> MinPackedValue、MaxPackedValue</h3><p>  在一个段Segment中，所有的点数据按照<strong>点数据域</strong>（Point Field，即图1中的oneDim、twoDim、threeDim）进行划分，对于某个点数据域，在<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2020/1027/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bkdd&amp;kdi&amp;kdm" >索引文件.kdm<i class="fas fa-external-link-alt"></i></a>中会存储下面两个字段MinPackedValue、MaxPackedValue：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/2.png"><p>  MinPackedValue描述的是这个点数据域中<strong>每个维度</strong>的最小值，同理MaxPackedValue描述的是这个点数据域中每个维度的最大值。例如有某个点数据域中包含如下的二维点数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">2</span>, <span class="number">3</span>&#125;, &#123;<span class="number">6</span>, <span class="number">2</span>&#125;, &#123;<span class="number">7</span>, <span class="number">6</span>&#125;, &#123;<span class="number">8</span>, <span class="number">4</span>&#125;, &#123;<span class="number">5</span>, <span class="number">4</span>&#125;, &#123;<span class="number">3</span>, <span class="number">5</span>&#125;</span><br></pre></td></tr></table></figure><p>  可以看出在这个点数据集合中，第一个维度的最小值是2，第二个维度最小值是2，故MinPackedValue的值为{2, 2}，同理，第一个维度的最大值是8，第二个维度的最大值是6，故MaxPackedValue的值为{8, 6}。</p><p>  如果我们把这个点数据集放到一个平面上，如下所示：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/3.png"><p>  由图3可知，MinPackedValue、MaxPackedValue<strong>不一定</strong>是索引中的数据。同时可以看出，Lucene使用这两个值生成一个矩形，索引中所有的点数据都在这个矩形内。</p><h3 id="lowervalue-uppervalue"><a class="markdownIt-Anchor" href="#lowervalue-uppervalue"></a> lowerValue、upperValue</h3><p>  我们在定义一个PointRangeQuery时，需要指定两个参数lowerValue、upperValue，分别表示我们这次范围查询的上界跟下界。</p><p>  如果设定的查询条件为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lowerValue = &#123;<span class="number">1</span>, <span class="number">1</span>&#125;</span><br><span class="line">upperValue = &#123;<span class="number">7</span>, <span class="number">5</span>&#125;</span><br></pre></td></tr></table></figure><p>  同MinPackedValue、MaxPackedValue一样，lowerValue、upperValue这两个点也可以形成一个矩形：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/4.png"><p>  所以从图4可以看出，对于二维的点数据，PointRangeQuery的查询核心原理即：找出两个矩形相交（重叠）部分的所有点数据。</p><p>  这两个矩形的相交关系在源码中使用Relation定义，它描述了三种相交关系：</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/5.png"><h3 id="cell_outside_query"><a class="markdownIt-Anchor" href="#cell_outside_query"></a> CELL_OUTSIDE_QUERY</h3><p>  CELL_OUTSIDE_QUERY描述的是查询条件跟索引中点数据的数值范围没有交集，即没有重叠（overlap），如下所示：</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/6.png"><h3 id="cell_crosses_query"><a class="markdownIt-Anchor" href="#cell_crosses_query"></a> CELL_CROSSES_QUERY</h3><p>  CELL_CROSSES_QUERY描述的是查询条件跟索引中点数据的数值范围部分重叠（partially overlaps）。</p><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/7.png"><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/8.png"><h3 id="cell_inside_query"><a class="markdownIt-Anchor" href="#cell_inside_query"></a> CELL_INSIDE_QUERY</h3><p>  CELL_INSIDE_QUERY描述的是查询条件的数值范围包含索引中所有的点数据。</p><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/9.png"><h3 id="基于relation访问节点"><a class="markdownIt-Anchor" href="#基于relation访问节点"></a> 基于Relation访问节点</h3><p>  在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0410/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件的生成（十一）之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>中我们说到，在生成BKD树的过程中，每次生成一个内部节点（inner node），都需要计算这个节点对应的MinPackedValue、MaxPackedValue，他们描述了这个内部节点对应的所有叶子节点（leave node）中的点数据都在MinPackedValue、MaxPackedValue对应的矩形内。</p><p>  那么当我们从根节点开始深度遍历后，查询条件跟每一个内部节点的MinPackedValue、MaxPackedValue在计算Relation后，会采取不同的方式访问其子节点。</p><ul><li>CELL_OUTSIDE_QUERY：说明当前内部节点下的所有叶子节点都不满足查询条件，那么就不用再处理这个内部节点下的所有子节点了。</li><li>CELL_INSIDE_QUERY：说明当前内部节点下的所有叶子节点中的点数据都满足查询条件，那么随后只从当前内部节点出发执行深度遍历，并且不需要再对内部节点进行Relation的计算，直到访问到叶子节点，并读取其包含的文档号。</li><li>CELL_CROSSES_QUERY：说明当前内部节点下的所有叶子节点只有部分满足查询条件，那么在分别访问内部节点的左右子节点（内部节点）时，都需要计算Relation。</li></ul><h2 id="收集文档号集合的策略"><a class="markdownIt-Anchor" href="#收集文档号集合的策略"></a> 收集文档号集合的策略</h2><p>  在深度遍历BKD树的过程中，在读取叶子节点后，文档号会被收集。在PointRangeQuery中，遍历之前会根据索引中的一些信息执行不同的收集策略：</p><h3 id="策略一根据段中的最大文档号生成文档号集合"><a class="markdownIt-Anchor" href="#策略一根据段中的最大文档号生成文档号集合"></a> 策略一：根据段中的最大文档号生成文档号集合</h3><p>  执行策略一需要同时满足两个条件：</p><ul><li>条件一：段中每一篇文档都包含某个域的点数据</li><li>条件二：索引中某个域的点数据都满足查询条件</li></ul><h4 id="条件一"><a class="markdownIt-Anchor" href="#条件一"></a> 条件一</h4><p>  如果<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2020/1027/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bkdd&amp;kdi&amp;kdm" >索引文件.kdm<i class="fas fa-external-link-alt"></i></a>中的DocCount字段的值跟<strong>段中的文档数量segSize</strong>相同，那么满足条件一：段中每一篇文档都包含某个域的点数据。</p><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/10.png"><p>  在生成索引文件.kdm期间，会使用<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0404/FixedBitSet" >FixedBitSet<i class="fas fa-external-link-alt"></i></a>来收集文档号，FixedBitSet使用类似bitmap原理存储文档号，所以它不会重复存储相同的文档号。DocCount字段描述的是包含某个点数据域的文档数量，所以即使一篇文档中定义了多个相同域名的点数据域，对于DocCount只会执行+1操作。</p><p>图11：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/PointRangeQuery/PointRangeQuery（一）/11.png"><p>  另外，段中的文档号数量segSize通过<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0428/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件.si<i class="fas fa-external-link-alt"></i></a>获得，在代码中可以通过reader.maxDoc()方法获得。</p><h4 id="条件二"><a class="markdownIt-Anchor" href="#条件二"></a> 条件二</h4><p>  通过比较图10中的MinPackedValue、MaxPackedValue与查询条件的上下界进行比较，如果他们的Relation为CELL_INSIDE_QUERY，那么满足条件二：索引中某个域的点数据都满足查询条件。</p><p>  执行策略一后，我们就可以在不遍历BKD树的情况下收集到满足查询条件的结果，即[0, reader.maxDoc()]这个区间的文档集合。</p><h3 id="策略二反向收集文档号信息"><a class="markdownIt-Anchor" href="#策略二反向收集文档号信息"></a> 策略二：反向收集文档号信息</h3><p>  执行策略一需要同时满足三个条件：</p><ul><li><p>条件一：段中每一篇文档都包含某个域的点数据</p><ul><li>同策略一中的条件一，不赘述</li></ul></li><li><p>条件二：每篇文档中只包含一个某个点数据域的点数据</p><ul><li>如果<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2020/1027/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bkdd&amp;kdi&amp;kdm" >索引文件.kdm<i class="fas fa-external-link-alt"></i></a>中的PointCount字段（见图10）的值跟DocCount相同，那么满足条件二</li><li>PointCount字段描述的是所有文档中的所有某个点数据的点数据的数量，比如一篇文档中定义了3个相同域名的点数据域，对于PointCount会执行+3操作，而DocCount只会执行+1操作</li></ul></li><li><p>条件三：满足查询条件的点数据数量（估算值cost，下一篇文章中会介绍cost的计算方式）占段中的文档数量的一半以上（&gt; 50%）</p></li></ul><p>  这三个条件针对的是对于这类索引数据的优化：如果所有文档中<strong>有且只有一个</strong>某个点数据域的点数据，如果cost大于文档数量的一半，那么就收集不满足查询条件的文档号。见源码中的注释：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">If all docs have exactly one value and the cost is greater than half the leaf size then maybe we can make things faster by computing the set of documents that do NOT match the range.</span><br></pre></td></tr></table></figure><p>  执行策略二后，在随后遍历BKD树的过程中，我们只收集那些不满足查询条件的文档号。</p><h3 id="策略三收集满足查询条件的文档号"><a class="markdownIt-Anchor" href="#策略三收集满足查询条件的文档号"></a> 策略三：收集满足查询条件的文档号</h3><p>  在无法满足策略一跟策略二的条件，那么就执行默认的策略三，即在随后遍历BKD树的过程中，我们收集那些满足查询条件的文档号。</p><h3 id="节点访问规则intersectvisitor"><a class="markdownIt-Anchor" href="#节点访问规则intersectvisitor"></a> 节点访问规则IntersectVisitor</h3><p>  上文说道，对于策略二，它获取的是不满足查询条件的文档号，而对于策略三，它则是获取满足查询条件的文档号。不管哪一种策略，他们的<strong>相同点都是使用深度遍历读取BKD树，不同点则是访问内部节点跟叶子节点的规则，这个规则即IntersectVisitor</strong>。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  基于篇幅，我们将在下一篇文章中介绍节点访问规则IntersectVisitor以及策略二中条件三的cost的计算过程。</p><p><a class="link"   href="http://www.amazingkoala.com.cn/attachment/Lucene/Search/PointRangeQuery/PointRangeQuery%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  该系列文章开始介绍数值类型的范围查询PointRangeQuery，该类数据在Lucene中被称为点数据Point Value。&lt;/p&gt;
&lt;p&gt;  点数据按照基本类型（primitive type）可以划分IntPoint、LongPoint、FloatPoint、Do</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="point" scheme="http://example.com/tags/point/"/>
    
    <category term="query" scheme="http://example.com/tags/query/"/>
    
    <category term="dim" scheme="http://example.com/tags/dim/"/>
    
    <category term="dii" scheme="http://example.com/tags/dii/"/>
    
    <category term="rangeQuery" scheme="http://example.com/tags/rangeQuery/"/>
    
  </entry>
  
  <entry>
    <title>IndexSort（一）（Lucene 8.9.0）</title>
    <link href="http://example.com/Lucene/Index/2021/0915/IndexSort%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/Index/2021/0915/IndexSort%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2021-09-14T16:00:00.000Z</published>
    <updated>2023-09-28T08:43:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>  段内排序IndexSort是Lucene在索引（Indexing）阶段提供的一个功能，该功能使得在执行<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2019/0716/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E4%B8%80%EF%BC%89" >flush<i class="fas fa-external-link-alt"></i></a>、<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2019/0906/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bcommit%EF%BC%88%E4%B8%80%EF%BC%89" >commit<i class="fas fa-external-link-alt"></i></a>或者<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2019/0916/%E8%BF%91%E5%AE%9E%E6%97%B6%E6%90%9C%E7%B4%A2NRT%EF%BC%88%E4%B8%80%EF%BC%89" >NRT<i class="fas fa-external-link-alt"></i></a>操作后，新生成的段其包含的文档是有序的，即在索引阶段实现了文档的排序。</p><p>  在之前的一些文章中已经简单的介绍了IndexSort，例如在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2019/1111/%E6%9E%84%E9%80%A0IndexWriter%E5%AF%B9%E8%B1%A1%EF%BC%88%E4%B8%80%EF%BC%89" >构造IndexWriter对象（一） <i class="fas fa-external-link-alt"></i></a>说到，通过在IndexWriter的配置信息中添加IndexSort信息来开启段内排序的功能；在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2019/0725/%E6%96%87%E6%A1%A3%E6%8F%90%E4%BA%A4%E4%B9%8Bflush%EF%BC%88%E4%B8%89%EF%BC%89" >文档提交之flush（三）<i class="fas fa-external-link-alt"></i></a>中提到了对文档进行段内排序的时机点；在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2019/0814/Collector%EF%BC%88%E4%B8%89%EF%BC%89" >Collector（三）<i class="fas fa-external-link-alt"></i></a>中提到了在Search阶段，如何通过IndexSort实现高效（提前结束）收集满足查询条件的文档集合。</p><p>  本系列文章将会详细介绍IndexSort在索引阶段相关的内容，以及它将如何影响索引文件的生成、段的合并、以及在查询阶段的用途。</p><h2 id="indexsort的应用"><a class="markdownIt-Anchor" href="#indexsort的应用"></a> IndexSort的应用</h2><p>  我们先通过一个例子来了解如何使用IndexSort这个功能。完整的demo地址见：<a class="link"   href="https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.9.0/src/main/java/index/IndexSortTest.java" >https://github.com/LuXugang/Lucene-7.5.0/blob/master/LuceneDemo8.9.0/src/main/java/index/IndexSortTest.java<i class="fas fa-external-link-alt"></i></a> 。</p><p>图1：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/1.png"><p>图2：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/2.png"><p>  图1中的第44、45行代码定义了两个排序规则。在继续展开介绍之前，我们先简单的说下Lucene中正排索引SortedSetDocValuesField（见图2）的一些概念。</p><h3 id="sortedsetdocvaluesfield"><a class="markdownIt-Anchor" href="#sortedsetdocvaluesfield"></a> SortedSetDocValuesField</h3><p>  使用SortedSetDocValuesField可以使得我们在同一篇文档中定义一个或多个具有<strong>相同域名</strong>、<strong>不同域值</strong>的SortedSetDocValuesField域。这种域的其中一个应用方式即在索引阶段，对于一篇文档，我们可以选择其包含的SortedSetDocValuesField域的某一个域值参与段内排序。</p><p>  例如在图2中，文档3中（代码第80、81行）定义了2个域名为&quot;sort0&quot;，域值分别为&quot;b1&quot;、“b2&quot;的SortedSetDocValuesField域。并且在图1中的第44行代码定义了一个段内排序规则，该规则描述的是每个文档会使用域名为&quot;sort0”，并且将最小的域值来参与排序。那么对于文档3，它将使用域值为&quot;b1&quot;（字符串使用字典序进行排序）参与段内排序。</p><h4 id="sortedsetselectortype"><a class="markdownIt-Anchor" href="#sortedsetselectortype"></a> SortedSetSelector.Type</h4><p>  图2中SortedSetSelector.Type.MIN规定了使用最小的域值参与段内排序。SortedSetSelector.Type的所有选项如下所示：</p><p>图3：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/3.png"><p>  图3中MIN、MAX就不做介绍了，我们说下MIDDLE_MIN跟MIDDLE_MAX。</p><h5 id="middle_min-middle_max"><a class="markdownIt-Anchor" href="#middle_min-middle_max"></a> MIDDLE_MIN、MIDDLE_MAX</h5><p>  如果域值的数量是<strong>奇数</strong>，那么MIDDLE_MIN、MIDDLE_MAX具有相同的作用，比如有以下的域值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;, &quot;b5&quot;&#125;</span><br></pre></td></tr></table></figure><p>  那么将会选择&quot;b3&quot;参与排序。</p><p>  如果域值的数量是<strong>偶数</strong>，假设有以下的域值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;, &quot;b5&quot;, &quot;b6&quot;&#125;</span><br></pre></td></tr></table></figure><p>  那么在MIDDLE_MIN条件下会选择&quot;b3&quot;、在MIDDLE_MAX下会选择&quot;b4&quot;。</p><p>  另外使用SortedSetDocValuesField的一个场景是，我们在搜索阶段可以根据SortedSetDocValuesField对查询结果进行排序，并且可以通过指定不同的SortedSetSelector.Type获取不同的排序结果。</p><h3 id="排序方式概述"><a class="markdownIt-Anchor" href="#排序方式概述"></a> 排序方式概述</h3><p>  我们接着看下图1。图1中定义了两个规则，那么在段内排序的过程中，先按照&quot;sort0&quot;进行排序，当无法比较出先后关系时，接着按照&quot;sort1&quot;进行排序，如果两个排序规则都无法比较出先后关系，则最终比较文档的添加顺序。</p><h3 id="文档之间的排序比较方式"><a class="markdownIt-Anchor" href="#文档之间的排序比较方式"></a> 文档之间的排序比较方式</h3><p>图4：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/4.png"><p>图5：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/5.png"><p>  图4中，<strong>我们的搜索条件没有对结果增加额外的排序规则，那么查询结果将会按照段内排序后的顺序输出</strong>。</p><p>  我们结合图2，分别介绍下一些文档之间排序比较的方式。</p><h4 id="文档0"><a class="markdownIt-Anchor" href="#文档0"></a> 文档0</h4><p>  这里说的文档0指的是图2中添加的顺序，如图2中第53行的代码所示。</p><p>  根据&quot;sort0&quot;的排序规则，将按照域值从大到小排序（SortedSetSortField的第二个参数reverse的值为true），所以文档0~3这四篇文档将分别使用&quot;c1&quot;、“b1”、“b1”、“b1&quot;进行比较，另外由于文档4中没有&quot;sort0&quot;域，那么它将被排到最末位置。可见根据&quot;sort0”，<strong>只能</strong>确定文档0是排在最前面的以及文档4是排在最后面，如下所示。故需要通过&quot;sort1&quot;对文档1、2、3进一步排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文档<span class="number">0</span> --&gt; 文档<span class="number">1</span>、<span class="number">2</span>、<span class="number">3</span> --&gt; 文档<span class="number">4</span></span><br></pre></td></tr></table></figure><h4 id="文档1-2-3"><a class="markdownIt-Anchor" href="#文档1-2-3"></a> 文档1、2、3</h4><p>  根据&quot;sort1&quot;的排序规则，将按照域值从大到小排序，所以文档1、2、3这四篇文档将分别使用&quot;e2&quot;、“f2”、&quot;e2&quot;进行比较，可见文档2在这三篇文档中排在最前面，由于文档1、3无法通过&quot;sort1&quot;区分出先后关系，并且没有其他的排序排序规则了，那么由于文档1先被添加，故文档1排在文档3前面，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文档<span class="number">0</span> --&gt; 文档<span class="number">2</span> --&gt; 文档<span class="number">1</span> --&gt; 文档<span class="number">3</span> --&gt; 文档<span class="number">4</span></span><br></pre></td></tr></table></figure><h3 id="搜索阶段的排序"><a class="markdownIt-Anchor" href="#搜索阶段的排序"></a> 搜索阶段的排序</h3><p>  上文中说到SortedSetDocValuesField可以用于在索引阶段排序，同样的它也可以用于搜索阶段的排序。在设置了图1中的排序规则前提下，如果我们在搜索阶段提供了以下的排序规则：</p><p>图6：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/6.png"><p>  其查询结果如下所示：</p><p>图7：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/7.png"><p>  其排序的比较过程跟IndexSort是一样的，这里就不赘述了。</p><h3 id="用于排序的域"><a class="markdownIt-Anchor" href="#用于排序的域"></a> 用于排序的域</h3><p>  下图中列出了其他可以用于段内排序的域：</p><p>图8：</p><img src="https://www.amazingkoala.com.cn/uploads/lucene/index/段内排序IndexSort/段内排序IndexSort（一）/8.png"><p>  这些域的使用方法可以通过查看每个类的注释就可以完全理解，故不展开介绍了。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  下一篇文章中，我们将介绍段内排序在索引阶段的排序方式、排序时机点以及其他相关内容。</p><p><a class="link"   href="https://www.amazingkoala.com.cn/attachment/Lucene/Index/%E6%AE%B5%E5%86%85%E6%8E%92%E5%BA%8FIndexSort/%E6%AE%B5%E5%86%85%E6%8E%92%E5%BA%8FIndexSort%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  段内排序IndexSort是Lucene在索引（Indexing）阶段提供的一个功能，该功能使得在执行&lt;a class=&quot;link&quot;   href=&quot;https://www.amazingkoala.com.cn/Lucene/Index/2019/0716/%E6%9</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Index" scheme="http://example.com/categories/Lucene/Index/"/>
    
    
    <category term="index" scheme="http://example.com/tags/index/"/>
    
    <category term="IndexSort" scheme="http://example.com/tags/IndexSort/"/>
    
  </entry>
  
  <entry>
    <title>GeoQuery（二）（Lucene 8.8.0）</title>
    <link href="http://example.com/Lucene/Search/2021/0902/GeoQuery%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2021/0902/GeoQuery%EF%BC%88%E4%BA%8C%EF%BC%89/</id>
    <published>2021-09-01T16:00:00.000Z</published>
    <updated>2023-10-09T08:18:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="geoquery二"><a class="markdownIt-Anchor" href="#geoquery二"></a> <a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/" >GeoQuery（二）<i class="fas fa-external-link-alt"></i></a>（Lucene 8.8.0）</h1><p>  在上一篇文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2021/0817/GeoQuery%EF%BC%88%E4%B8%80%EF%BC%89" >GeoQuery（一）<i class="fas fa-external-link-alt"></i></a>中，我们基于下面的例子介绍了在GeoHash编码在Elasticsearch中的部分实现，我们继续介绍其剩余内容。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">latitude：<span class="number">32</span></span><br><span class="line">longitude：<span class="number">50</span></span><br></pre></td></tr></table></figure><h2 id="base32编码"><a class="markdownIt-Anchor" href="#base32编码"></a> base32编码</h2><p>  在前面的步骤中，先将经纬度的值<strong>量化</strong>为两个int类型的数值，随后在<strong>交叉编码</strong>后，将两个int类型的数值用一个long类型的表示。为了便于介绍，我们称这个long类型的值为interleave。最后，interleave在经过base32编码后，我们就能获得GeoHash编码值。</p><p>  同样的我们根据Elasticsearch中的源码来介绍其过程。由于在es中地理位置的精度等级为12（Geohash类中的PRECISION变量定义，如图1所示），加上base32核心处理方式是将5个bit用一个字符（char）描述，故意味着interleave的高60个bit是有效的，同时低4个bit可以用来描述其精度值。</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（二）/1.png"><h3 id="低4个bit"><a class="markdownIt-Anchor" href="#低4个bit"></a> 低4个bit</h3><p>  上一篇文章中我们获得的interleave的值如下所示：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（二）/2.png"><p>  当低4个bit用来描述精度12（0b<font color=Red>1100</font>）后，interleave的值如下所示：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（二）/3.png"><h3 id="高60个bit"><a class="markdownIt-Anchor" href="#高60个bit"></a> 高60个bit</h3><p>  接着就可以对高60个bit进行base32编码了。总体流程为从这60个bit的末尾开始，每次取出5个bit，其对应的十进制值作为<strong>编码表</strong>的下标值，在<strong>编码表</strong>中找到对应的字符，最终生成一个包含12个字符的字符串。如下所示：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（二）/4.png"><p>  从图4中可以看出，我们首先取出5个bit，以<strong>10110</strong>为例，它对应的十进制的值为22，随后将22作为编码表BASE_32的下标值，取出数组元素q，把该值写入到GeoHash编码中。为了图片的整洁性，故图4中只描述了将60个bit的末尾15个bit进行编码的过程。当所有的bit都处理结束后，其最终的GeoHash编码如下所示：</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（二）/5.png"><p><strong>为什么介绍GeoHash编码</strong></p><p>  在Elasticsearch的文档中我们可以知道，以<a class="link"   href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-geo-bounding-box-query.html" >geo_bounding_box query<i class="fas fa-external-link-alt"></i></a>为例，它支持提供GeoHash编码跟经纬度进行地理位置查询，如下所示：</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（二）/6.png"><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  下一篇文章将正式开始介绍Elasticsearch中提供的Geo Queries。</p><p><a class="link"   href="http://www.amazingkoala.com.cn/attachment/Lucene/Search/GeoQuery/GeoQuery%EF%BC%88%E4%BA%8C%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;geoquery二&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#geoquery二&quot;&gt;&lt;/a&gt; &lt;a class=&quot;link&quot;   href=&quot;https://www.amazingkoala.com.cn/Lucene/Search</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="query" scheme="http://example.com/tags/query/"/>
    
    <category term="geo" scheme="http://example.com/tags/geo/"/>
    
    <category term="dim" scheme="http://example.com/tags/dim/"/>
    
    <category term="dii" scheme="http://example.com/tags/dii/"/>
    
  </entry>
  
  <entry>
    <title>GeoQuery（一）（Lucene 8.8.0）</title>
    <link href="http://example.com/Lucene/Search/2021/0817/GeoQuery%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2021/0817/GeoQuery%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2021-08-16T16:00:00.000Z</published>
    <updated>2023-10-09T08:15:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>  本系列文章将介绍下Elasticsearch中提供的几个<a class="link"   href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/geo-queries.html" >地理查询<i class="fas fa-external-link-alt"></i></a>（Geo Query）在Lucene层的相关内容。Elasticsearch 7.13版本中提供了以下的Geo Queries：</p><ul><li><a class="link"   href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-geo-bounding-box-query.html" >geo_bounding_box<i class="fas fa-external-link-alt"></i></a> query</li><li><a class="link"   href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-geo-distance-query.html" >geo_distance<i class="fas fa-external-link-alt"></i></a> query</li><li><a class="link"   href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-geo-polygon-query.html" >geo_polygon<i class="fas fa-external-link-alt"></i></a> query</li><li><a class="link"   href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-geo-shape-query.html" >geo_shape<i class="fas fa-external-link-alt"></i></a> query</li></ul><h2 id="预备知识"><a class="markdownIt-Anchor" href="#预备知识"></a> 预备知识</h2><p>  为了能深入理解GeoQuery，我们需要先介绍下两个预备知识：</p><ul><li>Lucene中点数据的索引与查询</li><li>GeoHash编码</li></ul><h3 id="lucene中点数据的索引与查询"><a class="markdownIt-Anchor" href="#lucene中点数据的索引与查询"></a> Lucene中点数据的索引与查询</h3><h4 id="点数据"><a class="markdownIt-Anchor" href="#点数据"></a> 点数据</h4><p>  Lucene中，点数据域用于存储数值类型的信息，并且点数据可以是多维的，例如图1中的第48行、49行分别写入了一个int类型的二维点数据跟三维点数据。</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/1.png"><p>  其他类型的点数据如下所示：</p><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/2.png"><p>  基于篇幅，后续的文章中只会介绍跟上文中的Geo Queries相关的点数据域。</p><h4 id="索引indexing和搜索search点数据"><a class="markdownIt-Anchor" href="#索引indexing和搜索search点数据"></a> 索引（indexing）和搜索（search）点数据</h4><p>  在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdim&amp;&amp;dii" >索引文件之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>中介绍了Lucene中点数据对应的索引数据结构，以及在文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0422/Bkd-Tree" >Bkd-Tree<i class="fas fa-external-link-alt"></i></a>中通过一个例子简单的介绍了如何对点数据集合进行划分，并用于生成一棵BKD树，以及在系列文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Index/2020/0329/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E7%94%9F%E6%88%90%EF%BC%88%E5%85%AB%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件的生成（八）~（十四）<i class="fas fa-external-link-alt"></i></a>中详细的介绍了生成<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdim&amp;&amp;dii" >索引文件之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>的过程，最后在系列文章<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/Search/2020/0427/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E7%9A%84%E8%AF%BB%E5%8F%96%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8Bdim&amp;&amp;dii" >索引文件的读取（一）~（四）<i class="fas fa-external-link-alt"></i></a>中介绍了<a class="link"   href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0424/%E7%B4%A2%E5%BC%95%E6%96%87%E4%BB%B6%E4%B9%8Bdim&amp;&amp;dii" >索引文件之dim&amp;&amp;dii<i class="fas fa-external-link-alt"></i></a>的读取过程，即点数据的搜索过程。</p><p>  如果阅读过上述的文章，那么相信能非常容易理解GeoQuery在Lucene中的实现。</p><h3 id="geohash编码"><a class="markdownIt-Anchor" href="#geohash编码"></a> GeoHash编码</h3><p>  接着我们将先介绍区域编码的概念，随后介绍下在Elasticsearch中GeoHash编码的实现。</p><h4 id="区域编码domain-encode"><a class="markdownIt-Anchor" href="#区域编码domain-encode"></a> 区域编码（domain encode）</h4><p>  区域编码描述的是使用唯一的编码值来描述平面上的一块区域。对于一个有边界的二维空间，可以在水平方向或者垂直方向对空间进行划分，那么空间将被划分为四块区域。这四块子区域可以用2个bit号来进行编码。如果规定在水平方向划分后，左边的区域用0表示，右边的区域的用1表示；在垂直方向划分后，下面的区域用0表示，上面的区域用1表示，那么这四个区域就可以分别用00、01、11、10进行编码，如下所示：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/3.png"><p>  按照上述的切分方式，如果我们继续分别对四块区域进行水平方向跟垂直方向的划分，如下所示：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/4.png"><p>  区域编码常用于计算空间里面点与点之间的距离关系，从这种区域编码方式能发现：拥有相同前缀的长度越长，空间上的位置就越接近，不过反过来就不一定成立了，因为空间中的两个点可能非常接近，但是这两个点所属的区域可能拥有很短的前缀，甚至没有相同前缀。</p><p>  图5中，**<font color=black>黑点</font><strong>跟<font color=Red>红点</font>没有相同的编码前缀，跟<font color=green>绿点</font>有相同的编码前缀，但是</strong><font color=black>黑点</font>**跟<font color=Red>红点</font>更为接近。</p><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/5.png"><h4 id="查找附近的点"><a class="markdownIt-Anchor" href="#查找附近的点"></a> 查找附近的点</h4><p>  使用区域编码来实现查找附近的点的直观方法就是找出与查询点所属的编码值前缀最长的区域，这些区域中的点都是认为是在查询点附近的。由于存在图5中描述的问题，所以这种方式会遗漏一些附近点（<font color=Red>红点</font>）。</p><p>  为了解决这个遗漏问题，我们可以把查询点所属的区域的周围8个区域中的点都找出来就可以了，当然这种粗暴的做法的缺点是可能会获得大量的结果集。为了防止出现大量结果集，那么可以对当前最小区域再进行划分。</p><p>  那么问题就转变为如何通过查询点所在的区域编码获取它周围8个区域的编码值了。</p><h5 id="计算规则"><a class="markdownIt-Anchor" href="#计算规则"></a> 计算规则</h5><p>  以图5中**<font color=black>黑点</font>**所在区域为例，区域编码为<font color=red>0</font><font color=blue>0</font><font color=red>1</font><font color=blue>1</font>。它的水平编码为<font color=red>01</font>，垂直编码为<font color=blue>01</font>，该区域上下左右四个区域的编码值如下所示：</p><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/6.png"><p>  同理可以计算出查询点所属的区域的左上、左下、右上、右下四个区域编码值。</p><h5 id="精度"><a class="markdownIt-Anchor" href="#精度"></a> 精度</h5><p>  由上面的划分方式可以看出，划分次数越多，区域越小，意味着精度越高。精度高一方面描述了区域编码值能表示更少的点，另一方面也为查找附近点能提高更少的结果集。</p><h4 id="elastisearch中的geohash编码"><a class="markdownIt-Anchor" href="#elastisearch中的geohash编码"></a> Elastisearch中的GeoHash编码</h4><p>  Geohash编码指的是将经纬度坐标转换为字符串的编码方式。它同样通过区域编码的处理方式对地理位置进行了划分。由于地理位置可以用经纬度来表示，其中纬度的取值范围为[-90, 90]，经度的取值范围为[-180, 180]，即对一个有边界的二维区域进行区域编码。</p><h5 id="编码"><a class="markdownIt-Anchor" href="#编码"></a> 编码</h5><p>  我们先看下Elasticsearch中如何将经纬度值转变为GeoHash编码。</p><h6 id="量化"><a class="markdownIt-Anchor" href="#量化"></a> 量化</h6><p>  首先将经纬度两个值分别量化（quantizing）为区间为[0, 2^32 - 1]的值，使得可以用int类型来描述经纬度，其量化公式如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">latEnc</span> <span class="operator">=</span> (<span class="type">int</span>) Math.floor(latitude / (<span class="number">180.0D</span>/(<span class="number">0x1L</span>&lt;&lt;<span class="number">32</span>)))</span><br><span class="line"><span class="type">int</span> <span class="variable">lonEnc</span> <span class="operator">=</span> (<span class="type">int</span>) Math.floor(longitude / (<span class="number">360.0D</span>/(<span class="number">0x1L</span>&lt;&lt;<span class="number">32</span>)))</span><br></pre></td></tr></table></figure><p>  比如纬度的取值范围为[-90, 90]，那么维度值-90跟90将分别量化为 0、2^32 - 1。</p><h6 id="交叉编码"><a class="markdownIt-Anchor" href="#交叉编码"></a> 交叉编码</h6><p>  将量化后的维度、经度值，即两个32位的int类型的值交叉编码为一个64个bit的long类型的值。在这个64个bit中，奇数位共32个bit为量化后的维度值对应的32个bit，偶数位共32个bit为量化后的经度值对应的32个bit。</p><p>  例如有一个坐标值如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">latitude：<span class="number">32</span></span><br><span class="line">longitude：<span class="number">50</span></span><br></pre></td></tr></table></figure><p>  量化后的值为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">latEnc</span> <span class="operator">=</span> <span class="number">0b10101101_10000010_11011000_00101101</span></span><br><span class="line"><span class="type">int</span> <span class="variable">lonEnc</span> <span class="operator">=</span> <span class="number">0b10100011_10001110_00111000_11100011</span></span><br></pre></td></tr></table></figure><p>  最后对latEnc跟lonEnc进行交叉编码，其处理过程如下图所示：</p><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/7.png"><p>  图7中，我们先将int类型的latEnc中的32个bit塞到一个long类型的v1中。</p><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/8.png"><p>  同图7的处理方式一样，将int类型的lonEnc中的32个bit塞到一个long类型的v2中。</p><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/9.png"><p>  图9中先将v2左移一位，然后 v2 跟v1执行或操作获得一个long类型的值interleave。</p><p>  图7~图8的处理过程对应Elasticsearch中的源码BitUtil类中的interleave方法如下所示：</p><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/GeoQuery/GeoQuery（一）/10.png"><h6 id="base32编码"><a class="markdownIt-Anchor" href="#base32编码"></a> base32编码</h6><p>  在上文中我们获得了interleave的值后，需要对其进行base32编码，在此之后就获得了GeoHash编码。基于篇幅，该内容将在下一篇文章中展开。</p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  Elasticsearch中GeoHash编码原理基于莫顿编码，感兴趣的同学可以深入理解下：<a class="link"   href="http://graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN" >http://graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN<i class="fas fa-external-link-alt"></i></a> 。</p><p><a class="link"   href="http://www.amazingkoala.com.cn/attachment/Lucene/Search/GeoQuery/GeoQuery%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  本系列文章将介绍下Elasticsearch中提供的几个&lt;a class=&quot;link&quot;   href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/7.13/geo-queries.html&quot; &gt;地理查</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="query" scheme="http://example.com/tags/query/"/>
    
    <category term="geo" scheme="http://example.com/tags/geo/"/>
    
    <category term="dim" scheme="http://example.com/tags/dim/"/>
    
    <category term="dii" scheme="http://example.com/tags/dii/"/>
    
  </entry>
  
  <entry>
    <title>DisjunctionMaxQuery（Lucene 8.9.0）</title>
    <link href="http://example.com/Lucene/Search/2021/0804/DisjunctionMaxQuery%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://example.com/Lucene/Search/2021/0804/DisjunctionMaxQuery%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2021-08-03T16:00:00.000Z</published>
    <updated>2023-10-09T07:51:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>  本系列的内容将会先介绍DisjunctionMaxQuery在Lucene中的实现原理，随后再介绍在Elasticsearch中的应用。我们先直接通过图1中DisjunctionMaxQuery的注释跟图2的构造函数来简单了解下这个Query的功能：</p><p>图1：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/1.png"><p>图2：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/2.png"><p>  <font color=gray>灰色框</font>标注的注释中说道：DisjunctionMaxQuery中封装了多个子Query（subqueries，这些子Query之间的关系相当于<strong>BooleanQuery中SHOULD的关系</strong>），即图2中的参数disjuncts。每个子Query都会生成一个文档集合，并且如果某篇文档被多个子Query命中，虽然这篇文档的打分值在不同的子Query中是不同的，但是这篇文档的<strong>最终打分值</strong>（在收集器Collector中的打分值）会选择分数最高的。不过如果图2中构造函数的参数tieBreakerMultiplier是一个不为0的合法值（合法取值范围为[0, 1)），那么文档的打分值还会所有考虑命中这个文档的所有子Query对应的打分值，其计算公式如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">float</span> <span class="variable">score</span> <span class="operator">=</span> (<span class="type">float</span>) (max + otherSum * tieBreakerMultiplier);</span><br></pre></td></tr></table></figure><p>  上述公式中，max为子Query对应最高的那个打分值，而otherSum则为剩余的子Query对应打分值的和值，由于tieBreakerMultiplier的取值范围为[0, 1]，所以当tieBreakerMultiplier<mark>0时，文档的最终打分值为max；而当tieBreakerMultiplier</mark>1时，则是所有子Query对应的打分值的和值，这个时候DisjunctionMaxQuery就相当于minimumNumberShouldMatch<mark>1的BooleanQuery。在源码中，如果tieBreakerMultiplier</mark>1，那么将会使用BooleanQuery进行查询。如下所示：</p><p>图3：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/3.png"><p>  图3中，当tieBreakerMultiplier==1，会从disjuncts中读取所有的子Query，用于生成一个新的BooleanQuery，并且子Query之间的关系为should。</p><p>  使用DisjunctionMaxQuery很有帮助的（useful）一个场景是：多个域中都包含某个term时，我们可以对不同的域设置加分因子（boost factor），如果某篇文档中包含这些域，我们获得的文档打分值<strong>可以只是</strong>加分因子最高的那个域对应的打分值，而不像在BooleanQuery中，文档打分值是所有域对应的打分值的和值。在随后介绍在Elasticsearch的应用中再详细展开。</p><h2 id="例子"><a class="markdownIt-Anchor" href="#例子"></a> 例子</h2><p>  我们用一个例子来理解下图1中<font color=gray>灰色框</font>标注的注释：</p><p>图4：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/4.png"><h3 id="booleanqueryminimumnumbershouldmatch-1"><a class="markdownIt-Anchor" href="#booleanqueryminimumnumbershouldmatch-1"></a> BooleanQuery（minimumNumberShouldMatch == 1）</h3><p>图5：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/5.png"><p>图6：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/6.png"><p>  图5中，文档0跟文档1中由于只包含了&quot;title&quot;跟&quot;body&quot;域的一个，并且查询条件BooleanQuery的minimumNumberShouldMatch的值为1，那么文档0跟文档1<strong>分别满足</strong>titleTermQuery跟bodyTermQuery，文档分数分别为0.113950975跟0.082873434。由于文档2<strong>同时满足</strong>了titleTermQuery跟bodyTermQuery这两个子Query ，所以文档2的分数为0.113950975跟0.082873434的和值，即0.1968244。</p><p>  正如上文中说道：BooleanQuery中，文档打分值是所有域对应的打分值的和值。</p><h3 id="disjunctionmaxquerytiebreakermultiplier-0"><a class="markdownIt-Anchor" href="#disjunctionmaxquerytiebreakermultiplier-0"></a> DisjunctionMaxQuery（tieBreakerMultiplier == 0）</h3><p>图7：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/7.png"><p>图8：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/8.png"><p>  对于文档0跟文档1，跟使用BooleanQuery获得打分值是一致的，他们不会tieBreakerMultiplier 的影响，因为这两篇文档分别只满足一个Query的条件。而对于文档2，它同时满足titleTermQuery跟bodyTermQuery这两个子Query后对应的文档分数分别为0.113950975跟0.082873434，由于tieBreakerMultiplier == 0，意味着最终文档2的打分值<strong>只会</strong>选择打分最高的子Query，对应的分数，即0.113950975。结合上文中给出的计算公式，其计算过程如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">float</span> <span class="variable">score</span> <span class="operator">=</span> (<span class="type">float</span>) (<span class="number">0.113950975</span> + <span class="number">0.082873434</span> * <span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>  正如上文中说道：如果某篇文档被多个子Query命中，虽然这篇文档的打分值在不同的子Query中是不同的，但是这篇文档的<strong>最终打分值</strong>（在收集器Collector中的打分值）会选择分数最高的。</p><h3 id="disjunctionmaxquerytiebreakermultiplier-1"><a class="markdownIt-Anchor" href="#disjunctionmaxquerytiebreakermultiplier-1"></a> DisjunctionMaxQuery（tieBreakerMultiplier == 1）</h3><p>图9：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/9.png"><p>图10：</p><img src="http://www.amazingkoala.com.cn/uploads/lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery（一）/10.png"><p>  对于文档0跟文档1，他们不受tieBreakerMultiplier 的影响，因为这两篇文档分别只满足一个Query的条件。而对于文档2，它同时满足titleTermQuery跟bodyTermQuery这两个子Query后对应的文档分数分别为0.113950975跟0.082873434，由于tieBreakerMultiplier == 1，该值不为0，意味着文档2的打分值不但会选择子Query对应的打分值最高的那个，还会考虑其他子Query对应的打分值。结合上文中给出的计算公式，其计算过程如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">float</span> <span class="variable">score</span> <span class="operator">=</span> (<span class="type">float</span>) (<span class="number">0.113950975</span> + <span class="number">0.082873434</span> * <span class="number">1</span>);</span><br></pre></td></tr></table></figure><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><p>  基于篇幅原因，剩余的内容将在下一篇文章中展开。</p><p><a class="link"   href="http://www.amazingkoala.com.cn/attachment/Lucene/Search/DisjunctionMaxQuery/DisjunctionMaxQuery%EF%BC%88%E4%B8%80%EF%BC%89.zip" >点击<i class="fas fa-external-link-alt"></i></a>下载附件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  本系列的内容将会先介绍DisjunctionMaxQuery在Lucene中的实现原理，随后再介绍在Elasticsearch中的应用。我们先直接通过图1中DisjunctionMaxQuery的注释跟图2的构造函数来简单了解下这个Query的功能：&lt;/p&gt;
&lt;p&gt;图1</summary>
      
    
    
    
    <category term="Lucene" scheme="http://example.com/categories/Lucene/"/>
    
    <category term="Search" scheme="http://example.com/categories/Lucene/Search/"/>
    
    
    <category term="query" scheme="http://example.com/tags/query/"/>
    
    <category term="DisjunctionMaxQuery" scheme="http://example.com/tags/DisjunctionMaxQuery/"/>
    
  </entry>
  
</feed>
